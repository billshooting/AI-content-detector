[
  {
    "codes": "public ServiceFuture<LuisResult> resolveAsync(String appId, String query, ResolveOptionalParameter resolveOptionalParameter, final ServiceCallback<LuisResult> serviceCallback) {\n        return ServiceFuture.fromResponse(resolveWithServiceResponseAsync(appId, query, resolveOptionalParameter), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```sql\n-- Select customer IDs along with their total purchase amounts\nSELECT customer_id, SUM(purchase_amount) AS total_purchase\nFROM purchases\n-- Group the results by customer ID to aggregate purchase amounts\nGROUP BY customer_id\n-- Order the results in descending order to get the highest total purchases first\nORDER BY total_purchase DESC\n-- Limit the results to only the top 5 customers\nLIMIT 5;\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<Page<EventHubConsumerGroupInfoInner>>> listEventHubConsumerGroupsSinglePageAsync(final String resourceGroupName, final String resourceName, final String eventHubEndpointName) {\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (resourceName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceName is required and cannot be null.\");\n        }\n        if (eventHubEndpointName == null) {\n            throw new IllegalArgumentException(\"Parameter eventHubEndpointName is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        return service.listEventHubConsumerGroups(this.client.subscriptionId(), resourceGroupName, resourceName, eventHubEndpointName, this.client.apiVersion(), this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<Page<EventHubConsumerGroupInfoInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<EventHubConsumerGroupInfoInner>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<PageImpl<EventHubConsumerGroupInfoInner>> result = listEventHubConsumerGroupsDelegate(response);\n                        return Observable.just(new ServiceResponse<Page<EventHubConsumerGroupInfoInner>>(result.body(), result.response()));\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```java\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutionException;\n\npublic class AsyncTaskWithFallback {\n    public static void main(String[] args) {\n        CompletableFuture<String> future = CompletableFuture.supplyAsync(() -> {\n            // Primary task\n            if (Math.random() > 0.5) {\n                throw new RuntimeException(\"Primary task failed!\");\n            }\n            return \"Result from primary task\";\n        }).exceptionally(ex -> {\n            // Fallback for primary task failure\n            return \"Fallback result after primary task failure\";\n        }).thenApplyAsync(result -> {\n            // Secondary async task\n            if (Math.random() > 0.5) {\n                throw new RuntimeException(\"Secondary task failed!\");\n            }\n            return result + \": Extended by secondary task\";\n        }).exceptionally(ex -> {\n            // Fallback for secondary task failure\n            return \"Fallback result after secondary task failure\";\n        });\n\n        try {\n            System.out.println(future.get());\n        } catch (InterruptedException | ExecutionException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```powershell\n# Import necessary module\nImport-Module Microsoft.PowerShell.Security\n\n# Define a function to audit registry permissions\nFunction Audit-RegistryPermissions {\n    Param (\n        [string]$RegistryPath = \"HKLM:\\\"\n    )\n\n    # Retrieve the subkeys under the specified registry path\n    $subKeys = Get-ChildItem -Path $RegistryPath -Recurse -ErrorAction SilentlyContinue\n\n    # Initialize array to store weak permission entries\n    $weakPermissions = @()\n\n    # Check each subkey's permissions\n    foreach ($key in $subKeys) {\n        try {\n            # Get access control list for the registry key\n            $acl = Get-Acl -Path $key.PSPath\n\n            # Iterate over each access rule in the ACL\n            foreach ($accessRule in $acl.Access) {\n                # Check if the access rule is not secure\n                if (($accessRule.AccessControlType -eq \"Allow\") -and\n                    (($accessRule.FileSystemRights -eq \"FullControl\") -or ($accessRule.FileSystemRights -eq \"WriteKey\")) -and\n                    ($accessRule.IdentityReference -notlike \"NT AUTHORITY\\SYSTEM\") -and\n                    ($accessRule.IdentityReference -notlike \"BUILTIN\\Administrators\")\n                   ) {\n                    # Add weak permissions to the array\n                    $weakPermissions += New-Object PSObject -Property @{\n                        Key = $key.PSPath\n                        User = $accessRule.IdentityReference\n                        Rights = $accessRule.FileSystemRights\n                    }\n                }\n            }\n        } catch {\n            Write-Error \"Unable to retrieve permissions for $($key.PSPath): $_\"\n        }\n    }\n\n    # Output the results\n    if ($weakPermissions.Count -gt 0) {\n        Write-Output \"Weak Permissions Found:\"\n        $weakPermissions | Format-Table -AutoSize\n    } else {\n        Write-Output \"No weak permissions found.\"\n    }\n}\n\n# Run the audit on standard registry hives\nAudit-RegistryPermissions -RegistryPath \"HKLM:\\\"\nAudit-RegistryPermissions -RegistryPath \"HKCU:\\\"\nAudit-RegistryPermissions -RegistryPath \"HKCR:\\\"\nAudit-RegistryPermissions -RegistryPath \"HKU:\\\"\nAudit-RegistryPermissions -RegistryPath \"HKCC:\\\"\n```",
    "label": 1
  },
  {
    "codes": "def context(self, size, placeholder=None, scope=None):\n        \"\"\"Returns this word in context, {size} words to the left, the current word, and {size} words to the right\"\"\"\n        return self.leftcontext(size, placeholder,scope) + [self] + self.rightcontext(size, placeholder,scope)",
    "label": 0
  },
  {
    "codes": "```swift\nimport Security\n\nfunc saveKeyToKeychain(key: String, value: String, service: String) -> Bool {\n    // Convert the key and value to Data\n    guard let keyData = key.data(using: .utf8),\n          let valueData = value.data(using: .utf8) else {\n        return false\n    }\n    \n    // Create a query dictionary for the keychain\n    let query: [String: Any] = [\n        kSecClass as String: kSecClassGenericPassword,\n        kSecAttrService as String: service,\n        kSecAttrAccount as String: keyData,\n        kSecValueData as String: valueData\n    ]\n    \n    // Delete any existing item before adding the new one\n    SecItemDelete(query as CFDictionary)\n    \n    // Add the new item to the keychain\n    let status = SecItemAdd(query as CFDictionary, nil)\n    \n    return status == errSecSuccess\n}\n\nfunc retrieveKeyFromKeychain(key: String, service: String) -> String? {\n    // Convert the key to Data\n    guard let keyData = key.data(using: .utf8) else {\n        return nil\n    }\n    \n    // Create a query dictionary for the keychain\n    let query: [String: Any] = [\n        kSecClass as String: kSecClassGenericPassword,\n        kSecAttrService as String: service,\n        kSecAttrAccount as String: keyData,\n        kSecReturnData as String: true,\n        kSecMatchLimit as String: kSecMatchLimitOne\n    ]\n    \n    // Retrieve the item from the keychain\n    var dataTypeRef: AnyObject?\n    let status = SecItemCopyMatching(query as CFDictionary, &dataTypeRef)\n    \n    // Check if the retrieval was successful\n    if status == errSecSuccess, let retrievedData = dataTypeRef as? Data {\n        return String(data: retrievedData, encoding: .utf8)\n    }\n    \n    return nil\n}\n```",
    "label": 1
  },
  {
    "codes": "function write_BrtBookView(idx, o) {\n\tif(!o) o = new_buf(29);\n\to.write_shift(-4, 0);\n\to.write_shift(-4, 460);\n\to.write_shift(4,  28800);\n\to.write_shift(4,  17600);\n\to.write_shift(4,  500);\n\to.write_shift(4,  idx);\n\to.write_shift(4,  idx);\n\tvar flags = 0x78;\n\to.write_shift(1,  flags);\n\treturn o.length > o.l ? o.slice(0, o.l) : o;\n}",
    "label": 0
  },
  {
    "codes": "```kotlin\nfun hasCycle(graph: Map<Int, List<Int>>): Boolean {\n    val visited = mutableSetOf<Int>() // Track visited nodes\n    val recStack = mutableSetOf<Int>() // Track nodes in the current recursion stack\n\n    fun dfs(node: Int): Boolean {\n        if (node in recStack) return true // Cycle detected\n        if (node in visited) return false // Already visited, no cycle\n\n        visited.add(node)\n        recStack.add(node)\n\n        for (neighbor in graph[node] ?: emptyList()) {\n            if (dfs(neighbor)) return true\n        }\n\n        recStack.remove(node) // Remove node from recursion stack\n        return false\n    }\n\n    for (node in graph.keys) {\n        if (dfs(node)) return true\n    }\n\n    return false // No cycle found\n}\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Required variables\nBUCKET_NAME=\"your-s3-bucket-name\"\nBACKUP_PATH=\"/path/to/your/backup\"\nDIRECTORIES_TO_BACKUP=\"/path/to/dir1 /path/to/dir2\"\nLOG_FILE=\"/var/log/s3-backup.log\"\nDATE=$(date +\"%Y-%m-%d\")\n\n# Make sure AWS CLI is installed\ncommand -v aws >/dev/null 2>&1 || { echo >&2 \"AWS CLI is required but it's not installed. Aborting.\"; exit 1; }\n\n# Create the backup directory if it doesn't exist\nmkdir -p \"${BACKUP_PATH}\"\n\n# Archive and compress each directory\nfor DIR in ${DIRECTORIES_TO_BACKUP}\ndo\n    TAR_FILE=\"${BACKUP_PATH}/$(basename ${DIR})-${DATE}.tar.gz\"\n    tar -czf \"${TAR_FILE}\" -C \"$(dirname \"${DIR}\")\" \"$(basename \"${DIR}\")\"\ndone\n\n# Upload each archive to S3\nfor FILE in ${BACKUP_PATH}/*.tar.gz\ndo\n    aws s3 cp \"${FILE}\" \"s3://${BUCKET_NAME}/backups/\"\n    if [[ $? -eq 0 ]]; then\n        echo \"${DATE} - Successfully uploaded ${FILE} to S3\" >> \"${LOG_FILE}\"\n    else\n        echo \"${DATE} - Failed to upload ${FILE} to S3\" >> \"${LOG_FILE}\"\n    fi\ndone\n\n# Clean up old backup files (retaining the last 7 days)\nfind \"${BACKUP_PATH}\" -type f -name \"*.tar.gz\" -mtime +7 -exec rm {} \\;\n```",
    "label": 1
  },
  {
    "codes": "func (s *JobParameters) SetFormat(v string) *JobParameters {\n\ts.Format = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *GetVaultNotificationsOutput) SetVaultNotificationConfig(v *VaultNotificationConfig) *GetVaultNotificationsOutput {\n\ts.VaultNotificationConfig = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function parse_PtgAttrIfError(blob) {\n\tvar bitIf = (blob[blob.l+1] & 0xFF) ? 1 : 0;\n\tblob.l += 2;\n\treturn [bitIf, blob.read_shift(2)];\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"os/signal\"\n\t\"sync\"\n\n\t\"github.com/segmentio/kafka-go\"\n)\n\nconst (\n\tbrokerAddress = \"localhost:9092\"\n\ttopic         = \"example-topic\"\n\tgroupID       = \"consumer-group\"\n\tworkerCount   = 5\n)\n\nfunc main() {\n\t// Setup signal handling to gracefully shutdown\n\tsignalChannel := make(chan os.Signal, 1)\n\tsignal.Notify(signalChannel, os.Interrupt)\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\t// Create a Kafka reader for fan-in processing\n\treader := kafka.NewReader(kafka.ReaderConfig{\n\t\tBrokers: []string{brokerAddress},\n\t\tTopic:   topic,\n\t\tGroupID: groupID,\n\t})\n\tdefer reader.Close()\n\n\t// Create a channel for messages to be processed by workers\n\tmessageChannel := make(chan kafka.Message)\n\n\t// WaitGroup to synchronize the Go routines\n\tvar wg sync.WaitGroup\n\n\t// Start worker go routines for fan-out processing\n\tfor i := 0; i < workerCount; i++ {\n\t\twg.Add(1)\n\t\tgo func(workerID int) {\n\t\t\tdefer wg.Done()\n\t\t\tfor msg := range messageChannel {\n\t\t\t\t// Simulate processing message\n\t\t\t\tfmt.Printf(\"Worker %d: Processing message with key: %s, value: %s\\n\",\n\t\t\t\t\tworkerID, string(msg.Key), string(msg.Value))\n\t\t\t}\n\t\t}(i)\n\t}\n\n\t// Fan-in: Read messages and send to messageChannel\n\tgo func() {\n\t\tdefer close(messageChannel)\n\t\tfor {\n\t\t\tmsg, err := reader.ReadMessage(ctx)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"Error reading message: %v\\n\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase messageChannel <- msg:\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Wait for termination signal\n\t<-signalChannel\n\tfmt.Println(\"Shutting down...\")\n\n\t// Cancel the context to stop the Kafka reader\n\tcancel()\n\n\t// Wait for all workers to finish processing\n\twg.Wait()\n\tfmt.Println(\"Shutdown complete.\")\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"bufio\"\n    \"fmt\"\n    \"net\"\n    \"strings\"\n    \"sync\"\n)\n\nfunc handleConnection(conn net.Conn, wg *sync.WaitGroup) {\n    defer wg.Done() // Decrement the WaitGroup counter when the function exits\n    defer conn.Close() // Close the connection when the function exits\n\n    // Create a new buffered reader for the connection\n    reader := bufio.NewReader(conn)\n\n    for {\n        // Read a line from the client\n        message, err := reader.ReadString('\\n')\n        if err != nil {\n            fmt.Println(\"Error reading from client:\", err)\n            return\n        }\n\n        // Trim any extra whitespace and print the received message\n        message = strings.TrimSpace(message)\n        fmt.Printf(\"Received: %s\\n\", message)\n\n        // Echo the message back to the client\n        _, err = conn.Write([]byte(message + \"\\n\"))\n        if err != nil {\n            fmt.Println(\"Error writing to client:\", err)\n            return\n        }\n    }\n}\n\nfunc main() {\n    // Create a WaitGroup to wait for all goroutines to finish\n    var wg sync.WaitGroup\n\n    // Start listening on port 8080\n    listener, err := net.Listen(\"tcp\", \":8080\")\n    if err != nil {\n        fmt.Println(\"Error starting server:\", err)\n        return\n    }\n    defer listener.Close() // Close the listener when the function exits\n\n    fmt.Println(\"Server started, listening on port 8080\")\n\n    for {\n        // Accept a new connection\n        conn, err := listener.Accept()\n        if err != nil {\n            fmt.Println(\"Error accepting connection:\", err)\n            continue\n        }\n\n        // Increment the WaitGroup counter for each new connection\n        wg.Add(1)\n\n        // Handle the connection in a new goroutine\n        go handleConnection(conn, &wg)\n    }\n\n    // Wait for all connections to finish\n    wg.Wait()\n}\n```",
    "label": 1
  },
  {
    "codes": "function (srcDirectory) {\n  let manifest\n  let manifestContent\n\n  try {\n    manifestContent = fs.readFileSync(path.join(srcDirectory, 'manifest.json'))\n  } catch (readError) {\n    console.warn(`Reading ${path.join(srcDirectory, 'manifest.json')} failed.`)\n    console.warn(readError.stack || readError)\n    throw readError\n  }\n\n  try {\n    manifest = JSON.parse(manifestContent)\n  } catch (parseError) {\n    console.warn(`Parsing ${path.join(srcDirectory, 'manifest.json')} failed.`)\n    console.warn(parseError.stack || parseError)\n    throw parseError\n  }\n\n  if (!manifestNameMap[manifest.name]) {\n    const extensionId = generateExtensionIdFromName(manifest.name)\n    manifestMap[extensionId] = manifestNameMap[manifest.name] = manifest\n    Object.assign(manifest, {\n      srcDirectory: srcDirectory,\n      extensionId: extensionId,\n      // We can not use 'file://' directly because all resources in the extension\n      // will be treated as relative to the root in Chrome.\n      startPage: url.format({\n        protocol: 'chrome-extension',\n        slashes: true,\n        hostname: extensionId,\n        pathname: manifest.devtools_page\n      })\n    })\n    return manifest\n  } else if (manifest && manifest.name) {\n    console.warn(`Attempted to load extension \"${manifest.name}\" that has already been loaded.`)\n    return manifest\n  }\n}",
    "label": 0
  },
  {
    "codes": "```rust\nuse std::collections::HashMap;\n\n// Function to calculate the nth Fibonacci number using memoization\nfn fibonacci(n: u32, memo: &mut HashMap<u32, u64>) -> u64 {\n    // Base cases\n    if n == 0 {\n        return 0;\n    }\n    if n == 1 {\n        return 1;\n    }\n\n    // Check if the value is already in the memo\n    if let Some(&result) = memo.get(&n) {\n        return result;\n    }\n\n    // Recursive calculation with memoization\n    let result = fibonacci(n - 1, memo) + fibonacci(n - 2, memo);\n    memo.insert(n, result);\n\n    result\n}\n\nfn main() {\n    let mut memo = HashMap::new();\n    let n = 10;\n    println!(\"Fibonacci({}) = {}\", n, fibonacci(n, &mut memo));\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.animation as animation\n\n# Function to perform insertion sort\ndef insertion_sort(arr, bar_subs):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i-1\n        while j >= 0 and key < arr[j]:\n            arr[j+1] = arr[j]\n            j -= 1\n            arr[j+1] = key\n            bar_subs.append(arr.copy())\n\n# Function to initialize the animation\ndef init():\n    return bars,\n\n# Function to update the figure for each frame\ndef update_fig(frame):\n    for bar, val in zip(bars, bar_subs[frame]):\n        bar.set_height(val)\n    return bars,\n\n# Create random data\nnp.random.seed(0)\nn = 50\ndata = np.random.rand(n)\n\n# List to store the states of array during sorting\nbar_subs = [data.copy()]\n\n# Sort the data and capture each step\ninsertion_sort(data, bar_subs)\n\n# Create figure and axis\nfig, ax = plt.subplots()\n# Create bars\nbars = ax.bar(range(n), bar_subs[0], color='blue')\n\n# Set limits and labels\nax.set_xlim(0, n)\nax.set_ylim(0, max(data))\nax.set_title(\"Insertion Sort Visualization\")\nax.set_xlabel(\"Index\")\nax.set_ylabel(\"Value\")\n\n# Create the animation\nani = animation.FuncAnimation(fig, update_fig, frames=range(len(bar_subs)),\n                              init_func=init, blit=True, interval=100, repeat=False)\n\nplt.show()\n```",
    "label": 1
  },
  {
    "codes": "function inspectNode(node) {\n            if (!node) {\n                return \"[No node]\";\n            }\n            if (crashyTextNodes && isBrokenNode(node)) {\n                return \"[Broken node]\";\n            }\n            if (isCharacterDataNode(node)) {\n                return '\"' + node.data + '\"';\n            }\n            if (node.nodeType == 1) {\n                var idAttr = node.id ? ' id=\"' + node.id + '\"' : \"\";\n                return \"<\" + node.nodeName + idAttr + \">[index:\" + getNodeIndex(node) + \",length:\" + node.childNodes.length + \"][\" + (node.innerHTML || \"[innerHTML not supported]\").slice(0, 25) + \"]\";\n            }\n            return node.nodeName;\n        }",
    "label": 0
  },
  {
    "codes": "public PagedList<DeletedVaultInner> listDeletedNext(final String nextPageLink) {\n        ServiceResponse<Page<DeletedVaultInner>> response = listDeletedNextSinglePageAsync(nextPageLink).toBlocking().single();\n        return new PagedList<DeletedVaultInner>(response.body()) {\n            @Override\n            public Page<DeletedVaultInner> nextPage(String nextPageLink) {\n                return listDeletedNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport pytest\nimport requests\n\n# Test case to validate the API response schema\ndef test_api_response_schema():\n    # Make a GET request to the API\n    response = requests.get('https://api.example.com/data')\n    \n    # Assert the status code is 200 (OK)\n    assert response.status_code == 200, \"Expected status code 200, but got {}\".format(response.status_code)\n    \n    # Validate the response schema\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"id\": {\"type\": \"number\"},\n            \"name\": {\"type\": \"string\"},\n            \"is_active\": {\"type\": \"boolean\"}\n        },\n        \"required\": [\"id\", \"name\"]\n    }\n    \n    # Assert the response matches the schema\n    assert response.json() == schema, \"Response does not match the expected schema\"\n\n# Test case to validate the API response schema with nested objects\ndef test_api_response_schema_nested():\n    # Make a GET request to the API\n    response = requests.get('https://api.example.com/data/nested')\n    \n    # Assert the status code is 200 (OK)\n    assert response.status_code == 200, \"Expected status code 200, but got {}\".format(response.status_code)\n    \n    # Validate the response schema with nested objects\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"id\": {\"type\": \"number\"},\n            \"name\": {\"type\": \"string\"},\n            \"details\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"age\": {\"type\": \"number\"},\n                    \"email\": {\"type\": \"string\"}\n                },\n                \"required\": [\"age\"]\n            }\n        },\n        \"required\": [\"id\", \"name\", \"details\"]\n    }\n    \n    # Assert the response matches the schema\n    assert response.json() == schema, \"Response does not match the expected schema\"\n\n# Test case to validate the API response schema with arrays\ndef test_api_response_schema_array():\n    # Make a GET request to the API\n    response = requests.get('https://api.example.com/data/array')\n    \n    # Assert the status code is 200 (OK)\n    assert response.status_code == 200, \"Expected status code 200, but got {}\".format(response.status_code)\n    \n    # Validate the response schema with arrays\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"items\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"id\": {\"type\": \"number\"},\n                        \"name\": {\"type\": \"string\"}\n                    },\n                    \"required\": [\"id\", \"name\"]\n                }\n            }\n        },\n        \"required\": [\"items\"]\n    }\n    \n    # Assert the response matches the schema\n    assert response.json() == schema, \"Response does not match the expected schema\"\n\n# Test case to validate the API response schema with optional fields\ndef test_api_response_schema_optional():\n    # Make a GET request to the API\n    response = requests.get('https://api.example.com/data/optional')\n    \n    # Assert the status code is 200 (OK)\n    assert response.status_code == 200, \"Expected status code 200, but got {}\".format(response.status_code)\n    \n    # Validate the response schema with optional fields\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"id\": {\"type\": \"number\"},\n            \"name\": {\"type\": \"string\"},\n            \"is_active\": {\"type\": \"boolean\", \"optional\": True}\n        },\n        \"required\": [\"id\", \"name\"]\n    }\n    \n    # Assert the response matches the schema\n    assert response.json() == schema, \"Response does not match the expected schema\"\n```",
    "label": 1
  },
  {
    "codes": "```dart\nString formatNumberCompact(double number) {\n  // Define the suffixes for thousands, millions, etc.\n  const suffixes = [\"\", \"K\", \"M\", \"B\", \"T\"];\n\n  // Determine the appropriate suffix index\n  int suffixIndex = 0;\n  while (number >= 1000 && suffixIndex < suffixes.length - 1) {\n    number /= 1000;\n    suffixIndex++;\n  }\n\n  // Format the number to one decimal place and append the suffix\n  return '${number.toStringAsFixed(1)}${suffixes[suffixIndex]}';\n}\n```",
    "label": 1
  },
  {
    "codes": "def save(self, filename=None):\n        \"\"\"Save the document to file.\n\n        Arguments:\n            * filename (str): The filename to save to. If not set (``None``, default), saves to the same file as loaded from.\n        \"\"\"\n        if not filename:\n            filename = self.filename\n        if not filename:\n            raise Exception(\"No filename specified\")\n        if filename[-4:].lower() == '.bz2':\n            f = bz2.BZ2File(filename,'wb')\n            f.write(self.xmlstring().encode('utf-8'))\n            f.close()\n        elif filename[-3:].lower() == '.gz':\n            f = gzip.GzipFile(filename,'wb') #pylint: disable=redefined-variable-type\n            f.write(self.xmlstring().encode('utf-8'))\n            f.close()\n        else:\n            f = io.open(filename,'w',encoding='utf-8')\n            f.write(self.xmlstring())\n            f.close()",
    "label": 0
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport gym\nimport numpy as np\n\n# Define the Q-learning agent class\nclass QLearningAgent:\n    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n        self.env = env\n        self.alpha = alpha  # Learning rate\n        self.gamma = gamma  # Discount factor\n        self.epsilon = epsilon  # Exploration rate\n        self.epsilon_decay = epsilon_decay  # Decay rate for exploration\n        self.epsilon_min = epsilon_min  # Minimum exploration rate\n        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))  # Initialize Q-table\n\n    def choose_action(self, state):\n        # Epsilon-greedy action selection\n        if np.random.rand() < self.epsilon:\n            return self.env.action_space.sample()  # Explore: random action\n        return np.argmax(self.q_table[state])  # Exploit: best action from Q-table\n\n    def learn(self, state, action, reward, next_state, done):\n        # Q-learning update rule\n        best_next_action = np.argmax(self.q_table[next_state])\n        td_target = reward + self.gamma * self.q_table[next_state][best_next_action] * (not done)\n        td_error = td_target - self.q_table[state][action]\n        self.q_table[state][action] += self.alpha * td_error\n\n        # Decay epsilon\n        if done:\n            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n# Initialize the environment and agent\nenv = gym.make('FrozenLake-v1')\nagent = QLearningAgent(env)\n\n# Training loop\nnum_episodes = 1000\nfor episode in range(num_episodes):\n    state = env.reset()\n    done = False\n\n    while not done:\n        action = agent.choose_action(state)\n        next_state, reward, done, info = env.step(action)\n        agent.learn(state, action, reward, next_state, done)\n        state = next_state\n\n    if (episode + 1) % 100 == 0:\n        print(f\"Episode: {episode + 1}, Epsilon: {agent.epsilon:.2f}\")\n\n# Test the trained agent\nstate = env.reset()\ndone = False\nwhile not done:\n    action = np.argmax(agent.q_table[state])\n    state, reward, done, info = env.step(action)\n    env.render()\n\nenv.close()\n```",
    "label": 1
  },
  {
    "codes": "```lua\n-- Destructible Terrain Script for Roblox\n\n-- Variables\nlocal Terrain = workspace.Terrain\nlocal DestructionRadius = 10\n\n-- Function to create explosion and destruct the terrain\nlocal function createExplosion(position, power)\n    local explosion = Instance.new(\"Explosion\")\n    explosion.Position = position\n    explosion.BlastRadius = power\n    explosion.Parent = workspace\n\n    explosion.Hit:Connect(function(part, distance)\n        local hitPosition = part.Position\n        if part:IsA(\"Terrain\") then\n            destroyTerrain(hitPosition, math.min(DestructionRadius, power - distance))\n        end\n    end)\nend\n\n-- Function to carve out the terrain\nlocal function destroyTerrain(center, radius)\n    -- Create a sphere shape to remove terrain\n    local region = Region3.new(center - Vector3.new(radius, radius, radius), \n                               center + Vector3.new(radius, radius, radius))\n    Terrain:FillRegion(region, Enum.Material.Air, 4)\nend\n\n-- Event connection for when a player clicks to cause an explosion\nlocal Players = game:GetService(\"Players\")\nPlayers.PlayerAdded:Connect(function(player)\n    player.CharacterAdded:Connect(function(character)\n        character:WaitForChild(\"HumanoidRootPart\").Touched:Connect(function(hit)\n            if hit:IsA(\"Part\") and hit.Material == Enum.Material.Neon then\n                createExplosion(hit.Position, DestructionRadius)\n            end\n        end)\n    end)\nend)\n```",
    "label": 1
  },
  {
    "codes": "function getBackgroundPoint(vm, size, alignment, chart) {\n\t// Background Position\n\tvar x = vm.x;\n\tvar y = vm.y;\n\n\tvar caretSize = vm.caretSize;\n\tvar caretPadding = vm.caretPadding;\n\tvar cornerRadius = vm.cornerRadius;\n\tvar xAlign = alignment.xAlign;\n\tvar yAlign = alignment.yAlign;\n\tvar paddingAndSize = caretSize + caretPadding;\n\tvar radiusAndPadding = cornerRadius + caretPadding;\n\n\tif (xAlign === 'right') {\n\t\tx -= size.width;\n\t} else if (xAlign === 'center') {\n\t\tx -= (size.width / 2);\n\t\tif (x + size.width > chart.width) {\n\t\t\tx = chart.width - size.width;\n\t\t}\n\t\tif (x < 0) {\n\t\t\tx = 0;\n\t\t}\n\t}\n\n\tif (yAlign === 'top') {\n\t\ty += paddingAndSize;\n\t} else if (yAlign === 'bottom') {\n\t\ty -= size.height + paddingAndSize;\n\t} else {\n\t\ty -= (size.height / 2);\n\t}\n\n\tif (yAlign === 'center') {\n\t\tif (xAlign === 'left') {\n\t\t\tx += paddingAndSize;\n\t\t} else if (xAlign === 'right') {\n\t\t\tx -= paddingAndSize;\n\t\t}\n\t} else if (xAlign === 'left') {\n\t\tx -= radiusAndPadding;\n\t} else if (xAlign === 'right') {\n\t\tx += radiusAndPadding;\n\t}\n\n\treturn {\n\t\tx: x,\n\t\ty: y\n\t};\n}",
    "label": 0
  },
  {
    "codes": "function addonsManager_waitForSearchFilter(aSpec) {\n    var spec = aSpec || { };\n    var filter = spec.filter;\n    var timeout = (spec.timeout == undefined) ? TIMEOUT : spec.timeout;\n\n    if (!filter)\n      throw new Error(arguments.callee.name + \": Search filter not specified.\");\n\n    // TODO: restore after 1.5.1 has landed\n    // var self = this;\n    // \n    // mozmill.utils.waitFor(function () {\n    //   return self.selectedSearchFilter.getNode() == filter.getNode();\n    // }, timeout, 100, \"Search filter '\" + filter.getNode().value + \"' has been set\");\n    \n    mozmill.utils.waitForEval(\"subject.self.selectedSearchFilter.getNode() == subject.aFilter.getNode()\",\n                              timeout, 100,\n                              {self: this, aFilter: filter});\n  }",
    "label": 0
  },
  {
    "codes": "```scala\nimport org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}\nimport org.apache.kafka.clients.consumer.ConsumerRecords\nimport java.time.Duration\nimport java.util.Properties\nimport scala.jdk.CollectionConverters._\n\nobject KafkaStreamConsumer {\n  def main(args: Array[String]): Unit = {\n    // Set up the properties for the Kafka consumer\n    val props = new Properties()\n    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\")\n    props.put(ConsumerConfig.GROUP_ID_CONFIG, \"scala-consumer-group\")\n    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\")\n    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\")\n    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\")\n\n    // Create the Kafka consumer\n    val consumer = new KafkaConsumer[String, String](props)\n    consumer.subscribe(List(\"your-topic-name\").asJava)\n\n    // Start consuming records\n    try {\n      while (true) {\n        val records: ConsumerRecords[String, String] = consumer.poll(Duration.ofMillis(100))\n        records.asScala.foreach { record =>\n          // Process each record\n          println(s\"Consumed record with key ${record.key()} and value: ${record.value()} from partition ${record.partition()} offset ${record.offset()}\")\n        }\n      }\n    } finally {\n      consumer.close()\n    }\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "function resolveCollisions(nodesByBreadth, nodeGap, height, width, orient) {\n    var keyAttr = orient === 'vertical' ? 'x' : 'y';\n    zrUtil.each(nodesByBreadth, function (nodes) {\n        nodes.sort(function (a, b) {\n            return a.getLayout()[keyAttr] - b.getLayout()[keyAttr];\n        });\n        var nodeX;\n        var node;\n        var dy;\n        var y0 = 0;\n        var n = nodes.length;\n        var nodeDyAttr = orient === 'vertical' ? 'dx' : 'dy';\n        for (var i = 0; i < n; i++) {\n            node = nodes[i];\n            dy = y0 - node.getLayout()[keyAttr];\n            if (dy > 0) {\n                nodeX = node.getLayout()[keyAttr] + dy;\n                orient === 'vertical'\n                    ? node.setLayout({x: nodeX}, true)\n                    : node.setLayout({y: nodeX}, true);\n            }\n            y0 = node.getLayout()[keyAttr] + node.getLayout()[nodeDyAttr] + nodeGap;\n        }\n        var viewWidth = orient === 'vertical' ? width : height;\n        // If the bottommost node goes outside the bounds, push it back up\n        dy = y0 - nodeGap - viewWidth;\n        if (dy > 0) {\n            nodeX = node.getLayout()[keyAttr] - dy;\n            orient === 'vertical'\n                ? node.setLayout({x: nodeX}, true)\n                : node.setLayout({y: nodeX}, true);\n\n            y0 = nodeX;\n            for (i = n - 2; i >= 0; --i) {\n                node = nodes[i];\n                dy = node.getLayout()[keyAttr] + node.getLayout()[nodeDyAttr] + nodeGap - y0;\n                if (dy > 0) {\n                    nodeX = node.getLayout()[keyAttr] - dy;\n                    orient === 'vertical'\n                        ? node.setLayout({x: nodeX}, true)\n                        : node.setLayout({y: nodeX}, true);\n                }\n                y0 = node.getLayout()[keyAttr];\n            }\n        }\n    });\n}",
    "label": 0
  },
  {
    "codes": "```java\n// Method to convert a decimal number to binary\npublic static String decimalToBinary(int decimal) {\n    // Handle the case for 0 separately\n    if (decimal == 0) {\n        return \"0\";\n    }\n    \n    StringBuilder binary = new StringBuilder();\n    \n    // Convert decimal to binary by repeatedly dividing by 2\n    while (decimal > 0) {\n        int remainder = decimal % 2; // Get the remainder\n        binary.insert(0, remainder); // Insert the remainder at the beginning\n        decimal = decimal / 2;       // Divide the number by 2\n    }\n    \n    return binary.toString(); // Return the binary string\n}\n```",
    "label": 1
  },
  {
    "codes": "def groupby(self, by=None, **kwargs):\n        \"\"\"\n        Group by and do not sort (unless specified)\n\n        For plydata use cases, there is no need to specify\n        group columns.\n        \"\"\"\n        if by is None:\n            by = self.plydata_groups\n\n        # Turn off sorting by groups messes with some verbs\n        if 'sort' not in kwargs:\n            kwargs['sort'] = False\n\n        return super().groupby(by, **kwargs)",
    "label": 0
  },
  {
    "codes": "Certainly! Below are pytest cases to validate an API response schema by checking response status codes and ensuring correct schema structure:\n\n```python\nimport pytest\nimport requests\nfrom jsonschema import validate, ValidationError\n\n# Define the expected schema for the API response\nexpected_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"id\": {\"type\": \"integer\"},\n        \"name\": {\"type\": \"string\"},\n        \"email\": {\"type\": \"string\", \"format\": \"email\"},\n        \"age\": {\"type\": \"integer\"},\n    },\n    \"required\": [\"id\", \"name\", \"email\", \"age\"]\n}\n\n# Example API endpoint\nAPI_URL = \"https://api.example.com/data\"\n\n@pytest.fixture\ndef get_api_response():\n    return requests.get(API_URL)\n\ndef test_status_code_200(get_api_response):\n    # Test whether the API response status code is 200 OK\n    assert get_api_response.status_code == 200\n\ndef test_response_schema(get_api_response):\n    # Test whether the API response follows the expected JSON schema\n    try:\n        validate(instance=get_api_response.json(), schema=expected_schema)\n    except ValidationError as e:\n        pytest.fail(f\"Response schema validation failed: {e.message}\")\n\ndef test_status_code_4xx(get_api_response):\n    # Example of testing for a 4xx client error response code\n    assert get_api_response.status_code != 404, \"Expected no 404 Not Found\"\n\ndef test_status_code_5xx(get_api_response):\n    # Example of testing for a 5xx server error response code\n    assert not 500 <= get_api_response.status_code < 600, \"Unexpected server error\"\n```\n\nComments have been included as requested, while additional content has been omitted per your instructions.",
    "label": 1
  },
  {
    "codes": "```dart\nimport 'dart:async';\nimport 'package:flutter/services.dart';\n\nclass NativeMLAccelerator {\n  static const MethodChannel _channel = MethodChannel('native_ml_accelerator');\n\n  /// Initializes the native machine learning accelerator.\n  static Future<void> initialize() async {\n    try {\n      await _channel.invokeMethod('initialize');\n    } on PlatformException catch (e) {\n      throw 'Failed to initialize: ${e.message}';\n    }\n  }\n\n  /// Loads a machine learning model.\n  static Future<void> loadModel(String modelPath) async {\n    try {\n      await _channel.invokeMethod('loadModel', {'modelPath': modelPath});\n    } on PlatformException catch (e) {\n      throw 'Failed to load model: ${e.message}';\n    }\n  }\n\n  /// Runs inference on the given input data.\n  static Future<dynamic> runInference(Map<String, dynamic> inputData) async {\n    try {\n      final result = await _channel.invokeMethod('runInference', inputData);\n      return result;\n    } on PlatformException catch (e) {\n      throw 'Failed to run inference: ${e.message}';\n    }\n  }\n\n  /// Releases resources associated with loaded models.\n  static Future<void> dispose() async {\n    try {\n      await _channel.invokeMethod('dispose');\n    } on PlatformException catch (e) {\n      throw 'Failed to dispose resources: ${e.message}';\n    }\n  }\n}\n```\n\n```kotlin\n// Android platform code (Kotlin)\nclass NativeMLAcceleratorPlugin: FlutterPlugin, MethodCallHandler {\n  private lateinit var channel : MethodChannel\n\n  override fun onAttachedToEngine(@NonNull flutterPluginBinding: FlutterPlugin.FlutterPluginBinding) {\n    channel = MethodChannel(flutterPluginBinding.binaryMessenger, \"native_ml_accelerator\")\n    channel.setMethodCallHandler(this)\n  }\n\n  override fun onMethodCall(@NonNull call: MethodCall, @NonNull result: Result) {\n    when (call.method) {\n      \"initialize\" -> {\n        // Initialize ML acceleration\n        result.success(null)\n      }\n      \"loadModel\" -> {\n        val modelPath = call.argument<String>(\"modelPath\")\n        // Load model logic here\n        result.success(null)\n      }\n      \"runInference\" -> {\n        val inputData = call.arguments as Map<String, Any>\n        // Inference logic here\n        result.success(null)\n      }\n      \"dispose\" -> {\n        // Dispose resources logic here\n        result.success(null)\n      }\n      else -> {\n        result.notImplemented()\n      }\n    }\n  }\n\n  override fun onDetachedFromEngine(@NonNull binding: FlutterPlugin.FlutterPluginBinding) {\n    channel.setMethodCallHandler(null)\n  }\n}\n```\n\n```swift\n// iOS platform code (Swift)\npublic class SwiftNativeMLAcceleratorPlugin: NSObject, FlutterPlugin {\n  public static func register(with registrar: FlutterPluginRegistrar) {\n    let channel = FlutterMethodChannel(name: \"native_ml_accelerator\", binaryMessenger: registrar.messenger())\n    let instance = SwiftNativeMLAcceleratorPlugin()\n    registrar.addMethodCallDelegate(instance, channel: channel)\n  }\n\n  public func handle(_ call: FlutterMethodCall, result: @escaping FlutterResult) {\n    switch call.method {\n    case \"initialize\":\n      // Initialize ML acceleration\n      result(nil)\n    case \"loadModel\":\n      let args = call.arguments as? [String: Any]\n      let modelPath = args?[\"modelPath\"] as? String ?? \"\"\n      // Load model logic here\n      result(nil)\n    case \"runInference\":\n      let inputData = call.arguments as? [String: Any] ?? [:]\n      // Inference logic here\n      result(nil)\n    case \"dispose\":\n      // Dispose resources logic here\n      result(nil)\n    default:\n      result(FlutterMethodNotImplemented)\n    }\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "public SecretBundle getSecret(String vaultBaseUrl, String secretName, String secretVersion) {\n        return getSecretWithServiceResponseAsync(vaultBaseUrl, secretName, secretVersion).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <Servo.h>\n#include <SoftwareSerial.h>\n\n// Define Bluetooth RX and TX pins\n#define BT_RX 10\n#define BT_TX 11\n\n// Create a SoftwareSerial object for Bluetooth communication\nSoftwareSerial bluetooth(BT_RX, BT_TX);\n\n// Create Servo objects\nServo servo1;\nServo servo2;\n\nvoid setup() {\n  // Attach servos to pins\n  servo1.attach(9);  // Servo 1 on pin 9\n  servo2.attach(8);  // Servo 2 on pin 8\n\n  // Initialize Bluetooth communication\n  bluetooth.begin(9600);\n}\n\nvoid loop() {\n  // Check if data is available from Bluetooth\n  if (bluetooth.available()) {\n    // Read the incoming byte\n    char command = bluetooth.read();\n\n    // Control servo1 based on the command\n    if (command == 'A') {\n      servo1.write(0);  // Move servo1 to 0 degrees\n    } else if (command == 'B') {\n      servo1.write(90);  // Move servo1 to 90 degrees\n    } else if (command == 'C') {\n      servo1.write(180);  // Move servo1 to 180 degrees\n    }\n\n    // Control servo2 based on the command\n    if (command == 'D') {\n      servo2.write(0);  // Move servo2 to 0 degrees\n    } else if (command == 'E') {\n      servo2.write(90);  // Move servo2 to 90 degrees\n    } else if (command == 'F') {\n      servo2.write(180);  // Move servo2 to 180 degrees\n    }\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponseWithHeaders<Page<NodeFile>, FileListFromTaskHeaders>> listFromTaskSinglePageAsync(final String jobId, final String taskId, final Boolean recursive, final FileListFromTaskOptions fileListFromTaskOptions) {\n        if (this.client.batchUrl() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.batchUrl() is required and cannot be null.\");\n        }\n        if (jobId == null) {\n            throw new IllegalArgumentException(\"Parameter jobId is required and cannot be null.\");\n        }\n        if (taskId == null) {\n            throw new IllegalArgumentException(\"Parameter taskId is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        Validator.validate(fileListFromTaskOptions);\n        String filter = null;\n        if (fileListFromTaskOptions != null) {\n            filter = fileListFromTaskOptions.filter();\n        }\n        Integer maxResults = null;\n        if (fileListFromTaskOptions != null) {\n            maxResults = fileListFromTaskOptions.maxResults();\n        }\n        Integer timeout = null;\n        if (fileListFromTaskOptions != null) {\n            timeout = fileListFromTaskOptions.timeout();\n        }\n        UUID clientRequestId = null;\n        if (fileListFromTaskOptions != null) {\n            clientRequestId = fileListFromTaskOptions.clientRequestId();\n        }\n        Boolean returnClientRequestId = null;\n        if (fileListFromTaskOptions != null) {\n            returnClientRequestId = fileListFromTaskOptions.returnClientRequestId();\n        }\n        DateTime ocpDate = null;\n        if (fileListFromTaskOptions != null) {\n            ocpDate = fileListFromTaskOptions.ocpDate();\n        }\n        String parameterizedHost = Joiner.on(\", \").join(\"{batchUrl}\", this.client.batchUrl());\n        DateTimeRfc1123 ocpDateConverted = null;\n        if (ocpDate != null) {\n            ocpDateConverted = new DateTimeRfc1123(ocpDate);\n        }\n        return service.listFromTask(jobId, taskId, recursive, this.client.apiVersion(), this.client.acceptLanguage(), filter, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDateConverted, parameterizedHost, this.client.userAgent())\n                .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponseWithHeaders<Page<NodeFile>, FileListFromTaskHeaders>>>() {\n                    @Override\n                    public Observable<ServiceResponseWithHeaders<Page<NodeFile>, FileListFromTaskHeaders>> call(Response<ResponseBody> response) {\n                        try {\n                            ServiceResponseWithHeaders<PageImpl<NodeFile>, FileListFromTaskHeaders> result = listFromTaskDelegate(response);\n                            return Observable.just(new ServiceResponseWithHeaders<Page<NodeFile>, FileListFromTaskHeaders>(result.body(), result.headers(), result.response()));\n                        } catch (Throwable t) {\n                            return Observable.error(t);\n                        }\n                    }\n                });\n    }",
    "label": 0
  },
  {
    "codes": "```xml\n<!-- AndroidManifest.xml -->\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.example.objectdetection\">\n    \n    <application\n        android:allowBackup=\"true\"\n        android:icon=\"@mipmap/ic_launcher\"\n        android:label=\"@string/app_name\"\n        android:roundIcon=\"@mipmap/ic_launcher_round\"\n        android:supportsRtl=\"true\"\n        android:theme=\"@style/Theme.AppCompat.Light.NoActionBar\">\n        <activity android:name=\".MainActivity\">\n            <intent-filter>\n                <action android:name=\"android.intent.action.MAIN\" />\n                <category android:name=\"android.intent.category.LAUNCHER\" />\n            </intent-filter>\n        </activity>\n    </application>\n    \n    <!-- Permissions to access camera -->\n    <uses-permission android:name=\"android.permission.CAMERA\" />\n    <uses-feature android:name=\"android.hardware.camera\" />\n    <uses-feature android:name=\"android.hardware.camera.autofocus\" />\n</manifest>\n```\n\n```java\n// MainActivity.java\npackage com.example.objectdetection;\n\nimport android.Manifest;\nimport android.content.pm.PackageManager;\nimport android.graphics.Bitmap;\nimport android.os.Bundle;\nimport android.util.Size;\nimport android.widget.Toast;\nimport androidx.annotation.NonNull;\nimport androidx.appcompat.app.AppCompatActivity;\nimport androidx.core.app.ActivityCompat;\nimport androidx.core.content.ContextCompat;\nimport com.example.objectdetection.databinding.ActivityMainBinding;\nimport org.tensorflow.lite.Interpreter;\nimport org.tensorflow.lite.support.common.FileUtil;\nimport org.tensorflow.lite.support.image.TensorImage;\nimport org.tensorflow.lite.support.image.ops.ResizeOp;\nimport org.tensorflow.lite.support.tensorbuffer.TensorBuffer;\nimport java.io.IOException;\nimport java.nio.MappedByteBuffer;\nimport java.util.List;\n\npublic class MainActivity extends AppCompatActivity {\n    \n    private ActivityMainBinding binding;\n    private Interpreter tflite;\n\n    private final int REQUEST_CAMERA_PERMISSION = 200;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n\n        binding = ActivityMainBinding.inflate(getLayoutInflater());\n        setContentView(binding.getRoot());\n\n        // Check for camera permission\n        if (ContextCompat.checkSelfPermission(this, Manifest.permission.CAMERA)\n                != PackageManager.PERMISSION_GRANTED) {\n            ActivityCompat.requestPermissions(this,\n                    new String[]{Manifest.permission.CAMERA}, REQUEST_CAMERA_PERMISSION);\n        } else {\n            setupCamera();\n        }\n\n        // Load TFLite model\n        try {\n            MappedByteBuffer tfliteModel = FileUtil.loadMappedFile(this, \"model.tflite\");\n            tflite = new Interpreter(tfliteModel);\n        } catch (IOException e) {\n            e.printStackTrace();\n            Toast.makeText(this, \"Error loading model\", Toast.LENGTH_SHORT).show();\n        }\n    }\n\n    // Handling camera permission request result\n    @Override\n    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions,\n                                           @NonNull int[] grantResults) {\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults);\n\n        if (requestCode == REQUEST_CAMERA_PERMISSION) {\n            if (grantResults.length > 0 && grantResults[0] == PackageManager.PERMISSION_GRANTED) {\n                setupCamera();\n            } else {\n                Toast.makeText(this, \"Camera permission denied\", Toast.LENGTH_SHORT).show();\n            }\n        }\n    }\n\n    private void setupCamera() {\n        // Setup camera capture session to get real-time frames\n        // This involves initializing a CameraX or Camera2 API session\n\n        // Start the camera stream, handle frame processing...\n    }\n\n    private void processImage(Bitmap bitmap) {\n        // Perform inference with the TensorFlow Lite model\n        TensorImage tensorImage = TensorImage.fromBitmap(bitmap);\n        tensorImage = TensorImage.createFromResizeOps(tensorImage, new ResizeOp(300, 300, ResizeOp.ResizeMethod.BILINEAR), true);\n\n        TensorBuffer probabilityBuffer = TensorBuffer.createFixedSize(new int[]{1, 10}, DataType.FLOAT32);\n        tflite.run(tensorImage.getBuffer(), probabilityBuffer.getBuffer().rewind());\n\n        // Post-process the output to extract detected objects, draw bounding boxes, etc.\n    }\n}\n```\n\n```gradle\n// build.gradle (Module: app)\nplugins {\n    id 'com.android.application'\n}\n\nandroid {\n    compileSdk 33\n\n    defaultConfig {\n        applicationId \"com.example.objectdetection\"\n        minSdk 21\n        targetSdk 33\n        versionCode 1\n        versionName \"1.0\"\n\n        testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\"\n    }\n\n    buildTypes {\n        release {\n            minifyEnabled false\n            proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'\n        }\n    }\n}\n\ndependencies {\n    implementation 'org.tensorflow:tensorflow-lite:2.4.0'\n    implementation 'org.tensorflow:tensorflow-lite-support:0.1.0'\n    implementation 'androidx.appcompat:appcompat:1.3.1'\n    implementation 'androidx.constraintlayout:constraintlayout:2.1.0'\n    implementation 'com.google.android.material:material:1.4.0'\n    implementation 'androidx.camera:camera-camera2:1.0.1'\n    implementation 'androidx.camera:camera-lifecycle:1.0.1'\n    implementation 'androidx.camera:camera-view:1.0.0-alpha27'\n}\n```\n\n```xml\n<!-- res/layout/activity_main.xml -->\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n    \n    <!-- A simple camera preview layout -->\n    <androidx.camera.view.PreviewView\n        android:id=\"@+id/viewFinder\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"/>\n    \n    <!-- Overlay to draw bounding boxes -->\n    <com.example.objectdetection.OverlayView\n        android:id=\"@+id/overlay\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"/>\n</RelativeLayout>\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse std::env;\nuse serde_json::{Value, from_str};\n\nfn main() {\n    // Parse command line arguments\n    let args: Vec<String> = env::args().collect();\n    if args.len() < 3 {\n        eprintln!(\"Usage: <json-string> <query>\");\n        std::process::exit(1);\n    }\n\n    let json_input = &args[1];\n    let query = &args[2];\n\n    // Deserialize JSON input\n    let parsed_json: Value = match from_str(json_input) {\n        Ok(val) => val,\n        Err(e) => {\n            eprintln!(\"Error parsing JSON: {}\", e);\n            std::process::exit(1);\n        }\n    };\n\n    // Execute query\n    match execute_query(&parsed_json, query) {\n        Some(result) => println!(\"{}\", result),\n        None => eprintln!(\"Query resulted in no output\"),\n    }\n}\n\n// Function to execute a simple query\nfn execute_query(value: &Value, query: &str) -> Option<Value> {\n    match query {\n        \".\" => Some(value.clone()), // Return the whole JSON\n        _ => value.get(query).cloned(), // Return specific field\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *JobUpdate) SetNotificationProperty(v *NotificationProperty) *JobUpdate {\n\ts.NotificationProperty = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Drop rows with any null values\ndf_cleaned = df.dropna()\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```\n\n```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Drop columns with any null values\ndf_cleaned = df.dropna(axis=1)\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```\n\n```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Fill null values with a specific value (e.g., 0)\ndf_cleaned = df.fillna(0)\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```\n\n```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Drop rows where all values are null\ndf_cleaned = df.dropna(how='all')\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```\n\n```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Drop rows with null values in specific columns\ndf_cleaned = df.dropna(subset=['column1', 'column2'])\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary modules\nimport coremltools as ct\nfrom tensorflow import keras\n\n# Load your Keras model\nkeras_model = keras.models.load_model('path_to_your_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model, \n                          inputs=[ct.TensorType(shape=(None, *keras_model.input.shape[1:]))])\n\n# Specify the path where the CoreML model will be saved\ncoreml_model_path = 'path_to_save_coreml_model.mlmodel'\n\n# Save the converted CoreML model\ncoreml_model.save(coreml_model_path)\n\n# Confirm conversion was successful\nprint(f'Model successfully converted and saved at {coreml_model_path}')\n```",
    "label": 1
  },
  {
    "codes": "```hcl\nprovider \"aws\" {\n  region = \"us-east-1\"\n}\n\nresource \"aws_sagemaker_model\" \"example\" {\n  name                  = \"example-model\"\n  execution_role_arn    = aws_iam_role.sagemaker_execution_role.arn\n  primary_container {\n    image               = \"123456789012.dkr.ecr.us-east-1.amazonaws.com/my-custom-image:latest\"\n    model_data_url      = \"s3://my-bucket/model/model.tar.gz\"\n  }\n}\n\nresource \"aws_iam_role\" \"sagemaker_execution_role\" {\n  name = \"sagemaker_execution_role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Effect = \"Allow\"\n      Principal = {\n        Service = \"sagemaker.amazonaws.com\"\n      }\n      Action = \"sts:AssumeRole\"\n    }]\n  })\n}\n\nresource \"aws_iam_role_policy\" \"sagemaker_policy\" {\n  name = \"sagemaker_policy\"\n  role = aws_iam_role.sagemaker_execution_role.id\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Action = [\n          \"s3:GetObject\",\n          \"s3:PutObject\"\n        ]\n        Resource = \"arn:aws:s3:::my-bucket/*\"\n      },\n      {\n        Effect = \"Allow\"\n        Action = [\n          \"cloudwatch:PutMetricData\"\n        ]\n        Resource = \"*\"\n      }\n    ]\n  })\n}\n```\n",
    "label": 1
  },
  {
    "codes": "public PagedList<IntegrationAccountSchemaInner> listByIntegrationAccounts(final String resourceGroupName, final String integrationAccountName) {\n        ServiceResponse<Page<IntegrationAccountSchemaInner>> response = listByIntegrationAccountsSinglePageAsync(resourceGroupName, integrationAccountName).toBlocking().single();\n        return new PagedList<IntegrationAccountSchemaInner>(response.body()) {\n            @Override\n            public Page<IntegrationAccountSchemaInner> nextPage(String nextPageLink) {\n                return listByIntegrationAccountsNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport re\n\ndef validate_email(email):\n    # Regex pattern to validate email addresses\n    pattern = r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$'\n    \n    # Check if the email matches the pattern\n    if re.match(pattern, email):\n        return True\n    else:\n        return False\n```\n\n```python\n# Example usage:\n# print(validate_email(\"example@example.com\"))  # Output: True\n# print(validate_email(\"invalid-email@\"))       # Output: False\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <sys/inotify.h>\n#include <unistd.h>\n#include <cstring>\n#include <csignal>\n#include <vector>\n#include <poll.h>\n\n#define EVENT_SIZE  (sizeof(struct inotify_event))\n#define EVENT_BUF_LEN (1024 * (EVENT_SIZE + 16))\n\nbool running = true;\n\nvoid signalHandler(int signum) {\n    running = false;\n}\n\nvoid handleEvent(const struct inotify_event* event) {\n    if (event->len) {\n        if (event->mask & IN_CREATE) {\n            std::cout << \"Directory \" << event->name << \" was created.\" << std::endl;\n        }\n        else if (event->mask & IN_DELETE) {\n            std::cout << \"Directory \" << event->name << \" was deleted.\" << std::endl;\n        }\n        else if (event->mask & IN_MODIFY) {\n            std::cout << \"Directory \" << event->name << \" was modified.\" << std::endl;\n        }\n    }\n}\n\nint main() {\n    // Setup signal handling for safe daemon termination\n    signal(SIGINT, signalHandler);\n    signal(SIGTERM, signalHandler);\n\n    // Initialize inotify\n    int inotifyFd = inotify_init();\n    if (inotifyFd < 0) {\n        std::cerr << \"Failed to initialize inotify: \" << strerror(errno) << std::endl;\n        return 1;\n    }\n\n    // Add watch on the specified directory\n    const char* dir_to_watch = \"/path/to/watch\";\n    int watchDescriptor = inotify_add_watch(inotifyFd, dir_to_watch, IN_CREATE | IN_DELETE | IN_MODIFY);\n    if (watchDescriptor < 0) {\n        std::cerr << \"Failed to add watch: \" << strerror(errno) << std::endl;\n        return 1;\n    }\n\n    std::vector<char> buffer(EVENT_BUF_LEN);\n\n    // Main loop\n    while (running) {\n        // Using poll to wait for an event with a timeout of 500 ms\n        struct pollfd fds;\n        fds.fd = inotifyFd;\n        fds.events = POLLIN;\n\n        int pollResult = poll(&fds, 1, 500);\n        if (pollResult < 0) {\n            std::cerr << \"Polling error occurred: \" << strerror(errno) << std::endl;\n            break;\n        } else if (pollResult > 0) {\n            int length = read(inotifyFd, buffer.data(), EVENT_BUF_LEN);\n            if (length < 0) {\n                std::cerr << \"Failed to read events: \" << strerror(errno) << std::endl;\n                break;\n            }\n\n            int i = 0;\n            while (i < length) {\n                struct inotify_event* event = (struct inotify_event*)&buffer[i];\n                handleEvent(event);\n                i += EVENT_SIZE + event->len;\n            }\n        }\n    }\n\n    // Clean up\n    inotify_rm_watch(inotifyFd, watchDescriptor);\n    close(inotifyFd);\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *Crawler) SetVersion(v int64) *Crawler {\n\ts.Version = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```java\n// Import necessary JavaFX libraries\nimport javafx.application.Application;\nimport javafx.scene.Scene;\nimport javafx.scene.canvas.Canvas;\nimport javafx.scene.canvas.GraphicsContext;\nimport javafx.scene.layout.StackPane;\nimport javafx.scene.paint.Color;\nimport javafx.stage.Stage;\n\n// Main class extending Application\npublic class FractalPatternApp extends Application {\n\n    // Constants for canvas size\n    private static final int WIDTH = 800;\n    private static final int HEIGHT = 800;\n\n    @Override\n    public void start(Stage primaryStage) {\n        // Create a canvas to draw the fractal\n        Canvas canvas = new Canvas(WIDTH, HEIGHT);\n        GraphicsContext gc = canvas.getGraphicsContext2D();\n\n        // Set initial background color\n        gc.setFill(Color.BLACK);\n        gc.fillRect(0, 0, WIDTH, HEIGHT);\n\n        // Draw the fractal pattern\n        drawFractal(gc, WIDTH / 2, HEIGHT / 2, 300, 0);\n\n        // Create a layout and add the canvas\n        StackPane root = new StackPane(canvas);\n\n        // Set up the scene and stage\n        Scene scene = new Scene(root, WIDTH, HEIGHT);\n        primaryStage.setTitle(\"Interactive Fractal Patterns\");\n        primaryStage.setScene(scene);\n        primaryStage.show();\n    }\n\n    // Recursive method to draw a fractal pattern\n    private void drawFractal(GraphicsContext gc, double x, double y, double size, int depth) {\n        if (depth > 8) { // Base case to stop recursion\n            return;\n        }\n\n        // Draw a circle at the current position\n        gc.setStroke(Color.WHITE);\n        gc.strokeOval(x - size / 2, y - size / 2, size, size);\n\n        // Recursively draw smaller circles\n        drawFractal(gc, x - size / 2, y, size / 2, depth + 1);\n        drawFractal(gc, x + size / 2, y, size / 2, depth + 1);\n        drawFractal(gc, x, y - size / 2, size / 2, depth + 1);\n        drawFractal(gc, x, y + size / 2, size / 2, depth + 1);\n    }\n\n    // Main method to launch the application\n    public static void main(String[] args) {\n        launch(args);\n    }\n}\n```\n\nThis code creates a JavaFX application that displays an interactive fractal pattern using recursive circles. The fractal is drawn using a recursive method `drawFractal`, which stops after a certain depth to prevent infinite recursion. The canvas is set up with a black background, and the fractal is drawn in white.",
    "label": 1
  },
  {
    "codes": "```swift\nimport Foundation\n\n// Struct to represent a Polynomial\nstruct Polynomial {\n    // Coefficients array where the index represents the power of x\n    var coefficients: [Double]\n    \n    // Method to evaluate the polynomial at a given value of x\n    func evaluate(at x: Double) -> Double {\n        var result: Double = 0.0\n        for (exponent, coefficient) in coefficients.enumerated() {\n            result += coefficient * pow(x, Double(exponent))\n        }\n        return result\n    }\n}\n\n// Example usage:\nlet polynomial = Polynomial(coefficients: [1.0, -3.0, 2.0]) // Represents the polynomial 1 - 3x + 2x^2\nlet value = polynomial.evaluate(at: 2.0) // Evaluate at x = 2\nprint(value) // Output will be 3.0\n```",
    "label": 1
  },
  {
    "codes": "def bootstrap_prompt(prompt_kwargs, group):\n    \"\"\"\n    Bootstrap prompt_toolkit kwargs or use user defined values.\n\n    :param prompt_kwargs: The user specified prompt kwargs.\n    \"\"\"\n    prompt_kwargs = prompt_kwargs or {}\n\n    defaults = {\n        \"history\": InMemoryHistory(),\n        \"completer\": ClickCompleter(group),\n        \"message\": u\"> \",\n    }\n\n    for key in defaults:\n        default_value = defaults[key]\n        if key not in prompt_kwargs:\n            prompt_kwargs[key] = default_value\n\n    return prompt_kwargs",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define the directory containing log files\nLOG_DIR=\"/var/log\"\n\n# Define the archive directory\nARCHIVE_DIR=\"/var/log/archive\"\n\n# Create the archive directory if it doesn't exist\nmkdir -p \"$ARCHIVE_DIR\"\n\n# Find log files older than 30 days and archive them\nfind \"$LOG_DIR\" -type f -name \"*.log\" -mtime +30 -exec tar -czf \"$ARCHIVE_DIR/logs_$(date +%Y%m%d).tar.gz\" {} +\n\n# Optionally, remove the original log files after archiving\n# find \"$LOG_DIR\" -type f -name \"*.log\" -mtime +30 -exec rm {} \\;\n\n# Print completion message\necho \"Log files older than 30 days have been archived.\"\n```",
    "label": 1
  },
  {
    "codes": "```powershell\n# Define the directory containing the files\n$directoryPath = \"C:\\Path\\To\\Files\"\n\n# Define the regex pattern for matching old file names\n$regexPattern = \"^old_(.*)\\.txt$\" # Example pattern\n\n# Define the replacement pattern for new file names\n$replacementPattern = \"new_$1.txt\" # Corresponding replacement\n\n# Get all files in the directory\n$files = Get-ChildItem -Path $directoryPath\n\n# Iterate over each file\nforeach ($file in $files) {\n    # Check if the file name matches the regex pattern\n    if ($file.Name -match $regexPattern) {\n        # Construct the new file name using the replacement pattern\n        $newFileName = [regex]::Replace($file.Name, $regexPattern, $replacementPattern)\n        \n        # Construct full paths for moving the file\n        $oldFilePath = Join-Path $directoryPath $file.Name\n        $newFilePath = Join-Path $directoryPath $newFileName\n\n        # Rename the file with the new name\n        Rename-Item -Path $oldFilePath -NewName $newFileName\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *GetJobRunsOutput) SetJobRuns(v []*JobRun) *GetJobRunsOutput {\n\ts.JobRuns = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```java\npublic void batchInsertEntities(List<MyEntity> entities, SessionFactory sessionFactory) {\n    int batchSize = 50; // Set a reasonable batch size\n    Session session = sessionFactory.openSession();\n    Transaction transaction = session.beginTransaction();\n\n    try {\n        for (int i = 0; i < entities.size(); i++) {\n            session.save(entities.get(i));\n            if (i > 0 && i % batchSize == 0) {\n                session.flush(); // Synchronize session with the database\n                session.clear(); // Clear session to manage memory\n            }\n        }\n        transaction.commit();\n    } catch (Exception e) {\n        if (transaction != null) transaction.rollback();\n        e.printStackTrace();\n    } finally {\n        session.close(); // Ensure the session is closed\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```python\nimport os\nfrom contextlib import contextmanager\n\n@contextmanager\ndef temporary_env(**kwargs):\n    original_env = {key: os.environ.get(key) for key in kwargs}\n    os.environ.update(kwargs)\n    try:\n        yield\n    finally:\n        # Restore the original environment variables\n        for key, value in original_env.items():\n            if value is None:\n                del os.environ[key]  # Remove if it was absent originally\n            else:\n                os.environ[key] = value  # Restore original value\n\n# Example usage:\n# with temporary_env(MY_VAR='test'):\n#     print(os.environ.get('MY_VAR'))\n# print(os.environ.get('MY_VAR'))\n```\n",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<StorageBundle> restoreStorageAccountAsync(String vaultBaseUrl, byte[] storageBundleBackup, final ServiceCallback<StorageBundle> serviceCallback) {\n        return ServiceFuture.fromResponse(restoreStorageAccountWithServiceResponseAsync(vaultBaseUrl, storageBundleBackup), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```R\n# Load necessary libraries\nlibrary(shiny)\nlibrary(shinydashboard)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\n\n# Read vaccination data (assumes a CSV file with 'country', 'vaccination_rate' columns)\nvaccination_data <- read_csv(\"path/to/covid_vaccination_data.csv\")\n\n# Define UI for application\nui <- dashboardPage(\n  dashboardHeader(title = \"COVID-19 Vaccination Rates by Country\"),\n  dashboardSidebar(\n    selectInput(\"selected_country\", \"Select a Country:\", \n                choices = unique(vaccination_data$country)),\n    actionButton(\"plot_button\", \"Plot Vaccination Rate\")\n  ),\n  dashboardBody(\n    fluidRow(\n      box(title = \"Vaccination Rate Plot\", status = \"primary\", solidHeader = TRUE, \n          plotOutput(\"vaccination_plot\")),\n      box(title = \"Country Data\", status = \"info\", solidHeader = TRUE, \n          tableOutput(\"country_data\"))\n    )\n  )\n)\n\n# Define server logic\nserver <- function(input, output, session) {\n  \n  selected_data <- reactive({\n    req(input$selected_country)\n    vaccination_data %>% filter(country == input$selected_country)\n  })\n  \n  output$vaccination_plot <- renderPlot({\n    req(input$plot_button)\n    ggplot(selected_data(), aes(x = country, y = vaccination_rate)) +\n      geom_bar(stat=\"identity\", fill=\"steelblue\") +\n      labs(title = paste(\"Vaccination Rate in\", input$selected_country),\n           x = \"Country\", y = \"Vaccination Rate (%)\") +\n      theme_minimal()\n  })\n\n  output$country_data <- renderTable({\n    selected_data()\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model)\n\n# Save the CoreML model to a file\ncoreml_model.save('converted_model.mlmodel')\n\n# Optional: Add metadata to the CoreML model\ncoreml_model.author = 'Your Name'\ncoreml_model.license = 'MIT'\ncoreml_model.short_description = 'A converted Keras model to CoreML format.'\n\n# Save the updated CoreML model\ncoreml_model.save('converted_model_with_metadata.mlmodel')\n```",
    "label": 1
  },
  {
    "codes": "```typescript\n// Define the User type\ntype User = {\n  id: ID!\n  username: String!\n  email: String!\n  posts: [Post!]! // A user can have multiple posts\n  followers: [User!]! // A user can have multiple followers\n  following: [User!]! // A user can follow multiple users\n  createdAt: String!\n  updatedAt: String!\n};\n\n// Define the Post type\ntype Post = {\n  id: ID!\n  content: String!\n  author: User! // Each post has one author\n  likes: [User!]! // A post can have multiple likes\n  comments: [Comment!]! // A post can have multiple comments\n  createdAt: String!\n  updatedAt: String!\n};\n\n// Define the Comment type\ntype Comment = {\n  id: ID!\n  content: String!\n  author: User! // Each comment has one author\n  post: Post! // Each comment belongs to one post\n  createdAt: String!\n  updatedAt: String!\n};\n\n// Define the Query type\ntype Query = {\n  // Fetch a user by ID\n  user(id: ID!): User\n  // Fetch all users\n  users: [User!]!\n  // Fetch a post by ID\n  post(id: ID!): Post\n  // Fetch all posts\n  posts: [Post!]!\n  // Fetch a comment by ID\n  comment(id: ID!): Comment\n  // Fetch all comments\n  comments: [Comment!]!\n};\n\n// Define the Mutation type\ntype Mutation = {\n  // Create a new user\n  createUser(username: String!, email: String!): User!\n  // Create a new post\n  createPost(content: String!, authorId: ID!): Post!\n  // Create a new comment\n  createComment(content: String!, authorId: ID!, postId: ID!): Comment!\n  // Like a post\n  likePost(userId: ID!, postId: ID!): Post!\n  // Follow a user\n  followUser(followerId: ID!, followeeId: ID!): User!\n};\n\n// Define the Subscription type\ntype Subscription = {\n  // Subscribe to new posts by a specific user\n  newPostByUser(userId: ID!): Post!\n  // Subscribe to new comments on a specific post\n  newCommentOnPost(postId: ID!): Comment!\n  // Subscribe to new likes on a specific post\n  newLikeOnPost(postId: ID!): User!\n};\n\n// Export the schema\nexport const schema = `\n  ${User}\n  ${Post}\n  ${Comment}\n  ${Query}\n  ${Mutation}\n  ${Subscription}\n`;\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <vector>\n#include <queue>\n#include <future>\n#include <mutex>\n#include <condition_variable>\n#include <thread>\n\nclass ThreadPool {\npublic:\n    explicit ThreadPool(size_t numThreads) : stop(false) {\n        for(size_t i = 0; i < numThreads; ++i) {\n            workers.emplace_back([this, i] {\n                while(true) {\n                    std::function<void()> task;\n                    {\n                        std::unique_lock<std::mutex> lock(queueMutex);\n\n                        condition.wait(lock, [this, i] {\n                            // Attempt to steal work from other threads\n                            return stop || !taskQueues[i].empty() || tryStealTask(i, task);\n                        });\n\n                        if (stop && taskQueues[i].empty())\n                            return;\n\n                        if (taskQueues[i].empty()) {\n                            continue;  // If task is stolen continue loop\n                        }\n\n                        task = std::move(taskQueues[i].front());\n                        taskQueues[i].pop();\n                    }\n                    task();\n                }\n            });\n            taskQueues.emplace_back();\n        }\n    }\n    \n    template<class F, class... Args>\n    auto enqueue(F&& f, Args&&... args) \n        -> std::future<typename std::result_of<F(Args...)>::type> {\n        using returnType = typename std::result_of<F(Args...)>::type;\n\n        auto task = std::make_shared<std::packaged_task<returnType()>>(\n            std::bind(std::forward<F>(f), std::forward<Args>(args)...)\n        );\n        \n        std::future<returnType> res = task->get_future();\n        {\n            std::unique_lock<std::mutex> lock(queueMutex);\n\n            // Workers are indexed by modulo to ensure tasks are distributed across queues\n            taskQueues[taskCount++ % taskQueues.size()].emplace([task]() { (*task)(); });\n        }\n        condition.notify_one();\n        return res;\n    }\n\n    ~ThreadPool() {\n        {\n            std::unique_lock<std::mutex> lock(queueMutex);\n            stop = true;\n        }\n        condition.notify_all();\n        for(std::thread &worker: workers)\n            worker.join();\n    }\n\nprivate:\n    // Stealing tasks from other queues\n    bool tryStealTask(size_t currentIndex, std::function<void()>& task) {\n        for (size_t i = 0; i < taskQueues.size(); ++i) {\n            if (i == currentIndex || taskQueues[i].empty()) {\n                continue;\n            }\n\n            task = std::move(taskQueues[i].front());\n            taskQueues[i].pop();\n            return true;\n        }\n        return false;\n    }\n\n    std::vector<std::thread> workers; // Worker threads\n    std::vector<std::queue<std::function<void()>>> taskQueues; // Task queues\n    std::mutex queueMutex; // Synchronization\n    std::condition_variable condition; // Condition variable\n    size_t taskCount{0}; // Task count for indexing\n    bool stop; // Stop flag\n};\n\nint main() {\n    ThreadPool pool(4);\n    auto result = pool.enqueue([](int answer) { return answer; }, 42);\n    std::cout << \"The answer is: \" << result.get() << std::endl;\n    return 0;\n}\n```\n",
    "label": 1
  },
  {
    "codes": "def _do_retrieve(url, fname):\n        \"\"\"Retrieve given url to target filepath fname.\"\"\"\n        folder = os.path.dirname(fname)\n        if not os.path.exists(folder):\n            os.makedirs(folder)\n            print(\"{}/ created.\".format(folder))\n        if not os.path.exists(fname):\n            with open(fname, 'wb') as fout:\n                print(\"retrieving {}.\".format(url))\n                resp = urlopen(url)\n                fout.write(resp.read())\n            print(\"{} saved.\".format(fname))\n        else:\n            print(\"re-using artifact {}\".format(fname))\n        return fname",
    "label": 0
  },
  {
    "codes": "func (c *Glue) GetDevEndpointRequest(input *GetDevEndpointInput) (req *request.Request, output *GetDevEndpointOutput) {\n\top := &request.Operation{\n\t\tName:       opGetDevEndpoint,\n\t\tHTTPMethod: \"POST\",\n\t\tHTTPPath:   \"/\",\n\t}\n\n\tif input == nil {\n\t\tinput = &GetDevEndpointInput{}\n\t}\n\n\toutput = &GetDevEndpointOutput{}\n\treq = c.newRequest(op, input, output)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization\n\n# Sample text data\ntexts = [\n    \"TensorFlow is an end-to-end open-source platform for machine learning.\",\n    \"It has a comprehensive ecosystem of tools, libraries, and resources.\",\n    \"Keras is an API designed for human beings, not machines.\"\n]\n\n# Parameters for the TextVectorization layer\nmax_vocab_size = 10000  # Maximum number of words in the vocabulary\nmax_sequence_length = 100  # Output sequence length\n\n# Create a TextVectorization layer\nvectorizer = TextVectorization(\n    max_tokens=max_vocab_size,  # Limit on the number of tokens\n    output_sequence_length=max_sequence_length,  # Output shape of the vectorized sequences\n    output_mode='int'  # Output integer encoded sequences\n)\n\n# Adapt the vectorizer to the text data\nvectorizer.adapt(texts)\n\n# Create a TensorFlow data pipeline\ntext_dataset = tf.data.Dataset.from_tensor_slices(texts)\n\n# Function to map text data to its vectorized form\ndef vectorize_text(text):\n    return vectorizer(text)\n\n# Apply preprocessing to the text dataset\nvectorized_text_dataset = text_dataset.map(vectorize_text)\n\n# Print out vectorized text data\nfor vectorized_text in vectorized_text_dataset:\n    print(vectorized_text.numpy())\n\n# Example model using the TextVectorization layer\nmodel = tf.keras.Sequential([\n    tf.keras.Input(shape=(1,), dtype=tf.string),  # Input layer to accept raw text\n    vectorizer,  # Text vectorization layer\n    tf.keras.layers.Embedding(input_dim=max_vocab_size, output_dim=16),  # Embedding layer\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer (suitable for binary classification)\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import required libraries from Hugging Face Transformers\nfrom transformers import pipeline\n\n# Create a summarization pipeline using a pre-trained model\nsummarizer = pipeline(\"summarization\")\n\n# Example usage of the summarization pipeline\ntext = \"\"\"\nArtificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence\ndisplayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\":\nany device that perceives its environment and takes actions that maximize its chance of successfully achieving\nits goals. Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers)\nthat mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\".\n\"\"\"\n\n# Perform summarization on the provided text\nsummary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n\n# Output the summary\nprint(summary[0]['summary_text'])\n```\n",
    "label": 1
  },
  {
    "codes": "def _get_webapi_requests(self):\n        \"\"\"Update headers of webapi for Requests.\"\"\"\n\n        headers = {\n            'Accept':\n            '*/*',\n            'Accept-Language':\n            'zh-CN,zh;q=0.8,gl;q=0.6,zh-TW;q=0.4',\n            'Connection':\n            'keep-alive',\n            'Content-Type':\n            'application/x-www-form-urlencoded',\n            'Referer':\n            'http://music.163.com',\n            'Host':\n            'music.163.com',\n            'User-Agent':\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.152 Safari/537.36'\n        }\n        NCloudBot.req.headers.update(headers)\n        return NCloudBot.req",
    "label": 0
  },
  {
    "codes": "func (c *Glue) DeleteCrawlerWithContext(ctx aws.Context, input *DeleteCrawlerInput, opts ...request.Option) (*DeleteCrawlerOutput, error) {\n\treq, out := c.DeleteCrawlerRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<OperationStatus> deletePatternAsync(UUID appId, String versionId, UUID patternId, final ServiceCallback<OperationStatus> serviceCallback) {\n        return ServiceFuture.fromResponse(deletePatternWithServiceResponseAsync(appId, versionId, patternId), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "def register_header(self, name):\n        \"\"\"\n        Register support for the SHIM header with the given `name`.\n\n        If the header has already been registered as supported,\n        :class:`ValueError` is raised.\n        \"\"\"\n\n        self._node.register_feature(\n            \"#\".join([namespaces.xep0131_shim, name])\n        )",
    "label": 0
  },
  {
    "codes": "```java\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.springframework.stereotype.Component;\n\nimport java.util.concurrent.atomic.AtomicInteger;\n\n@Aspect\n@Component\npublic class CircuitBreakerAspect {\n\n    private static final int THRESHOLD = 3;\n    private static final int TIMEOUT = 5000; // milliseconds\n\n    private AtomicInteger failureCount = new AtomicInteger(0);\n    private long lastFailureTime = 0;\n\n    @Around(\"@annotation(CircuitBreaker)\")\n    public Object circuitBreaker(ProceedingJoinPoint joinPoint) throws Throwable {\n        if (isCircuitOpen()) {\n            throw new RuntimeException(\"Circuit is open. Method is not allowed to execute.\");\n        }\n\n        try {\n            Object result = joinPoint.proceed();\n            resetCircuit();\n            return result;\n        } catch (Throwable throwable) {\n            recordFailure();\n            throw throwable;\n        }\n    }\n\n    private boolean isCircuitOpen() {\n        return failureCount.get() >= THRESHOLD && \n               (System.currentTimeMillis() - lastFailureTime) < TIMEOUT;\n    }\n\n    private void recordFailure() {\n        failureCount.incrementAndGet();\n        lastFailureTime = System.currentTimeMillis();\n    }\n\n    private void resetCircuit() {\n        failureCount.set(0);\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```powershell\n# Define the directory containing the files\n$directory = \"C:\\Path\\To\\Your\\Files\"\n\n# Define the regex pattern to match and the replacement string\n$regexPattern = \"OldPattern\"\n$replacement = \"NewPattern\"\n\n# Get all files in the directory\n$files = Get-ChildItem -Path $directory\n\n# Loop through each file and rename if it matches the regex pattern\nforeach ($file in $files) {\n    if ($file.Name -match $regexPattern) {\n        $newName = $file.Name -replace $regexPattern, $replacement\n        Rename-Item -Path $file.FullName -NewName $newName\n        Write-Host \"Renamed '$($file.Name)' to '$newName'\"\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport asyncio\nimport logging\nfrom typing import Dict, Optional\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Connection pool to manage active connections\nconnection_pool: Dict[str, asyncio.StreamWriter] = {}\n\nasync def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter):\n    \"\"\"Handle incoming client connections.\"\"\"\n    client_addr = writer.get_extra_info('peername')\n    logger.info(f\"New connection from {client_addr}\")\n\n    # Check if a connection to the target server already exists in the pool\n    target_writer: Optional[asyncio.StreamWriter] = connection_pool.get(client_addr[0])\n\n    if not target_writer:\n        # Establish a new connection to the target server\n        target_reader, target_writer = await asyncio.open_connection('target_server_ip', 8080)\n        connection_pool[client_addr[0]] = target_writer\n        logger.info(f\"Created new connection to target server for {client_addr}\")\n\n    async def forward(src: asyncio.StreamReader, dst: asyncio.StreamWriter):\n        \"\"\"Forward data from source to destination.\"\"\"\n        try:\n            while True:\n                data = await src.read(4096)\n                if not data:\n                    break\n                dst.write(data)\n                await dst.drain()\n        except Exception as e:\n            logger.error(f\"Error forwarding data: {e}\")\n        finally:\n            dst.close()\n            await dst.wait_closed()\n\n    # Forward data between client and target server\n    await asyncio.gather(\n        forward(reader, target_writer),\n        forward(target_reader, writer)\n    )\n\n    # Clean up\n    writer.close()\n    await writer.wait_closed()\n    del connection_pool[client_addr[0]]\n    logger.info(f\"Connection closed for {client_addr}\")\n\nasync def start_proxy_server():\n    \"\"\"Start the TCP proxy server.\"\"\"\n    server = await asyncio.start_server(handle_client, '0.0.0.0', 8888)\n    logger.info(\"Proxy server started on 0.0.0.0:8888\")\n\n    async with server:\n        await server.serve_forever()\n\nif __name__ == \"__main__\":\n    asyncio.run(start_proxy_server())\n```",
    "label": 1
  },
  {
    "codes": "```kotlin\nclass Graph(private val vertices: Int) {\n    private val adjacencyList: Array<MutableList<Int>> = Array(vertices) { mutableListOf() }\n\n    fun addEdge(source: Int, destination: Int) {\n        adjacencyList[source].add(destination)\n    }\n\n    fun hasCycle(): Boolean {\n        val visited = BooleanArray(vertices)\n        val recStack = BooleanArray(vertices)\n\n        for (i in 0 until vertices) {\n            if (dfs(i, visited, recStack)) {\n                return true\n            }\n        }\n        return false\n    }\n\n    private fun dfs(vertex: Int, visited: BooleanArray, recStack: BooleanArray): Boolean {\n        if (recStack[vertex]) {\n            // Found a back edge, thus a cycle\n            return true\n        }\n        if (visited[vertex]) {\n            // Already visited this node; no cycle found in this path\n            return false\n        }\n        visited[vertex] = true\n        recStack[vertex] = true\n\n        for (neighbor in adjacencyList[vertex]) {\n            if (dfs(neighbor, visited, recStack)) {\n                return true\n            }\n        }\n        \n        recStack[vertex] = false // Remove from recursion stack\n        return false\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"net\"\n\t\"time\"\n)\n\nfunc main() {\n\t// Seed random number generator\n\trand.Seed(time.Now().UnixNano())\n\n\t// Specify local address for incoming connections\n\tlocalAddr := \":8080\"\n\n\t// Listen for incoming TCP connections\n\tlistener, err := net.Listen(\"tcp\", localAddr)\n\tif err != nil {\n\t\tfmt.Println(\"Error starting TCP server:\", err)\n\t\treturn\n\t}\n\tdefer listener.Close()\n\tfmt.Println(\"Server listening on\", localAddr)\n\n\tfor {\n\t\t// Accept an incoming connection\n\t\tconn, err := listener.Accept()\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error accepting connection:\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Handle the connection in a new goroutine\n\t\tgo handleConnection(conn)\n\t}\n}\n\n// Simulate packet loss with a given loss probability\nfunc simulatePacketLoss() bool {\n\tconst lossProbability = 0.1 // 10% chance of packet loss\n\treturn rand.Float64() < lossProbability\n}\n\n// Handle incoming TCP connection\nfunc handleConnection(conn net.Conn) {\n\tdefer conn.Close()\n\tbuffer := make([]byte, 1024)\n\n\tfor {\n\t\t// Read data from the connection\n\t\tn, err := conn.Read(buffer)\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error reading from connection:\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Simulate packet loss\n\t\tif simulatePacketLoss() {\n\t\t\tfmt.Println(\"Packet loss simulated, data not sent back\")\n\t\t\tcontinue\n\t\t}\n\n\t\t// Echo the data back to the client\n\t\t_, err = conn.Write(buffer[:n])\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error writing to connection:\", err)\n\t\t\treturn\n\t\t}\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "```rust\n// Cargo.toml\n[package]\nname = \"zip_password_protected\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\nzip = \"0.5\"\ncrypto = \"0.2.36\"\n\n// lib.rs\nextern crate zip;\nextern crate crypto;\n\nuse std::fs::File;\nuse std::io::{Read, Write, Seek};\nuse zip::write::FileOptions;\nuse zip::CompressionMethod;\nuse crypto::sha2::Sha256;\nuse crypto::digest::Digest;\nuse crypto::symmetriccipher::SynchronousStreamCipher;\nuse crypto::aes::KeySize::KeySize256;\nuse crypto::aes::ctr::AesCtr;\nuse crypto::buffer::{RefReadBuffer, RefWriteBuffer};\n\npub struct ZipPasswordProtected {\n    password: String\n}\n\nimpl ZipPasswordProtected {\n    pub fn new(password: &str) -> Self {\n        ZipPasswordProtected {\n            password: password.to_owned()\n        }\n    }\n\n    fn encrypt(&self, data: &[u8]) -> Vec<u8> {\n        let mut hasher = Sha256::new();\n        hasher.input_str(&self.password);\n        let key = hasher.result_str().into_bytes();\n        \n        let nonce: [u8; 16] = [0; 16];\n        let mut encryptor = AesCtr::new(KeySize256, &key, &nonce);\n\n        let mut read_buffer = RefReadBuffer::new(data);\n        let mut buffer = [0; 4096];\n        let mut write_buffer = RefWriteBuffer::new(&mut buffer);\n\n        let mut encrypted_data = Vec::new();\n        \n        loop {\n            let result = encryptor.encrypt(&mut read_buffer, &mut write_buffer, true).unwrap();\n            encrypted_data.extend(write_buffer.take_read_buffer().take_remaining().iter().copied());\n            if let crypto::symmetriccipher::BufferResult::BufferUnderflow = result {\n                break;\n            }\n        }\n        encrypted_data\n    }\n\n    fn decrypt(&self, data: &[u8]) -> Vec<u8> {\n        let mut hasher = Sha256::new();\n        hasher.input_str(&self.password);\n        let key = hasher.result_str().into_bytes();\n        \n        let nonce: [u8; 16] = [0; 16];\n        let mut decryptor = AesCtr::new(KeySize256, &key, &nonce);\n\n        let mut read_buffer = RefReadBuffer::new(data);\n        let mut buffer = [0; 4096];\n        let mut write_buffer = RefWriteBuffer::new(&mut buffer);\n\n        let mut decrypted_data = Vec::new();\n\n        loop {\n            let result = decryptor.decrypt(&mut read_buffer, &mut write_buffer, true).unwrap();\n            decrypted_data.extend(write_buffer.take_read_buffer().take_remaining().iter().copied());\n            if let crypto::symmetriccipher::BufferResult::BufferUnderflow = result {\n                break;\n            }\n        }\n        decrypted_data\n    }\n\n    pub fn create_zip(&self, files: Vec<(&str, &str)>, zip_name: &str) -> zip::result::ZipResult<()> {\n        let path = std::path::Path::new(zip_name);\n        let file = File::create(&path)?;\n\n        let mut zip = zip::ZipWriter::new(file);\n        let options = FileOptions::default().compression_method(CompressionMethod::Stored);\n\n        for (name, path) in files.into_iter() {\n            zip.start_file(name, options)?;\n            let mut f = File::open(path)?;\n            let mut buffer = Vec::new();\n            f.read_to_end(&mut buffer)?;\n            let data = self.encrypt(&buffer);\n            zip.write_all(&data)?;\n        }\n\n        zip.finish()?;\n        Ok(())\n    }\n\n    pub fn extract_zip(&self, zip_name: &str, destination: &str) -> zip::result::ZipResult<()> {\n        let path = std::path::Path::new(zip_name);\n        let file = File::open(&path)?;\n        let mut archive = zip::ZipArchive::new(file)?;\n\n        for i in 0..archive.len() {\n            let mut file = archive.by_index(i)?;\n            let file_path = std::path::Path::new(destination).join(file.name());\n\n            if file.is_dir() {\n                std::fs::create_dir_all(&file_path)?;\n            } else {\n                if let Some(p) = file_path.parent() {\n                    if !p.exists() {\n                        std::fs::create_dir_all(&p)?;\n                    }\n                }\n\n                let mut buffer = Vec::new();\n                file.read_to_end(&mut buffer)?;\n                let data = self.decrypt(&buffer);\n                let mut outfile = File::create(&file_path)?;\n                outfile.write_all(&data)?;\n            }\n        }\n\n        Ok(())\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport gym\nimport numpy as np\nimport random\n\n# Initialize the environment\nenv = gym.make('CartPole-v1')\n\n# Define the Q-learning parameters\nalpha = 0.1  # Learning rate\ngamma = 0.99  # Discount factor\nepsilon = 1.0  # Exploration rate\nepsilon_decay = 0.995  # Decay rate for exploration\nepsilon_min = 0.01  # Minimum exploration rate\n\n# Initialize Q-table with zeros\nstate_space_size = env.observation_space.shape[0]\naction_space_size = env.action_space.n\nq_table = np.zeros((state_space_size, action_space_size))\n\n# Training parameters\nnum_episodes = 1000\nmax_steps_per_episode = 200\n\n# Training loop\nfor episode in range(num_episodes):\n    state = env.reset()\n    state = np.reshape(state, [1, state_space_size])\n    \n    for step in range(max_steps_per_episode):\n        # Exploration-exploitation trade-off\n        if random.uniform(0, 1) < epsilon:\n            action = env.action_space.sample()  # Explore action space\n        else:\n            action = np.argmax(q_table[state])  # Exploit learned values\n        \n        # Take action and observe the result\n        next_state, reward, done, _ = env.step(action)\n        next_state = np.reshape(next_state, [1, state_space_size])\n        \n        # Update Q-value for the state-action pair\n        old_value = q_table[state, action]\n        next_max = np.max(q_table[next_state])\n        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n        q_table[state, action] = new_value\n        \n        # Move to the next state\n        state = next_state\n        \n        if done:\n            break\n    \n    # Decay epsilon\n    if epsilon > epsilon_min:\n        epsilon *= epsilon_decay\n\n# Close the environment\nenv.close()\n```\n\n```python\n# Test the trained agent\nenv = gym.make('CartPole-v1')\nstate = env.reset()\nstate = np.reshape(state, [1, state_space_size])\n\nfor step in range(max_steps_per_episode):\n    action = np.argmax(q_table[state])\n    next_state, reward, done, _ = env.step(action)\n    next_state = np.reshape(next_state, [1, state_space_size])\n    env.render()\n    state = next_state\n    \n    if done:\n        break\n\nenv.close()\n```\n\n```python\n# Comments:\n# - The agent uses Q-learning to learn the optimal policy.\n# - The Q-table is updated using the Bellman equation.\n# - Exploration is controlled by the epsilon-greedy strategy.\n# - The agent is tested after training to evaluate its performance.\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glue) DeleteDevEndpointWithContext(ctx aws.Context, input *DeleteDevEndpointInput, opts ...request.Option) (*DeleteDevEndpointOutput, error) {\n\treq, out := c.DeleteDevEndpointRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "function merge(acc, item) {\n  if (!item) {\n    return acc;\n  }\n\n  return deepmerge(acc, item, {\n    clone: false, // No need to clone deep, it's way faster.\n  });\n}",
    "label": 0
  },
  {
    "codes": "```kotlin\nfun hasCycle(graph: Map<Int, List<Int>>): Boolean {\n    val visited = mutableSetOf<Int>() // Track visited nodes\n    val recStack = mutableSetOf<Int>() // Track nodes in the current recursion stack\n\n    fun dfs(node: Int): Boolean {\n        if (node in recStack) return true // Cycle detected\n        if (node in visited) return false // Already visited, no cycle\n\n        visited.add(node)\n        recStack.add(node)\n\n        for (neighbor in graph[node] ?: emptyList()) {\n            if (dfs(neighbor)) return true\n        }\n\n        recStack.remove(node) // Backtrack\n        return false\n    }\n\n    for (node in graph.keys) {\n        if (dfs(node)) return true\n    }\n\n    return false // No cycle found\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nconst express = require('express');\nconst bodyParser = require('body-parser');\n\nconst app = express();\napp.use(bodyParser.json());\n\n// In-memory user data for testing (in a real application, use a database)\nconst users = [\n  { id: 1, username: 'user1', password: 'pass1' },\n  { id: 2, username: 'user2', password: 'pass2' }\n];\n\n// Login endpoint\napp.post('/login', (req, res) => {\n  const { username, password } = req.body;\n\n  // Find the user by username\n  const user = users.find(u => u.username === username);\n\n  // User not found\n  if (!user) {\n    return res.status(404).send('User not found');\n  }\n\n  // Invalid password\n  if (user.password !== password) {\n    return res.status(401).send('Invalid password');\n  }\n\n  // Successful login\n  res.status(200).send('Login successful');\n});\n\nconst PORT = process.env.PORT || 3000;\napp.listen(PORT, () => {\n  console.log(`Server is running on port ${PORT}`);\n});\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"testing\"\n\n\t\"github.com/tidwall/gjson\"\n)\n\n// benchmarkData is the JSON data used in the benchmark.\nvar benchmarkData = `{\"name\":\"John\",\"age\":30,\"city\":\"New York\",\"interests\":[\"music\",\"movies\",\"sports\"]}`\n\n// Benchmark for the encoding/json library.\nfunc BenchmarkEncodingJSON(b *testing.B) {\n\tfor i := 0; i < b.N; i++ {\n\t\tvar result map[string]interface{}\n\t\tdecoder := json.NewDecoder(bytes.NewReader([]byte(benchmarkData)))\n\t\tif err := decoder.Decode(&result); err != nil {\n\t\t\tb.Errorf(\"Error decoding JSON: %v\", err)\n\t\t}\n\t}\n}\n\n// Benchmark for the tidwall/gjson library.\nfunc BenchmarkGJSON(b *testing.B) {\n\tfor i := 0; i < b.N; i++ {\n\t\tresult := gjson.ParseBytes([]byte(benchmarkData))\n\t\tif !result.Exists() {\n\t\t\tb.Error(\"Error parsing JSON with gjson\")\n\t\t}\n\t}\n}\n\n// The main function is required to run the benchmarks.\nfunc main() {\n\tvar benchmarks = []testing.InternalBenchmark{\n\t\t{Name: \"BenchmarkEncodingJSON\", F: BenchmarkEncodingJSON},\n\t\t{Name: \"BenchmarkGJSON\", F: BenchmarkGJSON},\n\t}\n\n\ttesting.Main(func(_, _ string, _ testing.InternalTest) (bool, error) { return false, nil },\n\t\tbenchmarks, nil, nil)\n}\n```\n\n- The code compares JSON parsing performance using Go's standard `encoding/json` package and the `gjson` package.\n- It defines benchmark functions for both libraries, using `testing.B` to measure execution time.\n- The benchmarks run conversions of a JSON string into a `map[string]interface{}` and parse it with `gjson`.",
    "label": 1
  },
  {
    "codes": "func (s *TableVersion) SetVersionId(v string) *TableVersion {\n\ts.VersionId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```rust\nuse bevy::prelude::*;\nuse bevy::input::mouse::{MouseButtonInput, MouseMotion};\nuse bevy::render::camera::Camera;\nuse bevy::ecs::query::WorldQuery;\n\n#[derive(Component)]\nstruct Selectable;\n\n#[derive(Component)]\nstruct Selected;\n\n#[derive(Default)]\nstruct SelectionBox {\n    start: Option<Vec2>,\n    end: Option<Vec2>,\n}\n\nfn main() {\n    App::build()\n        .add_plugins(DefaultPlugins)\n        .add_startup_system(setup.system())\n        .add_system(selection_start.system())\n        .add_system(selection_end.system())\n        .add_system(selection_update.system())\n        .run();\n}\n\n// Sets up the camera and some example entities\nfn setup(mut commands: Commands) {\n    // Camera\n    commands.spawn_bundle(OrthographicCameraBundle::new_2d());\n    \n    // Example entities\n    commands.spawn_bundle(SpriteBundle::default()).insert(Selectable);\n    commands.spawn_bundle(SpriteBundle::default()).insert(Selectable);\n}\n\n// Handles the start of the selection rectangle\nfn selection_start(\n    mut mouse_button_input_events: EventReader<MouseButtonInput>,\n    mut selection_box: ResMut<SelectionBox>,\n) {\n    for event in mouse_button_input_events.iter() {\n        if event.button == MouseButton::Left && event.state.is_pressed() {\n            selection_box.start = Some(Vec2::new(event.position.x, event.position.y));\n        }\n    }\n}\n\n// Handles the updating of the selection rectangle\nfn selection_update(\n    mut mouse_motion_events: EventReader<MouseMotion>,\n    mut selection_box: ResMut<SelectionBox>,\n) {\n    if selection_box.start.is_some() {\n        for event in mouse_motion_events.iter() {\n            if let Some(start) = selection_box.start {\n                selection_box.end = Some(start + event.delta);\n            }\n        }\n    }\n}\n\n// Handles the end of the selection rectangle and selects units\nfn selection_end(\n    mut commands: Commands,\n    mut mouse_button_input_events: EventReader<MouseButtonInput>,\n    mut query: Query<(&Selectable, &mut Transform, Entity)>,\n    selection_box: Res<SelectionBox>,\n) {\n    for event in mouse_button_input_events.iter() {\n        if event.button == MouseButton::Left && event.state.is_released() {\n            if let (Some(start), Some(end)) = (selection_box.start, selection_box.end) {\n                let min_x = start.x.min(end.x);\n                let max_x = start.x.max(end.x);\n                let min_y = start.y.min(end.y);\n                let max_y = start.y.max(end.y);\n\n                for (_, transform, entity) in query.iter_mut() {\n                    let position = transform.translation.truncate();\n                    if position.x >= min_x && position.x <= max_x && position.y >= min_y && position.y <= max_y {\n                        commands.entity(entity).insert(Selected);\n                    } else {\n                        commands.entity(entity).remove::<Selected>();\n                    }\n                }\n            }\n            selection_box.start = None;\n            selection_box.end = None;\n        }\n    }\n}\n```\n\nThis code sets up a basic unit selection system for a real-time strategy game using the Bevy engine. It involves starting and stopping selection rectangle draws and applies selection to instances of the `Selectable` component within the rectangle's bounds.",
    "label": 1
  },
  {
    "codes": "func (s *BatchDeletePartitionInput) SetTableName(v string) *BatchDeletePartitionInput {\n\ts.TableName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function parse_rpr(rpr, intro, outro) {\n\t\tvar font = {}, cp = 65001, align = \"\";\n\t\tvar pass = false;\n\t\tvar m = rpr.match(tagregex), i = 0;\n\t\tif(m) for(;i!=m.length; ++i) {\n\t\t\tvar y = parsexmltag(m[i]);\n\t\t\tswitch(y[0].replace(/\\w*:/g,\"\")) {\n\t\t\t\t/* 18.8.12 condense CT_BooleanProperty */\n\t\t\t\t/* ** not required . */\n\t\t\t\tcase '<condense': break;\n\t\t\t\t/* 18.8.17 extend CT_BooleanProperty */\n\t\t\t\t/* ** not required . */\n\t\t\t\tcase '<extend': break;\n\t\t\t\t/* 18.8.36 shadow CT_BooleanProperty */\n\t\t\t\t/* ** not required . */\n\t\t\t\tcase '<shadow':\n\t\t\t\t\tif(!y.val) break;\n\t\t\t\t\t/* falls through */\n\t\t\t\tcase '<shadow>':\n\t\t\t\tcase '<shadow/>': font.shadow = 1; break;\n\t\t\t\tcase '</shadow>': break;\n\n\t\t\t\t/* 18.4.1 charset CT_IntProperty TODO */\n\t\t\t\tcase '<charset':\n\t\t\t\t\tif(y.val == '1') break;\n\t\t\t\t\tcp = CS2CP[parseInt(y.val, 10)];\n\t\t\t\t\tbreak;\n\n\t\t\t\t/* 18.4.2 outline CT_BooleanProperty TODO */\n\t\t\t\tcase '<outline':\n\t\t\t\t\tif(!y.val) break;\n\t\t\t\t\t/* falls through */\n\t\t\t\tcase '<outline>':\n\t\t\t\tcase '<outline/>': font.outline = 1; break;\n\t\t\t\tcase '</outline>': break;\n\n\t\t\t\t/* 18.4.5 rFont CT_FontName */\n\t\t\t\tcase '<rFont': font.name = y.val; break;\n\n\t\t\t\t/* 18.4.11 sz CT_FontSize */\n\t\t\t\tcase '<sz': font.sz = y.val; break;\n\n\t\t\t\t/* 18.4.10 strike CT_BooleanProperty */\n\t\t\t\tcase '<strike':\n\t\t\t\t\tif(!y.val) break;\n\t\t\t\t\t/* falls through */\n\t\t\t\tcase '<strike>':\n\t\t\t\tcase '<strike/>': font.strike = 1; break;\n\t\t\t\tcase '</strike>': break;\n\n\t\t\t\t/* 18.4.13 u CT_UnderlineProperty */\n\t\t\t\tcase '<u':\n\t\t\t\t\tif(!y.val) break;\n\t\t\t\t\tswitch(y.val) {\n\t\t\t\t\t\tcase 'double': font.uval = \"double\"; break;\n\t\t\t\t\t\tcase 'singleAccounting': font.uval = \"single-accounting\"; break;\n\t\t\t\t\t\tcase 'doubleAccounting': font.uval = \"double-accounting\"; break;\n\t\t\t\t\t}\n\t\t\t\t\t/* falls through */\n\t\t\t\tcase '<u>':\n\t\t\t\tcase '<u/>': font.u = 1; break;\n\t\t\t\tcase '</u>': break;\n\n\t\t\t\t/* 18.8.2 b */\n\t\t\t\tcase '<b':\n\t\t\t\t\tif(y.val == '0') break;\n\t\t\t\t\t/* falls through */\n\t\t\t\tcase '<b>':\n\t\t\t\tcase '<b/>': font.b = 1; break;\n\t\t\t\tcase '</b>': break;\n\n\t\t\t\t/* 18.8.26 i */\n\t\t\t\tcase '<i':\n\t\t\t\t\tif(y.val == '0') break;\n\t\t\t\t\t/* falls through */\n\t\t\t\tcase '<i>':\n\t\t\t\tcase '<i/>': font.i = 1; break;\n\t\t\t\tcase '</i>': break;\n\n\t\t\t\t/* 18.3.1.15 color CT_Color TODO: tint, theme, auto, indexed */\n\t\t\t\tcase '<color':\n\t\t\t\t\tif(y.rgb) font.color = y.rgb.slice(2,8);\n\t\t\t\t\tbreak;\n\n\t\t\t\t/* 18.8.18 family ST_FontFamily */\n\t\t\t\tcase '<family': font.family = y.val; break;\n\n\t\t\t\t/* 18.4.14 vertAlign CT_VerticalAlignFontProperty TODO */\n\t\t\t\tcase '<vertAlign': align = y.val; break;\n\n\t\t\t\t/* 18.8.35 scheme CT_FontScheme TODO */\n\t\t\t\tcase '<scheme': break;\n\n\t\t\t\t/* 18.2.10 extLst CT_ExtensionList ? */\n\t\t\t\tcase '<extLst': case '<extLst>': case '</extLst>': break;\n\t\t\t\tcase '<ext': pass = true; break;\n\t\t\t\tcase '</ext>': pass = false; break;\n\t\t\t\tdefault:\n\t\t\t\t\tif(y[0].charCodeAt(1) !== 47 && !pass) throw new Error('Unrecognized rich format ' + y[0]);\n\t\t\t}\n\t\t}\n\t\tvar style = [];\n\n\t\tif(font.u) style.push(\"text-decoration: underline;\");\n\t\tif(font.uval) style.push(\"text-underline-style:\" + font.uval + \";\");\n\t\tif(font.sz) style.push(\"font-size:\" + font.sz + \"pt;\");\n\t\tif(font.outline) style.push(\"text-effect: outline;\");\n\t\tif(font.shadow) style.push(\"text-shadow: auto;\");\n\t\tintro.push('<span style=\"' + style.join(\"\") + '\">');\n\n\t\tif(font.b) { intro.push(\"<b>\"); outro.push(\"</b>\"); }\n\t\tif(font.i) { intro.push(\"<i>\"); outro.push(\"</i>\"); }\n\t\tif(font.strike) { intro.push(\"<s>\"); outro.push(\"</s>\"); }\n\n\t\tif(align == \"superscript\") align = \"sup\";\n\t\telse if(align == \"subscript\") align = \"sub\";\n\t\tif(align != \"\") { intro.push(\"<\" + align + \">\"); outro.push(\"</\" + align + \">\"); }\n\n\t\toutro.push(\"</span>\");\n\t\treturn cp;\n\t}",
    "label": 0
  },
  {
    "codes": "```python\nimport functools\nimport time\n\ndef measure_time_ns(func):\n    \"\"\"Decorator to measure the execution time of a function in nanoseconds.\"\"\"\n    @functools.wraps(func)\n    def wrapper_measure_time_ns(*args, **kwargs):\n        start_time = time.perf_counter_ns()  # Record the start time in nanoseconds\n        result = func(*args, **kwargs)       # Execute the function\n        end_time = time.perf_counter_ns()    # Record the end time in nanoseconds\n        execution_time = end_time - start_time  # Calculate the execution time\n        print(f\"Function {func.__name__} executed in {execution_time:,} ns\")  # Output the execution time\n        return result  # Return the result of the function\n    return wrapper_measure_time_ns\n```\n",
    "label": 1
  },
  {
    "codes": "```java\n// Import necessary libraries\nimport static org.mockito.Mockito.*;\nimport org.junit.jupiter.api.Test;\nimport javax.sql.DataSource;\nimport java.sql.Connection;\nimport java.sql.SQLException;\n\npublic class DatabaseServiceTest {\n\n    @Test\n    public void testTransactionRollback() throws SQLException {\n        // Mock the DataSource and Connection\n        DataSource dataSource = mock(DataSource.class);\n        Connection connection = mock(Connection.class);\n\n        // Stub the DataSource to return the mocked Connection\n        when(dataSource.getConnection()).thenReturn(connection);\n\n        // Create an instance of the service under test\n        DatabaseService databaseService = new DatabaseService(dataSource);\n\n        // Simulate a SQLException to trigger a rollback\n        doThrow(new SQLException(\"Database error\")).when(connection).commit();\n\n        // Perform the operation that should trigger a rollback\n        try {\n            databaseService.performTransaction();\n        } catch (SQLException e) {\n            // Expected exception\n        }\n\n        // Verify that rollback was called\n        verify(connection, times(1)).rollback();\n    }\n}\n\n// DatabaseService class for demonstration\nclass DatabaseService {\n    private DataSource dataSource;\n\n    public DatabaseService(DataSource dataSource) {\n        this.dataSource = dataSource;\n    }\n\n    public void performTransaction() throws SQLException {\n        Connection connection = dataSource.getConnection();\n        try {\n            connection.setAutoCommit(false);\n            // Simulate database operations\n            connection.commit();\n        } catch (SQLException e) {\n            connection.rollback();\n            throw e;\n        } finally {\n            connection.close();\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<List<ResourceMetricDefinitionInner>> listWorkerPoolInstanceMetricDefinitionsAsync(final String resourceGroupName, final String name, final String workerPoolName, final String instance, final ListOperationCallback<ResourceMetricDefinitionInner> serviceCallback) {\n        return AzureServiceFuture.fromPageResponse(\n            listWorkerPoolInstanceMetricDefinitionsSinglePageAsync(resourceGroupName, name, workerPoolName, instance),\n            new Func1<String, Observable<ServiceResponse<Page<ResourceMetricDefinitionInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<ResourceMetricDefinitionInner>>> call(String nextPageLink) {\n                    return listWorkerPoolInstanceMetricDefinitionsNextSinglePageAsync(nextPageLink);\n                }\n            },\n            serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```typescript\nfunction Retry(maxRetries: number = 3, delay: number = 1000) {\n    return function (target: any, propertyKey: string, descriptor: PropertyDescriptor) {\n        const originalMethod = descriptor.value;\n\n        descriptor.value = async function (...args: any[]) {\n            let retries = 0;\n\n            while (retries < maxRetries) {\n                try {\n                    const result = await originalMethod.apply(this, args);\n                    return result; // Success, return the result\n                } catch (error) {\n                    retries++;\n                    if (retries >= maxRetries) {\n                        throw error; // Max retries reached, rethrow the error\n                    }\n                    await new Promise(resolve => setTimeout(resolve, delay)); // Wait before retrying\n                }\n            }\n        };\n\n        return descriptor;\n    };\n}\n\n// Example usage:\nclass MyService {\n    @Retry(5, 2000) // Retry up to 5 times with a 2-second delay\n    async fetchData() {\n        // Simulate a network request that may fail\n        const success = Math.random() > 0.5;\n        if (!success) {\n            throw new Error('Failed to fetch data');\n        }\n        return 'Data fetched successfully';\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\n// Import necessary libraries\nconst xss = require('xss');\n\n// Middleware to sanitize user input\nconst sanitizeInput = (req, res, next) => {\n    // Sanitize request body\n    if (req.body) {\n        for (let key in req.body) {\n            if (typeof req.body[key] === 'string') {\n                req.body[key] = xss(req.body[key]);\n            }\n        }\n    }\n\n    // Sanitize request query parameters\n    if (req.query) {\n        for (let key in req.query) {\n            if (typeof req.query[key] === 'string') {\n                req.query[key] = xss(req.query[key]);\n            }\n        }\n    }\n\n    // Sanitize request params\n    if (req.params) {\n        for (let key in req.params) {\n            if (typeof req.params[key] === 'string') {\n                req.params[key] = xss(req.params[key]);\n            }\n        }\n    }\n\n    // Proceed to the next middleware\n    next();\n};\n\nmodule.exports = sanitizeInput;\n```",
    "label": 1
  },
  {
    "codes": "public PagedList<WorkerPoolResourceInner> listMultiRolePoolsNext(final String nextPageLink) {\n        ServiceResponse<Page<WorkerPoolResourceInner>> response = listMultiRolePoolsNextSinglePageAsync(nextPageLink).toBlocking().single();\n        return new PagedList<WorkerPoolResourceInner>(response.body()) {\n            @Override\n            public Page<WorkerPoolResourceInner> nextPage(String nextPageLink) {\n                return listMultiRolePoolsNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "func (s *GetJobRunsInput) Validate() error {\n\tinvalidParams := request.ErrInvalidParams{Context: \"GetJobRunsInput\"}\n\tif s.JobName == nil {\n\t\tinvalidParams.Add(request.NewErrParamRequired(\"JobName\"))\n\t}\n\tif s.JobName != nil && len(*s.JobName) < 1 {\n\t\tinvalidParams.Add(request.NewErrParamMinLen(\"JobName\", 1))\n\t}\n\tif s.MaxResults != nil && *s.MaxResults < 1 {\n\t\tinvalidParams.Add(request.NewErrParamMinValue(\"MaxResults\", 1))\n\t}\n\n\tif invalidParams.Len() > 0 {\n\t\treturn invalidParams\n\t}\n\treturn nil\n}",
    "label": 0
  },
  {
    "codes": "def _try_decode_utf8_content(self, content, content_type):\n        \"\"\"Generic function to decode content.\n\n        :param object content:\n        :return:\n        \"\"\"\n        if not self._auto_decode or not content:\n            return content\n        if content_type in self._decode_cache:\n            return self._decode_cache[content_type]\n        if isinstance(content, dict):\n            content = self._try_decode_dict(content)\n        else:\n            content = try_utf8_decode(content)\n        self._decode_cache[content_type] = content\n        return content",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"testing\"\n\n\t\"github.com/json-iterator/go\"\n)\n\n// Sample JSON payload for benchmarking\nvar jsonData = []byte(`{\"name\": \"Test\", \"age\": 30, \"email\": \"test@example.com\"}`)\n\n// Struct to match the JSON data\ntype Person struct {\n\tName  string `json:\"name\"`\n\tAge   int    `json:\"age\"`\n\tEmail string `json:\"email\"`\n}\n\n// Benchmark using the standard library\nfunc BenchmarkJsonUnmarshal(b *testing.B) {\n\tfor i := 0; i < b.N; i++ {\n\t\tvar person Person\n\t\terr := json.Unmarshal(jsonData, &person)\n\t\tif err != nil {\n\t\t\tb.Error(err)\n\t\t}\n\t}\n}\n\n// Benchmark using json-iterator\nfunc BenchmarkJsonIterUnmarshal(b *testing.B) {\n\tfor i := 0; i < b.N; i++ {\n\t\tvar person Person\n\t\terr := jsoniter.Unmarshal(jsonData, &person)\n\t\tif err != nil {\n\t\t\tb.Error(err)\n\t\t}\n\t}\n}\n\n// Run the benchmarks to compare performance\n// `go test -bench=.` will run these benchmarks\n```\n",
    "label": 1
  },
  {
    "codes": "```python\nimport asyncio\nimport logging\nfrom collections import deque\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Connection pool to manage active connections\nconnection_pool = deque(maxlen=10)\n\nasync def handle_client(reader, writer):\n    \"\"\"\n    Handle incoming client connections and proxy data to the target server.\n    \"\"\"\n    client_addr = writer.get_extra_info('peername')\n    logger.info(f\"New connection from {client_addr}\")\n\n    # Get a connection from the pool or create a new one\n    if connection_pool:\n        target_reader, target_writer = connection_pool.pop()\n        logger.info(f\"Reusing connection from pool for {client_addr}\")\n    else:\n        target_reader, target_writer = await asyncio.open_connection('target.server.com', 8080)\n        logger.info(f\"Created new connection for {client_addr}\")\n\n    async def forward_data(src_reader, dst_writer):\n        \"\"\"\n        Forward data from source to destination.\n        \"\"\"\n        try:\n            while True:\n                data = await src_reader.read(4096)\n                if not data:\n                    break\n                dst_writer.write(data)\n                await dst_writer.drain()\n        except Exception as e:\n            logger.error(f\"Error forwarding data: {e}\")\n        finally:\n            dst_writer.close()\n            await dst_writer.wait_closed()\n\n    # Start forwarding data between client and target server\n    await asyncio.gather(\n        forward_data(reader, target_writer),\n        forward_data(target_reader, writer)\n    )\n\n    # Return the connection to the pool if it's still open\n    if not target_writer.is_closing():\n        connection_pool.append((target_reader, target_writer))\n        logger.info(f\"Connection returned to pool for {client_addr}\")\n\nasync def start_proxy_server():\n    \"\"\"\n    Start the TCP proxy server.\n    \"\"\"\n    server = await asyncio.start_server(handle_client, '0.0.0.0', 8888)\n    logger.info(\"Proxy server started on 0.0.0.0:8888\")\n\n    async with server:\n        await server.serve_forever()\n\nif __name__ == \"__main__\":\n    asyncio.run(start_proxy_server())\n```",
    "label": 1
  },
  {
    "codes": "function createMultipartUpload (req, res, next) {\n    // @ts-ignore The `uppy` property is added by middleware before reaching here.\n    const client = req.uppy.s3Client\n    const key = config.getKey(req, req.body.filename)\n    const { type } = req.body\n    if (typeof key !== 'string') {\n      return res.status(500).json({ error: 's3: filename returned from `getKey` must be a string' })\n    }\n    if (typeof type !== 'string') {\n      return res.status(400).json({ error: 's3: content type must be a string' })\n    }\n\n    client.createMultipartUpload({\n      Bucket: config.bucket,\n      Key: key,\n      ACL: config.acl,\n      ContentType: type,\n      Expires: ms('5 minutes') / 1000\n    }, (err, data) => {\n      if (err) {\n        next(err)\n        return\n      }\n      res.json({\n        key: data.Key,\n        uploadId: data.UploadId\n      })\n    })\n  }",
    "label": 0
  },
  {
    "codes": "def get_downbeat_steps(self):\n        \"\"\"\n        Return the indices of time steps that contain downbeats.\n\n        Returns\n        -------\n        downbeat_steps : list\n            The indices of time steps that contain downbeats.\n\n        \"\"\"\n        if self.downbeat is None:\n            return []\n        downbeat_steps = np.nonzero(self.downbeat)[0].tolist()\n        return downbeat_steps",
    "label": 0
  },
  {
    "codes": "def remove_tracks(self, track_indices):\n        \"\"\"\n        Remove tracks specified by `track_indices`.\n\n        Parameters\n        ----------\n        track_indices : list\n            The indices of the tracks to be removed.\n\n        \"\"\"\n        if isinstance(track_indices, int):\n            track_indices = [track_indices]\n        self.tracks = [track for idx, track in enumerate(self.tracks)\n                       if idx not in track_indices]",
    "label": 0
  },
  {
    "codes": "def _wait_for_connection_state(self, state=Stateful.OPEN, rpc_timeout=30):\n        \"\"\"Wait for a Connection state.\n\n        :param int state: State that we expect\n\n        :raises AMQPConnectionError: Raises if we are unable to establish\n                                     a connection to RabbitMQ.\n\n        :return:\n        \"\"\"\n        start_time = time.time()\n        while self.current_state != state:\n            self.check_for_errors()\n            if time.time() - start_time > rpc_timeout:\n                raise AMQPConnectionError('Connection timed out')\n            sleep(IDLE_WAIT)",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"log\"\n\t\"net/http\"\n)\n\n// RecoverMiddleware is a middleware function that recovers from panics.\nfunc RecoverMiddleware(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\t// Use defer to ensure recovery happens before returning a response.\n\t\tdefer func() {\n\t\t\tif err := recover(); err != nil {\n\t\t\t\t// Log the panic message for debugging.\n\t\t\t\tlog.Printf(\"Recovered from panic: %v\", err)\n\n\t\t\t\t// Write an internal server error response to the client.\n\t\t\t\thttp.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError)\n\t\t\t}\n\t\t}()\n\n\t\t// Proceed with the next handler in the chain.\n\t\tnext.ServeHTTP(w, r)\n\t})\n}\n\n// Example of how to use the middleware with an HTTP handler.\nfunc ExampleHandler(w http.ResponseWriter, r *http.Request) {\n\t// Deliberately cause a panic to demonstrate recovery.\n\tpanic(\"something went wrong\")\n}\n\nfunc main() {\n\tmux := http.NewServeMux()\n\tmux.HandleFunc(\"/\", ExampleHandler)\n\n\t// Add the RecoverMiddleware to the server's handler chain.\n\thttp.ListenAndServe(\":8080\", RecoverMiddleware(mux))\n}\n```",
    "label": 1
  },
  {
    "codes": "function centerGraphic(rect, boundingRect) {\n    // Set rect to center, keep width / height ratio.\n    var aspect = boundingRect.width / boundingRect.height;\n    var width = rect.height * aspect;\n    var height;\n    if (width <= rect.width) {\n        height = rect.height;\n    }\n    else {\n        width = rect.width;\n        height = width / aspect;\n    }\n    var cx = rect.x + rect.width / 2;\n    var cy = rect.y + rect.height / 2;\n\n    return {\n        x: cx - width / 2,\n        y: cy - height / 2,\n        width: width,\n        height: height\n    };\n}",
    "label": 0
  },
  {
    "codes": "func (s *ArchiveCreationOutput) SetChecksum(v string) *ArchiveCreationOutput {\n\ts.Checksum = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport org.hibernate.Session;\nimport org.hibernate.SessionFactory;\nimport org.hibernate.Transaction;\nimport org.hibernate.cfg.Configuration;\n\npublic class BatchInsertExample {\n\n    public void batchInsert(List<MyEntity> entities) {\n        SessionFactory sessionFactory = new Configuration().configure().buildSessionFactory();\n        try (Session session = sessionFactory.openSession()) {\n            Transaction transaction = session.beginTransaction();\n            int batchSize = 50;\n            for (int i = 0; i < entities.size(); i++) {\n                session.save(entities.get(i));\n                if (i % batchSize == 0) { // Flush and clear every batchSize\n                    session.flush();\n                    session.clear();\n                }\n            }\n            transaction.commit();\n        } catch (Exception e) {\n            e.printStackTrace();\n        } finally {\n            sessionFactory.close();\n        }\n    }\n}\n```\n\nNote: Ensure you have entities list passed to the method, and `MyEntity` should be your entity class mapped with Hibernate. Adjust `batchSize` as necessary based on your application's memory constraints.",
    "label": 1
  },
  {
    "codes": "```ruby\n# Define a simple Fiber-based scheduler\nclass FiberScheduler\n  def initialize\n    @fibers = []  # Queue to hold fibers\n  end\n\n  # Add a fiber to the scheduler\n  def schedule(&block)\n    fiber = Fiber.new(&block)\n    @fibers << fiber\n    fiber\n  end\n\n  # Run the scheduler and execute fibers\n  def run\n    while !@fibers.empty?\n      fiber = @fibers.shift  # Get the next fiber\n      fiber.resume  # Resume the fiber\n      @fibers << fiber if fiber.alive?  # Requeue if not finished\n    end\n  end\nend\n\n# Example usage\nscheduler = FiberScheduler.new\n\n# Schedule tasks\nscheduler.schedule do\n  3.times do |i|\n    puts \"Task 1: Step #{i}\"\n    Fiber.yield  # Yield control back to the scheduler\n  end\nend\n\nscheduler.schedule do\n  3.times do |i|\n    puts \"Task 2: Step #{i}\"\n    Fiber.yield  # Yield control back to the scheduler\n  end\nend\n\n# Run the scheduler\nscheduler.run\n```",
    "label": 1
  },
  {
    "codes": "def group_indices(self):\n        \"\"\"\n        Return group indices\n        \"\"\"\n        # No groups\n        if not self.plydata_groups:\n            return np.ones(len(self), dtype=int)\n\n        grouper = self.groupby()\n        indices = np.empty(len(self), dtype=int)\n        for i, (_, idx) in enumerate(sorted(grouper.indices.items())):\n            indices[idx] = i\n        return indices",
    "label": 0
  },
  {
    "codes": "```python\nimport pytest\nimport requests\n\n# Test case to validate the API response schema\ndef test_api_response_schema():\n    # Make a GET request to the API endpoint\n    response = requests.get('https://api.example.com/data')\n    \n    # Assert that the response status code is 200 (OK)\n    assert response.status_code == 200, \"Expected status code 200, but got {}\".format(response.status_code)\n    \n    # Validate the response schema\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"id\": {\"type\": \"number\"},\n            \"name\": {\"type\": \"string\"},\n            \"is_active\": {\"type\": \"boolean\"}\n        },\n        \"required\": [\"id\", \"name\"]\n    }\n    \n    # Assert that the response JSON matches the schema\n    assert response.json() == schema, \"Response JSON does not match the expected schema\"\n\n# Test case to validate the API response schema with nested objects\ndef test_api_response_schema_with_nested_objects():\n    # Make a GET request to the API endpoint\n    response = requests.get('https://api.example.com/data')\n    \n    # Assert that the response status code is 200 (OK)\n    assert response.status_code == 200, \"Expected status code 200, but got {}\".format(response.status_code)\n    \n    # Validate the response schema with nested objects\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"id\": {\"type\": \"number\"},\n            \"name\": {\"type\": \"string\"},\n            \"details\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"age\": {\"type\": \"number\"},\n                    \"email\": {\"type\": \"string\"}\n                },\n                \"required\": [\"age\", \"email\"]\n            }\n        },\n        \"required\": [\"id\", \"name\", \"details\"]\n    }\n    \n    # Assert that the response JSON matches the schema\n    assert response.json() == schema, \"Response JSON does not match the expected schema\"\n\n# Test case to validate the API response schema with arrays\ndef test_api_response_schema_with_arrays():\n    # Make a GET request to the API endpoint\n    response = requests.get('https://api.example.com/data')\n    \n    # Assert that the response status code is 200 (OK)\n    assert response.status_code == 200, \"Expected status code 200, but got {}\".format(response.status_code)\n    \n    # Validate the response schema with arrays\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"id\": {\"type\": \"number\"},\n            \"name\": {\"type\": \"string\"},\n            \"items\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"item_id\": {\"type\": \"number\"},\n                        \"item_name\": {\"type\": \"string\"}\n                    },\n                    \"required\": [\"item_id\", \"item_name\"]\n                }\n            }\n        },\n        \"required\": [\"id\", \"name\", \"items\"]\n    }\n    \n    # Assert that the response JSON matches the schema\n    assert response.json() == schema, \"Response JSON does not match the expected schema\"\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class MazeGenerator : MonoBehaviour\n{\n    public int width = 10;\n    public int height = 10;\n    public GameObject wallPrefab;\n    private int[,] maze;\n\n    void Start()\n    {\n        maze = new int[width, height];\n        GenerateMaze();\n        DrawMaze();\n    }\n\n    void GenerateMaze()\n    {\n        InitializeMaze();\n        PerformDepthFirstSearch(1, 1);\n    }\n\n    void InitializeMaze()\n    {\n        // Create maze filled with walls\n        for (int x = 0; x < width; x++)\n        {\n            for (int y = 0; y < height; y++)\n            {\n                maze[x, y] = 1;\n            }\n        }\n    }\n\n    void PerformDepthFirstSearch(int x, int y)\n    {\n        // Directions array represents N, E, S, W\n        int[] dX = { 0, 1, 0, -1 };\n        int[] dY = { -1, 0, 1, 0 };\n\n        maze[x, y] = 0;\n        List<int> directions = new List<int> { 0, 1, 2, 3 };\n        Shuffle(directions);\n\n        foreach (int direction in directions)\n        {\n            int nx = x + dX[direction] * 2;\n            int ny = y + dY[direction] * 2;\n\n            if (IsWithinBounds(nx, ny) && maze[nx, ny] == 1)\n            {\n                maze[x + dX[direction], y + dY[direction]] = 0; // Remove wall between cells\n                PerformDepthFirstSearch(nx, ny);\n            }\n        }\n    }\n\n    void Shuffle(List<int> list)\n    {\n        // Shuffle directions to ensure randomness\n        for (int i = 0; i < list.Count; i++)\n        {\n            int temp = list[i];\n            int randomIndex = Random.Range(i, list.Count);\n            list[i] = list[randomIndex];\n            list[randomIndex] = temp;\n        }\n    }\n\n    bool IsWithinBounds(int x, int y)\n    {\n        return x > 0 && x < width && y > 0 && y < height;\n    }\n\n    void DrawMaze()\n    {\n        // Instantiate walls based on generated maze\n        for (int x = 0; x < width; x++)\n        {\n            for (int y = 0; y < height; y++)\n            {\n                if (maze[x, y] == 1)\n                {\n                    Instantiate(wallPrefab, new Vector3(x, 0, y), Quaternion.identity);\n                }\n            }\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<AvailableProvidersListInner>> beginListAvailableProvidersWithServiceResponseAsync(String resourceGroupName, String networkWatcherName, AvailableProvidersListParameters parameters) {\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (networkWatcherName == null) {\n            throw new IllegalArgumentException(\"Parameter networkWatcherName is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (parameters == null) {\n            throw new IllegalArgumentException(\"Parameter parameters is required and cannot be null.\");\n        }\n        Validator.validate(parameters);\n        final String apiVersion = \"2018-06-01\";\n        return service.beginListAvailableProviders(resourceGroupName, networkWatcherName, this.client.subscriptionId(), parameters, apiVersion, this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<AvailableProvidersListInner>>>() {\n                @Override\n                public Observable<ServiceResponse<AvailableProvidersListInner>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<AvailableProvidersListInner> clientResponse = beginListAvailableProvidersDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.utils.dates import days_ago\nimport pandas as pd\nimport logging\n\n# Define default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': days_ago(1),\n    'retries': 1,\n}\n\n# Define the DAG\ndag = DAG(\n    'etl_redshift_pipeline',\n    default_args=default_args,\n    description='ETL pipeline to load data into Redshift',\n    schedule_interval='@daily',\n)\n\n# Function to extract data from S3\ndef extract_data_from_s3():\n    s3_hook = S3Hook(aws_conn_id='aws_default')\n    bucket_name = 'your-s3-bucket'\n    key = 'your-data-file.csv'\n    local_path = '/tmp/data.csv'\n    s3_hook.download_file(key=key, bucket_name=bucket_name, local_path=local_path)\n    logging.info(\"Data extracted from S3\")\n\n# Function to transform data\ndef transform_data():\n    local_path = '/tmp/data.csv'\n    df = pd.read_csv(local_path)\n    # Example transformation: Convert a column to uppercase\n    df['column_name'] = df['column_name'].str.upper()\n    df.to_csv(local_path, index=False)\n    logging.info(\"Data transformed\")\n\n# Function to load data into Redshift\ndef load_data_to_redshift():\n    local_path = '/tmp/data.csv'\n    redshift_hook = PostgresHook(postgres_conn_id='redshift_default')\n    df = pd.read_csv(local_path)\n    # Example: Load data into a table\n    redshift_hook.insert_rows(table='your_redshift_table', rows=df.values.tolist())\n    logging.info(\"Data loaded into Redshift\")\n\n# Define tasks in the DAG\nextract_task = PythonOperator(\n    task_id='extract_data_from_s3',\n    python_callable=extract_data_from_s3,\n    dag=dag,\n)\n\ntransform_task = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_data,\n    dag=dag,\n)\n\nload_task = PythonOperator(\n    task_id='load_data_to_redshift',\n    python_callable=load_data_to_redshift,\n    dag=dag,\n)\n\n# Set task dependencies\nextract_task >> transform_task >> load_task\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define a convolutional neural network\nclass CIFAR10Model(nn.Module):\n    def __init__(self):\n        super(CIFAR10Model, self).__init__()\n        # First convolutional layer, 3 input channels (RGB), 32 output channels, 3x3 kernel\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        # Second convolutional layer, 32 input channels, 64 output channels, 3x3 kernel\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        # A fully connected layer with 64*8*8 input features and 512 output\n        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n        # Output layer with 512 input features and 10 output classes (CIFAR-10 classes)\n        self.fc2 = nn.Linear(512, 10)\n        # Max-pooling layer with 2x2 pool size\n        self.pool = nn.MaxPool2d(2, 2)\n        # Dropout layer with a 50% drop probability\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        # Apply convolution, activation, and pooling to first layer\n        x = self.pool(F.relu(self.conv1(x)))\n        # Apply dropout after first pooling\n        x = self.dropout(x)\n        # Apply convolution, activation, and pooling to second layer\n        x = self.pool(F.relu(self.conv2(x)))\n        # Apply dropout after second pooling\n        x = self.dropout(x)\n        # Flatten feature map to a vector\n        x = x.view(-1, 64 * 8 * 8)\n        # Pass through first fully connected layer and apply activation\n        x = F.relu(self.fc1(x))\n        # Apply dropout after fully connected layer\n        x = self.dropout(x)\n        # Output layer\n        x = self.fc2(x)\n        return x\n\n# Instantiate the model, and move it to a CUDA device if available\nmodel = CIFAR10Model()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<Page<ResourceMetricInner>>> listMultiRolePoolInstanceMetricsNextSinglePageAsync(final String nextPageLink) {\n        if (nextPageLink == null) {\n            throw new IllegalArgumentException(\"Parameter nextPageLink is required and cannot be null.\");\n        }\n        String nextUrl = String.format(\"%s\", nextPageLink);\n        return service.listMultiRolePoolInstanceMetricsNext(nextUrl, this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<Page<ResourceMetricInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<ResourceMetricInner>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<PageImpl<ResourceMetricInner>> result = listMultiRolePoolInstanceMetricsNextDelegate(response);\n                        return Observable.just(new ServiceResponse<Page<ResourceMetricInner>>(result.body(), result.response()));\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def calc_networkmeasure(self, networkmeasure, **measureparams):\n        \"\"\"\n        Calculate network measure.\n\n        Parameters\n        -----------\n        networkmeasure : str\n            Function to call. Functions available are in teneto.networkmeasures\n\n        measureparams : kwargs\n            kwargs for teneto.networkmeasure.[networkmeasure]\n        \"\"\"\n        availablemeasures = [f for f in dir(\n            teneto.networkmeasures) if not f.startswith('__')]\n        if networkmeasure not in availablemeasures:\n            raise ValueError(\n                'Unknown network measure. Available network measures are: ' + ', '.join(availablemeasures))\n        funs = inspect.getmembers(teneto.networkmeasures)\n        funs = {m[0]: m[1] for m in funs if not m[0].startswith('__')}\n        measure = funs[networkmeasure](self, **measureparams)\n        return measure",
    "label": 0
  },
  {
    "codes": "```java\n@Aspect\n@Component\npublic class CircuitBreakerAspect {\n\n    private final CircuitBreaker circuitBreaker;\n\n    public CircuitBreakerAspect(CircuitBreakerRegistry circuitBreakerRegistry) {\n        this.circuitBreaker = circuitBreakerRegistry.circuitBreaker(\"myCircuitBreaker\");\n    }\n\n    @Around(\"@annotation(org.springframework.web.bind.annotation.GetMapping) || \" +\n            \"@annotation(org.springframework.web.bind.annotation.PostMapping) || \" +\n            \"@annotation(org.springframework.web.bind.annotation.PutMapping) || \" +\n            \"@annotation(org.springframework.web.bind.annotation.DeleteMapping)\")\n    public Object wrapWithCircuitBreaker(ProceedingJoinPoint pjp) throws Throwable {\n        // Executes method with circuit breaker pattern\n        return circuitBreaker.executeCallable(pjp::proceed);\n    }\n}\n```\n\n```xml\n<!-- Add Resilience4j dependency to your pom.xml for the circuit breaker functionality -->\n<dependency>\n    <groupId>io.github.resilience4j</groupId>\n    <artifactId>resilience4j-spring-boot2</artifactId>\n    <version>1.7.1</version>\n</dependency>\n```\n\n```yaml\n# Configure the Circuit Breaker in application.yml\nresilience4j:\n  circuitbreaker:\n    instances:\n      myCircuitBreaker:\n        registerHealthIndicator: true\n        slidingWindowSize: 100\n        failureRateThreshold: 50\n        waitDurationInOpenState: 60s\n        enableAutomaticTransitionFromOpenToHalfOpen: true\n```\n\n```java\n// Configuration class to initialize Circuit Breaker Registry\n@Configuration\npublic class CircuitBreakerConfig {\n\n    @Bean\n    public CircuitBreakerRegistry circuitBreakerRegistry() {\n        return CircuitBreakerRegistry.ofDefaults();\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glue) GetDatabases(input *GetDatabasesInput) (*GetDatabasesOutput, error) {\n\treq, out := c.GetDatabasesRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "public PagedList<JobResponseInner> listJobs(final String resourceGroupName, final String resourceName) {\n        ServiceResponse<Page<JobResponseInner>> response = listJobsSinglePageAsync(resourceGroupName, resourceName).toBlocking().single();\n        return new PagedList<JobResponseInner>(response.body()) {\n            @Override\n            public Page<JobResponseInner> nextPage(String nextPageLink) {\n                return listJobsNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "public CertificateOperation getCertificateOperation(String vaultBaseUrl, String certificateName) {\n        return getCertificateOperationWithServiceResponseAsync(vaultBaseUrl, certificateName).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<SasDefinitionBundle>> recoverDeletedSasDefinitionWithServiceResponseAsync(String vaultBaseUrl, String storageAccountName, String sasDefinitionName) {\n        if (vaultBaseUrl == null) {\n            throw new IllegalArgumentException(\"Parameter vaultBaseUrl is required and cannot be null.\");\n        }\n        if (storageAccountName == null) {\n            throw new IllegalArgumentException(\"Parameter storageAccountName is required and cannot be null.\");\n        }\n        if (sasDefinitionName == null) {\n            throw new IllegalArgumentException(\"Parameter sasDefinitionName is required and cannot be null.\");\n        }\n        if (this.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.apiVersion() is required and cannot be null.\");\n        }\n        String parameterizedHost = Joiner.on(\", \").join(\"{vaultBaseUrl}\", vaultBaseUrl);\n        return service.recoverDeletedSasDefinition(storageAccountName, sasDefinitionName, this.apiVersion(), this.acceptLanguage(), parameterizedHost, this.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<SasDefinitionBundle>>>() {\n                @Override\n                public Observable<ServiceResponse<SasDefinitionBundle>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<SasDefinitionBundle> clientResponse = recoverDeletedSasDefinitionDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *GetCrawlerMetricsInput) SetCrawlerNameList(v []*string) *GetCrawlerMetricsInput {\n\ts.CrawlerNameList = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *Table) SetDescription(v string) *Table {\n\ts.Description = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public DeletedSecretBundle deleteSecret(String vaultBaseUrl, String secretName) {\n        return deleteSecretWithServiceResponseAsync(vaultBaseUrl, secretName).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "def replace(self, **kwargs):\n        \"\"\"\n        Construct a new :class:`JID` object, using the values of the current\n        JID. Use the arguments to override specific attributes on the new\n        object.\n\n        All arguments are keyword arguments.\n\n        :param localpart: Set the local part of the resulting JID.\n        :param domain: Set the domain of the resulting JID.\n        :param resource: Set the resource part of the resulting JID.\n        :raises: See :class:`JID`\n        :return: A new :class:`JID` object with the corresponding\n            substitutions performed.\n        :rtype: :class:`JID`\n\n        The attributes of parameters which are omitted are not modified and\n        copied down to the result.\n        \"\"\"\n\n        new_kwargs = {}\n\n        strict = kwargs.pop(\"strict\", True)\n\n        try:\n            localpart = kwargs.pop(\"localpart\")\n        except KeyError:\n            pass\n        else:\n            if localpart:\n                localpart = nodeprep(\n                    localpart,\n                    allow_unassigned=not strict\n                )\n            new_kwargs[\"localpart\"] = localpart\n\n        try:\n            domain = kwargs.pop(\"domain\")\n        except KeyError:\n            pass\n        else:\n            if not domain:\n                raise ValueError(\"domain must not be empty or None\")\n            new_kwargs[\"domain\"] = nameprep(\n                domain,\n                allow_unassigned=not strict\n            )\n\n        try:\n            resource = kwargs.pop(\"resource\")\n        except KeyError:\n            pass\n        else:\n            if resource:\n                resource = resourceprep(\n                    resource,\n                    allow_unassigned=not strict\n                )\n            new_kwargs[\"resource\"] = resource\n\n        if kwargs:\n            raise TypeError(\"replace() got an unexpected keyword argument\"\n                            \" {!r}\".format(\n                                next(iter(kwargs))))\n\n        return super()._replace(**new_kwargs)",
    "label": 0
  },
  {
    "codes": "```java\n// Step 1: Add TensorFlow Lite dependencies in your app-level build.gradle\ndependencies {\n    implementation 'org.tensorflow:tensorflow-lite:2.3.0'\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\n    implementation 'org.tensorflow:tensorflow-lite-support:0.1.0'\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'\n}\n\n// Step 2: Add permissions for camera access in AndroidManifest.xml\n<manifest>\n    <uses-permission android:name=\"android.permission.CAMERA\"/>\n    <application>\n        <!-- Required for CameraX library -->\n        <meta-data\n            android:name=\"androidx.camera.camera2.enabled\"\n            android:value=\"true\"/>\n    </application>\n</manifest>\n\n// Step 3: Implement TensorFlow Lite model in Android application.\n// Assume your object detection model is converted to TFLite and named 'detect.tflite'.\n\n// Import necessary libraries\nimport org.tensorflow.lite.Interpreter;\nimport org.tensorflow.lite.gpu.GpuDelegate;\nimport org.tensorflow.lite.support.tensorbuffer.TensorBuffer;\nimport org.tensorflow.lite.support.image.TensorImage;\nimport org.tensorflow.lite.support.common.FileUtil;\n\n// ... \n\n// Step 4: Load the TFLite model\npublic class ObjectDetectionModel {\n    private Interpreter tflite;\n    private GpuDelegate gpuDelegate = null;\n\n    public ObjectDetectionModel(Context context) {\n        try {\n            // Load the model file as an InputStream\n            MappedByteBuffer tfliteModel = FileUtil.loadMappedFile(context, \"detect.tflite\");\n            Interpreter.Options options = new Interpreter.Options();\n            gpuDelegate = new GpuDelegate();\n            options.addDelegate(gpuDelegate);\n            tflite = new Interpreter(tfliteModel, options);\n        } catch (Exception e) {\n            // Handle the exception\n        }\n    }\n\n    // Perform detection on input image\n    public void detectObjects(TensorImage image) {\n        // Create input and output buffers\n        TensorBuffer outputBuffer = TensorBuffer.createFixedSize(new int[]{1, 10, 4}, DataType.FLOAT32);\n\n        // Run inference\n        tflite.run(image.getBuffer(), outputBuffer.getBuffer());\n\n        // Process results\n        // Interpret the outputBuffer to extract detection results.\n        ByteBuffer output = outputBuffer.getBuffer();\n        // Handle detections.\n    }\n}\n\n// Step 5: Setup CameraX for real-time capture.\n// Add CameraX dependencies in your app-level build.gradle\ndependencies {\n    def camerax_version = \"1.0.0\"\n    implementation \"androidx.camera:camera-core:$camerax_version\"\n    implementation \"androidx.camera:camera-camera2:$camerax_version\"\n    implementation \"androidx.camera:camera-lifecycle:$camerax_version\"\n    implementation \"androidx.camera:camera-view:1.0.0-alpha18\" // or use a stable release\n}\n\n// Setup camera in your activity\nimport androidx.camera.core.CameraX;\nimport androidx.camera.core.Preview;\n\n// Add a preview use case\nprivate void startCamera() {\n    final ListenableFuture<ProcessCameraProvider> cameraProviderFuture =\n        ProcessCameraProvider.getInstance(this);\n        \n    cameraProviderFuture.addListener(() -> {\n        try {\n            ProcessCameraProvider cameraProvider = cameraProviderFuture.get();\n            Preview preview = new Preview.Builder().build();\n            // Attach the viewfinder's surface provider to preview use case\n            preview.setSurfaceProvider(viewFinder.getSurfaceProvider());\n\n            // Bind use cases to camera\n            cameraProvider.bindToLifecycle(this, CameraX.CameraSelector.DEFAULT_BACK_CAMERA, preview);\n\n        } catch (ExecutionException | InterruptedException e) {\n            // Handle exceptions\n        }\n    }, ContextCompat.getMainExecutor(this));\n}\n\n// Ensure your app processes each frame from the camera\n// by leveraging the Preview's surface listener or use an ImageAnalysis use case to analyze camera frames in real-time.\n```\n\n**Note:** The above code provides a basic outline but is missing detailed implementation aspects like handling result processing, threading for running the model, managing lifecycle changes, handling image rotation/scaling, and drawing detection results back onto a UI overlay. These would be essential in a full implementation.",
    "label": 1
  },
  {
    "codes": "func (s *CreateCrawlerInput) SetSchemaChangePolicy(v *SchemaChangePolicy) *CreateCrawlerInput {\n\ts.SchemaChangePolicy = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define a custom CNN model for CIFAR-10 classification\nclass CustomCNN(nn.Module):\n    def __init__(self):\n        super(CustomCNN, self).__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        # Max pooling layers\n        self.pool = nn.MaxPool2d(2, 2)\n        # Fully connected layers\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 10)  # 10 classes for CIFAR-10\n        # Dropout layer\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        # Apply convolutional layers with ReLU and pooling\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        # Flatten the output for fully connected layers\n        x = x.view(-1, 128 * 4 * 4)\n        # Apply fully connected layers with ReLU and dropout\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        # Output layer\n        x = self.fc3(x)\n        return x\n\n# Instantiate the model\nmodel = CustomCNN()\n\n# Print the model architecture\nprint(model)\n```",
    "label": 1
  },
  {
    "codes": "def remove_class(self, cls):\n        \"\"\"\n        Remove a XSO class `cls` from parsing. This method raises\n        :class:`KeyError` with the classes :attr:`TAG` attribute as argument if\n        removing fails because the class is not registered.\n        \"\"\"\n        del self._tag_map[cls.TAG]\n        del self._class_map[cls]",
    "label": 0
  },
  {
    "codes": "public Observable<Contacts> deleteCertificateContactsAsync(String vaultBaseUrl) {\n        return deleteCertificateContactsWithServiceResponseAsync(vaultBaseUrl).map(new Func1<ServiceResponse<Contacts>, Contacts>() {\n            @Override\n            public Contacts call(ServiceResponse<Contacts> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```csharp\nusing System;\nusing System.ServiceProcess;\nusing System.Management;\nusing System.IO;\n\npublic class UsbMonitorService : ServiceBase\n{\n    private ManagementEventWatcher insertWatcher;\n    private ManagementEventWatcher removeWatcher;\n    private static string logFilePath = @\"C:\\UsbDeviceLog.txt\";\n\n    public UsbMonitorService()\n    {\n        this.ServiceName = \"UsbMonitorService\";\n    }\n\n    protected override void OnStart(string[] args)\n    {\n        StartWatchingUsbDevices();\n    }\n\n    protected override void OnStop()\n    {\n        StopWatchingUsbDevices();\n    }\n\n    private void StartWatchingUsbDevices()\n    {\n        WqlEventQuery insertQuery = new WqlEventQuery(\"__InstanceCreationEvent\", new TimeSpan(0, 0, 1),\n            \"TargetInstance ISA 'Win32_USBHub'\");\n        insertWatcher = new ManagementEventWatcher(insertQuery);\n        insertWatcher.EventArrived += new EventArrivedEventHandler(DeviceInsertedEvent);\n        insertWatcher.Start();\n\n        WqlEventQuery removeQuery = new WqlEventQuery(\"__InstanceDeletionEvent\", new TimeSpan(0, 0, 1),\n            \"TargetInstance ISA 'Win32_USBHub'\");\n        removeWatcher = new ManagementEventWatcher(removeQuery);\n        removeWatcher.EventArrived += new EventArrivedEventHandler(DeviceRemovedEvent);\n        removeWatcher.Start();\n    }\n\n    private void StopWatchingUsbDevices()\n    {\n        insertWatcher.Stop();\n        insertWatcher.Dispose();\n\n        removeWatcher.Stop();\n        removeWatcher.Dispose();\n    }\n\n    private void DeviceInsertedEvent(object sender, EventArrivedEventArgs e)\n    {\n        LogEvent(\"USB device inserted.\");\n    }\n\n    private void DeviceRemovedEvent(object sender, EventArrivedEventArgs e)\n    {\n        LogEvent(\"USB device removed.\");\n    }\n\n    private void LogEvent(string message)\n    {\n        // Logs the event to a text file\n        File.AppendAllText(logFilePath, $\"{DateTime.Now}: {message}{Environment.NewLine}\");\n    }\n\n    public static void Main()\n    {\n        ServiceBase.Run(new UsbMonitorService());\n    }\n}\n```\n\n```xml\n<!-- Install the service using a simple service installer script -->\n<configuration>\n  <runtime>\n    <assemblyBinding xmlns=\"urn:schemas-microsoft-com:asm.v1\">\n      <dependentAssembly>\n        <assemblyIdentity name=\"System.Management\" publicKeyToken=\"b03f5f7f11d50a3a\" culture=\"neutral\" />\n        <bindingRedirect oldVersion=\"0.0.0.0-2555.2555.2555.2555\" newVersion=\"2.0.0.0\" />\n      </dependentAssembly>\n    </assemblyBinding>\n  </runtime>\n</configuration>\n```\n\n```shell\nREM Create Service Installer\nsc create UsbMonitorService binPath= \"C:\\Path\\To\\UsbMonitorService.exe\" start= auto\nsc start UsbMonitorService\n```\n\n<!--\n- This C# code defines a Windows Service named 'UsbMonitorService' that monitors USB device connection events.\n- `ManagementEventWatcher` is used to track both USB insertions and removals via WMI queries.\n- The service writes events to a log file located at `C:\\UsbDeviceLog.txt`.\n- A shell script to create and start the service is provided for ease of installation.\n-->",
    "label": 1
  },
  {
    "codes": "```elixir\ndefmodule DistributedStateManager do\n  use GenServer\n\n  # Client API\n\n  def start_link(opts \\\\ []) do\n    GenServer.start_link(__MODULE__, :ok, opts)\n  end\n\n  def set_state(pid, key, value) do\n    GenServer.call(pid, {:set_state, key, value})\n  end\n\n  def get_state(pid, key) do\n    GenServer.call(pid, {:get_state, key})\n  end\n\n  def delete_state(pid, key) do\n    GenServer.call(pid, {:delete_state, key})\n  end\n\n  # Server Callbacks\n\n  @impl true\n  def init(:ok) do\n    {:ok, %{}}\n  end\n\n  @impl true\n  def handle_call({:set_state, key, value}, _from, state) do\n    new_state = Map.put(state, key, value)\n    {:reply, :ok, new_state}\n  end\n\n  @impl true\n  def handle_call({:get_state, key}, _from, state) do\n    value = Map.get(state, key)\n    {:reply, value, state}\n  end\n\n  @impl true\n  def handle_call({:delete_state, key}, _from, state) do\n    new_state = Map.delete(state, key)\n    {:reply, :ok, new_state}\n  end\nend\n```\n\n```erlang\n-module(distributed_state_manager).\n-behaviour(gen_server).\n\n-export([start_link/0, set_state/2, get_state/1, delete_state/1]).\n-export([init/1, handle_call/3, handle_cast/2, handle_info/2, terminate/2, code_change/3]).\n\n% Client API\n\nstart_link() ->\n    gen_server:start_link({local, ?MODULE}, ?MODULE, [], []).\n\nset_state(Key, Value) ->\n    gen_server:call(?MODULE, {set_state, Key, Value}).\n\nget_state(Key) ->\n    gen_server:call(?MODULE, {get_state, Key}).\n\ndelete_state(Key) ->\n    gen_server:call(?MODULE, {delete_state, Key}).\n\n% Server Callbacks\n\ninit([]) ->\n    {ok, #{}}.\n\nhandle_call({set_state, Key, Value}, _From, State) ->\n    NewState = maps:put(Key, Value, State),\n    {reply, ok, NewState};\n\nhandle_call({get_state, Key}, _From, State) ->\n    Value = maps:get(Key, State, undefined),\n    {reply, Value, State};\n\nhandle_call({delete_state, Key}, _From, State) ->\n    NewState = maps:remove(Key, State),\n    {reply, ok, NewState};\n\nhandle_call(_Request, _From, State) ->\n    {reply, ok, State}.\n\nhandle_cast(_Msg, State) ->\n    {noreply, State}.\n\nhandle_info(_Info, State) ->\n    {noreply, State}.\n\nterminate(_Reason, _State) ->\n    ok.\n\ncode_change(_OldVsn, State, _Extra) ->\n    {ok, State}.\n```\n\n```elixir\n# Example usage:\n# {:ok, pid} = DistributedStateManager.start_link()\n# DistributedStateManager.set_state(pid, :key1, \"value1\")\n# DistributedStateManager.get_state(pid, :key1)  # => \"value1\"\n# DistributedStateManager.delete_state(pid, :key1)\n```\n\n```erlang\n% Example usage:\n% {ok, Pid} = distributed_state_manager:start_link().\n% distributed_state_manager:set_state(key1, \"value1\").\n% distributed_state_manager:get_state(key1).  % => \"value1\"\n% distributed_state_manager:delete_state(key1).\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport psutil\nimport time\nimport argparse\n\n# Function to get network usage per process\ndef get_network_usage():\n    processes = {}\n    for proc in psutil.process_iter(['pid', 'name']):\n        try:\n            connections = proc.connections()\n            if connections:\n                for conn in connections:\n                    if conn.status == psutil.CONN_ESTABLISHED:\n                        pid = proc.info['pid']\n                        name = proc.info['name']\n                        if pid not in processes:\n                            processes[pid] = {'name': name, 'sent': 0, 'recv': 0}\n                        io_counters = proc.io_counters()\n                        processes[pid]['sent'] += io_counters.bytes_sent\n                        processes[pid]['recv'] += io_counters.bytes_recv\n        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n            continue\n    return processes\n\n# Function to monitor network usage\ndef monitor_network_usage(interval=1):\n    try:\n        while True:\n            processes = get_network_usage()\n            for pid, data in processes.items():\n                print(f\"PID: {pid}, Name: {data['name']}, Sent: {data['sent']} bytes, Received: {data['recv']} bytes\")\n            time.sleep(interval)\n    except KeyboardInterrupt:\n        print(\"Monitoring stopped.\")\n\n# Main function to handle CLI arguments\ndef main():\n    parser = argparse.ArgumentParser(description=\"Monitor network bandwidth usage per process.\")\n    parser.add_argument('-i', '--interval', type=int, default=1, help=\"Interval in seconds to refresh the data.\")\n    args = parser.parse_args()\n    monitor_network_usage(args.interval)\n\nif __name__ == \"__main__\":\n    main()\n```",
    "label": 1
  },
  {
    "codes": "public Observable<Boolean> checkExistenceAsync(String resourceGroupName) {\n        return checkExistenceWithServiceResponseAsync(resourceGroupName).map(new Func1<ServiceResponse<Boolean>, Boolean>() {\n            @Override\n            public Boolean call(ServiceResponse<Boolean> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"crypto/rand\"\n\t\"fmt\"\n)\n\n// Generates a random password\nfunc generatePassword(length int, useLower, useUpper, useDigits, useSymbols bool) (string, error) {\n\tconst (\n\t\tlower   = \"abcdefghijklmnopqrstuvwxyz\"\n\t\tupper   = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\t\tdigits  = \"0123456789\"\n\t\tsymbols = \"!@#$%^&*()-_+=<>?\"\n\t)\n\n\t// Create a character set based on specified options\n\tcharSet := \"\"\n\tif useLower {\n\t\tcharSet += lower\n\t}\n\tif useUpper {\n\t\tcharSet += upper\n\t}\n\tif useDigits {\n\t\tcharSet += digits\n\t}\n\tif useSymbols {\n\t\tcharSet += symbols\n\t}\n\n\tif len(charSet) == 0 {\n\t\treturn \"\", fmt.Errorf(\"no character sets selected\")\n\t}\n\n\t// Create a byte slice to hold the password\n\tpassword := make([]byte, length)\n\n\t// Fill the password with random characters from the character set\n\tfor i := range password {\n\t\tnum, err := rand.Int(rand.Reader, big.NewInt(int64(len(charSet))))\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tpassword[i] = charSet[num.Int64()]\n\t}\n\n\treturn string(password), nil\n}\n\nfunc main() {\n\t// Specify password parameters\n\tlength := 12\n\tuseLower := true\n\tuseUpper := true\n\tuseDigits := true\n\tuseSymbols := true\n\n\t// Generate the password\n\tpassword, err := generatePassword(length, useLower, useUpper, useDigits, useSymbols)\n\tif err != nil {\n\t\tfmt.Println(\"Error:\", err)\n\t\treturn\n\t}\n\n\tfmt.Println(\"Generated Password:\", password)\n}\n```",
    "label": 1
  },
  {
    "codes": "def _process_features(features):\n    \"\"\"\n    Generate the `Features String` from an iterable of features.\n\n    :param features: The features to generate the features string from.\n    :type features: :class:`~collections.abc.Iterable` of :class:`str`\n    :return: The `Features String`\n    :rtype: :class:`bytes`\n\n    Generate the `Features String` from the given `features` as specified in\n    :xep:`390`.\n    \"\"\"\n    parts = [\n        feature.encode(\"utf-8\")+b\"\\x1f\"\n        for feature in features\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"",
    "label": 0
  },
  {
    "codes": "def connection_from_list_slice(list_slice, args=None, connection_type=None,\n                               edge_type=None, pageinfo_type=None,\n                               slice_start=0, list_length=0, list_slice_length=None):\n    '''\n    Given a slice (subset) of an array, returns a connection object for use in\n    GraphQL.\n    This function is similar to `connectionFromArray`, but is intended for use\n    cases where you know the cardinality of the connection, consider it too large\n    to materialize the entire array, and instead wish pass in a slice of the\n    total result large enough to cover the range specified in `args`.\n    '''\n    connection_type = connection_type or Connection\n    edge_type = edge_type or Edge\n    pageinfo_type = pageinfo_type or PageInfo\n\n    args = args or {}\n\n    before = args.get('before')\n    after = args.get('after')\n    first = args.get('first')\n    last = args.get('last')\n    if list_slice_length is None:\n        list_slice_length = len(list_slice)\n    slice_end = slice_start + list_slice_length\n    before_offset = get_offset_with_default(before, list_length)\n    after_offset = get_offset_with_default(after, -1)\n\n    start_offset = max(\n        slice_start - 1,\n        after_offset,\n        -1\n    ) + 1\n    end_offset = min(\n        slice_end,\n        before_offset,\n        list_length\n    )\n    if isinstance(first, int):\n        end_offset = min(\n            end_offset,\n            start_offset + first\n        )\n    if isinstance(last, int):\n        start_offset = max(\n            start_offset,\n            end_offset - last\n        )\n\n    # If supplied slice is too large, trim it down before mapping over it.\n    _slice = list_slice[\n        max(start_offset - slice_start, 0):\n        list_slice_length - (slice_end - end_offset)\n    ]\n    edges = [\n        edge_type(\n            node=node,\n            cursor=offset_to_cursor(start_offset + i)\n        )\n        for i, node in enumerate(_slice)\n    ]\n\n\n    first_edge_cursor = edges[0].cursor if edges else None\n    last_edge_cursor = edges[-1].cursor if edges else None\n    lower_bound = after_offset + 1 if after else 0\n    upper_bound = before_offset if before else list_length\n\n    return connection_type(\n        edges=edges,\n        page_info=pageinfo_type(\n            start_cursor=first_edge_cursor,\n            end_cursor=last_edge_cursor,\n            has_previous_page=isinstance(last, int) and start_offset > lower_bound,\n            has_next_page=isinstance(first, int) and end_offset < upper_bound\n        )\n    )",
    "label": 0
  },
  {
    "codes": "```python\nfrom locust import HttpUser, task, between\n\nclass ApiUser(HttpUser):\n    wait_time = between(1, 5)  # Random wait time between requests\n\n    @task\n    def get_resource(self):\n        # GET request to fetch a resource\n        response = self.client.get(\"/api/resource/1\")\n        if response.status_code == 200:\n            print(\"GET request successful\")\n        else:\n            print(f\"GET request failed with status code: {response.status_code}\")\n\n    @task(3)  # Higher weight, more frequent execution\n    def create_resource(self):\n        # POST request to create a resource\n        payload = {\"name\": \"test\", \"value\": \"123\"}\n        response = self.client.post(\"/api/resource\", json=payload)\n        if response.status_code == 201:\n            print(\"POST request successful\")\n        else:\n            print(f\"POST request failed with status code: {response.status_code}\")\n\n    @task(2)\n    def update_resource(self):\n        # PUT request to update a resource\n        payload = {\"name\": \"updated\", \"value\": \"456\"}\n        response = self.client.put(\"/api/resource/1\", json=payload)\n        if response.status_code == 200:\n            print(\"PUT request successful\")\n        else:\n            print(f\"PUT request failed with status code: {response.status_code}\")\n\n    @task(1)\n    def delete_resource(self):\n        # DELETE request to remove a resource\n        response = self.client.delete(\"/api/resource/1\")\n        if response.status_code == 204:\n            print(\"DELETE request successful\")\n        else:\n            print(f\"DELETE request failed with status code: {response.status_code}\")\n```",
    "label": 1
  },
  {
    "codes": "```gd\nshader_type canvas_item;\n\nuniform sampler2D water_texture;\nuniform sampler2D refraction_texture;\nuniform vec2 uv_scale : hint_range(0.0, 1.0);\n\nvoid fragment() {\n    // Clear the background\n    vec4 water_color = texture(water_texture, UV * uv_scale);\n    \n    // Refraction simulation\n    vec2 refraction_uv = UV + (water_color.rg - 0.5) * 0.1;\n    \n    // Fetch the refracted background texture\n    vec4 refracted_color = texture(refraction_texture, refraction_uv);\n    \n    // Combine both colors for effect\n    vec4 final_color = mix(water_color, refracted_color, 0.5);\n    \n    // Assign the final color output\n    COLOR = final_color;\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom flask import Flask, request, jsonify\nfrom PIL import Image\nimport numpy as np\nimport tensorflow as tf\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Load pre-trained model\nmodel = tf.keras.models.load_model('path_to_your_pretrained_model.h5')\n\n# Define image preprocessing function\ndef preprocess_image(image):\n    image = image.resize((224, 224))  # Resize image to model's expected input size\n    image = np.array(image) / 255.0  # Normalize pixel values\n    image = np.expand_dims(image, axis=0)  # Add batch dimension\n    return image\n\n# Define API endpoint for image classification\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    try:\n        image = Image.open(file.stream)\n        processed_image = preprocess_image(image)\n        predictions = model.predict(processed_image)\n        predicted_class = np.argmax(predictions, axis=1)\n        return jsonify({'predicted_class': int(predicted_class)})\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n# Run the Flask app\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n```",
    "label": 1
  },
  {
    "codes": "public CompletableFuture<Boolean> queueExistsAsync(String path) {\n        EntityNameHelper.checkValidQueueName(path);\n\n        CompletableFuture<Boolean> existsFuture = new CompletableFuture<>();\n        this.getQueueAsync(path).handleAsync((qd, ex) -> {\n            if (ex != null) {\n                if (ex instanceof MessagingEntityNotFoundException) {\n                    existsFuture.complete(Boolean.FALSE);\n                    return false;\n                }\n\n                existsFuture.completeExceptionally(ex);\n                return false;\n            }\n\n            existsFuture.complete(Boolean.TRUE);\n            return true;\n        }, MessagingFactory.INTERNAL_THREAD_POOL);\n\n        return existsFuture;\n    }",
    "label": 0
  },
  {
    "codes": "```html\n<template>\n  <div>\n    <h2>Stock Market Data</h2>\n    <canvas id=\"stockChart\"></canvas>\n  </div>\n</template>\n\n<script>\nimport { Line } from 'vue-chartjs'\nimport { Chart as ChartJS, Title, Tooltip, Legend, LineElement, CategoryScale, LinearScale, PointElement } from 'chart.js'\n\nChartJS.register(Title, Tooltip, Legend, LineElement, CategoryScale, LinearScale, PointElement)\n\nexport default {\n  name: 'StockChart',\n  components: {\n    Line\n  },\n  data() {\n    return {\n      chartData: {\n        labels: [], // Dates will be populated here\n        datasets: [\n          {\n            label: 'Stock Price',\n            backgroundColor: 'rgba(75, 192, 192, 0.2)',\n            borderColor: 'rgba(75, 192, 192, 1)',\n            data: [] // Stock prices will be populated here\n          }\n        ]\n      },\n      chartOptions: {\n        responsive: true,\n        maintainAspectRatio: false,\n        scales: {\n          x: {\n            type: 'time',\n            time: {\n              unit: 'day'\n            }\n          }\n        },\n        plugins: {\n          legend: {\n            display: true\n          },\n          tooltip: {\n            enabled: true\n          }\n        }\n      }\n    }\n  },\n  mounted() {\n    // Fetch the data from an API or other source\n    this.fetchData()\n  },\n  methods: {\n    async fetchData() {\n      // Simulate an API call for stock data\n      const response = await fetch('https://api.example.com/stock-data')\n      const data = await response.json()\n      this.chartData.labels = data.map(item => item.date)\n      this.chartData.datasets[0].data = data.map(item => item.price)\n    }\n  }\n}\n</script>\n\n<style scoped>\n#stockChart {\n  position: relative;\n  height: 400px;\n}\n</style>\n```\n\nNote: Replace the URL `'https://api.example.com/stock-data'` with the actual API URL you intend to use for fetching stock market data. Additionally, make sure that your `fetchData` method aligns with the actual data format received from your chosen API.",
    "label": 1
  },
  {
    "codes": "```swift\nimport SpriteKit\n\n// Function to create an action sequence for puzzle piece animations\nfunc createPuzzlePieceAnimation() -> SKAction {\n    // Define an initial scale down action for a popping effect\n    let scaleDown = SKAction.scale(to: 0.9, duration: 0.1)\n    \n    // Define a scale up action to return to original size\n    let scaleUp = SKAction.scale(to: 1.0, duration: 0.2)\n    \n    // Define a slight rotation to give dynamic movement\n    let rotate = SKAction.rotate(byAngle: CGFloat.pi / 8, duration: 0.2)\n    \n    // Define a reverse rotation to return to original orientation\n    let rotateBack = SKAction.rotate(byAngle: -CGFloat.pi / 8, duration: 0.2)\n    \n    // Define a slight movement upwards by 10 points\n    let moveUp = SKAction.moveBy(x: 0, y: 10, duration: 0.2)\n    \n    // Define a reverse movement downwards by 10 points\n    let moveDown = SKAction.moveBy(x: 0, y: -10, duration: 0.2)\n    \n    // Group scale and rotate actions for simultaneous execution\n    let scaleAndRotate = SKAction.group([scaleDown, rotate])\n    \n    // Group reverse scale and rotate actions\n    let scaleAndRotateBack = SKAction.group([scaleUp, rotateBack])\n    \n    // Create a sequence of actions\n    let sequence = SKAction.sequence([\n        scaleAndRotate,  // Scale down and rotate\n        scaleAndRotateBack, // Scale up and rotate back\n        moveUp,  // Move upwards\n        moveDown  // Move downwards\n    ])\n    \n    // Return the completed sequence\n    return sequence\n}\n\n// Usage example\nlet puzzlePieceNode = SKSpriteNode(imageNamed: \"puzzlePiece\")\npuzzlePieceNode.run(createPuzzlePieceAnimation())\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse std::collections::HashMap;\n\n// Function to calculate the nth Fibonacci number using memoization\nfn fibonacci(n: u32, memo: &mut HashMap<u32, u64>) -> u64 {\n    // Check if the result is already in the memo\n    if let Some(&result) = memo.get(&n) {\n        return result;\n    }\n\n    // Base cases\n    if n == 0 {\n        return 0;\n    }\n    if n == 1 {\n        return 1;\n    }\n\n    // Recursive case with memoization\n    let result = fibonacci(n - 1, memo) + fibonacci(n - 2, memo);\n    memo.insert(n, result);\n\n    result\n}\n\n// Example usage\nfn main() {\n    let mut memo = HashMap::new();\n    let n = 10;\n    println!(\"Fibonacci({}) = {}\", n, fibonacci(n, &mut memo));\n}\n```",
    "label": 1
  },
  {
    "codes": "def split_sentences(tokens):\n    \"\"\"Split sentences (based on tokenised data), returns sentences as a list of lists of tokens, each sentence is a list of tokens\"\"\"\n    begin = 0\n    for i, token in enumerate(tokens):\n        if is_end_of_sentence(tokens, i):\n            yield tokens[begin:i+1]\n            begin = i+1\n    if begin <= len(tokens)-1:\n        yield tokens[begin:]",
    "label": 0
  },
  {
    "codes": "func (s *CreateCrawlerInput) SetTablePrefix(v string) *CreateCrawlerInput {\n\ts.TablePrefix = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *ResetJobBookmarkOutput) SetJobBookmarkEntry(v *JobBookmarkEntry) *ResetJobBookmarkOutput {\n\ts.JobBookmarkEntry = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *PutEventSelectorsOutput) SetEventSelectors(v []*EventSelector) *PutEventSelectorsOutput {\n\ts.EventSelectors = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define a simple CNN for CIFAR-10 classification\nclass CIFAR10Classifier(nn.Module):\n    def __init__(self):\n        super(CIFAR10Classifier, self).__init__()\n        # First convolutional layer (3 input channels, 32 output channels, kernel size 3x3)\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n        # Second convolutional layer (32 input channels, 64 output channels, kernel size 3x3)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        # Max pooling layer (kernel size 2x2, stride 2)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        # Fully connected layer (64 * 8 * 8 input features, 512 output features)\n        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n        # Fully connected layer (512 input features, 10 output features for CIFAR-10 classes)\n        self.fc2 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        # Apply first conv layer with ReLU activation and max pooling\n        x = self.pool(F.relu(self.conv1(x)))\n        # Apply second conv layer with ReLU activation and max pooling\n        x = self.pool(F.relu(self.conv2(x)))\n        # Flatten the output from the conv layers\n        x = x.view(-1, 64 * 8 * 8)\n        # Apply first fully connected layer with ReLU activation\n        x = F.relu(self.fc1(x))\n        # Apply second fully connected layer for classification output\n        x = self.fc2(x)\n        return x\n\n# Function to create the model\ndef create_model():\n    return CIFAR10Classifier()\n\n# Instantiate the model\nmodel = create_model()\n```",
    "label": 1
  },
  {
    "codes": "```c\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <string.h>\n\n// Constants for AES-GCM\n#define AES_GCM_IV_LENGTH 12\n#define AES_GCM_TAG_LENGTH 16\n\n// Function to perform AES-GCM encryption\nint encrypt_aes_gcm(const unsigned char *plaintext, int plaintext_len,\n                    const unsigned char *aad, int aad_len,\n                    const unsigned char *key,\n                    unsigned char *iv, unsigned char *ciphertext, unsigned char *tag)\n{\n    EVP_CIPHER_CTX *ctx = EVP_CIPHER_CTX_new();\n    int len;\n    int ciphertext_len;\n\n    // Initialize the encryption operation\n    EVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL);\n\n    // Set IV length, should be done before setting key and IV\n    EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_IVLEN, AES_GCM_IV_LENGTH, NULL);\n\n    // Initialize key and IV\n    EVP_EncryptInit_ex(ctx, NULL, NULL, key, iv);\n\n    // Provide AAD data if present\n    if (aad && aad_len > 0)\n        EVP_EncryptUpdate(ctx, NULL, &len, aad, aad_len);\n\n    // Encrypt the plaintext\n    EVP_EncryptUpdate(ctx, ciphertext, &len, plaintext, plaintext_len);\n    ciphertext_len = len;\n\n    // Finalize encryption\n    EVP_EncryptFinal_ex(ctx, ciphertext + len, &len);\n    ciphertext_len += len;\n\n    // Get the tag\n    EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_GET_TAG, AES_GCM_TAG_LENGTH, tag);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    return ciphertext_len;\n}\n\n// Function to perform AES-GCM decryption\nint decrypt_aes_gcm(const unsigned char *ciphertext, int ciphertext_len,\n                    const unsigned char *aad, int aad_len,\n                    const unsigned char *tag,\n                    const unsigned char *key, unsigned char *iv,\n                    unsigned char *plaintext)\n{\n    EVP_CIPHER_CTX *ctx = EVP_CIPHER_CTX_new();\n    int len;\n    int plaintext_len;\n    int ret;\n\n    // Initialize the decryption operation\n    EVP_DecryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL);\n\n    // Set IV length\n    EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_IVLEN, AES_GCM_IV_LENGTH, NULL);\n\n    // Initialize key and IV\n    EVP_DecryptInit_ex(ctx, NULL, NULL, key, iv);\n\n    // Provide AAD data if present\n    if (aad && aad_len > 0)\n        EVP_DecryptUpdate(ctx, NULL, &len, aad, aad_len);\n\n    // Decrypt the ciphertext\n    EVP_DecryptUpdate(ctx, plaintext, &len, ciphertext, ciphertext_len);\n    plaintext_len = len;\n\n    // Set expected tag value\n    EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_TAG, AES_GCM_TAG_LENGTH, (void *)tag);\n\n    // Finalize decryption\n    ret = EVP_DecryptFinal_ex(ctx, plaintext + len, &len);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    if (ret > 0) {\n        // Successful decryption\n        plaintext_len += len;\n        return plaintext_len;\n    } else {\n        // Decryption failed, probably due to tag mismatch\n        return -1;\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```elixir\ndefmodule DistributedStateServer do\n  use GenServer\n\n  # Public API\n\n  def start_link(initial_state) do\n    GenServer.start_link(__MODULE__, initial_state, name: __MODULE__)\n  end\n\n  def read_state do\n    GenServer.call(__MODULE__, :read_state)\n  end\n\n  def update_state(update_func) do\n    GenServer.call(__MODULE__, {:update_state, update_func})\n  end\n\n  # GenServer Callbacks\n\n  @impl true\n  def init(initial_state) do\n    {:ok, initial_state}\n  end\n\n  @impl true\n  def handle_call(:read_state, _from, state) do\n    {:reply, state, state}\n  end\n\n  @impl true\n  def handle_call({:update_state, update_func}, _from, state) do\n    new_state = update_func.(state)\n    # Notify other nodes about state change\n    broadcast_state_change(new_state)\n    {:reply, :ok, new_state}\n  end\n\n  # Private Functions\n\n  defp broadcast_state_change(new_state) do\n    :global.registered_names()\n    |> Enum.each(fn\n      server when server != __MODULE__ ->\n        send(server, {:state_change, new_state})\n      _ -> :ok\n    end)\n  end\n  \n  @impl true\n  def handle_info({:state_change, new_state}, _current_state) do\n    {:noreply, new_state}\n  end\nend\n\n# Usage Example:\n# Ensure nodes are connected in advance using Node.connect/1\n# Node A\n{:ok, _pid} = DistributedStateServer.start_link(%{})\n\n# Node B\n{:ok, _pid} = DistributedStateServer.start_link(%{})\n\n# Update state\nDistributedStateServer.update_state(fn state ->\n  Map.put(state, :key, \"value\")\nend)\n\n# Read state\nDistributedStateServer.read_state()\n```\n",
    "label": 1
  },
  {
    "codes": "public KeyOperationResult encrypt(String vaultBaseUrl, String keyName, String keyVersion, JsonWebKeyEncryptionAlgorithm algorithm, byte[] value) {\n        return encryptWithServiceResponseAsync(vaultBaseUrl, keyName, keyVersion, algorithm, value).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "```kotlin\n// Base sealed class for domain-specific errors\nsealed class DomainError {\n    // Represents validation errors\n    sealed class ValidationError : DomainError() {\n        object InvalidEmail : ValidationError()\n        object InvalidPassword : ValidationError()\n        object MissingRequiredField : ValidationError()\n    }\n\n    // Represents network-related errors\n    sealed class NetworkError : DomainError() {\n        object NoInternetConnection : NetworkError()\n        object Timeout : NetworkError()\n        object ServerUnavailable : NetworkError()\n    }\n\n    // Represents database-related errors\n    sealed class DatabaseError : DomainError() {\n        object RecordNotFound : DatabaseError()\n        object DuplicateEntry : DatabaseError()\n        object ConnectionFailed : DatabaseError()\n    }\n\n    // Represents business logic errors\n    sealed class BusinessError : DomainError() {\n        object InsufficientFunds : BusinessError()\n        object UnauthorizedAccess : BusinessError()\n        object ResourceLimitExceeded : BusinessError()\n    }\n\n    // Represents unknown or unexpected errors\n    object UnknownError : DomainError()\n}\n```",
    "label": 1
  },
  {
    "codes": "```typescript\ntype RGBTuple = [number, number, number];\n\nfunction isRGBTuple(value: any): value is RGBTuple {\n  // Check if value is an array with exactly 3 elements\n  if (!Array.isArray(value) || value.length !== 3) {\n    return false;\n  }\n\n  // Check if each element is a number within the RGB range (0-255)\n  return value.every(element =>\n    typeof element === 'number' && element >= 0 && element <= 255\n  );\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# CircuitPython NMEA GPS Parser Library\n\nimport time\n\nclass NMEAParser:\n    def __init__(self):\n        self.sentence = \"\"\n        self.data = {}\n        self.valid_sentence = False\n\n    def parse(self, sentence):\n        \"\"\"Parse an NMEA sentence and store the data.\"\"\"\n        self.sentence = sentence.strip()\n        self.valid_sentence = self._validate_checksum()\n        if not self.valid_sentence:\n            return False\n\n        parts = self.sentence.split(',')\n        sentence_type = parts[0][1:]  # Remove the '$' or '!' prefix\n\n        if sentence_type == \"GPGGA\":\n            self._parse_gpgga(parts)\n        elif sentence_type == \"GPRMC\":\n            self._parse_gprmc(parts)\n        # Add more sentence types as needed\n\n        return True\n\n    def _validate_checksum(self):\n        \"\"\"Validate the NMEA sentence checksum.\"\"\"\n        if '*' not in self.sentence:\n            return False\n\n        sentence, checksum = self.sentence.split('*')\n        calculated_checksum = 0\n        for char in sentence[1:]:  # Skip the '$' or '!'\n            calculated_checksum ^= ord(char)\n\n        return hex(calculated_checksum)[2:].upper() == checksum.upper()\n\n    def _parse_gpgga(self, parts):\n        \"\"\"Parse GPGGA sentence.\"\"\"\n        self.data['time'] = parts[1]\n        self.data['latitude'] = self._convert_to_decimal(parts[2], parts[3])\n        self.data['longitude'] = self._convert_to_decimal(parts[4], parts[5])\n        self.data['fix_quality'] = int(parts[6])\n        self.data['satellites'] = int(parts[7])\n        self.data['altitude'] = float(parts[9])\n\n    def _parse_gprmc(self, parts):\n        \"\"\"Parse GPRMC sentence.\"\"\"\n        self.data['time'] = parts[1]\n        self.data['status'] = parts[2]\n        self.data['latitude'] = self._convert_to_decimal(parts[3], parts[4])\n        self.data['longitude'] = self._convert_to_decimal(parts[5], parts[6])\n        self.data['speed'] = float(parts[7])\n        self.data['course'] = float(parts[8])\n        self.data['date'] = parts[9]\n\n    def _convert_to_decimal(self, value, direction):\n        \"\"\"Convert NMEA latitude/longitude to decimal degrees.\"\"\"\n        degrees = float(value[:2]) if len(value) > 2 else float(value[:1])\n        minutes = float(value[2:]) if len(value) > 2 else float(value[1:])\n        decimal = degrees + (minutes / 60.0)\n        if direction in ['S', 'W']:\n            decimal *= -1\n        return decimal\n\n    def get_data(self):\n        \"\"\"Return parsed data.\"\"\"\n        return self.data if self.valid_sentence else None\n```",
    "label": 1
  },
  {
    "codes": "```kotlin\nfun hasCycle(graph: Map<Int, List<Int>>): Boolean {\n    val visited = mutableSetOf<Int>()\n    val recursionStack = mutableSetOf<Int>()\n\n    fun dfs(node: Int): Boolean {\n        if (node in recursionStack) return true\n        if (node in visited) return false\n\n        visited.add(node)\n        recursionStack.add(node)\n\n        for (neighbor in graph[node] ?: emptyList()) {\n            if (dfs(neighbor)) return true\n        }\n\n        recursionStack.remove(node)\n        return false\n    }\n\n    for (node in graph.keys) {\n        if (dfs(node)) return true\n    }\n    return false\n}\n```\n",
    "label": 1
  },
  {
    "codes": "def _add_conversation(self, conversation):\n        \"\"\"\n        Add the conversation and fire the :meth:`on_conversation_added` event.\n\n        :param conversation: The conversation object to add.\n        :type conversation: :class:`~.AbstractConversation`\n\n        The conversation is added to the internal list of conversations which\n        can be queried at :attr:`conversations`. The\n        :meth:`on_conversation_added` event is fired.\n\n        In addition, the :class:`ConversationService` subscribes to the\n        :meth:`~.AbstractConversation.on_exit` event to remove the conversation\n        from the list automatically. There is no need to remove a conversation\n        from the list explicitly.\n        \"\"\"\n        handler = functools.partial(\n            self._handle_conversation_exit,\n            conversation\n        )\n        tokens = []\n\n        def linked_token(signal, handler):\n            return signal, signal.connect(handler)\n\n        tokens.append(linked_token(conversation.on_exit, handler))\n        tokens.append(linked_token(conversation.on_failure, handler))\n        tokens.append(linked_token(conversation.on_message, functools.partial(\n            self.on_message,\n            conversation,\n        )))\n\n        self._conversation_meta[conversation] = (\n            tokens,\n        )\n        self._conversation_map[conversation.jid] = conversation\n        self.on_conversation_added(conversation)",
    "label": 0
  },
  {
    "codes": "```java\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.security.KeyStore;\nimport java.security.cert.Certificate;\nimport java.security.cert.X509Certificate;\n\npublic class KeyStoreWrapper {\n\n    private KeyStore keyStore;\n    private String keyStorePath;\n    private char[] password;\n\n    // Constructor to initialize KeyStore\n    public KeyStoreWrapper(String keyStorePath, char[] password) throws Exception {\n        this.keyStorePath = keyStorePath;\n        this.password = password;\n        this.keyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n\n        // Load existing KeyStore or create a new one\n        try (FileInputStream fis = new FileInputStream(keyStorePath)) {\n            keyStore.load(fis, password);\n        } catch (Exception e) {\n            keyStore.load(null, password); // Create a new KeyStore\n        }\n    }\n\n    // Add a certificate to the KeyStore\n    public void addCertificate(String alias, X509Certificate certificate) throws Exception {\n        keyStore.setCertificateEntry(alias, certificate);\n        saveKeyStore();\n    }\n\n    // Retrieve a certificate from the KeyStore\n    public X509Certificate getCertificate(String alias) throws Exception {\n        Certificate cert = keyStore.getCertificate(alias);\n        if (cert instanceof X509Certificate) {\n            return (X509Certificate) cert;\n        }\n        return null;\n    }\n\n    // Delete a certificate from the KeyStore\n    public void deleteCertificate(String alias) throws Exception {\n        keyStore.deleteEntry(alias);\n        saveKeyStore();\n    }\n\n    // Save the KeyStore to the file\n    private void saveKeyStore() throws Exception {\n        try (FileOutputStream fos = new FileOutputStream(keyStorePath)) {\n            keyStore.store(fos, password);\n        }\n    }\n\n    // List all aliases in the KeyStore\n    public String[] listAliases() throws Exception {\n        return keyStore.aliases().asIterator().toArray(String[]::new);\n    }\n\n    // Check if an alias exists in the KeyStore\n    public boolean containsAlias(String alias) throws Exception {\n        return keyStore.containsAlias(alias);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```powershell\n# Define registry paths to audit\n$registryPaths = @(\n    \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run\",\n    \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\RunOnce\",\n    \"HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run\",\n    \"HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\RunOnce\"\n)\n\n# Function to check permissions\nfunction Check-RegistryPermissions {\n    param (\n        [string]$path\n    )\n    try {\n        # Get ACL for the registry path\n        $acl = Get-Acl -Path $path\n        foreach ($access in $acl.Access) {\n            # Check if permissions are too permissive\n            if ($access.IdentityReference -notmatch \"SYSTEM|Administrators\" -and $access.FileSystemRights -match \"FullControl|Write\") {\n                Write-Host \"Weak permissions found on $path for $($access.IdentityReference)\"\n            }\n        }\n    } catch {\n        Write-Host \"Error accessing $path: $_\"\n    }\n}\n\n# Iterate through each registry path and check permissions\nforeach ($path in $registryPaths) {\n    Check-RegistryPermissions -path $path\n}\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse std::collections::HashMap;\n\n// Function to calculate the nth Fibonacci number using memoization\nfn fibonacci(n: u32, memo: &mut HashMap<u32, u32>) -> u32 {\n    // Check if the value has already been computed\n    if let Some(&result) = memo.get(&n) {\n        return result;\n    }\n\n    // Base cases\n    if n == 0 {\n        return 0;\n    } else if n == 1 {\n        return 1;\n    }\n\n    // Recursive calculation with memoization\n    let result = fibonacci(n - 1, memo) + fibonacci(n - 2, memo);\n\n    // Store the computed result in the memoization map\n    memo.insert(n, result);\n    \n    result\n}\n\nfn main() {\n    let n = 10; // Change this value to compute a different Fibonacci number\n    let mut memo = HashMap::new();\n    let nth_fibonacci = fibonacci(n, &mut memo);\n    println!(\"The {}th Fibonacci number is {}\", n, nth_fibonacci);\n}\n```",
    "label": 1
  },
  {
    "codes": "def xml(self, attribs = None,elements = None, skipchildren = False):\n        \"\"\"See :meth:`AbstractElement.xml`\"\"\"\n        attribs = {}\n        if not self.offset is None:\n            attribs['{' + NSFOLIA + '}offset'] = str(self.offset)\n        if self.parent and self.ref:\n            attribs['{' + NSFOLIA + '}ref'] = self.ref\n\n        #if self.cls != 'current' and not (self.cls == 'original' and any( isinstance(x, Original) for x in self.ancestors() )  ):\n        #    attribs['{' + NSFOLIA + '}class'] = self.cls\n        #else:\n        #    if '{' + NSFOLIA + '}class' in attribs:\n        #        del attribs['{' + NSFOLIA + '}class']\n        #return E.t(self.value, **attribs)\n\n        e = super(TextContent,self).xml(attribs,elements,skipchildren)\n        if '{' + NSFOLIA + '}class' in e.attrib and e.attrib['{' + NSFOLIA + '}class'] == \"current\":\n            #delete 'class=current'\n            del e.attrib['{' + NSFOLIA + '}class']\n\n        return e",
    "label": 0
  },
  {
    "codes": "public JobListFromJobScheduleNextOptions withOcpDate(DateTime ocpDate) {\n        if (ocpDate == null) {\n            this.ocpDate = null;\n        } else {\n            this.ocpDate = new DateTimeRfc1123(ocpDate);\n        }\n        return this;\n    }",
    "label": 0
  },
  {
    "codes": "def send(sms_to, sms_body, **kwargs):\n    \"\"\"\n    Site: http://smsaero.ru/\n    API: http://smsaero.ru/api/\n    \"\"\"\n    headers = {\n        \"User-Agent\": \"DBMail/%s\" % get_version(),\n    }\n\n    kwargs.update({\n        'user': settings.SMSAERO_LOGIN,\n        'password': settings.SMSAERO_MD5_PASSWORD,\n        'from': kwargs.pop('sms_from', settings.SMSAERO_FROM),\n        'to': sms_to.replace('+', ''),\n        'text': from_unicode(sms_body),\n        'answer': 'json',\n    })\n\n    http = HTTPConnection(kwargs.pop(\"api_url\", \"gate.smsaero.ru\"))\n    http.request(\"GET\", \"/send/?\" + urlencode(kwargs), headers=headers)\n    response = http.getresponse()\n\n    if response.status != 200:\n        raise AeroSmsError(response.reason)\n\n    read = response.read().decode(response.headers.get_content_charset())\n    data = json.loads(read)\n\n    status = None\n    if 'result' in data:\n        status = data['result']\n\n    sms_id = None\n    if 'id' in data:\n        sms_id = data['id']\n\n    if sms_id and status == 'accepted':\n        return True\n    return False",
    "label": 0
  },
  {
    "codes": "```zig\nconst std = @import(\"std\");\nconst os = @import(\"std\").os;\nconst nrf = @import(\"nrf52\");\n\npub fn main() void {\n    const state = try configure_gpio();\n    enter_sleep_mode(state);\n}\n\n/// Configures GPIO pins for low-power mode.\nfn configure_gpio() !nrf.low_power_state {\n    var gpio = try nrf.get_gpio_handle();\n\n    // Configure necessary pins to reduce power consumption\n    try gpio.set_pin_mode(13, nrf.pin_mode.OUTPUT, nrf.pin_pull.DISABLED);\n    try gpio.write_pin(13, false);\n\n    try gpio.set_pin_mode(14, nrf.pin_mode.INPUT, nrf.pin_pull.DISABLED);\n\n    return nrf.low_power_state{\n        .gpio_configured = true,\n    };\n}\n\n/// Enters a low-power sleep mode.\nfn enter_sleep_mode(state: nrf.low_power_state) void {\n    if (state.gpio_configured) {\n        // Enter low-power sleep mode\n        nrf.system_off();\n    }\n}\n\nnamespace nrf {\n    pub struct low_power_state {\n        gpio_configured: bool,\n    }\n\n    pub fn get_gpio_handle() !&GPIO {\n        // Simulated function for retrieving a GPIO handle\n        return &GPIO{};\n    }\n\n    pub const pin_mode = enum {\n        INPUT,\n        OUTPUT,\n    };\n\n    pub const pin_pull = enum {\n        ENABLED,\n        DISABLED,\n    };\n\n    pub fn system_off() void {\n        // Placeholder function to enter low-power mode\n    }\n\n    pub struct GPIO {\n        // Mock structure for GPIO operations\n    }\n\n    pub fn (gpio: *GPIO) set_pin_mode(pin: u8, mode: pin_mode, pull: pin_pull) !void {\n        // Mock implementation for setting pin mode\n    }\n\n    pub fn (gpio: *GPIO) write_pin(pin: u8, value: bool) !void {\n        // Mock implementation for writing pin state\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse std::fs::File;\nuse std::io::{BufReader, BufWriter};\nuse std::path::Path;\n\nuse avro_rs::schema::Schema;\nuse avro_rs::types::Value;\nuse avro_rs::Writer;\nuse serde_json::Value as JsonValue;\n\n/// Reads a JSON file and returns its content as a serde_json `Value`.\nfn read_json_file<P: AsRef<Path>>(path: P) -> Result<JsonValue, Box<dyn std::error::Error>> {\n    let file = File::open(path)?;\n    let reader = BufReader::new(file);\n    let json_value: JsonValue = serde_json::from_reader(reader)?;\n    Ok(json_value)\n}\n\n/// Converts a serde_json `Value` to Avro `Value`.\nfn json_to_avro(json_value: &JsonValue) -> Result<Value, Box<dyn std::error::Error>> {\n    match json_value {\n        JsonValue::Null => Ok(Value::Null),\n        JsonValue::Bool(b) => Ok(Value::Boolean(*b)),\n        JsonValue::Number(n) => {\n            if let Some(i) = n.as_i64() {\n                Ok(Value::Long(i))\n            } else if let Some(f) = n.as_f64() {\n                Ok(Value::Double(f))\n            } else {\n                Err(\"Unsupported number type\".into())\n            }\n        }\n        JsonValue::String(s) => Ok(Value::String(s.clone())),\n        JsonValue::Array(vec) => {\n            let avro_array: Result<Vec<Value>, _> = vec.iter().map(json_to_avro).collect();\n            Ok(Value::Array(avro_array?))\n        }\n        JsonValue::Object(map) => {\n            let avro_map: Result<Vec<(String, Value)>, _> =\n                map.iter().map(|(k, v)| Ok((k.clone(), json_to_avro(v)?))).collect();\n            Ok(Value::Record(avro_map?))\n        }\n    }\n}\n\n/// Writes Avro data to a file using a specified schema.\nfn write_avro_file<P: AsRef<Path>>(\n    path: P,\n    schema: &Schema,\n    avro_value: Value,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let file = File::create(path)?;\n    let writer = BufWriter::new(file);\n    let mut avro_writer = Writer::new(schema, writer);\n    avro_writer.append(avro_value)?;\n    avro_writer.flush()?;\n    Ok(())\n}\n\nfn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Specify file paths\n    let json_file_path = \"data.json\";\n    let avro_file_path = \"data.avro\";\n\n    // Read JSON file\n    let json_value = read_json_file(json_file_path)?;\n\n    // Define an Avro schema\n    let raw_schema = r#\"\n    {\n        \"type\": \"record\",\n        \"name\": \"Example\",\n        \"fields\": [\n            {\"name\": \"data\", \"type\": \"string\"}\n        ]\n    }\"#;\n    let schema = Schema::parse_str(raw_schema)?;\n\n    // Convert JSON to Avro\n    let avro_value = json_to_avro(&json_value)?;\n\n    // Write Avro data to a file\n    write_avro_file(avro_file_path, &schema, avro_value)?;\n\n    Ok(())\n}\n```",
    "label": 1
  },
  {
    "codes": "```powershell\n# Module Name: ServiceDependencyManager\n\n<#\n    .SYNOPSIS\n    This module provides functions to manage Windows service dependencies.\n\n    .DESCRIPTION\n    ServiceDependencyManager allows you to add or remove dependencies for Windows services.\n#>\n\n# Exported functions\nExport-ModuleMember -Function Add-ServiceDependency, Remove-ServiceDependency, Get-ServiceDependencies\n\n# Function to add a dependency to a service\nfunction Add-ServiceDependency {\n    [CmdletBinding()]\n    param (\n        [Parameter(Mandatory = $true)]\n        [string]$ServiceName,\n\n        [Parameter(Mandatory = $true)]\n        [string]$Dependency\n    )\n\n    # Retrieve the current dependencies\n    $currentDependencies = Get-Service -Name $ServiceName | Select-Object -ExpandProperty DependentServices\n\n    # Set new dependencies, including the existing ones, if any\n    $newDependencies = $currentDependencies + $Dependency\n\n    # Update service with new dependencies\n    $service = Get-WmiObject -Class Win32_Service -Filter \"Name='$ServiceName'\"\n    $service.Change($null, $null, $null, $newDependencies -join \",\", $null, $null, $null, $null, $null, $null)\n}\n\n# Function to remove a dependency from a service\nfunction Remove-ServiceDependency {\n    [CmdletBinding()]\n    param (\n        [Parameter(Mandatory = $true)]\n        [string]$ServiceName,\n\n        [Parameter(Mandatory = $true)]\n        [string]$Dependency\n    )\n\n    # Retrieve current dependencies\n    $currentDependencies = Get-Service -Name $ServiceName | Select-Object -ExpandProperty DependentServices\n\n    # Remove specified dependency\n    $newDependencies = $currentDependencies | Where-Object { $_ -ne $Dependency }\n\n    # Update service with new dependencies\n    $service = Get-WmiObject -Class Win32_Service -Filter \"Name='$ServiceName'\"\n    $service.Change($null, $null, $null, $newDependencies -join \",\", $null, $null, $null, $null, $null, $null)\n}\n\n# Function to get the dependencies of a service\nfunction Get-ServiceDependencies {\n    [CmdletBinding()]\n    param (\n        [Parameter(Mandatory = $true)]\n        [string]$ServiceName\n    )\n\n    # Retrieve current dependencies\n    $dependencies = Get-Service -Name $ServiceName | Select-Object -ExpandProperty DependentServices\n    return $dependencies\n}\n```\n\nThis PowerShell module allows users to manage the dependencies of Windows services. It includes functions to add, remove, and retrieve dependencies for a specified service.",
    "label": 1
  },
  {
    "codes": "func (c *Glue) GetTablesPagesWithContext(ctx aws.Context, input *GetTablesInput, fn func(*GetTablesOutput, bool) bool, opts ...request.Option) error {\n\tp := request.Pagination{\n\t\tNewRequest: func() (*request.Request, error) {\n\t\t\tvar inCpy *GetTablesInput\n\t\t\tif input != nil {\n\t\t\t\ttmp := *input\n\t\t\t\tinCpy = &tmp\n\t\t\t}\n\t\t\treq, _ := c.GetTablesRequest(inCpy)\n\t\t\treq.SetContext(ctx)\n\t\t\treq.ApplyOptions(opts...)\n\t\t\treturn req, nil\n\t\t},\n\t}\n\n\tcont := true\n\tfor p.Next() && cont {\n\t\tcont = fn(p.Page().(*GetTablesOutput), !p.HasNextPage())\n\t}\n\treturn p.Err()\n}",
    "label": 0
  },
  {
    "codes": "function getDomDocumentPrefix() {\n\tif (getDomDocumentPrefix.prefix)\n\t\treturn getDomDocumentPrefix.prefix;\n\t\n\tvar prefixes = [\"MSXML2\", \"Microsoft\", \"MSXML\", \"MSXML3\"];\n\tvar o;\n\tfor (var i = 0; i < prefixes.length; i++) {\n\t\ttry {\n\t\t\t// try to create the objects\n\t\t\to = new ActiveXObject(prefixes[i] + \".DomDocument\");\n\t\t\treturn getDomDocumentPrefix.prefix = prefixes[i];\n\t\t}\n\t\tcatch (ex) {};\n\t}\n\t\n\tthrow new Error(\"Could not find an installed XML parser\");\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<SasDefinitionBundle> updateSasDefinitionAsync(String vaultBaseUrl, String storageAccountName, String sasDefinitionName, final ServiceCallback<SasDefinitionBundle> serviceCallback) {\n        return ServiceFuture.fromResponse(updateSasDefinitionWithServiceResponseAsync(vaultBaseUrl, storageAccountName, sasDefinitionName), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```cpp\n// DynamicWeatherSystem.h\n#pragma once\n\n#include \"CoreMinimal.h\"\n#include \"GameFramework/Actor.h\"\n#include \"DynamicWeatherSystem.generated.h\"\n\nUCLASS()\nclass YOURGAME_API ADynamicWeatherSystem : public AActor\n{\n    GENERATED_BODY()\n    \npublic:    \n    // Sets default values for this actor's properties\n    ADynamicWeatherSystem();\n\nprotected:\n    // Called when the game starts or when spawned\n    virtual void BeginPlay() override;\n\npublic:    \n    // Called every frame\n    virtual void Tick(float DeltaTime) override;\n\n    // Function to transition weather\n    UFUNCTION(BlueprintCallable, Category = \"Weather\")\n    void TransitionWeather(FName NewWeatherType, float TransitionDuration);\n\nprivate:\n    // Current weather type\n    UPROPERTY(VisibleAnywhere, Category = \"Weather\")\n    FName CurrentWeatherType;\n\n    // Target weather type\n    UPROPERTY(VisibleAnywhere, Category = \"Weather\")\n    FName TargetWeatherType;\n\n    // Transition progress (0 to 1)\n    UPROPERTY(VisibleAnywhere, Category = \"Weather\")\n    float TransitionProgress;\n\n    // Duration of the transition\n    UPROPERTY(EditAnywhere, Category = \"Weather\")\n    float TransitionTime;\n\n    // Function to update weather effects\n    void UpdateWeatherEffects(float DeltaTime);\n};\n\n// DynamicWeatherSystem.cpp\n#include \"DynamicWeatherSystem.h\"\n\n// Sets default values\nADynamicWeatherSystem::ADynamicWeatherSystem()\n{\n    PrimaryActorTick.bCanEverTick = true;\n\n    // Initialize default values\n    CurrentWeatherType = \"Clear\";\n    TargetWeatherType = \"Clear\";\n    TransitionProgress = 0.0f;\n    TransitionTime = 5.0f; // Default transition time\n}\n\n// Called when the game starts or when spawned\nvoid ADynamicWeatherSystem::BeginPlay()\n{\n    Super::BeginPlay();\n}\n\n// Called every frame\nvoid ADynamicWeatherSystem::Tick(float DeltaTime)\n{\n    Super::Tick(DeltaTime);\n\n    // Update weather effects during transition\n    if (TransitionProgress < 1.0f)\n    {\n        TransitionProgress += DeltaTime / TransitionTime;\n        UpdateWeatherEffects(DeltaTime);\n    }\n    else\n    {\n        // Transition complete\n        CurrentWeatherType = TargetWeatherType;\n        TransitionProgress = 0.0f;\n    }\n}\n\n// Function to transition weather\nvoid ADynamicWeatherSystem::TransitionWeather(FName NewWeatherType, float TransitionDuration)\n{\n    TargetWeatherType = NewWeatherType;\n    TransitionTime = TransitionDuration;\n    TransitionProgress = 0.0f;\n}\n\n// Function to update weather effects\nvoid ADynamicWeatherSystem::UpdateWeatherEffects(float DeltaTime)\n{\n    // Implement logic to blend weather effects based on TransitionProgress\n    // Example: Blend sky color, fog density, particle effects, etc.\n    // This is where you would apply the visual and gameplay changes.\n}\n```\n\n### Comments:\n- **TransitionWeather**: Call this function to start a weather transition.\n- **UpdateWeatherEffects**: Implement your weather blending logic here (e.g., sky color, fog, particles).\n- **TransitionProgress**: Tracks the progress of the transition (0 to 1).\n- **CurrentWeatherType** and **TargetWeatherType**: Store the current and target weather states.",
    "label": 1
  },
  {
    "codes": "def idle_connections(self):\n        \"\"\"Return a list of idle connections\n\n        :rtype: list\n\n        \"\"\"\n        return [c for c in self.connections.values()\n                if not c.busy and not c.closed]",
    "label": 0
  },
  {
    "codes": "function checkDeprecatedProps(layerName, propsInstance, deprecatedProps) {\n  if (!propsInstance) {\n    return;\n  }\n\n  for (const name in deprecatedProps) {\n    if (hasOwnProperty(propsInstance, name)) {\n      const nameStr = `${layerName || 'Layer'}: ${name}`;\n\n      for (const newPropName of deprecatedProps[name]) {\n        if (!hasOwnProperty(propsInstance, newPropName)) {\n          propsInstance[newPropName] = propsInstance[name];\n        }\n      }\n\n      log.deprecated(nameStr, deprecatedProps[name].join('/'))();\n    }\n  }\n}",
    "label": 0
  },
  {
    "codes": "function zero_fill_array(n) {\n\tvar o = new Array(n);\n\tfor(var i = 0; i < n; ++i) o[i] = 0;\n\treturn o;\n}",
    "label": 0
  },
  {
    "codes": "function write_BrtBeginTableStyles(cnt, defTableStyle, defPivotStyle) {\n\tvar o = new_buf(4+256*2*4);\n\to.write_shift(4, cnt);\n\twrite_XLNullableWideString(defTableStyle, o);\n\twrite_XLNullableWideString(defPivotStyle, o);\n\treturn o.length > o.l ? o.slice(0, o.l) : o;\n}",
    "label": 0
  },
  {
    "codes": "func (s *Job) SetLastModifiedOn(v time.Time) *Job {\n\ts.LastModifiedOn = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "def settext(self, text, cls='current'):\n        \"\"\"Set the text for this element.\n\n        Arguments:\n            text (str): The text\n            cls (str): The class of the text, defaults to ``current`` (leave this unless you know what you are doing). There may be only one text content element of each class associated with the element.\n        \"\"\"\n        self.replace(TextContent, value=text, cls=cls)",
    "label": 0
  },
  {
    "codes": "def compare_local_file(self, file_key):\n        \"\"\"\n        Compares a local version of a file with what's already published.\n\n        If an update is needed, the file's key is added self.update_list.\n        \"\"\"\n        # Where is the file?\n        file_path = os.path.join(self.build_dir, file_key)\n\n        # If we're in force_publish mode just add it\n        if self.force_publish:\n            self.update_list.append((file_key, file_path))\n            # And quit now\n            return\n\n        # Does it exist in our s3 object list?\n        if file_key in self.s3_obj_dict:\n\n            # Get the md5 stored in Amazon's header\n            s3_md5 = self.s3_obj_dict[file_key].get('ETag').strip('\"').strip(\"'\")\n\n            # If there is a multipart ETag on S3, compare that to our local file after its chunked up.\n            # We are presuming this file was uploaded in multiple parts.\n            if \"-\" in s3_md5:\n                local_md5 = self.get_multipart_md5(file_path)\n            # Other, do it straight for the whole file\n            else:\n                local_md5 = self.get_md5(file_path)\n\n            # If their md5 hexdigests match, do nothing\n            if s3_md5 == local_md5:\n                pass\n            # If they don't match, we want to add it\n            else:\n                logger.debug(\"{} has changed\".format(file_key))\n                self.update_list.append((file_key, file_path))\n\n            # Remove the file from the s3 dict, we don't need it anymore\n            del self.s3_obj_dict[file_key]\n\n        # If the file doesn't exist, queue it for creation\n        else:\n            logger.debug(\"{} has been added\".format(file_key))\n            self.update_list.append((file_key, file_path))",
    "label": 0
  },
  {
    "codes": "```r\n# Load necessary libraries\nlibrary(shiny)\nlibrary(shinydashboard)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Define UI components\nui <- dashboardPage(\n    dashboardHeader(title = \"COVID-19 Vaccination Rates\"),\n    dashboardSidebar(\n        sidebarMenu(\n            menuItem(\"Dashboard\", tabName = \"dashboard\", icon = icon(\"dashboard\")),\n            menuItem(\"Data Table\", tabName = \"data_table\", icon = icon(\"table\"))\n        )\n    ),\n    dashboardBody(\n        tabItems(\n            tabItem(\n                tabName = \"dashboard\",\n                fluidRow(\n                    box(title = \"Select a Country\", status = \"primary\", solidHeader = TRUE,\n                        selectInput(\"country\", \"Country:\", choices = NULL, selected = NULL)\n                    ),\n                    box(title = \"Vaccination Rate Trend\", status = \"primary\", solidHeader = TRUE,\n                        plotOutput(\"countryPlot\", height = \"300px\")\n                    )\n                )\n            ),\n            tabItem(\n                tabName = \"data_table\",\n                fluidRow(\n                    box(title = \"Vaccination Rates Data Table\", status = \"primary\", solidHeader = TRUE,\n                        DTOutput(\"vaccinationTable\")\n                    )\n                )\n            )\n        )\n    )\n)\n\n# Define Server logic\nserver <- function(input, output, session) {\n    # Load and prepare the data\n    vaccination_data <- reactive({\n        # Assumes that a dataset named \"vaccination_data.csv\" is available in the working directory\n        data <- read.csv(\"vaccination_data.csv\")\n        data$date <- as.Date(data$date)\n        updateSelectInput(session, \"country\", choices = unique(data$country))\n        return(data)\n    })\n    \n    # Render plot\n    output$countryPlot <- renderPlot({\n        req(input$country)\n        country_data <- vaccination_data() %>%\n            filter(country == input$country)\n        \n        ggplot(country_data, aes(x = date, y = vaccination_rate)) +\n            geom_line(color = \"blue\") +\n            labs(title = paste(\"Vaccination Rate Trend in\", input$country),\n                 x = \"Date\", y = \"Vaccination Rate (%)\") +\n            theme_minimal()\n    })\n    \n    # Render data table\n    output$vaccinationTable <- renderDT({\n        req(vaccination_data())\n        datatable(vaccination_data(), options = list(pageLength = 10))\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n```\n",
    "label": 1
  },
  {
    "codes": "```csharp\n// Import necessary Polly namespaces\nusing Polly;\nusing Polly.Retry;\nusing System;\nusing System.Net.Http;\n\n// Define policy to handle transient faults\nAsyncRetryPolicy<HttpResponseMessage> transientFaultPolicy = Policy\n    // Define the policy to handle transient faults (e.g., HTTP 408, 500, 502, 503, 504)\n    .HandleResult<HttpResponseMessage>(response => \n        response.StatusCode == System.Net.HttpStatusCode.RequestTimeout ||      // 408\n        response.StatusCode == System.Net.HttpStatusCode.InternalServerError || // 500\n        response.StatusCode == System.Net.HttpStatusCode.BadGateway ||          // 502\n        response.StatusCode == System.Net.HttpStatusCode.ServiceUnavailable ||  // 503\n        response.StatusCode == System.Net.HttpStatusCode.GatewayTimeout)        // 504\n    // Specify the retry strategy: retry 3 times with exponential backoff\n    .WaitAndRetryAsync(\n        retryCount: 3,\n        sleepDurationProvider: attempt => TimeSpan.FromSeconds(Math.Pow(2, attempt)),\n        onRetry: (response, timespan, retryAttempt, context) => \n        {\n            // Log each retry with response status\n            Console.WriteLine($\"Retry {retryAttempt} encountered an error. Waiting {timespan} before next retry.\");\n        });\n\n// Example usage in an asynchronous method\nasync Task<HttpResponseMessage> ExecuteAsyncWithPolicy(HttpClient httpClient, HttpRequestMessage request)\n{\n    return await transientFaultPolicy.ExecuteAsync(() => httpClient.SendAsync(request));\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *ListJobsInput) SetTags(v map[string]*string) *ListJobsInput {\n\ts.Tags = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function(data){\n\t\t\tthis.segments = [];\n\t\t\t//Declare segment class as a chart instance specific class, so it can share props for this instance\n\t\t\tthis.SegmentArc = Chart.Arc.extend({\n\t\t\t\tshowStroke : this.options.segmentShowStroke,\n\t\t\t\tstrokeWidth : this.options.segmentStrokeWidth,\n\t\t\t\tstrokeColor : this.options.segmentStrokeColor,\n\t\t\t\tctx : this.chart.ctx,\n\t\t\t\tinnerRadius : 0,\n\t\t\t\tx : this.chart.width/2,\n\t\t\t\ty : this.chart.height/2\n\t\t\t});\n\t\t\tthis.scale = new Chart.RadialScale({\n\t\t\t\tdisplay: this.options.showScale,\n\t\t\t\tfontStyle: this.options.scaleFontStyle,\n\t\t\t\tfontSize: this.options.scaleFontSize,\n\t\t\t\tfontFamily: this.options.scaleFontFamily,\n\t\t\t\tfontColor: this.options.scaleFontColor,\n\t\t\t\tshowLabels: this.options.scaleShowLabels,\n\t\t\t\tshowLabelBackdrop: this.options.scaleShowLabelBackdrop,\n\t\t\t\tbackdropColor: this.options.scaleBackdropColor,\n\t\t\t\tbackdropPaddingY : this.options.scaleBackdropPaddingY,\n\t\t\t\tbackdropPaddingX: this.options.scaleBackdropPaddingX,\n\t\t\t\tlineWidth: (this.options.scaleShowLine) ? this.options.scaleLineWidth : 0,\n\t\t\t\tlineColor: this.options.scaleLineColor,\n\t\t\t\tlineArc: true,\n\t\t\t\twidth: this.chart.width,\n\t\t\t\theight: this.chart.height,\n\t\t\t\txCenter: this.chart.width/2,\n\t\t\t\tyCenter: this.chart.height/2,\n\t\t\t\tctx : this.chart.ctx,\n\t\t\t\ttemplateString: this.options.scaleLabel,\n\t\t\t\tvaluesCount: data.length\n\t\t\t});\n\n\t\t\tthis.updateScaleRange(data);\n\n\t\t\tthis.scale.update();\n\n\t\t\thelpers.each(data,function(segment,index){\n\t\t\t\tthis.addData(segment,index,true);\n\t\t\t},this);\n\n\t\t\t//Set up tooltip events on the chart\n\t\t\tif (this.options.showTooltips){\n\t\t\t\thelpers.bindEvents(this, this.options.tooltipEvents, function(evt){\n\t\t\t\t\tvar activeSegments = (evt.type !== 'mouseout') ? this.getSegmentsAtEvent(evt) : [];\n\t\t\t\t\thelpers.each(this.segments,function(segment){\n\t\t\t\t\t\tsegment.restore([\"fillColor\"]);\n\t\t\t\t\t});\n\t\t\t\t\thelpers.each(activeSegments,function(activeSegment){\n\t\t\t\t\t\tactiveSegment.fillColor = activeSegment.highlightColor;\n\t\t\t\t\t});\n\t\t\t\t\tthis.showTooltip(activeSegments);\n\t\t\t\t});\n\t\t\t}\n\n\t\t\tthis.render();\n\t\t}",
    "label": 0
  },
  {
    "codes": "func (s *Trail) SetCloudWatchLogsLogGroupArn(v string) *Trail {\n\ts.CloudWatchLogsLogGroupArn = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function parse_CellStyleXF(blob, length, style, opts) {\n\tvar o = {};\n\tvar a = blob.read_shift(4), b = blob.read_shift(4);\n\tvar c = blob.read_shift(4), d = blob.read_shift(2);\n\to.patternType = XLSFillPattern[c >> 26];\n\n\tif(!opts.cellStyles) return o;\n\to.alc = a & 0x07;\n\to.fWrap = (a >> 3) & 0x01;\n\to.alcV = (a >> 4) & 0x07;\n\to.fJustLast = (a >> 7) & 0x01;\n\to.trot = (a >> 8) & 0xFF;\n\to.cIndent = (a >> 16) & 0x0F;\n\to.fShrinkToFit = (a >> 20) & 0x01;\n\to.iReadOrder = (a >> 22) & 0x02;\n\to.fAtrNum = (a >> 26) & 0x01;\n\to.fAtrFnt = (a >> 27) & 0x01;\n\to.fAtrAlc = (a >> 28) & 0x01;\n\to.fAtrBdr = (a >> 29) & 0x01;\n\to.fAtrPat = (a >> 30) & 0x01;\n\to.fAtrProt = (a >> 31) & 0x01;\n\n\to.dgLeft = b & 0x0F;\n\to.dgRight = (b >> 4) & 0x0F;\n\to.dgTop = (b >> 8) & 0x0F;\n\to.dgBottom = (b >> 12) & 0x0F;\n\to.icvLeft = (b >> 16) & 0x7F;\n\to.icvRight = (b >> 23) & 0x7F;\n\to.grbitDiag = (b >> 30) & 0x03;\n\n\to.icvTop = c & 0x7F;\n\to.icvBottom = (c >> 7) & 0x7F;\n\to.icvDiag = (c >> 14) & 0x7F;\n\to.dgDiag = (c >> 21) & 0x0F;\n\n\to.icvFore = d & 0x7F;\n\to.icvBack = (d >> 7) & 0x7F;\n\to.fsxButton = (d >> 14) & 0x01;\n\treturn o;\n}",
    "label": 0
  },
  {
    "codes": "```vue\n<template>\n  <div class=\"stock-chart\">\n    <h2>{{ stockSymbol }} Stock Price Chart</h2>\n    <canvas ref=\"chartCanvas\"></canvas>\n  </div>\n</template>\n\n<script>\nimport { Line } from 'vue-chartjs';\nimport { Chart as ChartJS, Title, Tooltip, Legend, LineElement, PointElement, LinearScale, TimeScale } from 'chart.js';\nimport axios from 'axios';\n\nChartJS.register(Title, Tooltip, Legend, LineElement, PointElement, LinearScale, TimeScale);\n\nexport default {\n  name: 'StockChart',\n  components: {\n    LineChart: Line\n  },\n  props: {\n    stockSymbol: {\n      type: String,\n      required: true\n    }\n  },\n  data() {\n    return {\n      stockData: [],\n      chartData: {\n        labels: [],\n        datasets: [\n          {\n            label: this.stockSymbol,\n            borderColor: '#42A5F5',\n            backgroundColor: '#E3F2FD',\n            data: []\n          }\n        ]\n      }\n    };\n  },\n  mounted() {\n    this.fetchStockData();\n  },\n  methods: {\n    async fetchStockData() {\n      try {\n        const response = await axios.get(`https://api.example.com/stocks/${this.stockSymbol}`);\n        this.stockData = response.data;\n\n        this.stockData.forEach(entry => {\n          this.chartData.labels.push(entry.date);\n          this.chartData.datasets[0].data.push(entry.close);\n        });\n        \n        this.renderChart();\n      } catch (error) {\n        console.error('Error fetching stock data:', error);\n      }\n    },\n    renderChart() {\n      this.$refs.chartCanvas.getContext('2d').clearRect(0, 0, this.$refs.chartCanvas.width, this.$refs.chartCanvas.height);\n      new ChartJS(this.$refs.chartCanvas, {\n        type: 'line',\n        data: this.chartData,\n        options: {\n          responsive: true,\n          scales: {\n            x: {\n              type: 'time',\n              time: {\n                tooltipFormat: 'll',\n                unit: 'day',\n                unitStepSize: 1\n              }\n            },\n            y: {\n              beginAtZero: false\n            }\n          }\n        }\n      });\n    }\n  }\n};\n</script>\n\n<style scoped>\n.stock-chart {\n  max-width: 800px;\n  margin: auto;\n}\n</style>\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.utils import save_image\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import CelebA\nfrom torch.utils.data import DataLoader\n\n# Generator Model\nclass Generator(nn.Module):\n    def __init__(self, noise_dim, img_channels, feature_g):\n        super(Generator, self).__init__()\n        self.net = nn.Sequential(\n            self._block(noise_dim, feature_g * 16, 4, 1, 0),\n            self._block(feature_g * 16, feature_g * 8, 4, 2, 1),\n            self._block(feature_g * 8, feature_g * 4, 4, 2, 1),\n            self._block(feature_g * 4, feature_g * 2, 4, 2, 1),\n            nn.ConvTranspose2d(feature_g * 2, img_channels, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n    \n    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# Discriminator Model\nclass Discriminator(nn.Module):\n    def __init__(self, img_channels, feature_d):\n        super(Discriminator, self).__init__()\n        self.net = nn.Sequential(\n            self._block(img_channels, feature_d, 4, 2, 1, False),\n            self._block(feature_d, feature_d * 2, 4, 2, 1),\n            self._block(feature_d * 2, feature_d * 4, 4, 2, 1),\n            self._block(feature_d * 4, feature_d * 8, 4, 2, 1),\n            nn.Conv2d(feature_d * 8, 1, kernel_size=4, stride=1, padding=0),\n            nn.Sigmoid()\n        )\n    \n    def _block(self, in_channels, out_channels, kernel_size, stride, padding, batch_norm=True):\n        layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)]\n        if batch_norm:\n            layers.append(nn.BatchNorm2d(out_channels))\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n# Hyperparameters\nimg_size = 64\nimg_channels = 3\nfeature_g = 64\nfeature_d = 64\nnoise_dim = 100\nbatch_size = 128\nnum_epochs = 50\nlr = 0.0002\nbeta1 = 0.5\n\n# Data\ntransform = transforms.Compose([\n    transforms.Resize(img_size),\n    transforms.CenterCrop(img_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5] * 3, [0.5] * 3)\n])\ndataset = CelebA(root='data', split='train', transform=transform, download=True)\nloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n\n# Models\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngen = Generator(noise_dim, img_channels, feature_g).to(device)\ndisc = Discriminator(img_channels, feature_d).to(device)\n\n# Loss and Optimizer\ncriterion = nn.BCELoss()\noptimizer_gen = optim.Adam(gen.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizer_disc = optim.Adam(disc.parameters(), lr=lr, betas=(beta1, 0.999))\n\n# Training\nfor epoch in range(num_epochs):\n    for idx, (real, _) in enumerate(loader):\n        real = real.to(device)\n        noise = torch.randn(batch_size, noise_dim, 1, 1).to(device)\n\n        # Train Discriminator\n        fake = gen(noise)\n        disc_real = disc(real).reshape(-1)\n        disc_fake = disc(fake.detach()).reshape(-1)\n        loss_disc = criterion(disc_real, torch.ones_like(disc_real)) + criterion(disc_fake, torch.zeros_like(disc_fake))\n        optimizer_disc.zero_grad()\n        loss_disc.backward()\n        optimizer_disc.step()\n\n        # Train Generator\n        output = disc(fake).reshape(-1)\n        loss_gen = criterion(output, torch.ones_like(output))\n        optimizer_gen.zero_grad()\n        loss_gen.backward()\n        optimizer_gen.step()\n\n        if idx % 100 == 0:\n            print(f\"Epoch [{epoch}/{num_epochs}] Batch {idx}/{len(loader)} \\\n                   Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n\n    fake_images = gen(noise).detach().cpu()\n    save_image(fake_images, f\"outputs/epoch_{epoch}.png\", normalize=True)\n```",
    "label": 1
  },
  {
    "codes": "```rust\nfn levenshtein(a: &str, b: &str) -> usize {\n    let len_a = a.len();\n    let len_b = b.len();\n\n    // Create a 2D vector to store the distances.\n    let mut dist = vec![vec![0; len_b + 1]; len_a + 1];\n\n    // Initialize the distance for the empty substrings.\n    for i in 0..=len_a {\n        dist[i][0] = i;\n    }\n    for j in 0..=len_b {\n        dist[0][j] = j;\n    }\n\n    // Fill the distance vector based on previous computations.\n    for (i, c1) in a.chars().enumerate() {\n        for (j, c2) in b.chars().enumerate() {\n            let cost = if c1 == c2 { 0 } else { 1 };\n            dist[i + 1][j + 1] = *[\n                dist[i][j + 1] + 1,     // Deletion\n                dist[i + 1][j] + 1,     // Insertion\n                dist[i][j] + cost       // Substitution\n            ].iter().min().unwrap();\n        }\n    }\n\n    // Return the calculated Levenshtein distance for the entire strings.\n    dist[len_a][len_b]\n}\n\nfn main() {\n    let str1 = \"kitten\";\n    let str2 = \"sitting\";\n\n    println!(\"The Levenshtein distance is: {}\", levenshtein(str1, str2));\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<DeletedCertificateBundle> getDeletedCertificateAsync(String vaultBaseUrl, String certificateName) {\n        return getDeletedCertificateWithServiceResponseAsync(vaultBaseUrl, certificateName).map(new Func1<ServiceResponse<DeletedCertificateBundle>, DeletedCertificateBundle>() {\n            @Override\n            public DeletedCertificateBundle call(ServiceResponse<DeletedCertificateBundle> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```powershell\n# Define the registry paths to audit\n$registryPaths = @(\n    \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run\",\n    \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\RunOnce\",\n    \"HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run\",\n    \"HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\RunOnce\"\n)\n\n# Function to check ACLs for weak permissions\nfunction Check-RegistryPermissions {\n    param (\n        [string]$path\n    )\n    \n    # Get the ACL for the registry path\n    $acl = Get-Acl -Path $path\n    \n    # Check each access rule in the ACL\n    foreach ($rule in $acl.Access) {\n        # Check if the rule allows weak permissions (e.g., FullControl for Everyone)\n        if ($rule.IdentityReference -eq \"Everyone\" -and $rule.RegistryRights -match \"FullControl\") {\n            Write-Host \"Weak permission found in $path for $($rule.IdentityReference)\"\n        }\n    }\n}\n\n# Iterate through each registry path and check permissions\nforeach ($path in $registryPaths) {\n    if (Test-Path $path) {\n        Check-RegistryPermissions -path $path\n    } else {\n        Write-Host \"Path $path does not exist.\"\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nclass SpaceShooter extends Phaser.Scene {\n  constructor() {\n    super({ key: 'SpaceShooter' });\n  }\n\n  preload() {\n    this.load.image('spaceship', 'assets/spaceship.png');\n    this.load.image('bullet', 'assets/bullet.png');\n    this.load.image('particle', 'assets/particle.png');\n  }\n\n  create() {\n    // Setup spaceship\n    this.spaceship = this.physics.add.sprite(400, 300, 'spaceship');\n    this.spaceship.setCollideWorldBounds(true);\n  \n    // Setup bullets\n    this.bullets = this.physics.add.group({\n      classType: Phaser.Physics.Arcade.Image,\n      defaultKey: 'bullet',\n      maxSize: 10\n    });\n\n    // Setup particle effect for bullet trails\n    this.particles = this.add.particles('particle');\n    this.bulletEmitter = this.particles.createEmitter({\n      speed: 200,\n      scale: { start: 0.5, end: 0 },\n      blendMode: 'ADD'\n    });\n\n    // Setup keyboard input\n    this.cursors = this.input.keyboard.createCursorKeys();\n    this.spaceKey = this.input.keyboard.addKey(Phaser.Input.Keyboard.KeyCodes.SPACE);\n\n    // Timer for shooting\n    this.lastFired = 0;\n  }\n\n  update(time, delta) {\n    // Control spaceship movement\n    if (this.cursors.left.isDown) {\n      this.spaceship.setVelocityX(-200);\n    } else if (this.cursors.right.isDown) {\n      this.spaceship.setVelocityX(200);\n    } else {\n      this.spaceship.setVelocityX(0);\n    }\n\n    if (this.cursors.up.isDown) {\n      this.spaceship.setVelocityY(-200);\n    } else if (this.cursors.down.isDown) {\n      this.spaceship.setVelocityY(200);\n    } else {\n      this.spaceship.setVelocityY(0);\n    }\n\n    // Shooting logic\n    if (this.spaceKey.isDown && time > this.lastFired) {\n      const bullet = this.bullets.get(this.spaceship.x, this.spaceship.y - 20);\n      if (bullet) {\n        bullet.setActive(true);\n        bullet.setVisible(true);\n        bullet.body.velocity.y = -400;\n        this.bulletEmitter.startFollow(bullet);\n        this.lastFired = time + 100; // Fire rate of 100ms\n      }\n    }\n\n    // Recycling bullets\n    this.bullets.children.each(\n      function (bullet) {\n        if (bullet.active && bullet.y < 0) {\n          bullet.setActive(false);\n          bullet.setVisible(false);\n          bullet.body.stop();\n          this.bulletEmitter.removeEmitters();\n        }\n      },\n      this\n    );\n  }\n}\n\nconst config = {\n  type: Phaser.AUTO,\n  width: 800,\n  height: 600,\n  scene: SpaceShooter,\n  physics: {\n    default: 'arcade',\n    arcade: {\n      debug: false\n    }\n  }\n};\n\nconst game = new Phaser.Game(config);\n```\n",
    "label": 1
  },
  {
    "codes": "def delocate_tree_libs(lib_dict, lib_path, root_path):\n    \"\"\" Move needed libraries in `lib_dict` into `lib_path`\n\n    `lib_dict` has keys naming libraries required by the files in the\n    corresponding value.  Call the keys, \"required libs\".  Call the values\n    \"requiring objects\".\n\n    Copy all the required libs to `lib_path`.  Fix up the rpaths and install\n    names in the requiring objects to point to these new copies.\n\n    Exception: required libs within the directory tree pointed to by\n    `root_path` stay where they are, but we modify requiring objects to use\n    relative paths to these libraries.\n\n    Parameters\n    ----------\n    lib_dict : dict\n        Dictionary with (key, value) pairs of (``depended_lib_path``,\n        ``dependings_dict``) (see :func:`libsana.tree_libs`)\n    lib_path : str\n        Path in which to store copies of libs referred to in keys of\n        `lib_dict`.  Assumed to exist\n    root_path : str, optional\n        Root directory of tree analyzed in `lib_dict`.  Any required\n        library within the subtrees of `root_path` does not get copied, but\n        libraries linking to it have links adjusted to use relative path to\n        this library.\n\n    Returns\n    -------\n    copied_libs : dict\n        Filtered `lib_dict` dict containing only the (key, value) pairs from\n        `lib_dict` where the keys are the libraries copied to `lib_path``.\n    \"\"\"\n    copied_libs = {}\n    delocated_libs = set()\n    copied_basenames = set()\n    rp_root_path = realpath(root_path)\n    rp_lib_path = realpath(lib_path)\n    # Test for errors first to avoid getting half-way through changing the tree\n    for required, requirings in lib_dict.items():\n        if required.startswith('@'): # assume @rpath etc are correct\n            # But warn, because likely they are not\n            warnings.warn('Not processing required path {0} because it '\n                          'begins with @'.format(required))\n            continue\n        r_ed_base = basename(required)\n        if relpath(required, rp_root_path).startswith('..'):\n            # Not local, plan to copy\n            if r_ed_base in copied_basenames:\n                raise DelocationError('Already planning to copy library with '\n                                      'same basename as: ' + r_ed_base)\n            if not exists(required):\n                raise DelocationError('library \"{0}\" does not exist'.format(\n                    required))\n            copied_libs[required] = requirings\n            copied_basenames.add(r_ed_base)\n        else: # Is local, plan to set relative loader_path\n            delocated_libs.add(required)\n    # Modify in place now that we've checked for errors\n    for required in copied_libs:\n        shutil.copy(required, lib_path)\n        # Set rpath and install names for this copied library\n        for requiring, orig_install_name in lib_dict[required].items():\n            req_rel = relpath(rp_lib_path, dirname(requiring))\n            set_install_name(requiring, orig_install_name,\n                             '@loader_path/{0}/{1}'.format(\n                                 req_rel, basename(required)))\n    for required in delocated_libs:\n        # Set relative path for local library\n        for requiring, orig_install_name in lib_dict[required].items():\n            req_rel = relpath(required, dirname(requiring))\n            set_install_name(requiring, orig_install_name,\n                             '@loader_path/' + req_rel)\n    return copied_libs",
    "label": 0
  },
  {
    "codes": "```python\nimport hmac\nfrom secrets import compare_digest\n\ndef secure_compare_hmac(digest1: bytes, digest2: bytes, key: bytes) -> bool:\n    \"\"\"\n    Securely compare two HMAC digests in constant time.\n    \n    :param digest1: First HMAC digest to compare.\n    :param digest2: Second HMAC digest to compare.\n    :param key: The secret key used for HMAC generation.\n    :return: True if the digests match, False otherwise.\n    \"\"\"\n    # Generate HMAC for the first digest\n    hmac1 = hmac.new(key, digest1, digestmod='sha256').digest()\n    \n    # Generate HMAC for the second digest\n    hmac2 = hmac.new(key, digest2, digestmod='sha256').digest()\n    \n    # Compare the HMACs in constant time\n    return compare_digest(hmac1, hmac2)\n```",
    "label": 1
  },
  {
    "codes": "```go\n// main.go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"os\"\n    \"time\"\n\n    \"k8s.io/apimachinery/pkg/runtime\"\n    clientgoscheme \"k8s.io/client-go/kubernetes/scheme\"\n    \"sigs.k8s.io/controller-runtime/pkg/client/config\"\n    \"sigs.k8s.io/controller-runtime/pkg/controller\"\n    \"sigs.k8s.io/controller-runtime/pkg/handler\"\n    \"sigs.k8s.io/controller-runtime/pkg/log\"\n    \"sigs.k8s.io/controller-runtime/pkg/log/zap\"\n    \"sigs.k8s.io/controller-runtime/pkg/manager\"\n    \"sigs.k8s.io/controller-runtime/pkg/manager/signals\"\n    \"sigs.k8s.io/controller-runtime/pkg/source\"\n)\n\n// Define your custom resource type\ntype MyCustomResource struct {\n    Spec   MyCustomResourceSpec   `json:\"spec\"`\n    Status MyCustomResourceStatus `json:\"status\"`\n}\n\ntype MyCustomResourceSpec struct {\n    // Add fields here\n    Replicas int `json:\"replicas\"`\n}\n\ntype MyCustomResourceStatus struct {\n    // Add fields here\n    AvailableReplicas int `json:\"availableReplicas\"`\n}\n\nfunc main() {\n    // Set up logging\n    log.SetLogger(zap.New())\n\n    // Create a new manager\n    mgr, err := manager.New(config.GetConfigOrDie(), manager.Options{\n        Scheme: runtime.NewScheme(),\n    })\n    if err != nil {\n        log.Log.Error(err, \"unable to create manager\")\n        os.Exit(1)\n    }\n\n    // Register the custom resource with the scheme\n    if err := clientgoscheme.AddToScheme(mgr.GetScheme()); err != nil {\n        log.Log.Error(err, \"unable to add client-go scheme\")\n        os.Exit(1)\n    }\n\n    // Create a new controller\n    ctrl, err := controller.New(\"mycustomresource-controller\", mgr, controller.Options{\n        Reconciler: &MyCustomResourceReconciler{\n            Client: mgr.GetClient(),\n            Log:    log.Log.WithName(\"mycustomresource-controller\"),\n        },\n    })\n    if err != nil {\n        log.Log.Error(err, \"unable to create controller\")\n        os.Exit(1)\n    }\n\n    // Watch for changes to MyCustomResource\n    if err := ctrl.Watch(&source.Kind{Type: &MyCustomResource{}}, &handler.EnqueueRequestForObject{}); err != nil {\n        log.Log.Error(err, \"unable to watch MyCustomResource\")\n        os.Exit(1)\n    }\n\n    // Start the manager\n    if err := mgr.Start(signals.SetupSignalHandler()); err != nil {\n        log.Log.Error(err, \"unable to start manager\")\n        os.Exit(1)\n    }\n}\n\n// MyCustomResourceReconciler reconciles MyCustomResource objects\ntype MyCustomResourceReconciler struct {\n    client.Client\n    Log logr.Logger\n}\n\n// Reconcile handles the reconciliation logic\nfunc (r *MyCustomResourceReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n    log := r.Log.WithValues(\"mycustomresource\", req.NamespacedName)\n\n    // Fetch the MyCustomResource instance\n    instance := &MyCustomResource{}\n    if err := r.Get(ctx, req.NamespacedName, instance); err != nil {\n        log.Error(err, \"unable to fetch MyCustomResource\")\n        return ctrl.Result{}, client.IgnoreNotFound(err)\n    }\n\n    // Implement your reconciliation logic here\n    log.Info(\"Reconciling MyCustomResource\", \"name\", instance.Name)\n\n    // Example: Update status\n    instance.Status.AvailableReplicas = instance.Spec.Replicas\n    if err := r.Status().Update(ctx, instance); err != nil {\n        log.Error(err, \"unable to update MyCustomResource status\")\n        return ctrl.Result{}, err\n    }\n\n    return ctrl.Result{RequeueAfter: time.Minute}, nil\n}\n```\n\n```go\n// go.mod\nmodule mycustomresource-operator\n\ngo 1.19\n\nrequire (\n    k8s.io/apimachinery v0.24.0\n    k8s.io/client-go v0.24.0\n    sigs.k8s.io/controller-runtime v0.12.0\n)\n```\n\n```go\n// Dockerfile\nFROM golang:1.19 as builder\nWORKDIR /workspace\nCOPY . .\nRUN go mod download\nRUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o manager main.go\n\nFROM alpine:3.14\nWORKDIR /\nCOPY --from=builder /workspace/manager .\nENTRYPOINT [\"/manager\"]\n```",
    "label": 1
  },
  {
    "codes": "function recordRoute(call, callback) {\n  var point_count = 0;\n  var feature_count = 0;\n  var distance = 0;\n  var previous = null;\n  // Start a timer\n  var start_time = process.hrtime();\n  call.on('data', function(point) {\n    point_count += 1;\n    if (checkFeature(point).name !== '') {\n      feature_count += 1;\n    }\n    /* For each point after the first, add the incremental distance from the\n     * previous point to the total distance value */\n    if (previous != null) {\n      distance += getDistance(previous, point);\n    }\n    previous = point;\n  });\n  call.on('end', function() {\n    callback(null, {\n      point_count: point_count,\n      feature_count: feature_count,\n      // Cast the distance to an integer\n      distance: distance|0,\n      // End the timer\n      elapsed_time: process.hrtime(start_time)[0]\n    });\n  });\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<LongTermRetentionBackupInner> getAsync(String locationName, String longTermRetentionServerName, String longTermRetentionDatabaseName, String backupName, final ServiceCallback<LongTermRetentionBackupInner> serviceCallback) {\n        return ServiceFuture.fromResponse(getWithServiceResponseAsync(locationName, longTermRetentionServerName, longTermRetentionDatabaseName, backupName), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "function listCacheDelete(key) {\n  var data = this.__data__,\n    index = assocIndexOf(data, key)\n\n  if (index < 0) {\n    return false\n  }\n  var lastIndex = data.length - 1\n  if (index == lastIndex) {\n    data.pop()\n  } else {\n    splice.call(data, index, 1)\n  }\n  return true\n}",
    "label": 0
  },
  {
    "codes": "def starttls(self, ssl_context, post_handshake_callback=None):\n        \"\"\"\n        Start TLS on the transport and wait for it to complete.\n\n        The `ssl_context` and `post_handshake_callback` arguments are forwarded\n        to the transports\n        :meth:`aioopenssl.STARTTLSTransport.starttls` coroutine method.\n\n        If the transport does not support starttls, :class:`RuntimeError` is\n        raised; support for starttls can be discovered by querying\n        :meth:`can_starttls`.\n\n        After :meth:`starttls` returns, you must call :meth:`reset`. Any other\n        method may fail in interesting ways as the internal state is discarded\n        when starttls succeeds, for security reasons. :meth:`reset` re-creates\n        the internal structures.\n        \"\"\"\n        self._require_connection()\n        if not self.can_starttls():\n            raise RuntimeError(\"starttls not available on transport\")\n\n        yield from self._transport.starttls(ssl_context,\n                                            post_handshake_callback)\n        self._reset_state()",
    "label": 0
  },
  {
    "codes": "func (s *DataRetrievalPolicy) SetRules(v []*DataRetrievalRule) *DataRetrievalPolicy {\n\ts.Rules = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (c *Glue) BatchGetCrawlersWithContext(ctx aws.Context, input *BatchGetCrawlersInput, opts ...request.Option) (*BatchGetCrawlersOutput, error) {\n\treq, out := c.BatchGetCrawlersRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```rust\nuse wasm_bindgen::prelude::*;\nuse onnxruntime::{environment::Environment, session::Session};\n\n#[wasm_bindgen]\npub fn run_onnx_model(model_data: &[u8], input_data: &[f32]) -> Vec<f32> {\n    // Create an onnxruntime environment\n    let environment = Environment::builder()\n        .with_name(\"onnx-wasm-env\")\n        .build()\n        .expect(\"Failed to create ONNX environment\");\n\n    // Create a session for the provided model data\n    let session = Session::from_bytes(&environment, model_data)\n        .expect(\"Failed to create ONNX session\");\n\n    // Prepare input (assuming single input and output for simplicity)\n    let input_tensor = session\n        .new_tensor_standard_layout(input_data)\n        .expect(\"Failed to create input tensor\");\n\n    // Run the session\n    let outputs: Vec<Vec<f32>> = session.run(vec![input_tensor])\n        .expect(\"Failed to run the model\");\n\n    // Assuming single output, extract the results\n    outputs.into_iter().next().unwrap_or_else(Vec::new)\n}\n```\n\n```toml\n# Add this to your Cargo.toml\n[lib]\ncrate-type = [\"cdylib\"]\n\n[dependencies]\nonnxruntime = \"0.10\"\nwasm-bindgen = \"0.2\"\n```\n\n```sh\n# Instructions to build the WebAssembly module\nwasm-pack build --target web\n```",
    "label": 1
  },
  {
    "codes": "```java\n// MainActivity.java\npackage com.example.objectdetection;\n\nimport android.Manifest;\nimport android.content.pm.PackageManager;\nimport android.graphics.Bitmap;\nimport android.graphics.BitmapFactory;\nimport android.graphics.Canvas;\nimport android.graphics.Color;\nimport android.graphics.Matrix;\nimport android.graphics.Paint;\nimport android.graphics.RectF;\nimport android.hardware.Camera;\nimport android.os.Bundle;\nimport android.util.Log;\nimport android.util.Size;\nimport android.view.SurfaceHolder;\nimport android.view.SurfaceView;\nimport android.widget.Toast;\n\nimport androidx.annotation.NonNull;\nimport androidx.appcompat.app.AppCompatActivity;\nimport androidx.core.app.ActivityCompat;\nimport androidx.core.content.ContextCompat;\n\nimport org.tensorflow.lite.Interpreter;\nimport org.tensorflow.lite.support.common.FileUtil;\nimport org.tensorflow.lite.support.image.TensorImage;\nimport org.tensorflow.lite.support.label.Category;\nimport org.tensorflow.lite.task.vision.detector.Detection;\nimport org.tensorflow.lite.task.vision.detector.ObjectDetector;\n\nimport java.io.IOException;\nimport java.nio.MappedByteBuffer;\nimport java.util.List;\n\npublic class MainActivity extends AppCompatActivity implements SurfaceHolder.Callback, Camera.PreviewCallback {\n\n    private static final int CAMERA_REQUEST_CODE = 100;\n    private static final String MODEL_FILE = \"model.tflite\";\n    private static final String LABEL_FILE = \"labels.txt\";\n\n    private SurfaceView surfaceView;\n    private SurfaceHolder surfaceHolder;\n    private Camera camera;\n    private Interpreter interpreter;\n    private ObjectDetector objectDetector;\n    private List<String> labels;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        surfaceView = findViewById(R.id.surfaceView);\n        surfaceHolder = surfaceView.getHolder();\n        surfaceHolder.addCallback(this);\n\n        // Check and request camera permissions\n        if (ContextCompat.checkSelfPermission(this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {\n            ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.CAMERA}, CAMERA_REQUEST_CODE);\n        }\n\n        // Load TensorFlow Lite model and labels\n        try {\n            MappedByteBuffer modelBuffer = FileUtil.loadMappedFile(this, MODEL_FILE);\n            interpreter = new Interpreter(modelBuffer);\n            labels = FileUtil.loadLabels(this, LABEL_FILE);\n            objectDetector = ObjectDetector.createFromBufferAndOptions(modelBuffer, ObjectDetector.ObjectDetectorOptions.builder().build());\n        } catch (IOException e) {\n            Log.e(\"TFLite\", \"Error loading model or labels\", e);\n            Toast.makeText(this, \"Error loading model or labels\", Toast.LENGTH_SHORT).show();\n        }\n    }\n\n    @Override\n    public void surfaceCreated(@NonNull SurfaceHolder holder) {\n        // Open the camera and start preview\n        camera = Camera.open();\n        try {\n            camera.setPreviewDisplay(holder);\n            camera.setPreviewCallback(this);\n            camera.startPreview();\n        } catch (IOException e) {\n            Log.e(\"Camera\", \"Error setting camera preview\", e);\n        }\n    }\n\n    @Override\n    public void surfaceChanged(@NonNull SurfaceHolder holder, int format, int width, int height) {\n        // Handle surface changes (e.g., rotation)\n        if (surfaceHolder.getSurface() == null) return;\n        camera.stopPreview();\n        try {\n            camera.setPreviewDisplay(surfaceHolder);\n            camera.setPreviewCallback(this);\n            camera.startPreview();\n        } catch (IOException e) {\n            Log.e(\"Camera\", \"Error restarting camera preview\", e);\n        }\n    }\n\n    @Override\n    public void surfaceDestroyed(@NonNull SurfaceHolder holder) {\n        // Release the camera\n        camera.setPreviewCallback(null);\n        camera.stopPreview();\n        camera.release();\n        camera = null;\n    }\n\n    @Override\n    public void onPreviewFrame(byte[] data, Camera camera) {\n        // Convert preview frame to Bitmap\n        Camera.Size previewSize = camera.getParameters().getPreviewSize();\n        Bitmap bitmap = Bitmap.createBitmap(previewSize.width, previewSize.height, Bitmap.Config.ARGB_8888);\n        bitmap.copyPixelsFromBuffer(java.nio.ByteBuffer.wrap(data));\n\n        // Preprocess the bitmap (resize, normalize, etc.)\n        Bitmap resizedBitmap = Bitmap.createScaledBitmap(bitmap, 300, 300, true);\n        TensorImage tensorImage = TensorImage.fromBitmap(resizedBitmap);\n\n        // Run object detection\n        List<Detection> detections = objectDetector.detect(tensorImage);\n\n        // Draw bounding boxes and labels on the bitmap\n        Canvas canvas = new Canvas(bitmap);\n        Paint paint = new Paint();\n        paint.setColor(Color.RED);\n        paint.setStyle(Paint.Style.STROKE);\n        paint.setStrokeWidth(2.0f);\n\n        for (Detection detection : detections) {\n            RectF boundingBox = detection.getBoundingBox();\n            canvas.drawRect(boundingBox, paint);\n            canvas.drawText(labels.get(detection.getCategories().get(0).getIndex()), boundingBox.left, boundingBox.top, paint);\n        }\n\n        // Display the annotated bitmap on the SurfaceView\n        surfaceHolder.lockCanvas().drawBitmap(bitmap, new Matrix(), null);\n        surfaceHolder.unlockCanvasAndPost(surfaceHolder.getCanvas());\n    }\n\n    @Override\n    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults);\n        if (requestCode == CAMERA_REQUEST_CODE) {\n            if (grantResults.length > 0 && grantResults[0] == PackageManager.PERMISSION_GRANTED) {\n                // Permission granted, restart camera preview\n                surfaceCreated(surfaceHolder);\n            } else {\n                Toast.makeText(this, \"Camera permission denied\", Toast.LENGTH_SHORT).show();\n            }\n        }\n    }\n}\n```\n\n```xml\n<!-- activity_main.xml -->\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n    <SurfaceView\n        android:id=\"@+id/surfaceView\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\" />\n</RelativeLayout>\n```\n\n```xml\n<!-- AndroidManifest.xml -->\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.example.objectdetection\">\n\n    <uses-permission android:name=\"android.permission.CAMERA\" />\n\n    <application\n        android:allowBackup=\"true\"\n        android:icon=\"@mipmap/ic_launcher\"\n        android:label=\"@string/app_name\"\n        android:roundIcon=\"@mipmap/ic_launcher_round\"\n        android:supportsRtl=\"true\"\n        android:theme=\"@style/Theme.ObjectDetection\">\n        <activity android:name=\".MainActivity\">\n            <intent-filter>\n                <action android:name=\"android.intent.action.MAIN\" />\n                <category android:name=\"android.intent.category.LAUNCHER\" />\n            </intent-filter>\n        </activity>\n    </application>\n</manifest>\n```\n\n```gradle\n// build.gradle (Module: app)\napply plugin: 'com.android.application'\n\nandroid {\n    compileSdkVersion 30\n    defaultConfig {\n        applicationId \"com.example.objectdetection\"\n        minSdkVersion 21\n        targetSdkVersion 30\n        versionCode 1\n        versionName \"1.0\"\n    }\n    buildTypes {\n        release {\n            minifyEnabled false\n            proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'\n        }\n    }\n    aaptOptions {\n        noCompress \"tflite\"\n    }\n}\n\ndependencies {\n    implementation 'org.tensorflow:tensorflow-lite:2.8.0'\n    implementation 'org.tensorflow:tensorflow-lite-support:0.3.1'\n    implementation 'org.tensorflow:tensorflow-lite-task-vision:0.3.1'\n    implementation 'androidx.appcompat:appcompat:1.3.1'\n    implementation 'androidx.core:core-ktx:1.6.0'\n    implementation 'com.google.android.material:material:1.4.0'\n    implementation 'androidx.constraintlayout:constraintlayout:2.1.0'\n}\n```\n\n```txt\n# labels.txt\nperson\ncar\ndog\ncat\n```\n\n```txt\n# proguard-rules.pro\n-keep class org.tensorflow.lite.** { *; }\n```\n\n```txt\n# model.tflite\n# Place your TensorFlow Lite model file in the assets folder\n```",
    "label": 1
  },
  {
    "codes": "def clip(self, lower=0, upper=127):\n        \"\"\"\n        Clip the pianoroll by the given lower and upper bounds.\n\n        Parameters\n        ----------\n        lower : int or float\n            The lower bound to clip the pianoroll. Defaults to 0.\n        upper : int or float\n            The upper bound to clip the pianoroll. Defaults to 127.\n\n        \"\"\"\n        self.pianoroll = self.pianoroll.clip(lower, upper)",
    "label": 0
  },
  {
    "codes": "public DenyAssignmentInner get(String scope, String denyAssignmentId) {\n        return getWithServiceResponseAsync(scope, denyAssignmentId).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\n\n// Sample Product class\npublic class Product\n{\n    public string Name { get; set; }\n    public decimal Price { get; set; }\n    public string Category { get; set; }\n}\n\npublic class Program\n{\n    public static void Main()\n    {\n        // Sample data\n        var products = new List<Product>\n        {\n            new Product { Name = \"Laptop\", Price = 1200, Category = \"Electronics\" },\n            new Product { Name = \"Smartphone\", Price = 800, Category = \"Electronics\" },\n            new Product { Name = \"Desk\", Price = 200, Category = \"Furniture\" },\n            new Product { Name = \"Chair\", Price = 150, Category = \"Furniture\" },\n            new Product { Name = \"Headphones\", Price = 100, Category = \"Electronics\" }\n        };\n\n        // Define the price range and category\n        decimal minPrice = 100;\n        decimal maxPrice = 1000;\n        string category = \"Electronics\";\n\n        // LINQ query to filter products by price range and category\n        var filteredProducts = from p in products\n                               where p.Price >= minPrice && p.Price <= maxPrice\n                               && p.Category == category\n                               select p;\n\n        // Display the filtered products\n        foreach (var product in filteredProducts)\n        {\n            Console.WriteLine($\"{product.Name}: {product.Price:C} - {product.Category}\");\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *ListMultipartUploadsInput) SetMarker(v string) *ListMultipartUploadsInput {\n\ts.Marker = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport static org.mockito.Mockito.*;\nimport static org.junit.jupiter.api.Assertions.*;\nimport org.junit.jupiter.api.Test;\nimport org.mockito.InjectMocks;\nimport org.mockito.Mock;\nimport org.mockito.MockitoAnnotations;\nimport org.springframework.transaction.PlatformTransactionManager;\nimport org.springframework.transaction.TransactionStatus;\nimport org.springframework.transaction.support.DefaultTransactionDefinition;\nimport org.springframework.transaction.support.TransactionTemplate;\n\npublic class TransactionRollbackTest {\n\n    @Mock\n    private PlatformTransactionManager transactionManager;\n\n    @InjectMocks\n    private YourServiceClass service; // Replace with your actual service class\n    \n    public TransactionRollbackTest() {\n        MockitoAnnotations.openMocks(this);\n    }\n\n    @Test\n    public void testTransactionRollback() {\n        TransactionTemplate transactionTemplate = new TransactionTemplate(transactionManager);\n        TransactionStatus transactionStatus = mock(TransactionStatus.class);\n        when(transactionManager.getTransaction(any(DefaultTransactionDefinition.class))).thenReturn(transactionStatus);\n\n        doThrow(new RuntimeException(\"Database Error\")).when(service).yourMethodThatThrowsException();\n\n        assertThrows(RuntimeException.class, () -> {\n            service.methodUnderTest();\n        });\n\n        verify(transactionManager).rollback(transactionStatus);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "async function commitVersionBump (version) {\n  const gitDir = path.resolve(__dirname, '..')\n  const gitArgs = ['commit', '-a', '-m', `Bump v${version}`, '-n']\n  await GitProcess.exec(gitArgs, gitDir)\n}",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define the directory where logs are stored and the archive directory\nLOG_DIR=\"/path/to/logs\"\nARCHIVE_DIR=\"/path/to/archive\"\n\n# Find and archive log files older than 30 days\nfind \"$LOG_DIR\" -type f -name \"*.log\" -mtime +30 -exec tar -rvf \"$ARCHIVE_DIR/archived_logs.tar\" {} +\n\n# Remove files after they have been archived\nfind \"$LOG_DIR\" -type f -name \"*.log\" -mtime +30 -exec rm {} +\n\n# Print a message to indicate the completion of the archiving process\necho \"Archiving of log files older than 30 days has been completed.\"\n```\n",
    "label": 1
  },
  {
    "codes": "function(chart, e, options) {\n\t\t\tvar position = getRelativePosition(e, chart);\n\t\t\toptions.axis = options.axis || 'xy';\n\t\t\tvar distanceMetric = getDistanceMetricForAxis(options.axis);\n\t\t\tvar items = options.intersect ? getIntersectItems(chart, position) : getNearestItems(chart, position, false, distanceMetric);\n\n\t\t\tif (items.length > 0) {\n\t\t\t\titems = chart.getDatasetMeta(items[0]._datasetIndex).data;\n\t\t\t}\n\n\t\t\treturn items;\n\t\t}",
    "label": 0
  },
  {
    "codes": "```python\nimport time\n\ndef time_execution_ns(func):\n    def wrapper(*args, **kwargs):\n        start_time = time.perf_counter_ns()  # Start time in nanoseconds\n        result = func(*args, **kwargs)\n        end_time = time.perf_counter_ns()  # End time in nanoseconds\n        execution_time = end_time - start_time\n        print(f\"Function '{func.__name__}' executed in {execution_time} nanoseconds.\")\n        return result\n    return wrapper\n\n# Example usage\n@time_execution_ns\ndef example_function():\n    # Simulate some processing\n    sum(range(1000))\n\n# Call the function\nexample_function()\n```\n",
    "label": 1
  },
  {
    "codes": "```kotlin\nsealed class DomainError {\n    // 400 - Bad Request\n    data class InvalidInput(val message: String) : DomainError()\n\n    // 401 - Unauthorized\n    data class Unauthorized(val reason: String) : DomainError()\n\n    // 403 - Forbidden\n    data class Forbidden(val resource: String) : DomainError()\n\n    // 404 - Not Found\n    data class NotFound(val resource: String) : DomainError()\n\n    // 409 - Conflict\n    data class Conflict(val details: String) : DomainError()\n\n    // 500 - Internal Server Error\n    data class InternalServerError(val error: Throwable) : DomainError()\n\n    // 503 - Service Unavailable\n    data class ServiceUnavailable(val retryAfter: Long) : DomainError()\n}\n```",
    "label": 1
  },
  {
    "codes": "async function createModulePackages({ from, to }) {\n  const directoryPackages = glob.sync('*/index.js', { cwd: from }).map(path.dirname);\n\n  await Promise.all(\n    directoryPackages.map(async directoryPackage => {\n      const packageJson = {\n        sideEffects: false,\n        module: path.join('../esm', directoryPackage, 'index.js'),\n        typings: './index.d.ts',\n      };\n      const packageJsonPath = path.join(to, directoryPackage, 'package.json');\n\n      const [typingsExist] = await Promise.all([\n        fse.exists(path.join(to, directoryPackage, 'index.d.ts')),\n        fse.writeFile(packageJsonPath, JSON.stringify(packageJson, null, 2)),\n      ]);\n\n      if (!typingsExist) {\n        throw new Error(`index.d.ts for ${directoryPackage} is missing`);\n      }\n\n      return packageJsonPath;\n    }),\n  );\n}",
    "label": 0
  },
  {
    "codes": "```swift\n// Define the puzzle piece node\nlet puzzlePiece = SKSpriteNode(imageNamed: \"puzzlePiece\")\n\n// Set initial position and properties\npuzzlePiece.position = CGPoint(x: 100, y: 100)\npuzzlePiece.zPosition = 1\n\n// Create actions for the puzzle piece animation\nlet moveAction = SKAction.move(to: CGPoint(x: 300, y: 300), duration: 1.0)\nlet rotateAction = SKAction.rotate(byAngle: .pi / 2, duration: 0.5)\nlet scaleUpAction = SKAction.scale(to: 1.2, duration: 0.3)\nlet scaleDownAction = SKAction.scale(to: 1.0, duration: 0.3)\nlet fadeOutAction = SKAction.fadeOut(withDuration: 0.5)\nlet fadeInAction = SKAction.fadeIn(withDuration: 0.5)\n\n// Combine actions into a sequence\nlet scaleSequence = SKAction.sequence([scaleUpAction, scaleDownAction])\nlet fadeSequence = SKAction.sequence([fadeOutAction, fadeInAction])\nlet combinedSequence = SKAction.sequence([moveAction, rotateAction, scaleSequence, fadeSequence])\n\n// Run the sequence on the puzzle piece\npuzzlePiece.run(combinedSequence)\n```",
    "label": 1
  },
  {
    "codes": "```ruby\n# Define a simple Fiber-based scheduler\nclass FiberScheduler\n  def initialize\n    @fibers = [] # Queue to hold fibers\n  end\n\n  # Add a fiber to the scheduler\n  def schedule(&block)\n    fiber = Fiber.new(&block)\n    @fibers << fiber\n    fiber.resume # Start the fiber immediately\n  end\n\n  # Run the scheduler until all fibers are done\n  def run\n    while !@fibers.empty?\n      fiber = @fibers.shift # Get the next fiber\n      if fiber.alive?\n        fiber.resume # Resume the fiber\n        @fibers << fiber # Requeue if still alive\n      end\n    end\n  end\nend\n\n# Example usage\nscheduler = FiberScheduler.new\n\n# Schedule a task\nscheduler.schedule do\n  puts \"Task 1: Starting\"\n  Fiber.yield # Pause execution\n  puts \"Task 1: Resuming\"\nend\n\n# Schedule another task\nscheduler.schedule do\n  puts \"Task 2: Starting\"\n  Fiber.yield # Pause execution\n  puts \"Task 2: Resuming\"\nend\n\n# Run the scheduler\nscheduler.run\n```",
    "label": 1
  },
  {
    "codes": "```go\n// main.go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"time\"\n\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\tclientgoscheme \"k8s.io/client-go/kubernetes/scheme\"\n\t_ \"k8s.io/client-go/plugin/pkg/client/auth/gcp\"\n\tctrl \"sigs.k8s.io/controller-runtime\"\n\t\"sigs.k8s.io/controller-runtime/pkg/log/zap\"\n\n\tmygroupv1alpha1 \"github.com/example/my-operator/api/v1alpha1\"\n\t\"github.com/example/my-operator/controllers\"\n)\n\nvar (\n\tscheme   = runtime.NewScheme()\n\tsetupLog = ctrl.Log.WithName(\"setup\")\n)\n\nfunc init() {\n\t_ = clientgoscheme.AddToScheme(scheme)\n\t_ = mygroupv1alpha1.AddToScheme(scheme)\n}\n\nfunc main() {\n\tctrl.SetLogger(zap.New(zap.UseDevMode(true)))\n\n\tmgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{\n\t\tScheme:             scheme,\n\t\tMetricsBindAddress: \"0\",\n\t\tPort:               9443,\n\t\tLeaderElection:     false,\n\t\tLeaderElectionID:   \"my-operator-lock\",\n\t})\n\tif err != nil {\n\t\tsetupLog.Error(err, \"unable to start manager\")\n\t\tos.Exit(1)\n\t}\n\n\tif err = (&controllers.MyResourceReconciler{\n\t\tClient: mgr.GetClient(),\n\t\tLog:    ctrl.Log.WithName(\"controllers\").WithName(\"MyResource\"),\n\t\tScheme: mgr.GetScheme(),\n\t}).SetupWithManager(mgr); err != nil {\n\t\tsetupLog.Error(err, \"unable to create controller\", \"controller\", \"MyResource\")\n\t\tos.Exit(1)\n\t}\n\n\tsetupLog.Info(\"starting manager\")\n\tif err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {\n\t\tsetupLog.Error(err, \"problem running manager\")\n\t\tos.Exit(1)\n\t}\n}\n\n// controllers/myresource_controller.go\npackage controllers\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/go-logr/logr\"\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\tctrl \"sigs.k8s.io/controller-runtime\"\n\t\"sigs.k8s.io/controller-runtime/pkg/client\"\n\n\tmygroupv1alpha1 \"github.com/example/my-operator/api/v1alpha1\"\n)\n\ntype MyResourceReconciler struct {\n\tclient.Client\n\tLog    logr.Logger\n\tScheme *runtime.Scheme\n}\n\nfunc (r *MyResourceReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n\tlog := r.Log.WithValues(\"myresource\", req.NamespacedName)\n\n\tvar myResource mygroupv1alpha1.MyResource\n\tif err := r.Get(ctx, req.NamespacedName, &myResource); err != nil {\n\t\tlog.Error(err, \"unable to fetch MyResource\")\n\t\treturn ctrl.Result{}, client.IgnoreNotFound(err)\n\t}\n\n\t// Your custom logic here\n\tlog.Info(\"Reconciling MyResource\", \"name\", myResource.Name)\n\n\t// Example: Update status\n\tmyResource.Status.LastUpdated = time.Now().Format(time.RFC3339)\n\tif err := r.Status().Update(ctx, &myResource); err != nil {\n\t\tlog.Error(err, \"unable to update MyResource status\")\n\t\treturn ctrl.Result{}, err\n\t}\n\n\treturn ctrl.Result{RequeueAfter: time.Minute}, nil\n}\n\nfunc (r *MyResourceReconciler) SetupWithManager(mgr ctrl.Manager) error {\n\treturn ctrl.NewControllerManagedBy(mgr).\n\t\tFor(&mygroupv1alpha1.MyResource{}).\n\t\tComplete(r)\n}\n\n// api/v1alpha1/myresource_types.go\npackage v1alpha1\n\nimport (\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n)\n\ntype MyResourceSpec struct {\n\t// Add your spec fields here\n\tMessage string `json:\"message\"`\n}\n\ntype MyResourceStatus struct {\n\t// Add your status fields here\n\tLastUpdated string `json:\"lastUpdated\"`\n}\n\n// +kubebuilder:object:root=true\n// +kubebuilder:subresource:status\n\ntype MyResource struct {\n\tmetav1.TypeMeta   `json:\",inline\"`\n\tmetav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n\tSpec   MyResourceSpec   `json:\"spec,omitempty\"`\n\tStatus MyResourceStatus `json:\"status,omitempty\"`\n}\n\n// +kubebuilder:object:root=true\n\ntype MyResourceList struct {\n\tmetav1.TypeMeta `json:\",inline\"`\n\tmetav1.ListMeta `json:\"metadata,omitempty\"`\n\tItems           []MyResource `json:\"items\"`\n}\n\nfunc init() {\n\tSchemeBuilder.Register(&MyResource{}, &MyResourceList{})\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\n// Import required modules\nconst jwt = require('jsonwebtoken');\nconst { SECRET_KEY } = require('./config'); // Secret key for JWT\n\n// Middleware to authenticate users using JWT\nconst authenticateJWT = (req, res, next) => {\n    // Extract token from the Authorization header\n    const authHeader = req.headers.authorization;\n\n    // Check if the Authorization header exists and starts with 'Bearer '\n    if (authHeader && authHeader.startsWith('Bearer ')) {\n        const token = authHeader.split(' ')[1]; // Extract the token\n\n        // Verify the token\n        jwt.verify(token, SECRET_KEY, (err, user) => {\n            if (err) {\n                // Token is invalid or expired\n                return res.status(403).json({ message: 'Invalid or expired token' });\n            }\n\n            // Attach the decoded user payload to the request object\n            req.user = user;\n            next(); // Proceed to the next middleware or route handler\n        });\n    } else {\n        // No token provided\n        res.status(401).json({ message: 'No token provided' });\n    }\n};\n\nmodule.exports = authenticateJWT;\n```",
    "label": 1
  },
  {
    "codes": "```javascript\n// Web Worker Pool for CPU-intensive image processing\n\n// Worker pool class\nclass WorkerPool {\n  constructor(poolSize, workerScript) {\n    this.poolSize = poolSize;\n    this.workerScript = workerScript;\n    this.workers = [];\n    this.tasks = [];\n    this.init();\n  }\n\n  // Initialize the worker pool\n  init() {\n    for (let i = 0; i < this.poolSize; i++) {\n      const worker = new Worker(this.workerScript);\n      worker.onmessage = (event) => this.handleWorkerResponse(worker, event);\n      this.workers.push(worker);\n    }\n  }\n\n  // Handle worker response and assign next task\n  handleWorkerResponse(worker, event) {\n    const { taskId, result } = event.data;\n    const task = this.tasks.find(t => t.id === taskId);\n    if (task) {\n      task.resolve(result);\n      this.tasks = this.tasks.filter(t => t.id !== taskId);\n    }\n    this.assignTask(worker);\n  }\n\n  // Assign a task to an available worker\n  assignTask(worker) {\n    if (this.tasks.length > 0) {\n      const task = this.tasks.shift();\n      worker.postMessage({ taskId: task.id, data: task.data });\n    }\n  }\n\n  // Add a task to the queue\n  addTask(data) {\n    return new Promise((resolve) => {\n      const taskId = Date.now() + Math.random(); // Unique task ID\n      this.tasks.push({ id: taskId, data, resolve });\n      const availableWorker = this.workers.find(w => !w.busy);\n      if (availableWorker) {\n        this.assignTask(availableWorker);\n      }\n    });\n  }\n}\n\n// Example usage\nconst workerScript = 'imageProcessingWorker.js'; // Path to worker script\nconst pool = new WorkerPool(4, workerScript); // Pool of 4 workers\n\n// Add image processing tasks\npool.addTask(imageData1).then(result => console.log('Task 1 completed:', result));\npool.addTask(imageData2).then(result => console.log('Task 2 completed:', result));\n```\n\n```javascript\n// imageProcessingWorker.js (Web Worker script)\nself.onmessage = function(event) {\n  const { taskId, data } = event.data;\n\n  // Simulate CPU-intensive image processing\n  const processedData = processImage(data); // Replace with actual processing logic\n\n  // Send result back to main thread\n  self.postMessage({ taskId, result: processedData });\n};\n\nfunction processImage(imageData) {\n  // Placeholder for image processing logic\n  return imageData; // Replace with actual processing\n}\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/segmentio/kafka-go\"\n)\n\n// worker processes messages from the input channel and sends results to the output channel\nfunc worker(id int, in <-chan kafka.Message, out chan<- string, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\tfor msg := range in {\n\t\t// Simulate processing by sleeping for a short duration\n\t\ttime.Sleep(100 * time.Millisecond)\n\t\tresult := fmt.Sprintf(\"Worker %d processed message: %s\", id, string(msg.Value))\n\t\tout <- result\n\t}\n}\n\n// fanOut distributes Kafka messages to multiple worker goroutines\nfunc fanOut(messages []kafka.Message, workerCount int) <-chan string {\n\tin := make(chan kafka.Message, len(messages))\n\tout := make(chan string, len(messages))\n\n\tvar wg sync.WaitGroup\n\n\t// Start worker goroutines\n\tfor i := 0; i < workerCount; i++ {\n\t\twg.Add(1)\n\t\tgo worker(i, in, out, &wg)\n\t}\n\n\t// Send messages to the input channel\n\tgo func() {\n\t\tfor _, msg := range messages {\n\t\t\tin <- msg\n\t\t}\n\t\tclose(in)\n\t}()\n\n\t// Close the output channel once all workers are done\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\n\treturn out\n}\n\n// fanIn collects results from multiple channels into a single channel\nfunc fanIn(channels ...<-chan string) <-chan string {\n\tvar wg sync.WaitGroup\n\tout := make(chan string)\n\n\t// Start a goroutine for each input channel\n\tfor _, ch := range channels {\n\t\twg.Add(1)\n\t\tgo func(c <-chan string) {\n\t\t\tfor msg := range c {\n\t\t\t\tout <- msg\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(ch)\n\t}\n\n\t// Close the output channel once all input channels are done\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\n\treturn out\n}\n\nfunc main() {\n\t// Simulate Kafka messages\n\tmessages := []kafka.Message{\n\t\t{Value: []byte(\"Message 1\")},\n\t\t{Value: []byte(\"Message 2\")},\n\t\t{Value: []byte(\"Message 3\")},\n\t\t{Value: []byte(\"Message 4\")},\n\t}\n\n\t// Fan-out processing to multiple workers\n\tworkerCount := 3\n\tresults := fanOut(messages, workerCount)\n\n\t// Fan-in results from multiple channels (if needed)\n\tfinalResults := fanIn(results)\n\n\t// Collect and print final results\n\tfor res := range finalResults {\n\t\tfmt.Println(res)\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "```swift\n// Define a struct to model a polynomial equation\nstruct Polynomial {\n    // Coefficients for the polynomial, e.g., [a0, a1, a2] for a0 + a1*x + a2*x^2\n    let coefficients: [Double]\n    \n    // Initialize with coefficients\n    init(coefficients: [Double]) {\n        self.coefficients = coefficients\n    }\n    \n    // Evaluate the polynomial at a given value of x\n    func evaluate(at x: Double) -> Double {\n        return coefficients.enumerated().reduce(0.0) { result, term in\n            let (index, coefficient) = term\n            return result + coefficient * pow(x, Double(index))\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport gym\nimport numpy as np\n\n# Create the environment\nenv = gym.make(\"CartPole-v1\")\n\n# Initialize Q-table\nstate_space = env.observation_space.shape[0]\naction_space = env.action_space.n\nQ_table = np.zeros((state_space, action_space))\n\n# Hyperparameters\nalpha = 0.1  # Learning rate\ngamma = 0.99  # Discount factor\nepsilon = 1.0  # Exploration rate\nepsilon_decay = 0.995\nmin_epsilon = 0.01\n\n# Discretize the state space\ndef discretize(state, bins=(30, 30, 50, 50)):\n    upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)]\n    lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)]\n    ratios = [(state[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(state))]\n    new_state = [int(round((bins[i] - 1) * ratios[i])) for i in range(len(state))]\n    new_state = [min(bins[i] - 1, max(0, new_state[i])) for i in range(len(state))]\n    return tuple(new_state)\n\n# Q-learning algorithm\ndef train(episodes=1000):\n    global epsilon\n    for episode in range(episodes):\n        state = discretize(env.reset())\n        done = False\n        while not done:\n            if np.random.random() < epsilon:\n                action = env.action_space.sample()  # explore\n            else:\n                action = np.argmax(Q_table[state])  # exploit\n\n            next_state, reward, done, _ = env.step(action)\n            next_state = discretize(next_state)\n            \n            # Update Q-table\n            Q_table[state][action] = (1 - alpha) * Q_table[state][action] + \\\n                alpha * (reward + gamma * np.max(Q_table[next_state]))\n            state = next_state\n\n            if done:\n                break\n\n        # Decay epsilon\n        if epsilon > min_epsilon:\n            epsilon *= epsilon_decay\n\n    print(\"Training completed over {} episodes\".format(episodes))\n\n# Run training\ntrain()\n\n# Test the agent\ndef test(episodes=5):\n    for episode in range(episodes):\n        state = discretize(env.reset())\n        done = False\n        total_reward = 0\n        while not done:\n            action = np.argmax(Q_table[state])\n            next_state, reward, done, _ = env.step(action)\n            next_state = discretize(next_state)\n            total_reward += reward\n            state = next_state\n\n        print(f\"Episode {episode + 1}: Total reward: {total_reward}\")\n\n# Run testing\ntest()\n\nenv.close()\n```\n\nNote: Make sure you have installed `gym` using `pip install gym` before running the code, and, if necessary, install any additional dependencies for rendering or specific environments (like `CartPole`).",
    "label": 1
  },
  {
    "codes": "func (c *Glue) BatchDeleteTableVersionRequest(input *BatchDeleteTableVersionInput) (req *request.Request, output *BatchDeleteTableVersionOutput) {\n\top := &request.Operation{\n\t\tName:       opBatchDeleteTableVersion,\n\t\tHTTPMethod: \"POST\",\n\t\tHTTPPath:   \"/\",\n\t}\n\n\tif input == nil {\n\t\tinput = &BatchDeleteTableVersionInput{}\n\t}\n\n\toutput = &BatchDeleteTableVersionOutput{}\n\treq = c.newRequest(op, input, output)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "function unlistenArrayEvents(array, listener) {\n\tvar stub = array._chartjs;\n\tif (!stub) {\n\t\treturn;\n\t}\n\n\tvar listeners = stub.listeners;\n\tvar index = listeners.indexOf(listener);\n\tif (index !== -1) {\n\t\tlisteners.splice(index, 1);\n\t}\n\n\tif (listeners.length > 0) {\n\t\treturn;\n\t}\n\n\tarrayEvents.forEach(function(key) {\n\t\tdelete array[key];\n\t});\n\n\tdelete array._chartjs;\n}",
    "label": 0
  },
  {
    "codes": "def send(ch, message, **kwargs):\n    \"\"\"\n    Site: https://pushall.ru\n    API: https://pushall.ru/blog/api\n    Desc: App for notification to devices/browsers and messaging apps\n    \"\"\"\n    params = {\n        'type': kwargs.pop('req_type', 'self'),\n        'key': settings.PUSHALL_API_KEYS[ch]['key'],\n        'id': settings.PUSHALL_API_KEYS[ch]['id'],\n        'title': kwargs.pop(\n            \"title\", settings.PUSHALL_API_KEYS[ch].get('title') or \"\"),\n        'text': message,\n        'priority': kwargs.pop(\n            \"priority\", settings.PUSHALL_API_KEYS[ch].get('priority') or \"0\"),\n    }\n    if kwargs:\n        params.update(**kwargs)\n\n    response = urlopen(\n        Request('https://pushall.ru/api.php'),\n        urlencode(params),\n        timeout=10\n    )\n\n    if response.code != 200:\n        raise PushAllError(response.read())\n\n    json = loads(response.read())\n    if json.get('error'):\n        raise PushAllError(json.get('error'))\n\n    return True",
    "label": 0
  },
  {
    "codes": "```csharp\n// Assuming 'products' is a collection of Product objects with properties: Price and Category\n\n// Define price range and category\ndecimal minPrice = 50.0m;\ndecimal maxPrice = 200.0m;\nstring targetCategory = \"Electronics\";\n\n// LINQ query to filter products by price range and category\nvar filteredProducts = products\n    .Where(p => p.Price >= minPrice && p.Price <= maxPrice && p.Category == targetCategory)\n    .ToList();\n\n// filteredProducts now contains products within the specified price range and category\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport uuid\n\ndef generate_uuid_v4():\n    # Generate a UUID v4 and remove hyphens\n    return str(uuid.uuid4()).replace('-', '')\n```",
    "label": 1
  },
  {
    "codes": "```sql\n-- Assuming the table name is 'orders' with columns 'customer_id' and 'purchase_amount'\nSELECT customer_id, SUM(purchase_amount) AS total_spent\nFROM orders\nGROUP BY customer_id\nORDER BY total_spent DESC\nLIMIT 5;\n```",
    "label": 1
  },
  {
    "codes": "```powershell\n# Define the event log to query (Security log)\n$logName = \"Security\"\n\n# Define the time range for the audit (last 24 hours)\n$startTime = (Get-Date).AddHours(-24)\n$endTime = Get-Date\n\n# Query the Security log for specific event IDs (e.g., 4624: Successful logon, 4625: Failed logon)\n$eventIds = @(4624, 4625)\n\n# Retrieve events within the specified time range and event IDs\n$securityEvents = Get-WinEvent -LogName $logName -FilterXPath \"*[System[(EventID=$($eventIds -join ' or EventID=')) and TimeCreated[@SystemTime>='$($startTime.ToUniversalTime().ToString(\"o\"))' and @SystemTime<='$($endTime.ToUniversalTime().ToString(\"o\"))']]]\"\n\n# Output the results\n$securityEvents | Select-Object TimeCreated, Id, Message | Format-Table -AutoSize\n\n# Optionally, export the results to a CSV file\n$securityEvents | Export-Csv -Path \"SecurityAudit_$(Get-Date -Format 'yyyyMMdd_HHmmss').csv\" -NoTypeInformation\n```",
    "label": 1
  },
  {
    "codes": "public SecretBundle recoverDeletedSecret(String vaultBaseUrl, String secretName) {\n        return recoverDeletedSecretWithServiceResponseAsync(vaultBaseUrl, secretName).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "```kotlin\n// Extension function to reverse words in a string while preserving whitespace\nfun String.reverseWords(): String {\n    // Split the string into words and whitespace using a regex\n    val parts = this.split(\"(?<=\\\\s)|(?=\\\\s)\".toRegex())\n    \n    // Reverse each word while keeping whitespace unchanged\n    return parts.joinToString(\"\") { part ->\n        if (part.isNotBlank()) part.reversed() else part\n    }\n}\n\n// Example usage:\n// val input = \"Hello   world  \"\n// println(input.reverseWords())  // Output: \"olleH   dlrow  \"\n```",
    "label": 1
  },
  {
    "codes": "def discover_connectors(\n        domain: str,\n        loop=None,\n        logger=logger):\n    \"\"\"\n    Discover all connection options for a domain, in descending order of\n    preference.\n\n    This coroutine returns options discovered from SRV records, or if none are\n    found, the generic option using the domain name and the default XMPP client\n    port.\n\n    Each option is represented by a triple ``(host, port, connector)``.\n    `connector` is a :class:`aioxmpp.connector.BaseConnector` instance which is\n    suitable to connect to the given host and port.\n\n    `logger` is the logger used by the function.\n\n    The following sources are supported:\n\n    * :rfc:`6120` SRV records. One option is returned per SRV record.\n\n      If one of the SRV records points to the root name (``.``),\n      :class:`ValueError` is raised (the domain specifically said that XMPP is\n      not supported here).\n\n    * :xep:`368` SRV records. One option is returned per SRV record.\n\n    * :rfc:`6120` fallback process (only if no SRV records are found). One\n      option is returned for the host name with the default XMPP client port.\n\n    The options discovered from SRV records are mixed together, ordered by\n    priority and then within priorities are shuffled according to their weight.\n    Thus, if there are multiple records of equal priority, the result of the\n    function is not deterministic.\n\n    .. versionadded:: 0.6\n    \"\"\"\n\n    domain_encoded = domain.encode(\"idna\") + b\".\"\n    starttls_srv_failed = False\n    tls_srv_failed = False\n\n    try:\n        starttls_srv_records = yield from network.lookup_srv(\n            domain_encoded,\n            \"xmpp-client\",\n        )\n        starttls_srv_disabled = False\n    except dns.resolver.NoNameservers as exc:\n        starttls_srv_records = []\n        starttls_srv_disabled = False\n        starttls_srv_failed = True\n        starttls_srv_exc = exc\n        logger.debug(\"xmpp-client SRV lookup for domain %s failed \"\n                     \"(may not be fatal)\",\n                     domain_encoded,\n                     exc_info=True)\n    except ValueError:\n        starttls_srv_records = []\n        starttls_srv_disabled = True\n\n    try:\n        tls_srv_records = yield from network.lookup_srv(\n            domain_encoded,\n            \"xmpps-client\",\n        )\n        tls_srv_disabled = False\n    except dns.resolver.NoNameservers:\n        tls_srv_records = []\n        tls_srv_disabled = False\n        tls_srv_failed = True\n        logger.debug(\"xmpps-client SRV lookup for domain %s failed \"\n                     \"(may not be fatal)\",\n                     domain_encoded,\n                     exc_info=True)\n    except ValueError:\n        tls_srv_records = []\n        tls_srv_disabled = True\n\n    if starttls_srv_failed and (tls_srv_failed or tls_srv_records is None):\n        # the failure is probably more useful as a diagnostic\n        # if we find a good reason to allow this scenario, we might change it\n        # later.\n        raise starttls_srv_exc\n\n    if starttls_srv_disabled and (tls_srv_disabled or tls_srv_records is None):\n        raise ValueError(\n            \"XMPP not enabled on domain {!r}\".format(domain),\n        )\n\n    if starttls_srv_records is None and tls_srv_records is None:\n        # no SRV records published, fall back\n        logger.debug(\n            \"no SRV records found for %s, falling back\",\n            domain,\n        )\n        return [\n            (domain, 5222, connector.STARTTLSConnector()),\n        ]\n\n    starttls_srv_records = starttls_srv_records or []\n    tls_srv_records = tls_srv_records or []\n\n    srv_records = [\n        (prio, weight, (host.decode(\"ascii\"), port,\n                        connector.STARTTLSConnector()))\n        for prio, weight, (host, port) in starttls_srv_records\n    ]\n\n    srv_records.extend(\n        (prio, weight, (host.decode(\"ascii\"), port,\n                        connector.XMPPOverTLSConnector()))\n        for prio, weight, (host, port) in tls_srv_records\n    )\n\n    options = list(\n        network.group_and_order_srv_records(srv_records)\n    )\n\n    logger.debug(\n        \"options for %s: %r\",\n        domain,\n        options,\n    )\n\n    return options",
    "label": 0
  },
  {
    "codes": "```java\nimport org.junit.runner.RunWith;\nimport org.junit.runners.Suite;\nimport org.junit.runners.Suite.SuiteClasses;\n\n@RunWith(Suite.class)\n@SuiteClasses({\n    SortingAlgorithmTest.class\n})\npublic class SortingAlgorithmTestSuite {\n}\n\n// SortingAlgorithmTest.java\nimport static org.junit.Assert.assertArrayEquals;\nimport org.junit.Test;\n\npublic class SortingAlgorithmTest {\n\n    // Edge case: Empty array\n    @Test\n    public void testEmptyArray() {\n        int[] input = {};\n        int[] expected = {};\n        assertArrayEquals(expected, SortingAlgorithm.sort(input));\n    }\n\n    // Edge case: Single element array\n    @Test\n    public void testSingleElementArray() {\n        int[] input = {5};\n        int[] expected = {5};\n        assertArrayEquals(expected, SortingAlgorithm.sort(input));\n    }\n\n    // Edge case: Already sorted array\n    @Test\n    public void testAlreadySortedArray() {\n        int[] input = {1, 2, 3, 4, 5};\n        int[] expected = {1, 2, 3, 4, 5};\n        assertArrayEquals(expected, SortingAlgorithm.sort(input));\n    }\n\n    // Edge case: Array with identical elements\n    @Test\n    public void testIdenticalElements() {\n        int[] input = {7, 7, 7, 7};\n        int[] expected = {7, 7, 7, 7};\n        assertArrayEquals(expected, SortingAlgorithm.sort(input));\n    }\n\n    // Edge case: Array with negative numbers\n    @Test\n    public void testNegativeNumbers() {\n        int[] input = {-3, -1, -2, -4, 0};\n        int[] expected = {-4, -3, -2, -1, 0};\n        assertArrayEquals(expected, SortingAlgorithm.sort(input));\n    }\n\n    // Edge case: Array with mixed negative and positive numbers\n    @Test\n    public void testMixedNegativeAndPositiveNumbers() {\n        int[] input = {3, -1, 2, -5, 0};\n        int[] expected = {-5, -1, 0, 2, 3};\n        assertArrayEquals(expected, SortingAlgorithm.sort(input));\n    }\n\n    // Edge case: Large array with random numbers\n    @Test\n    public void testLargeRandomArray() {\n        int[] input = {10, -1, 4, 0, 2, 99, -25, 15, 3};\n        int[] expected = {-25, -1, 0, 2, 3, 4, 10, 15, 99};\n        assertArrayEquals(expected, SortingAlgorithm.sort(input));\n    }\n}\n\n// SortingAlgorithm.java (Stub for demonstration; the actual algorithm will be implemented here)\npublic class SortingAlgorithm {\n    public static int[] sort(int[] array) {\n        // Implement the sorting algorithm (e.g., quicksort, mergesort, etc.)\n        return array; // Placeholder\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define log file path\nLOG_FILE=\"/var/log/apache2/access.log\"\n\n# Parse log file and count 5xx errors per minute\ngrep $(date +\"%d/%b/%Y:%H:%M\" --date='1 minute ago') $LOG_FILE | awk '{print $9}' | grep '^5' | sort | uniq -c\n\n# Check if the count of 5xx errors exceeds a threshold (e.g., 10)\nif [ $(grep $(date +\"%d/%b/%Y:%H:%M\" --date='1 minute ago') $LOG_FILE | awk '{print $9}' | grep '^5' | wc -l) -gt 10 ]; then\n    echo \"5xx error spike detected in the last minute!\"\nfi\n```\n\n### Explanation:\n- **LOG_FILE**: Path to the Apache access log.\n- **grep $(date +\"%d/%b/%Y:%H:%M\" --date='1 minute ago')**: Filters logs from the last minute.\n- **awk '{print $9}'**: Extracts the HTTP status code.\n- **grep '^5'**: Filters only 5xx errors.\n- **sort | uniq -c**: Counts occurrences of each 5xx error code.\n- **wc -l**: Counts total 5xx errors in the last minute.\n- **if [ ... -gt 10 ]**: Checks if the count exceeds a threshold (e.g., 10). Adjust as needed.",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <opencv2/opencv.hpp>\n\n// Function to preprocess video frames\ncv::Mat preprocessFrame(const cv::Mat& frame) {\n    cv::Mat gray, resized, normalized;\n\n    // Convert to grayscale\n    cv::cvtColor(frame, gray, cv::COLOR_BGR2GRAY);\n\n    // Resize to desired dimensions (e.g., 224x224 for many models)\n    cv::resize(gray, resized, cv::Size(224, 224));\n\n    // Normalize to range [0, 1]\n    resized.convertTo(normalized, CV_32F, 1.0 / 255);\n\n    return normalized;\n}\n\nint main() {\n    // Open the video file\n    cv::VideoCapture cap(\"path_to_video.mp4\");\n    if (!cap.isOpened()) {\n        std::cerr << \"Error: Cannot open video file\\n\";\n        return -1;\n    }\n\n    cv::Mat frame;\n    while (cap.read(frame)) {\n        // Preprocess each frame\n        cv::Mat processedFrame = preprocessFrame(frame);\n\n        // Use processedFrame for your model input here\n        \n        // Display the processed frame (optional)\n        cv::imshow(\"Processed Frame\", processedFrame);\n        \n        // Break the loop if 'q' is pressed\n        if (cv::waitKey(10) == 'q') break;\n    }\n\n    // Release resources\n    cap.release();\n    cv::destroyAllWindows();\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/rwcarlsen/goexif/exif\"\n)\n\nfunc main() {\n\t// Open the JPEG file\n\tfilePath := \"example.jpg\" // Replace with your JPEG file\n\tfile, err := os.Open(filePath)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error opening file: %v\", err)\n\t}\n\tdefer file.Close()\n\n\t// Decode the EXIF data\n\tx, err := exif.Decode(file)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error decoding EXIF data: %v\", err)\n\t}\n\n\t// Print the EXIF tags and their values\n\tif tags, err := x.Tagged(); err == nil {\n\t\tfor _, tag := range tags {\n\t\t\tval, _ := x.Get(tag)\n\t\t\tfmt.Printf(\"%s: %s\\n\", tag, val)\n\t\t}\n\t} else {\n\t\tlog.Fatalf(\"Error getting tagged EXIF data: %v\", err)\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <fstream>\n#include <sstream>\n#include <map>\n#include <vector>\n#include <algorithm>\n\n// Structure to hold INI file data\nstruct INI {\n    std::map<std::string, std::map<std::string, std::string>> data;\n    std::vector<std::string> sections;\n\n    // Function to parse INI file\n    void parse(const std::string& filename) {\n        std::ifstream file(filename);\n        if (!file.is_open()) {\n            std::cerr << \"Error: Unable to open file \" << filename << std::endl;\n            return;\n        }\n\n        std::string line, currentSection;\n        while (std::getline(file, line)) {\n            line.erase(std::remove_if(line.begin(), line.end(), isspace), line.end()); // Remove whitespace\n            if (line.empty() || line[0] == ';' || line[0] == '#') continue; // Skip comments and empty lines\n\n            if (line[0] == '[' && line.back() == ']') { // Section header\n                currentSection = line.substr(1, line.size() - 2);\n                sections.push_back(currentSection);\n            } else if (line.find('=') != std::string::npos) { // Key-value pair\n                size_t delimiterPos = line.find('=');\n                std::string key = line.substr(0, delimiterPos);\n                std::string value = line.substr(delimiterPos + 1);\n\n                // Handle inheritance\n                if (value == \"inherit\") {\n                    if (sections.size() > 1) {\n                        std::string parentSection = sections[sections.size() - 2];\n                        if (data[parentSection].find(key) != data[parentSection].end()) {\n                            value = data[parentSection][key];\n                        }\n                    }\n                }\n\n                data[currentSection][key] = value;\n            }\n        }\n    }\n\n    // Function to get value from INI\n    std::string get(const std::string& section, const std::string& key) {\n        if (data.find(section) != data.end() && data[section].find(key) != data[section].end()) {\n            return data[section][key];\n        }\n        return \"\";\n    }\n};\n\nint main() {\n    INI ini;\n    ini.parse(\"config.ini\");\n\n    // Example usage\n    std::cout << \"Value of key1 in section1: \" << ini.get(\"section1\", \"key1\") << std::endl;\n    std::cout << \"Value of key2 in section2: \" << ini.get(\"section2\", \"key2\") << std::endl;\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<Page<DeletedCertificateItem>>> getDeletedCertificatesNextWithServiceResponseAsync(final String nextPageLink) {\n        return getDeletedCertificatesNextSinglePageAsync(nextPageLink)\n            .concatMap(new Func1<ServiceResponse<Page<DeletedCertificateItem>>, Observable<ServiceResponse<Page<DeletedCertificateItem>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<DeletedCertificateItem>>> call(ServiceResponse<Page<DeletedCertificateItem>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(getDeletedCertificatesNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport gym\nimport numpy as np\n\n# Hyperparameters\nalpha = 0.1\ngamma = 0.99\nepsilon = 0.1\nn_episodes = 5000\nmax_steps = 100\n\n# Create the environment\nenv = gym.make(\"Taxi-v3\")\n\n# Q-table initialization\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n\n# Epsilon-greedy policy for action selection\ndef choose_action(state):\n    if np.random.uniform(0, 1) < epsilon:\n        return env.action_space.sample()  # Explore\n    else:\n        return np.argmax(q_table[state])  # Exploit\n\n# Update Q-value\ndef update_q_table(state, action, reward, next_state):\n    best_next_action = np.argmax(q_table[next_state])\n    td_target = reward + gamma * q_table[next_state, best_next_action]\n    td_error = td_target - q_table[state, action]\n    q_table[state, action] += alpha * td_error\n\n# Training loop\nfor episode in range(n_episodes):\n    state = env.reset()\n    done = False\n    for _ in range(max_steps):\n        action = choose_action(state)\n        next_state, reward, done, _ = env.step(action)\n        update_q_table(state, action, reward, next_state)\n        state = next_state\n        if done:\n            break\n\n# Close the environment\nenv.close()\n\n# Print the learned Q-values\nprint(\"Q-table:\\n\", q_table)\n```\n",
    "label": 1
  },
  {
    "codes": "func (c *Glue) GetJobRunWithContext(ctx aws.Context, input *GetJobRunInput, opts ...request.Option) (*GetJobRunOutput, error) {\n\treq, out := c.GetJobRunRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "func (c *Glue) DeleteTriggerWithContext(ctx aws.Context, input *DeleteTriggerInput, opts ...request.Option) (*DeleteTriggerOutput, error) {\n\treq, out := c.DeleteTriggerRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```swift\nimport SpriteKit\n\n// Define a function to create the puzzle piece actions\nfunc createPuzzlePieceAnimation(for piece: SKSpriteNode) -> SKAction {\n    // Scale up to give a popping effect\n    let scaleUp = SKAction.scale(to: 1.2, duration: 0.2)\n    // Scale back to original size\n    let scaleDown = SKAction.scale(to: 1.0, duration: 0.2)\n    // Rotate slightly to give a \"snapping\" effect\n    let rotate = SKAction.rotate(byAngle: .pi / 8, duration: 0.2)\n    // Reverse the rotation to restore original orientation\n    let rotateReverse = rotate.reversed()\n    // Add a slight delay between actions\n    let delay = SKAction.wait(forDuration: 0.1)\n    \n    // Group scaling and rotation for simultaneous execution\n    let popAndSnap = SKAction.group([scaleUp, rotate])\n    \n    // Create sequence of actions\n    let actionSequence = SKAction.sequence([\n        popAndSnap,\n        scaleDown,\n        rotateReverse,\n        delay\n    ])\n    \n    return actionSequence\n}\n\n// Example usage in a puzzle scene to apply the animation\nlet puzzlePiece = SKSpriteNode(imageNamed: \"puzzlePiece\")\npuzzlePiece.run(createPuzzlePieceAnimation(for: puzzlePiece))\n```",
    "label": 1
  },
  {
    "codes": "function XNode(type, name, opt_value, opt_owner) {\n  this.attributes = [];\n  this.childNodes = [];\n\n  XNode.init.call(this, type, name, opt_value, opt_owner);\n}",
    "label": 0
  },
  {
    "codes": "func (s *Location) SetS3(v []*CodeGenNodeArg) *Location {\n\ts.S3 = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (c *Glacier) ListParts(input *ListPartsInput) (*ListPartsOutput, error) {\n\treq, out := c.ListPartsRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```csharp\n// Assuming 'products' is a collection of Product objects with properties 'Price' and 'Category'\n\n// Define the price range and category\ndecimal minPrice = 50.0m;\ndecimal maxPrice = 200.0m;\nstring targetCategory = \"Electronics\";\n\n// LINQ query to filter products by price range and category\nvar filteredProducts = products\n    .Where(p => p.Price >= minPrice && p.Price <= maxPrice && p.Category == targetCategory)\n    .ToList();\n\n// 'filteredProducts' now contains products within the specified price range and category\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<Void> beginResetServicePrincipalProfileAsync(String resourceGroupName, String resourceName, ManagedClusterServicePrincipalProfile parameters, final ServiceCallback<Void> serviceCallback) {\n        return ServiceFuture.fromResponse(beginResetServicePrincipalProfileWithServiceResponseAsync(resourceGroupName, resourceName, parameters), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "func (s *DeleteUserDefinedFunctionInput) SetFunctionName(v string) *DeleteUserDefinedFunctionInput {\n\ts.FunctionName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport functools\nimport traceback\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.ERROR)\n\ndef log_exceptions(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            # Log the exception with stack trace\n            logging.error(f\"Exception occurred in {func.__name__}: {str(e)}\")\n            logging.error(traceback.format_exc())\n            raise  # Re-raise the exception after logging\n    return wrapper\n\n# Example usage\n@log_exceptions\ndef risky_function():\n    return 1 / 0\n\nrisky_function()\n```",
    "label": 1
  },
  {
    "codes": "def normalize_tag(tag):\n    \"\"\"\n    Normalize an XML element tree `tag` into the tuple format. The following\n    input formats are accepted:\n\n    * ElementTree namespaced string, e.g. ``{uri:bar}foo``\n    * Unnamespaced tags, e.g. ``foo``\n    * Two-tuples consisting of `namespace_uri` and `localpart`; `namespace_uri`\n      may be :data:`None` if the tag is supposed to be namespaceless. Otherwise\n      it must be, like `localpart`, a :class:`str`.\n\n    Return a two-tuple consisting the ``(namespace_uri, localpart)`` format.\n    \"\"\"\n    if isinstance(tag, str):\n        namespace_uri, sep, localname = tag.partition(\"}\")\n        if sep:\n            if not namespace_uri.startswith(\"{\"):\n                raise ValueError(\"not a valid etree-format tag\")\n            namespace_uri = namespace_uri[1:]\n        else:\n            localname = namespace_uri\n            namespace_uri = None\n        return (namespace_uri, localname)\n    elif len(tag) != 2:\n        raise ValueError(\"not a valid tuple-format tag\")\n    else:\n        if any(part is not None and not isinstance(part, str) for part in tag):\n            raise TypeError(\"tuple-format tags must only contain str and None\")\n        if tag[1] is None:\n            raise ValueError(\"tuple-format localname must not be None\")\n    return tag",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n    \"sync\"\n    \"github.com/gorilla/mux\"\n)\n\n// Job represents a prediction task\ntype Job struct {\n    ID     int\n    Input  string\n    Result string\n}\n\n// Worker represents a GPU worker\ntype Worker struct {\n    ID      int\n    JobChan chan Job\n}\n\n// Create a pool of GPU workers\nfunc NewWorker(id int) Worker {\n    w := Worker{ID: id, JobChan: make(chan Job)}\n    go w.start()\n    return w\n}\n\n// Start processing jobs\nfunc (w Worker) start() {\n    for job := range w.JobChan {\n        // Simulate GPU processing\n        job.Result = fmt.Sprintf(\"Processed %s by worker %d\", job.Input, w.ID)\n        log.Printf(\"Worker %d finished job %d: %s\", w.ID, job.ID, job.Result)\n    }\n}\n\n// Job queue and worker pool\nvar (\n    workerCount = 4\n    jobQueue    = make(chan Job, 100)\n    workers     []Worker\n)\n\n// InitWorkers initializes a pool of workers\nfunc InitWorkers() {\n    for i := 0; i < workerCount; i++ {\n        workers = append(workers, NewWorker(i))\n    }\n}\n\n// Add job to the worker pool\nfunc AddJob(job Job) {\n    jobQueue <- job\n}\n\n// Handler for adding jobs\nfunc JobHandler(w http.ResponseWriter, r *http.Request) {\n    // Simulate creating a job\n    jobID := len(jobQueue) + 1 // Simple job ID assignment\n    job := Job{ID: jobID, Input: r.URL.Query().Get(\"input\")}\n    AddJob(job)\n    fmt.Fprintf(w, \"Job added: %d\\n\", job.ID)\n}\n\n// Main function\nfunc main() {\n    InitWorkers()\n\n    // Start workers to listen for jobs\n    go func() {\n        for job := range jobQueue {\n            // Distribute jobs to workers in a round-robin manner\n            workers[job.ID%workerCount].JobChan <- job\n        }\n    }()\n\n    // Setup HTTP server\n    r := mux.NewRouter()\n    r.HandleFunc(\"/submit\", JobHandler).Methods(\"POST\")\n    log.Println(\"Starting server on :8080\")\n    log.Fatal(http.ListenAndServe(\":8080\", r))\n}\n```",
    "label": 1
  },
  {
    "codes": "function addonsManager_disableAddon(aSpec) {\n    var spec = aSpec || { };\n    spec.button = \"disable\";\n\n    var button = this.getAddonButton(spec);\n    this._controller.click(button);\n  }",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define the directory to search for log files\nLOG_DIR=\"/path/to/log/files\"\n\n# Define the archive location\nARCHIVE_DIR=\"/path/to/archive\"\n\n# Find files older than 30 days and pass them to tar for archiving\nfind \"$LOG_DIR\" -name \"*.log\" -type f -mtime +30 -print0 | xargs -0 tar -czf \"$ARCHIVE_DIR/old_logs_$(date +%Y%m%d).tar.gz\" --remove-files\n\n# Notify about successful archiving\necho \"Log files older than 30 days have been archived.\"\n```",
    "label": 1
  },
  {
    "codes": "```javascript\n// Main script\n\n// Number of workers you want in your pool\nconst numWorkers = navigator.hardwareConcurrency || 4; \nconst workers = [];\nconst taskQueue = [];\nlet idleWorkers = [];\n\n// Initialize workers\nfor (let i = 0; i < numWorkers; i++) {\n    const worker = new Worker('worker.js');\n    worker.onmessage = handleWorkerResponse;\n    idleWorkers.push(worker);\n}\n\n// Handles messages back from the worker\nfunction handleWorkerResponse(event) {\n    console.log('Image processed:', event.data);\n\n    // After processing, check if there's any queued task to assign to the current worker.\n    if (taskQueue.length > 0) {\n        const nextTask = taskQueue.shift();\n        this.postMessage(nextTask);\n    } else {\n        idleWorkers.push(this);\n    }\n}\n\n// Function to add a task to the pool\nfunction addTaskToPool(taskData) {\n    if (idleWorkers.length > 0) {\n        // If there are any idle workers, assign the task immediately\n        const worker = idleWorkers.pop();\n        worker.postMessage(taskData);\n    } else {\n        // Otherwise, add the task to the queue\n        taskQueue.push(taskData);\n    }\n}\n\n// Example usage\nconst imageData = { /* image data */ };\naddTaskToPool(imageData);\n```\n\n```javascript\n// worker.js - Web Worker Script\n\nself.onmessage = function(event) {\n    const imageData = event.data;\n\n    // Image processing logic here\n    const processedData = processImage(imageData);\n\n    // Send processed image data back to main thread\n    self.postMessage(processedData);\n};\n\nfunction processImage(data) {\n    // Perform CPU-intensive operations here\n    // Example: Dummy processing; replace with actual image processing logic\n    for (let i = 0; i < data.length; i++) {\n        data[i] = 255 - data[i]; // Invert colors example\n    }\n    return data;\n}\n```\n\n",
    "label": 1
  },
  {
    "codes": "```java\nimport ddf.minim.*;\nimport ddf.minim.analysis.*;\n\nMinim minim;\nAudioInput in;\nFFT fft;\n\nvoid setup() {\n  size(800, 400);\n  minim = new Minim(this);\n  in = minim.getLineIn(Minim.STEREO, 1024);\n  fft = new FFT(in.bufferSize(), in.sampleRate());\n  fft.logAverages(22, 3); // Group frequencies into 22 bands, each spanning 3 octaves\n}\n\nvoid draw() {\n  background(0);\n  fft.forward(in.mix);\n\n  stroke(255);\n  noFill();\n  beginShape();\n  for (int i = 0; i < fft.avgSize(); i++) {\n    float x = map(i, 0, fft.avgSize(), 0, width);\n    float y = map(fft.getAvg(i), 0, 50, height, 0); // Adjust scaling for better visualization\n    vertex(x, y);\n  }\n  endShape();\n}\n\nvoid stop() {\n  in.close();\n  minim.stop();\n  super.stop();\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization\n\n# Define the TextVectorization layer\nmax_tokens = 10000  # Maximum number of unique tokens\nsequence_length = 100  # Length of each sequence\nvectorizer = TextVectorization(\n    max_tokens=max_tokens,\n    output_mode='int',\n    output_sequence_length=sequence_length\n)\n\n# Adapt the vectorizer to the text data\ntext_data = [\"Sample text data\", \"Another example of text data\"]\nvectorizer.adapt(text_data)\n\n# Create a Keras model pipeline\ninput_layer = tf.keras.Input(shape=(1,), dtype=tf.string)\nx = vectorizer(input_layer)\noutput_layer = tf.keras.layers.Embedding(input_dim=max_tokens, output_dim=128)(x)\n\n# Build the model\nmodel = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n\n# Example usage\nprocessed_text = model.predict([\"This is a sample text for preprocessing\"])\nprint(processed_text)\n```",
    "label": 1
  },
  {
    "codes": "```rust\nfn is_valid_isbn13(isbn: &str) -> bool {\n    // Remove any hyphens and ensure the length is 13\n    let cleaned_isbn: String = isbn.chars().filter(|c| c.is_digit(10)).collect();\n    if cleaned_isbn.len() != 13 {\n        return false;\n    }\n\n    // Calculate the checksum\n    let checksum: u32 = cleaned_isbn\n        .chars()\n        .enumerate()\n        .map(|(i, c)| {\n            let digit = c.to_digit(10).unwrap();\n            if i % 2 == 0 {\n                digit\n            } else {\n                digit * 3\n            }\n        })\n        .sum();\n\n    // Validate the checksum\n    checksum % 10 == 0\n}\n\n// Example usage:\n// println!(\"{}\", is_valid_isbn13(\"978-3-16-148410-0\")); // true\n// println!(\"{}\", is_valid_isbn13(\"978-3-16-148410-1\")); // false\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse reqwest::blocking::Client;\nuse serde::{Deserialize, Serialize};\nuse std::error::Error;\n\n// Define the structure for the token response\n#[derive(Serialize, Deserialize, Debug)]\nstruct TokenResponse {\n    access_token: String,\n    token_type: String,\n    expires_in: u64,\n}\n\n// Function to request an access token using client credentials flow\nfn get_access_token(client_id: &str, client_secret: &str, token_url: &str) -> Result<TokenResponse, Box<dyn Error>> {\n    let client = Client::new();\n\n    // Prepare the request body\n    let params = [\n        (\"grant_type\", \"client_credentials\"),\n        (\"client_id\", client_id),\n        (\"client_secret\", client_secret),\n    ];\n\n    // Send the POST request to the token endpoint\n    let response = client\n        .post(token_url)\n        .form(&params)\n        .send()?;\n\n    // Check if the request was successful\n    if response.status().is_success() {\n        let token_response: TokenResponse = response.json()?;\n        Ok(token_response)\n    } else {\n        Err(format!(\"Failed to get access token: {}\", response.status()).into())\n    }\n}\n\nfn main() -> Result<(), Box<dyn Error>> {\n    // Replace with your actual client ID, client secret, and token URL\n    let client_id = \"your_client_id\";\n    let client_secret = \"your_client_secret\";\n    let token_url = \"https://example.com/oauth2/token\";\n\n    // Request the access token\n    let token_response = get_access_token(client_id, client_secret, token_url)?;\n\n    // Print the access token\n    println!(\"Access Token: {}\", token_response.access_token);\n    println!(\"Token Type: {}\", token_response.token_type);\n    println!(\"Expires In: {} seconds\", token_response.expires_in);\n\n    Ok(())\n}\n```\n\n### Comments:\n- **TokenResponse**: Struct to deserialize the JSON response from the OAuth2 token endpoint.\n- **get_access_token**: Function to request an access token using the client credentials flow.\n- **reqwest::blocking::Client**: Used to send HTTP requests synchronously.\n- **serde**: Used for serializing and deserializing JSON data.\n- **main**: Entry point of the program, where the access token is requested and printed.",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.IO;\nusing System.ServiceProcess;\n\npublic class FileSystemMonitorService : ServiceBase\n{\n    private FileSystemWatcher fileSystemWatcher;\n\n    public FileSystemMonitorService()\n    {\n        ServiceName = \"FileSystemMonitorService\";\n    }\n\n    protected override void OnStart(string[] args)\n    {\n        fileSystemWatcher = new FileSystemWatcher\n        {\n            Path = @\"C:\\Path\\To\\Monitor\",  // Set the path to monitor\n            IncludeSubdirectories = true,  // Watch subdirectories\n            NotifyFilter = NotifyFilters.FileName | NotifyFilters.DirectoryName | NotifyFilters.LastWrite\n        };\n\n        fileSystemWatcher.Changed += OnChanged;\n        fileSystemWatcher.Created += OnChanged;\n        fileSystemWatcher.Deleted += OnChanged;\n        fileSystemWatcher.Renamed += OnRenamed;\n        \n        fileSystemWatcher.EnableRaisingEvents = true; // Begin monitoring\n    }\n\n    private void OnChanged(object sender, FileSystemEventArgs e)\n    {\n        // Handle file change events (Changed, Created, Deleted)\n        LogChange(e.ChangeType.ToString(), e.FullPath);\n    }\n\n    private void OnRenamed(object sender, RenamedEventArgs e)\n    {\n        // Handle renamed event specifically\n        LogChange(\"Renamed\", $\"from {e.OldFullPath} to {e.FullPath}\");\n    }\n\n    private void LogChange(string changeType, string path)\n    {\n        // Log the changes (Replace with actual logging)\n        Console.WriteLine($\"Change Type: {changeType}, Path: {path}\");\n    }\n\n    protected override void OnStop()\n    {\n        // Cleanup resources\n        if (fileSystemWatcher != null)\n        {\n            fileSystemWatcher.Dispose();\n            fileSystemWatcher = null;\n        }\n    }\n    \n    public static void Main()\n    {\n        ServiceBase.Run(new FileSystemMonitorService());\n    }\n}\n```\n\n**Comments:**\n- First, the `FileSystemWatcher` is configured to monitor a specific path for changes.\n- The service listens for `Changed`, `Created`, `Deleted`, and `Renamed` events, processing them in the respective event handlers.\n- The `OnStart` method initializes the watcher, while `OnStop` disposes of it.\n- Make sure to replace the logging mechanism with the appropriate logging infrastructure for production use.",
    "label": 1
  },
  {
    "codes": "func (s *Table) SetCreatedBy(v string) *Table {\n\ts.CreatedBy = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom flask import Flask, request, jsonify\nfrom PIL import Image\nimport numpy as np\nimport tensorflow as tf\nimport io\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Load pre-trained model\nmodel = tf.keras.models.load_model('path_to_your_model.h5')\n\n# Define image preprocessing function\ndef preprocess_image(image):\n    image = image.resize((224, 224))  # Resize image to model's expected input size\n    image = np.array(image) / 255.0   # Normalize pixel values\n    image = np.expand_dims(image, axis=0)  # Add batch dimension\n    return image\n\n# Define API endpoint for image classification\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    try:\n        # Read and preprocess the image\n        image = Image.open(io.BytesIO(file.read()))\n        processed_image = preprocess_image(image)\n\n        # Make prediction\n        predictions = model.predict(processed_image)\n        predicted_class = np.argmax(predictions, axis=1)[0]\n\n        # Return the result\n        return jsonify({'predicted_class': int(predicted_class)}), 200\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n# Run the Flask app\nif __name__ == '__main__':\n    app.run(debug=True)\n```",
    "label": 1
  },
  {
    "codes": "```rust\n// Cargo.toml dependencies\n[dependencies]\nzip = \"0.5.13\"\naes = \"0.5.0\"\naes-gcm = \"0.10.0\"\nrand = \"0.8\"\n\n// src/lib.rs\nuse std::fs::File;\nuse std::io::{Read, Write, Cursor};\nuse zip::write::FileOptions;\nuse zip::{ZipWriter, ZipArchive};\nuse aes_gcm::{Aes256Gcm, aead::{Aead, NewAead}};\nuse aes::Aes256;\nuse rand::Rng;\nuse rand::rngs::OsRng;\n\npub fn create_zip_with_password(input_files: Vec<(&str, &str)>, password: &[u8], zip_filename: &str) -> zip::result::ZipResult<()> {\n    let file = File::create(zip_filename)?;\n    let mut zip = ZipWriter::new(file);\n    let options = FileOptions::default();\n    \n    let cipher = Aes256Gcm::new(aes_gcm::Key::from_slice(password));\n    \n    for (path, name) in input_files {\n        let mut f = File::open(path)?;\n        let mut buffer = Vec::new();\n        f.read_to_end(&mut buffer)?;\n        \n        let nonce = aes_gcm::Nonce::from_slice(&rand::thread_rng().gen::<[u8; 12]>());\n        let encrypted_data = cipher.encrypt(\n            nonce, \n            buffer.as_ref()\n        ).expect(\"Encryption failed\");\n\n        zip.start_file(name, options)?;\n        zip.write_all(&encrypted_data)?;\n        zip.write_all(&nonce)?;\n    }\n    \n    zip.finish()?;\n    Ok(())\n}\n\npub fn extract_zip_with_password(zip_filename: &str, output_dir: &str, password: &[u8]) -> zip::result::ZipResult<()> {\n    let file = File::open(zip_filename)?;\n    let mut archive = ZipArchive::new(file)?;\n\n    let cipher = Aes256Gcm::new(aes_gcm::Key::from_slice(password));\n\n    for i in 0..archive.len() {\n        let mut file = archive.by_index(i)?;\n        let mut buffer = Vec::new();\n        file.read_to_end(&mut buffer)?;\n        \n        let nonce = &buffer[buffer.len()-12..];\n        let ciphertext = &buffer[..buffer.len()-12];\n        let decrypted_data = cipher.decrypt(\n            aes_gcm::Nonce::from_slice(nonce),\n            ciphertext,\n        ).expect(\"Decryption failed\");\n        \n        let out_path = std::path::Path::new(output_dir).join(file.name());\n        let mut out_file = File::create(out_path)?;\n        out_file.write_all(&decrypted_data)?;\n    }\n    Ok(())\n}\n```",
    "label": 1
  },
  {
    "codes": "```swift\nimport SwiftUI\nimport HealthKit\n\nstruct LiveHeartRateView: View {\n    @StateObject private var heartRateManager = HeartRateManager()\n    \n    var body: some View {\n        VStack {\n            Text(\"Live Heart Rate\")\n                .font(.largeTitle)\n                .padding()\n            \n            if let heartRate = heartRateManager.heartRate {\n                Text(\"\\(heartRate) BPM\")\n                    .font(.system(size: 72))\n                    .fontWeight(.bold)\n            } else {\n                Text(\"Fetching...\")\n            }\n        }\n        .onAppear {\n            heartRateManager.start()\n        }\n    }\n}\n\nfinal class HeartRateManager: ObservableObject {\n    private let healthStore = HKHealthStore()\n    private var heartRateQuery: HKAnchoredObjectQuery?\n    @Published var heartRate: Int?\n    \n    init() {\n        requestAuthorization()\n    }\n    \n    private func requestAuthorization() {\n        guard HKHealthStore.isHealthDataAvailable() else { return }\n        \n        let heartRateType = HKObjectType.quantityType(forIdentifier: .heartRate)!\n        \n        healthStore.requestAuthorization(toShare: [], read: [heartRateType]) { (success, error) in\n            if success {\n                self.startHeartRateQuery(for: heartRateType)\n            }\n        }\n    }\n    \n    private func startHeartRateQuery(for sampleType: HKSampleType) {\n        let predicate = HKQuery.predicateForSamples(withStart: Date.distantPast, end: Date(), options: .strictEndDate)\n        \n        heartRateQuery = HKAnchoredObjectQuery(\n            type: sampleType,\n            predicate: predicate,\n            anchor: nil,\n            limit: HKObjectQueryNoLimit\n        ) { query, samples, deletedObjects, newAnchor, error in\n            self.updateHeartRate(samples)\n        }\n        \n        heartRateQuery!.updateHandler = { query, samples, deletedObjects, newAnchor, error in\n            self.updateHeartRate(samples)\n        }\n        \n        healthStore.execute(heartRateQuery!)\n    }\n    \n    private func updateHeartRate(_ samples: [HKSample]?) {\n        guard let samples = samples as? [HKQuantitySample] else { return }\n        \n        DispatchQueue.main.async {\n            if let heartRateSample = samples.last {\n                let heartRateUnit = HKUnit(from: \"count/min\")\n                let heartRate = heartRateSample.quantity.doubleValue(for: heartRateUnit)\n                self.heartRate = Int(heartRate)\n            }\n        }\n    }\n    \n    func start() {\n        if heartRateQuery == nil {\n            let heartRateType = HKObjectType.quantityType(forIdentifier: .heartRate)!\n            startHeartRateQuery(for: heartRateType)\n        }\n    }\n}\n\nstruct ContentView: View {\n    var body: some View {\n        LiveHeartRateView()\n    }\n}\n\n@main\nstruct MyApp: App {\n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport ddf.minim.*;\nimport ddf.minim.analysis.*;\n\nMinim minim;\nAudioInput in;\nFFT fft;\n\nvoid setup() {\n  size(800, 400);\n  minim = new Minim(this);\n  \n  // Get the default audio input\n  in = minim.getLineIn(Minim.STEREO, 1024);\n  \n  // Initialize FFT with the buffer size and sample rate\n  fft = new FFT(in.bufferSize(), in.sampleRate());\n}\n\nvoid draw() {\n  background(0);\n  stroke(255);\n  \n  // Perform FFT on the audio input\n  fft.forward(in.mix);\n  \n  // Draw the frequency spectrum\n  for (int i = 0; i < fft.specSize(); i++) {\n    float x = map(i, 0, fft.specSize(), 0, width);\n    float h = map(fft.getBand(i), 0, 10, 0, height);\n    line(x, height, x, height - h);\n  }\n}\n\nvoid stop() {\n  // Close Minim audio classes\n  in.close();\n  minim.stop();\n  super.stop();\n}\n```",
    "label": 1
  },
  {
    "codes": "```typescript\n// Define a type for a valid RGB tuple\ntype RGB = [number, number, number];\n\n// Type guard function to check if a value is a valid RGB tuple\nfunction isRGB(value: unknown): value is RGB {\n    // Check if the value is an array with exactly 3 elements\n    if (!Array.isArray(value) || value.length !== 3) {\n        return false;\n    }\n\n    // Check if all elements are numbers between 0 and 255\n    return value.every((element) => typeof element === 'number' && element >= 0 && element <= 255);\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model)\n\n# Save the CoreML model to a file\ncoreml_model.save('your_model.mlmodel')\n\n# Print confirmation\nprint(\"Keras model successfully converted to CoreML format and saved as 'your_model.mlmodel'.\")\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glue) GetTags(input *GetTagsInput) (*GetTagsOutput, error) {\n\treq, out := c.GetTagsRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "def claim_pep_node(self, node_namespace, *,\n                       register_feature=True, notify=False):\n        \"\"\"\n        Claim node `node_namespace`.\n\n        :param node_namespace: the pubsub node whose events shall be\n            handled.\n        :param register_feature: Whether to publish the `node_namespace`\n            as feature.\n        :param notify: Whether to register the ``+notify`` feature to\n            receive notification without explicit subscription.\n        :raises RuntimeError: if a handler for `node_namespace` is already\n            set.\n        :returns: a :class:`~aioxmpp.pep.service.RegisteredPEPNode` instance\n            representing the claim.\n\n        .. seealso::\n\n            :class:`aioxmpp.pep.register_pep_node`\n              a descriptor which can be used with\n              :class:`~aioxmpp.service.Service` subclasses to claim a PEP node\n              automatically.\n\n        This registers `node_namespace` as feature for service discovery\n        unless ``register_feature=False`` is passed.\n\n        .. note::\n\n            For `notify` to work, it is required that\n            :class:`aioxmpp.EntityCapsService` is loaded and that presence is\n            re-sent soon after\n            :meth:`~aioxmpp.EntityCapsService.on_ver_changed` fires. See the\n            documentation of the class and the signal for details.\n\n        \"\"\"\n        if node_namespace in self._pep_node_claims:\n            raise RuntimeError(\n                \"claiming already claimed node\"\n            )\n        registered_node = RegisteredPEPNode(\n            self,\n            node_namespace,\n            register_feature=register_feature,\n            notify=notify,\n        )\n\n        finalizer = weakref.finalize(\n            registered_node,\n            weakref.WeakMethod(registered_node._unregister)\n        )\n        # we cannot guarantee that disco is not cleared up already,\n        # so we do not unclaim the feature on exit\n        finalizer.atexit = False\n\n        self._pep_node_claims[node_namespace] = registered_node\n\n        return registered_node",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<VaultInner> updateAsync(String resourceGroupName, String vaultName, VaultPatchParameters parameters, final ServiceCallback<VaultInner> serviceCallback) {\n        return ServiceFuture.fromResponse(updateWithServiceResponseAsync(resourceGroupName, vaultName, parameters), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "public CompletableFuture<QueueRuntimeInfo> getQueueRuntimeInfoAsync(String path) {\n        EntityNameHelper.checkValidQueueName(path);\n\n        CompletableFuture<String> contentFuture = getEntityAsync(path, null, true);\n        CompletableFuture<QueueRuntimeInfo> qdFuture = new CompletableFuture<>();\n        contentFuture.handleAsync((content, ex) -> {\n            if (ex != null) {\n                qdFuture.completeExceptionally(ex);\n            } else {\n                try {\n                    qdFuture.complete(QueueRuntimeInfoSerializer.parseFromContent(content));\n                } catch (MessagingEntityNotFoundException e) {\n                    qdFuture.completeExceptionally(e);\n                }\n            }\n            return null;\n        }, MessagingFactory.INTERNAL_THREAD_POOL);\n\n        return qdFuture;\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Drop rows with any null values\ndf_cleaned = df.dropna()\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```\n\n```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Drop columns with any null values\ndf_cleaned = df.dropna(axis=1)\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```\n\n```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Fill null values with a specific value (e.g., 0)\ndf_cleaned = df.fillna(0)\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```\n\n```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Drop rows where all values are null\ndf_cleaned = df.dropna(how='all')\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```\n\n```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Drop rows with null values in specific columns\ndf_cleaned = df.dropna(subset=['column1', 'column2'])\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glue) StartJobRun(input *StartJobRunInput) (*StartJobRunOutput, error) {\n\treq, out := c.StartJobRunRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <opencv2/opencv.hpp>\n#include <iostream>\n\nint main() {\n    // Open video file\n    cv::VideoCapture cap(\"input_video.mp4\");\n    if (!cap.isOpened()) {\n        std::cerr << \"Error: Could not open video file.\" << std::endl;\n        return -1;\n    }\n\n    // Define preprocessing parameters\n    const int target_width = 224;  // Target width for model input\n    const int target_height = 224; // Target height for model input\n    const double scale_factor = 1.0 / 255.0; // Normalization factor\n\n    cv::Mat frame, resized_frame, normalized_frame;\n\n    while (cap.read(frame)) {\n        // Resize frame to target dimensions\n        cv::resize(frame, resized_frame, cv::Size(target_width, target_height));\n\n        // Normalize pixel values to [0, 1]\n        resized_frame.convertTo(normalized_frame, CV_32F, scale_factor);\n\n        // Optional: Convert to grayscale if required by the model\n        // cv::cvtColor(normalized_frame, normalized_frame, cv::COLOR_BGR2GRAY);\n\n        // Optional: Apply additional preprocessing (e.g., mean subtraction)\n        // cv::Scalar mean_value = cv::mean(normalized_frame);\n        // normalized_frame -= mean_value;\n\n        // Display preprocessed frame (for debugging)\n        cv::imshow(\"Preprocessed Frame\", normalized_frame);\n        if (cv::waitKey(30) >= 0) break;\n    }\n\n    cap.release();\n    cv::destroyAllWindows();\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "def _translate_range(self, len_, start, end):\n        \"\"\"\n        Translate range to valid bounds.\n        \"\"\"\n        if start < 0:\n            start += len_\n        start = max(0, min(start, len_))\n        if end < 0:\n            end += len_\n        end = max(-1, min(end, len_ - 1))\n        return start, end",
    "label": 0
  },
  {
    "codes": "public Observable<Page<NodeFile>> listFromTaskAsync(final String jobId, final String taskId, final Boolean recursive, final FileListFromTaskOptions fileListFromTaskOptions) {\n        return listFromTaskWithServiceResponseAsync(jobId, taskId, recursive, fileListFromTaskOptions)\n                .map(new Func1<ServiceResponseWithHeaders<Page<NodeFile>, FileListFromTaskHeaders>, Page<NodeFile>>() {\n                    @Override\n                    public Page<NodeFile> call(ServiceResponseWithHeaders<Page<NodeFile>, FileListFromTaskHeaders> response) {\n                        return response.body();\n                    }\n                });\n    }",
    "label": 0
  },
  {
    "codes": "function () {\n        // Rect before any transform\n        var rawRect = this.getBoundingRect();\n        var cx = rawRect.x + rawRect.width / 2;\n        var cy = rawRect.y + rawRect.height / 2;\n\n        return [cx, cy];\n    }",
    "label": 0
  },
  {
    "codes": "```gdscript\nshader_type canvas_item;\n\nuniform sampler2D refraction_texture;\nuniform vec2 refraction_strength : hint_range(0.0, 1.0);\n\nvoid fragment() {\n    vec2 screen_uv = SCREEN_UV;\n    \n    // Get the normal from the normal map\n    vec3 normal = texture(TEXTURE, UV).rgb * 2.0 - 1.0;\n    \n    // Calculate the refraction effect\n    vec2 refraction = normal.xy * refraction_strength;\n    \n    // Apply refraction to the screen UV\n    vec2 refracted_uv = screen_uv + refraction;\n    \n    // Sample the refracted texture\n    vec4 refracted_color = texture(refraction_texture, refracted_uv);\n    \n    // Output the refracted color as the final color\n    COLOR = refracted_color;\n}\n```\n",
    "label": 1
  },
  {
    "codes": "function() {\n      var i        = 0,\n          length   = arr.length,\n          newArray = [];\n      for (; i<length; i++) {\n        newArray.push(arr[i]);\n      }\n      return newArray;\n    }",
    "label": 0
  },
  {
    "codes": "function killFirefox() {\n      helper.removeEventListeners(listeners);\n      if (firefoxProcess.pid && !firefoxProcess.killed && !firefoxClosed) {\n        // Force kill chrome.\n        try {\n          if (process.platform === 'win32')\n            childProcess.execSync(`taskkill /pid ${firefoxProcess.pid} /T /F`);\n          else\n            process.kill(-firefoxProcess.pid, 'SIGKILL');\n        } catch (e) {\n          // the process might have already stopped\n        }\n      }\n      // Attempt to remove temporary profile directory to avoid littering.\n      try {\n        removeFolder.sync(temporaryProfileDir);\n      } catch (e) { }\n    }",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <SoftwareSerial.h>\n#include <Servo.h>\n\nServo myServo;            // Create servo object to control a servo\nSoftwareSerial BTSerial(10, 11); // RX, TX for Bluetooth module\n\nchar command;             // Variable to store Bluetooth command\n\nvoid setup() {\n  myServo.attach(9);      // Attaches the servo on pin 9 to the servo object\n  BTSerial.begin(9600);   // Set Bluetooth communication speed\n  Serial.begin(9600);     // Start the serial communication\n  Serial.println(\"Servo Bluetooth Control Ready\");\n}\n\nvoid loop() {\n  // Check if data is available from the Bluetooth module\n  if (BTSerial.available()) {\n    command = BTSerial.read(); // Read the incoming data\n    \n    // Respond to commands: 'L' for left, 'R' for right, and 'C' for center\n    if (command == 'L') {\n      myServo.write(10);  // Rotate servo to 10 degrees\n      Serial.println(\"Turned Left\");\n    } else if (command == 'R') {\n      myServo.write(170); // Rotate servo to 170 degrees\n      Serial.println(\"Turned Right\");\n    } else if (command == 'C') {\n      myServo.write(90);  // Centering the servo\n      Serial.println(\"Centered\");\n    }\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n// Header file: DynamicWeatherSystem.h\n\n#pragma once\n\n#include \"CoreMinimal.h\"\n#include \"GameFramework/Actor.h\"\n#include \"DynamicWeatherSystem.generated.h\"\n\nUCLASS()\nclass YOURGAME_API ADynamicWeatherSystem : public AActor\n{\n    GENERATED_BODY()\n    \npublic:    \n    // Sets default values for this actor's properties\n    ADynamicWeatherSystem();\n\nprotected:\n    // Called when the game starts or when spawned\n    virtual void BeginPlay() override;\n\npublic:    \n    // Called every frame\n    virtual void Tick(float DeltaTime) override;\n\n    // Function to transition weather\n    UFUNCTION(BlueprintCallable, Category = \"Weather\")\n    void TransitionWeather(FName NewWeatherType, float TransitionDuration);\n\nprivate:\n    // Current weather type\n    UPROPERTY(VisibleAnywhere, Category = \"Weather\")\n    FName CurrentWeatherType;\n\n    // Target weather type for transition\n    UPROPERTY(VisibleAnywhere, Category = \"Weather\")\n    FName TargetWeatherType;\n\n    // Timer handle for weather transition\n    FTimerHandle WeatherTransitionTimer;\n\n    // Function to update weather parameters\n    void UpdateWeather(float BlendFactor);\n\n    // Function to handle weather transition completion\n    void OnWeatherTransitionComplete();\n};\n\n// Source file: DynamicWeatherSystem.cpp\n\n#include \"DynamicWeatherSystem.h\"\n#include \"Kismet/GameplayStatics.h\"\n\n// Sets default values\nADynamicWeatherSystem::ADynamicWeatherSystem()\n{\n    // Set this actor to call Tick() every frame\n    PrimaryActorTick.bCanEverTick = true;\n\n    // Initialize default weather type\n    CurrentWeatherType = \"Clear\";\n    TargetWeatherType = \"Clear\";\n}\n\n// Called when the game starts or when spawned\nvoid ADynamicWeatherSystem::BeginPlay()\n{\n    Super::BeginPlay();\n}\n\n// Called every frame\nvoid ADynamicWeatherSystem::Tick(float DeltaTime)\n{\n    Super::Tick(DeltaTime);\n}\n\n// Function to transition weather\nvoid ADynamicWeatherSystem::TransitionWeather(FName NewWeatherType, float TransitionDuration)\n{\n    if (CurrentWeatherType == NewWeatherType)\n    {\n        return; // No transition needed if already in the target weather\n    }\n\n    TargetWeatherType = NewWeatherType;\n\n    // Clear any existing transition timer\n    GetWorld()->GetTimerManager().ClearTimer(WeatherTransitionTimer);\n\n    // Start a new transition timer\n    GetWorld()->GetTimerManager().SetTimer(WeatherTransitionTimer, this, &ADynamicWeatherSystem::OnWeatherTransitionComplete, TransitionDuration, false);\n\n    // Update weather parameters over time\n    float BlendFactor = 0.0f;\n    while (BlendFactor < 1.0f)\n    {\n        BlendFactor += DeltaTime / TransitionDuration;\n        UpdateWeather(BlendFactor);\n    }\n}\n\n// Function to update weather parameters\nvoid ADynamicWeatherSystem::UpdateWeather(float BlendFactor)\n{\n    // Implement logic to blend between current and target weather parameters\n    // Example: Blend sky color, fog density, precipitation intensity, etc.\n}\n\n// Function to handle weather transition completion\nvoid ADynamicWeatherSystem::OnWeatherTransitionComplete()\n{\n    CurrentWeatherType = TargetWeatherType;\n    // Optionally trigger any post-transition events or effects\n}\n```\n\n### Comments:\n- **DynamicWeatherSystem.h**: Defines the `ADynamicWeatherSystem` class with properties and methods for managing weather transitions.\n- **DynamicWeatherSystem.cpp**: Implements the logic for transitioning between weather types over a specified duration.\n- **TransitionWeather**: Initiates a weather transition from the current weather to a new weather type over a given duration.\n- **UpdateWeather**: Handles the blending of weather parameters (e.g., sky color, fog density) during the transition.\n- **OnWeatherTransitionComplete**: Called when the transition is complete, updating the current weather type and triggering any post-transition effects.\n\nThis implementation allows for smooth transitions between different weather states, with customizable parameters and durations.",
    "label": 1
  },
  {
    "codes": "```c\n#include <stdio.h>\n#include <dirent.h>\n#include <sys/stat.h>\n#include <string.h>\n\n// Function to list files recursively\nvoid listFilesRecursively(char *basePath) {\n    char path[1000];\n    struct dirent *dp;\n    struct stat statbuf;\n    DIR *dir = opendir(basePath);\n\n    // Unable to open directory stream\n    if (!dir) return;\n\n    while ((dp = readdir(dir)) != NULL) {\n        if (strcmp(dp->d_name, \".\") != 0 && strcmp(dp->d_name, \"..\") != 0) {\n            // Construct new path from base path\n            strcpy(path, basePath);\n            strcat(path, \"/\");\n            strcat(path, dp->d_name);\n\n            // Check if the path is a directory\n            if (stat(path, &statbuf) == 0 && S_ISDIR(statbuf.st_mode)) {\n                // Recursively list files in the directory\n                listFilesRecursively(path);\n            } else {\n                // Print the file path\n                printf(\"%s\\n\", path);\n            }\n        }\n    }\n\n    closedir(dir);\n}\n\nint main() {\n    // Directory path to list files\n    char path[100] = \".\"; // Current directory\n\n    // Call the recursive function\n    listFilesRecursively(path);\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *UpdateTrailInput) SetCloudWatchLogsRoleArn(v string) *UpdateTrailInput {\n\ts.CloudWatchLogsRoleArn = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *S3Location) SetAccessControlList(v []*Grant) *S3Location {\n\ts.AccessControlList = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```c\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <openssl/err.h>\n#include <stdio.h>\n#include <string.h>\n\n// Wrapper function for AES-GCM encryption\nint aes_gcm_encrypt(const unsigned char *plaintext, size_t plaintext_len,\n                    const unsigned char *key, const unsigned char *iv,\n                    unsigned char *ciphertext, unsigned char *tag) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int ciphertext_len;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        ERR_print_errors_fp(stderr);\n        return -1;\n    }\n\n    // Initialize the encryption operation\n    if (1 != EVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, key, iv)) {\n        ERR_print_errors_fp(stderr);\n        return -1;\n    }\n\n    // Encrypt the plaintext\n    if (1 != EVP_EncryptUpdate(ctx, ciphertext, &len, plaintext, plaintext_len)) {\n        ERR_print_errors_fp(stderr);\n        return -1;\n    }\n    ciphertext_len = len;\n\n    // Finalize the encryption\n    if (1 != EVP_EncryptFinal_ex(ctx, ciphertext + len, &len)) {\n        ERR_print_errors_fp(stderr);\n        return -1;\n    }\n    ciphertext_len += len;\n\n    // Get the authentication tag\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_GET_TAG, 16, tag)) {\n        ERR_print_errors_fp(stderr);\n        return -1;\n    }\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    return ciphertext_len;\n}\n\n// Wrapper function for AES-GCM decryption\nint aes_gcm_decrypt(const unsigned char *ciphertext, size_t ciphertext_len,\n                    const unsigned char *key, const unsigned char *iv,\n                    const unsigned char *tag, unsigned char *plaintext) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int plaintext_len;\n    int ret;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        ERR_print_errors_fp(stderr);\n        return -1;\n    }\n\n    // Initialize the decryption operation\n    if (1 != EVP_DecryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, key, iv)) {\n        ERR_print_errors_fp(stderr);\n        return -1;\n    }\n\n    // Decrypt the ciphertext\n    if (1 != EVP_DecryptUpdate(ctx, plaintext, &len, ciphertext, ciphertext_len)) {\n        ERR_print_errors_fp(stderr);\n        return -1;\n    }\n    plaintext_len = len;\n\n    // Set the expected tag value\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_TAG, 16, (void *)tag)) {\n        ERR_print_errors_fp(stderr);\n        return -1;\n    }\n\n    // Finalize the decryption\n    ret = EVP_DecryptFinal_ex(ctx, plaintext + len, &len);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    if (ret > 0) {\n        plaintext_len += len;\n        return plaintext_len;\n    } else {\n        // Verification failed\n        return -1;\n    }\n}\n\n// Example usage\nint main() {\n    unsigned char key[32]; // 256-bit key\n    unsigned char iv[12];  // 96-bit IV\n    unsigned char plaintext[] = \"This is a secret message.\";\n    unsigned char ciphertext[128];\n    unsigned char decryptedtext[128];\n    unsigned char tag[16];\n\n    // Generate random key and IV\n    if (!RAND_bytes(key, sizeof(key)) || !RAND_bytes(iv, sizeof(iv))) {\n        fprintf(stderr, \"Error generating random bytes\\n\");\n        return 1;\n    }\n\n    // Encrypt the plaintext\n    int ciphertext_len = aes_gcm_encrypt(plaintext, strlen((char *)plaintext), key, iv, ciphertext, tag);\n    if (ciphertext_len == -1) {\n        fprintf(stderr, \"Encryption failed\\n\");\n        return 1;\n    }\n\n    // Decrypt the ciphertext\n    int decryptedtext_len = aes_gcm_decrypt(ciphertext, ciphertext_len, key, iv, tag, decryptedtext);\n    if (decryptedtext_len == -1) {\n        fprintf(stderr, \"Decryption failed\\n\");\n        return 1;\n    }\n\n    // Null-terminate the decrypted text\n    decryptedtext[decryptedtext_len] = '\\0';\n\n    // Print the results\n    printf(\"Original: %s\\n\", plaintext);\n    printf(\"Decrypted: %s\\n\", decryptedtext);\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```c\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <stdlib.h>\n#include <string.h>\n\n#define AES_KEYLEN 256\n#define AES_IVLEN 12\n#define AES_TAGLEN 16\n\ntypedef struct {\n    EVP_CIPHER_CTX *ctx;\n    unsigned char key[AES_KEYLEN / 8];\n    unsigned char iv[AES_IVLEN];\n} AESGCMContext;\n\n// Function to initialize AES-GCM context with a given key\nAESGCMContext* aesgcm_init(const unsigned char *key) {\n    AESGCMContext *ctx = malloc(sizeof(AESGCMContext));\n    if (!ctx) return NULL;\n\n    ctx->ctx = EVP_CIPHER_CTX_new();\n    if (!ctx->ctx) {\n        free(ctx);\n        return NULL;\n    }\n\n    memset(ctx->key, 0, AES_KEYLEN / 8);\n    memcpy(ctx->key, key, AES_KEYLEN / 8);\n\n    RAND_bytes(ctx->iv, AES_IVLEN); // Random IV\n\n    return ctx;\n}\n\n// Function to perform AES-GCM encryption\nint aesgcm_encrypt(AESGCMContext *ctx, const unsigned char *plaintext, int plaintext_len,\n                   const unsigned char *aad, int aad_len, unsigned char *ciphertext,\n                   unsigned char *tag) {\n    int len, ciphertext_len;\n\n    if (1 != EVP_EncryptInit_ex(ctx->ctx, EVP_aes_256_gcm(), NULL, NULL, NULL))\n        return -1;\n\n    if (1 != EVP_EncryptInit_ex(ctx->ctx, NULL, NULL, ctx->key, ctx->iv))\n        return -1;\n\n    if (1 != EVP_EncryptUpdate(ctx->ctx, NULL, &len, aad, aad_len))\n        return -1;\n\n    if (1 != EVP_EncryptUpdate(ctx->ctx, ciphertext, &len, plaintext, plaintext_len))\n        return -1;\n    ciphertext_len = len;\n\n    if (1 != EVP_EncryptFinal_ex(ctx->ctx, ciphertext + len, &len))\n        return -1;\n    ciphertext_len += len;\n\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx->ctx, EVP_CTRL_GCM_GET_TAG, AES_TAGLEN, tag))\n        return -1;\n\n    return ciphertext_len;\n}\n\n// Function to perform AES-GCM decryption\nint aesgcm_decrypt(AESGCMContext *ctx, const unsigned char *ciphertext, int ciphertext_len,\n                   const unsigned char *aad, int aad_len, unsigned char *tag,\n                   unsigned char *plaintext) {\n    int len, plaintext_len;\n    int ret;\n\n    if (!EVP_DecryptInit_ex(ctx->ctx, EVP_aes_256_gcm(), NULL, NULL, NULL))\n        return -1;\n\n    if (!EVP_DecryptInit_ex(ctx->ctx, NULL, NULL, ctx->key, ctx->iv))\n        return -1;\n\n    if (!EVP_DecryptUpdate(ctx->ctx, NULL, &len, aad, aad_len))\n        return -1;\n\n    if (!EVP_DecryptUpdate(ctx->ctx, plaintext, &len, ciphertext, ciphertext_len))\n        return -1;\n    plaintext_len = len;\n\n    if (!EVP_CIPHER_CTX_ctrl(ctx->ctx, EVP_CTRL_GCM_SET_TAG, AES_TAGLEN, tag))\n        return -1;\n\n    ret = EVP_DecryptFinal_ex(ctx->ctx, plaintext + len, &len);\n\n    return ret > 0 ? (plaintext_len + len) : -1;\n}\n\n// Function to free AES-GCM context\nvoid aesgcm_cleanup(AESGCMContext *ctx) {\n    if (ctx) {\n        if (ctx->ctx)\n            EVP_CIPHER_CTX_free(ctx->ctx);\n        free(ctx);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<CertificateBundle> mergeCertificateAsync(String vaultBaseUrl, String certificateName, List<byte[]> x509Certificates, CertificateAttributes certificateAttributes, Map<String, String> tags, final ServiceCallback<CertificateBundle> serviceCallback) {\n        return ServiceFuture.fromResponse(mergeCertificateWithServiceResponseAsync(vaultBaseUrl, certificateName, x509Certificates, certificateAttributes, tags), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```ruby\n# Define a simple Fiber-based scheduler\nclass FiberScheduler\n  def initialize\n    @fibers = []  # Queue to hold fibers\n  end\n\n  # Add a fiber to the scheduler\n  def schedule(&block)\n    fiber = Fiber.new(&block)\n    @fibers << fiber\n    fiber\n  end\n\n  # Run the scheduler and execute fibers\n  def run\n    while !@fibers.empty?\n      fiber = @fibers.shift  # Get the next fiber\n      fiber.resume  # Resume the fiber\n      @fibers << fiber if fiber.alive?  # Requeue if not finished\n    end\n  end\nend\n\n# Example usage\nscheduler = FiberScheduler.new\n\n# Schedule a task\nscheduler.schedule do\n  puts \"Task 1: Starting\"\n  Fiber.yield  # Yield control back to the scheduler\n  puts \"Task 1: Resuming\"\nend\n\n# Schedule another task\nscheduler.schedule do\n  puts \"Task 2: Starting\"\n  Fiber.yield  # Yield control back to the scheduler\n  puts \"Task 2: Resuming\"\nend\n\n# Run the scheduler\nscheduler.run\n```",
    "label": 1
  },
  {
    "codes": "def unregister_callback(self, type_, from_, *,\n                            wildcard_resource=True):\n        \"\"\"\n        Unregister a callback function.\n\n        :param type_: Stanza type to listen for, or :data:`None` for a\n                      wildcard match.\n        :param from_: Sender to listen for, or :data:`None` for a full wildcard\n                      match.\n        :type from_: :class:`aioxmpp.JID` or :data:`None`\n        :param wildcard_resource: Whether to wildcard the resourcepart of the\n                                  JID.\n        :type wildcard_resource: :class:`bool`\n\n        The callback must be disconnected with the same arguments as were used\n        to connect it.\n        \"\"\"\n        if from_ is None or not from_.is_bare:\n            wildcard_resource = False\n\n        self._map.pop((type_, from_, wildcard_resource))",
    "label": 0
  },
  {
    "codes": "```python\nclass NMEAParser:\n    def __init__(self):\n        self.sentence_types = {\n            \"GGA\": self.parse_gga,\n            \"RMC\": self.parse_rmc,\n            \"GSV\": self.parse_gsv,\n            \"GSA\": self.parse_gsa\n        }\n    \n    def parse(self, nmea_sentence):\n        try:\n            # Split the sentence into components\n            parts = nmea_sentence.strip().split(',')\n            # Determine sentence type (e.g., GGA, RMC)\n            sentence_type = parts[0][3:]\n            # Locate the parsing function\n            parse_function = self.sentence_types.get(sentence_type)\n            # If a function exists, apply it to the components\n            if parse_function:\n                return parse_function(parts)\n            else:\n                return None\n        except Exception as e:\n            # Return exception information if parsing fails\n            return {\"error\": str(e)}\n    \n    def parse_gga(self, data_parts):\n        try:\n            return {\n                \"sentence_type\": \"GGA\",\n                \"utc_time\": data_parts[1],\n                \"latitude\": self.coord_to_decimal(data_parts[2], data_parts[3]),\n                \"longitude\": self.coord_to_decimal(data_parts[4], data_parts[5]),\n                \"fix_quality\": data_parts[6],\n                \"num_satellites\": data_parts[7],\n                \"horizontal_dilution\": data_parts[8],\n                \"altitude\": data_parts[9]\n            }\n        except Exception as e:\n            # Handle any exceptions for GGA parsing\n            return {\"error\": f\"GGA parsing failed: {str(e)}\"}\n    \n    def parse_rmc(self, data_parts):\n        try:\n            return {\n                \"sentence_type\": \"RMC\",\n                \"utc_time\": data_parts[1],\n                \"status\": data_parts[2],\n                \"latitude\": self.coord_to_decimal(data_parts[3], data_parts[4]),\n                \"longitude\": self.coord_to_decimal(data_parts[5], data_parts[6]),\n                \"speed_over_ground\": data_parts[7],\n                \"track_angle\": data_parts[8],\n                \"date\": data_parts[9]\n            }\n        except Exception as e:\n            # Handle any exceptions for RMC parsing\n            return {\"error\": f\"RMC parsing failed: {str(e)}\"}\n    \n    def parse_gsv(self, data_parts):\n        try:\n            return {\n                \"sentence_type\": \"GSV\",\n                \"num_messages\": data_parts[1],\n                \"message_number\": data_parts[2],\n                \"satellites_in_view\": data_parts[3],\n            }\n        except Exception as e:\n            # Handle any exceptions for GSV parsing\n            return {\"error\": f\"GSV parsing failed: {str(e)}\"}\n    \n    def parse_gsa(self, data_parts):\n        try:\n            return {\n                \"sentence_type\": \"GSA\",\n                \"mode\": data_parts[1],\n                \"fix_type\": data_parts[2],\n                \"satellite_prn\": data_parts[3:15],\n                \"pdop\": data_parts[15],\n                \"hdop\": data_parts[16],\n                \"vdop\": data_parts[17]\n            }\n        except Exception as e:\n            # Handle any exceptions for GSA parsing\n            return {\"error\": f\"GSA parsing failed: {str(e)}\"}\n\n    def coord_to_decimal(self, coord, direction):\n        try:\n            # Convert NMEA coordinate format to decimal degrees\n            deg = float(coord[:2])\n            min = float(coord[2:])\n            dec = deg + (min / 60)\n            if direction in ['S', 'W']:\n                return -dec\n            return dec\n        except ValueError:\n            # Handle invalid coordinate conversion\n            raise ValueError(\"Invalid coordinate format\")\n```",
    "label": 1
  },
  {
    "codes": "public HostingEnvironmentDiagnosticsInner getDiagnosticsItem(String resourceGroupName, String name, String diagnosticsName) {\n        return getDiagnosticsItemWithServiceResponseAsync(resourceGroupName, name, diagnosticsName).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<List<HostingEnvironmentDiagnosticsInner>> listDiagnosticsAsync(String resourceGroupName, String name) {\n        return listDiagnosticsWithServiceResponseAsync(resourceGroupName, name).map(new Func1<ServiceResponse<List<HostingEnvironmentDiagnosticsInner>>, List<HostingEnvironmentDiagnosticsInner>>() {\n            @Override\n            public List<HostingEnvironmentDiagnosticsInner> call(ServiceResponse<List<HostingEnvironmentDiagnosticsInner>> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```swift\nimport Foundation\n\n// Struct to model polynomial equations\nstruct Polynomial {\n    // Coefficients of the polynomial, where the index represents the power of x\n    var coefficients: [Double]\n    \n    // Method to evaluate the polynomial at a given x value\n    func evaluate(at x: Double) -> Double {\n        // Use the reduce function to compute the polynomial value\n        return coefficients.enumerated().reduce(0.0) { (result, term) -> Double in\n            let (power, coefficient) = term\n            // Calculate term as coefficient * (x ^ power)\n            let termValue = coefficient * pow(x, Double(power))\n            // Add term value to the result\n            return result + termValue\n        }\n    }\n}\n\n// Example usage\nlet polynomial = Polynomial(coefficients: [3.0, 2.0, 5.0]) // Represents 3 + 2x + 5x^2\nlet result = polynomial.evaluate(at: 2.0) // Evaluates to 3 + 2*2 + 5*2^2 = 27\n```",
    "label": 1
  },
  {
    "codes": "public ManagedInstanceEncryptionProtectorInner get(String resourceGroupName, String managedInstanceName) {\n        return getWithServiceResponseAsync(resourceGroupName, managedInstanceName).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "function createPythonClass (className, baseClass, body, methods, async = false) {\n\n    const pythonStandardLibraries = {\n        'base64': 'base64',\n        'hashlib': 'hashlib',\n        'math': 'math',\n        'json.loads': 'json',\n    }\n\n    async = async ? 'async_support.' : ''\n\n    const importFrom = (baseClass == 'Exchange') ?\n        ('ccxt.' + async + 'base.exchange') :\n        ('ccxt.' + async + baseClass)\n\n    let bodyAsString = body.join (\"\\n\")\n\n    let header = [\n        \"# -*- coding: utf-8 -*-\\n\",\n        \"# PLEASE DO NOT EDIT THIS FILE, IT IS GENERATED AND WILL BE OVERWRITTEN:\",\n        \"# https://github.com/ccxt/ccxt/blob/master/CONTRIBUTING.md#how-to-contribute-code\\n\",\n        'from ' + importFrom + ' import ' + baseClass,\n        ... (bodyAsString.match (/basestring/) ? [\n            \"\\n# -----------------------------------------------------------------------------\\n\",\n            \"try:\",\n            \"    basestring  # Python 3\",\n            \"except NameError:\",\n            \"    basestring = str  # Python 2\",\n        ] : [])\n    ]\n\n    const libraries = []\n\n    for (let library in pythonStandardLibraries) {\n        const regex = new RegExp (\"[^\\\\']\" + library + \"[^\\\\'a-zA-Z]\")\n        if (bodyAsString.match (regex))\n            libraries.push ('import ' + pythonStandardLibraries[library])\n    }\n\n    const errorImports = []\n\n    for (let error in errors) {\n        const regex = new RegExp (\"[^\\\\'\\\"]\" + error + \"[^\\\\'\\\"]\")\n        if (bodyAsString.match (regex))\n            errorImports.push ('from ccxt.base.errors import ' + error)\n    }\n\n    const precisionImports = []\n\n    for (let constant in precisionConstants) {\n        if (bodyAsString.indexOf (constant) >= 0) {\n            precisionImports.push ('from ccxt.base.decimal_to_precision import ' + constant)\n        }\n    }\n\n    header = header.concat (libraries, errorImports, precisionImports)\n\n    for (let method of methods) {\n        const regex = new RegExp ('self\\\\.(' + method + ')\\\\s*\\\\(', 'g')\n        bodyAsString = bodyAsString.replace (regex,\n            (match, p1) => ('self.' + unCamelCase (p1) + '('))\n    }\n\n    header.push (\"\\n\\nclass \" + className + ' (' + baseClass + '):')\n\n    const footer = [\n        '', // footer (last empty line)\n    ]\n\n    const result = header.join (\"\\n\") + \"\\n\" + bodyAsString + \"\\n\" + footer.join ('\\n')\n    return result\n}",
    "label": 0
  },
  {
    "codes": "def _read_from_socket(self):\n        \"\"\"Read data from the socket.\n\n        :rtype: bytes\n        \"\"\"\n        if not self.use_ssl:\n            if not self.socket:\n                raise socket.error('connection/socket error')\n            return self.socket.recv(MAX_FRAME_SIZE)\n\n        with self._rd_lock:\n            if not self.socket:\n                raise socket.error('connection/socket error')\n            return self.socket.read(MAX_FRAME_SIZE)",
    "label": 0
  },
  {
    "codes": "func (s *DeleteJobInput) SetJobName(v string) *DeleteJobInput {\n\ts.JobName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```c\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <stdio.h>\n#include <string.h>\n\n// Function to perform AES-GCM encryption\nint aes_gcm_encrypt(const unsigned char *plaintext, size_t plaintext_len,\n                     const unsigned char *key, const unsigned char *iv,\n                     unsigned char *ciphertext, unsigned char *tag) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int ciphertext_len;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        return -1; // Error creating context\n    }\n\n    // Initialize the encryption operation\n    if (1 != EVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1; // Error initializing encryption\n    }\n\n    // Set the key and IV\n    if (1 != EVP_EncryptInit_ex(ctx, NULL, NULL, key, iv)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1; // Error setting key/IV\n    }\n\n    // Encrypt the plaintext\n    if (1 != EVP_EncryptUpdate(ctx, ciphertext, &len, plaintext, plaintext_len)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1; // Error encrypting plaintext\n    }\n    ciphertext_len = len;\n\n    // Finalize the encryption\n    if (1 != EVP_EncryptFinal_ex(ctx, ciphertext + len, &len)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1; // Error finalizing encryption\n    }\n    ciphertext_len += len;\n\n    // Get the authentication tag\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_GET_TAG, 16, tag)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1; // Error getting tag\n    }\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    return ciphertext_len;\n}\n\n// Function to perform AES-GCM decryption\nint aes_gcm_decrypt(const unsigned char *ciphertext, size_t ciphertext_len,\n                     const unsigned char *key, const unsigned char *iv,\n                     const unsigned char *tag, unsigned char *plaintext) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int plaintext_len;\n    int ret;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        return -1; // Error creating context\n    }\n\n    // Initialize the decryption operation\n    if (1 != EVP_DecryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1; // Error initializing decryption\n    }\n\n    // Set the key and IV\n    if (1 != EVP_DecryptInit_ex(ctx, NULL, NULL, key, iv)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1; // Error setting key/IV\n    }\n\n    // Decrypt the ciphertext\n    if (1 != EVP_DecryptUpdate(ctx, plaintext, &len, ciphertext, ciphertext_len)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1; // Error decrypting ciphertext\n    }\n    plaintext_len = len;\n\n    // Set the expected tag value\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_TAG, 16, (void *)tag)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1; // Error setting tag\n    }\n\n    // Finalize the decryption\n    ret = EVP_DecryptFinal_ex(ctx, plaintext + len, &len);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    if (ret > 0) {\n        plaintext_len += len;\n        return plaintext_len; // Success\n    } else {\n        return -1; // Authentication failed\n    }\n}\n\n// Example usage\nint main() {\n    unsigned char key[32]; // 256-bit key\n    unsigned char iv[12];  // 96-bit IV\n    unsigned char plaintext[] = \"Hello, AES-GCM!\";\n    unsigned char ciphertext[128];\n    unsigned char decryptedtext[128];\n    unsigned char tag[16];\n\n    // Generate random key and IV\n    RAND_bytes(key, sizeof(key));\n    RAND_bytes(iv, sizeof(iv));\n\n    // Encrypt\n    int ciphertext_len = aes_gcm_encrypt(plaintext, strlen((char *)plaintext), key, iv, ciphertext, tag);\n    if (ciphertext_len == -1) {\n        printf(\"Encryption failed\\n\");\n        return 1;\n    }\n\n    // Decrypt\n    int decryptedtext_len = aes_gcm_decrypt(ciphertext, ciphertext_len, key, iv, tag, decryptedtext);\n    if (decryptedtext_len == -1) {\n        printf(\"Decryption failed\\n\");\n        return 1;\n    }\n\n    // Null-terminate the decrypted text\n    decryptedtext[decryptedtext_len] = '\\0';\n\n    printf(\"Decrypted text: %s\\n\", decryptedtext);\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "function substituteTimeAgo(string, number, withoutSuffix, isFuture, locale) {\n    return locale.relativeTime(number || 1, !!withoutSuffix, string, isFuture);\n}",
    "label": 0
  },
  {
    "codes": "```julia\nusing VoronoiDelaunay\nusing Plots\n\n# Function to generate random points\nfunction generate_random_points(num_points::Int, xlim::Tuple{Float64, Float64}, ylim::Tuple{Float64, Float64})\n    xs = rand(xlim[1]:0.01:xlim[2], num_points)\n    ys = rand(ylim[1]:0.01:ylim[2], num_points)\n    return xs, ys\nend\n\n# Create and display Voronoi diagram\nfunction plot_voronoi(points::Tuple{Vector{Float64}, Vector{Float64}})\n    voronoi = VD(points[1], points[2])\n    plot(voronoi, xlim=(minimum(points[1]), maximum(points[1])), ylim=(minimum(points[2]), maximum(points[2])), legend=false)\nend\n\n# Parameters for the script\nnum_points = 30  # Number of random points\nxlim = (0.0, 1.0)  # x-axis limits for the random points\nylim = (0.0, 1.0)  # y-axis limits for the random points\n\n# Generate random points and plot Voronoi diagram\nrandom_points = generate_random_points(num_points, xlim, ylim)\nplot_voronoi(random_points)\n\n```\n",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<List<PatternRuleInfo>>> batchAddPatternsWithServiceResponseAsync(UUID appId, String versionId, List<PatternRuleCreateObject> patterns) {\n        if (this.client.endpoint() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.endpoint() is required and cannot be null.\");\n        }\n        if (appId == null) {\n            throw new IllegalArgumentException(\"Parameter appId is required and cannot be null.\");\n        }\n        if (versionId == null) {\n            throw new IllegalArgumentException(\"Parameter versionId is required and cannot be null.\");\n        }\n        if (patterns == null) {\n            throw new IllegalArgumentException(\"Parameter patterns is required and cannot be null.\");\n        }\n        Validator.validate(patterns);\n        String parameterizedHost = Joiner.on(\", \").join(\"{Endpoint}\", this.client.endpoint());\n        return service.batchAddPatterns(appId, versionId, patterns, this.client.acceptLanguage(), parameterizedHost, this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<List<PatternRuleInfo>>>>() {\n                @Override\n                public Observable<ServiceResponse<List<PatternRuleInfo>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<List<PatternRuleInfo>> clientResponse = batchAddPatternsDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```rust\nmacro_rules! error_type {\n    (\n        $(#[$meta:meta])*\n        $vis:vis enum $name:ident {\n            $(\n                $(#[$variant_meta:meta])*\n                $variant:ident$( ( $( $ty:ty ),* ) )?\n            ),* $(,)?\n        }\n    ) => {\n        $(#[$meta])*\n        #[derive(Debug)]\n        $vis enum $name {\n            $(\n                $(#[$variant_meta])*\n                $variant$( ( $( $ty ),* ) )?,\n            )*\n        }\n\n        impl std::fmt::Display for $name {\n            fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n                match self {\n                    $(\n                        $name::$variant$( ( $( $ty ),* ) )? => write!(f, stringify!($variant)),\n                    )*\n                }\n            }\n        }\n\n        impl std::error::Error for $name {}\n    };\n}\n\n// Example usage:\n\nerror_type! {\n    /// My custom error type\n    pub enum MyError {\n        /// An IO error occurred\n        IoError(std::io::Error),\n        /// A custom error type\n        CustomError,\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<IssuerBundle> setCertificateIssuerAsync(String vaultBaseUrl, String issuerName, String provider, IssuerCredentials credentials, OrganizationDetails organizationDetails, IssuerAttributes attributes) {\n        return setCertificateIssuerWithServiceResponseAsync(vaultBaseUrl, issuerName, provider, credentials, organizationDetails, attributes).map(new Func1<ServiceResponse<IssuerBundle>, IssuerBundle>() {\n            @Override\n            public IssuerBundle call(ServiceResponse<IssuerBundle> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport logging\nimport traceback\nfrom functools import wraps\n\n# Configure logging\nlogging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef log_exceptions(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            # Log the exception with traceback\n            logging.error(f\"Exception occurred in {func.__name__}: {e}\\n{traceback.format_exc()}\")\n            raise  # Re-raise the exception after logging\n    return wrapper\n\n# Example usage\n@log_exceptions\ndef risky_function(x, y):\n    return x / y\n\n# Test the decorator\nrisky_function(10, 0)\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom transformers import pipeline\n\n# Initialize the summarization pipeline\n# Using a different model for variety, e.g., \"facebook/bart-large-cnn\"\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n# Example text to summarize\ntext = \"\"\"\nThe Hugging Face Transformers library provides an easy-to-use interface for various NLP tasks, \nincluding text summarization. It supports multiple pre-trained models that can be used out-of-the-box \nor fine-tuned for specific tasks. Text summarization is particularly useful for condensing long \ndocuments into shorter, more manageable summaries.\n\"\"\"\n\n# Generate the summary\nsummary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n\n# Print the summary\nprint(summary[0]['summary_text'])\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\n\n# Define a simple CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        # Define a 2D convolution layer with 3 input channels, 32 output channels, and 3x3 kernel\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        # Define a batch normalization layer for 32 feature maps\n        self.bn1 = nn.BatchNorm2d(32)\n        # Define a 2D convolution layer with 32 input channels, 64 output channels, and 3x3 kernel\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        # Define a batch normalization layer for 64 feature maps\n        self.bn2 = nn.BatchNorm2d(64)\n        # Define a fully connected layer with input size 4096 and output size 256\n        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n        # Define a fully connected layer with input size 256 and output size 10\n        self.fc2 = nn.Linear(256, 10)\n\n    def forward(self, x):\n        # Apply first convolutional layer, batch norm, ReLU activation, then max pooling\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.max_pool2d(x, 2)\n        # Apply second convolutional layer, batch norm, ReLU activation, then max pooling\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.max_pool2d(x, 2)\n        # Flatten the feature map to a vector\n        x = x.view(x.size(0), -1)\n        # Apply first fully connected layer with ReLU activation\n        x = F.relu(self.fc1(x))\n        # Apply second fully connected layer (output)\n        x = self.fc2(x)\n        return x\n\n# Define transformations for training and test data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\n# Load training and test datasets\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n\n# Instantiate the model, define the loss function and the optimizer\nmodel = SimpleCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(10):  # loop over the dataset multiple times\n    running_loss = 0.0\n    for inputs, labels in trainloader:\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        # Print statistics\n        running_loss += loss.item()\n    \n    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}')\n\n# Print when done\nprint('Finished Training')\n```",
    "label": 1
  },
  {
    "codes": "public Observable<Page<DenyAssignmentInner>> listByResourceGroupAsync(final String resourceGroupName) {\n        return listByResourceGroupWithServiceResponseAsync(resourceGroupName)\n            .map(new Func1<ServiceResponse<Page<DenyAssignmentInner>>, Page<DenyAssignmentInner>>() {\n                @Override\n                public Page<DenyAssignmentInner> call(ServiceResponse<Page<DenyAssignmentInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *BatchDeleteConnectionOutput) SetSucceeded(v []*string) *BatchDeleteConnectionOutput {\n\ts.Succeeded = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function handleClick(event) {\n  const rootNode = document;\n  let element = event.target;\n\n  while (element && element !== rootNode) {\n    const category = element.getAttribute('data-ga-event-category');\n\n    // We reach a tracking element, no need to look higher in the dom tree.\n    if (category) {\n      window.ga('send', {\n        hitType: 'event',\n        eventCategory: category,\n        eventAction: element.getAttribute('data-ga-event-action'),\n        eventLabel: element.getAttribute('data-ga-event-label'),\n      });\n      break;\n    }\n\n    element = element.parentNode;\n  }\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/segmentio/kafka-go\"\n)\n\n// worker processes messages from the input channel and sends results to the output channel\nfunc worker(id int, in <-chan kafka.Message, out chan<- string, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\tfor msg := range in {\n\t\t// Simulate processing\n\t\ttime.Sleep(100 * time.Millisecond)\n\t\tresult := fmt.Sprintf(\"Worker %d processed message: %s\", id, string(msg.Value))\n\t\tout <- result\n\t}\n}\n\n// fanIn collects results from multiple workers into a single channel\nfunc fanIn(out chan<- string, ins ...<-chan string) {\n\tvar wg sync.WaitGroup\n\twg.Add(len(ins))\n\n\tfor _, in := range ins {\n\t\tgo func(ch <-chan string) {\n\t\t\tfor result := range ch {\n\t\t\t\tout <- result\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(in)\n\t}\n\n\twg.Wait()\n\tclose(out)\n}\n\nfunc main() {\n\t// Kafka reader configuration\n\treader := kafka.NewReader(kafka.ReaderConfig{\n\t\tBrokers:   []string{\"localhost:9092\"},\n\t\tTopic:     \"test-topic\",\n\t\tPartition: 0,\n\t\tMinBytes:  10e3, // 10KB\n\t\tMaxBytes:  10e6, // 10MB\n\t})\n\tdefer reader.Close()\n\n\t// Channels for fan-out/fan-in\n\tnumWorkers := 4\n\tin := make(chan kafka.Message, numWorkers)\n\touts := make([]chan string, numWorkers)\n\tfor i := range outs {\n\t\touts[i] = make(chan string)\n\t}\n\tfinalOut := make(chan string)\n\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < numWorkers; i++ {\n\t\twg.Add(1)\n\t\tgo worker(i, in, outs[i], &wg)\n\t}\n\n\t// Start fan-in goroutine\n\tgo fanIn(finalOut, outs...)\n\n\t// Read messages from Kafka and distribute to workers\n\tgo func() {\n\t\tfor {\n\t\t\tmsg, err := reader.ReadMessage(context.Background())\n\t\t\tif err != nil {\n\t\t\t\tfmt.Println(\"Error reading message:\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tin <- msg\n\t\t}\n\t}()\n\n\t// Collect and print results\n\tfor result := range finalOut {\n\t\tfmt.Println(result)\n\t}\n\n\t// Wait for all workers to finish\n\tclose(in)\n\twg.Wait()\n}\n```",
    "label": 1
  },
  {
    "codes": "function getSvgSpriteConfig(category) {\n  return {\n    shape: {\n      dimension: {\n        maxWidth: 24,\n        maxHeight: 24\n      },\n    },\n    mode: {\n      css : {\n        bust: false,\n        dest: './',\n        sprite: `./svg-sprite-${ category }.svg`,\n        example: {\n          dest: `./svg-sprite-${ category }.html`\n        },\n        render: {\n          css: {\n            dest: `./svg-sprite-${ category }.css`\n          }\n        }\n      },\n      symbol : {\n        bust: false,\n        dest: './',\n        sprite: `./svg-sprite-${ category }-symbol.svg`,\n        example: {\n          dest: `./svg-sprite-${ category }-symbol.html`\n        }\n      }\n    }\n  };\n}",
    "label": 0
  },
  {
    "codes": "public BackupShortTermRetentionPolicyInner createOrUpdate(String resourceGroupName, String serverName, String databaseName) {\n        return createOrUpdateWithServiceResponseAsync(resourceGroupName, serverName, databaseName).toBlocking().last().body();\n    }",
    "label": 0
  },
  {
    "codes": "def paragraphs(self, with_id = False):\n        \"\"\"Extracts paragraphs, returns list of plain-text(!) paragraphs\"\"\"\n        prevp = 0\n        partext = []\n        for word, id, pos, lemma in iter(self):\n            doc_id, ptype, p, s, w = re.findall('([\\w\\d-]+)\\.(p|head)\\.(\\d+)\\.s\\.(\\d+)\\.w\\.(\\d+)',id)[0]\n            if prevp != p and partext:\n                    yield ( doc_id + \".\" + ptype + \".\" + prevp , \" \".join(partext) )\n                    partext = []\n            partext.append(word)\n            prevp = p\n        if partext:\n            yield (doc_id + \".\" + ptype + \".\" + prevp, \" \".join(partext) )",
    "label": 0
  },
  {
    "codes": "func (s *Job) SetWorkerType(v string) *Job {\n\ts.WorkerType = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```java\n// MainActivity.java\npackage com.example.objectdetection;\n\nimport android.Manifest;\nimport android.content.pm.PackageManager;\nimport android.graphics.Bitmap;\nimport android.graphics.BitmapFactory;\nimport android.os.Bundle;\nimport android.util.Log;\nimport android.widget.ImageView;\nimport android.widget.TextView;\nimport androidx.annotation.NonNull;\nimport androidx.appcompat.app.AppCompatActivity;\nimport androidx.core.app.ActivityCompat;\nimport androidx.core.content.ContextCompat;\nimport org.tensorflow.lite.Interpreter;\nimport org.tensorflow.lite.support.image.TensorImage;\nimport org.tensorflow.lite.support.label.Category;\nimport org.tensorflow.lite.task.vision.detector.Detection;\nimport org.tensorflow.lite.task.vision.detector.ObjectDetector;\nimport java.io.IOException;\nimport java.nio.MappedByteBuffer;\nimport java.nio.channels.FileChannel;\nimport java.util.List;\n\npublic class MainActivity extends AppCompatActivity {\n    private static final int CAMERA_PERMISSION_CODE = 100;\n    private static final String MODEL_FILE = \"model.tflite\";\n    private Interpreter interpreter;\n    private ImageView imageView;\n    private TextView resultTextView;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        imageView = findViewById(R.id.imageView);\n        resultTextView = findViewById(R.id.resultTextView);\n\n        // Check and request camera permission\n        if (ContextCompat.checkSelfPermission(this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {\n            ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.CAMERA}, CAMERA_PERMISSION_CODE);\n        } else {\n            initializeModel();\n        }\n    }\n\n    private void initializeModel() {\n        try {\n            interpreter = new Interpreter(loadModelFile());\n        } catch (IOException e) {\n            Log.e(\"TFLite\", \"Error loading model\", e);\n        }\n    }\n\n    private MappedByteBuffer loadModelFile() throws IOException {\n        FileChannel fileChannel = new FileInputStream(getAssets().openFd(MODEL_FILE)).getChannel();\n        return fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, fileChannel.size());\n    }\n\n    private void detectObjects(Bitmap bitmap) {\n        TensorImage tensorImage = TensorImage.fromBitmap(bitmap);\n        ObjectDetector.ObjectDetectorOptions options = ObjectDetector.ObjectDetectorOptions.builder()\n                .setMaxResults(5)\n                .setScoreThreshold(0.5f)\n                .build();\n        ObjectDetector detector = ObjectDetector.createFromFileAndOptions(this, MODEL_FILE, options);\n\n        List<Detection> results = detector.detect(tensorImage);\n\n        StringBuilder resultText = new StringBuilder();\n        for (Detection detection : results) {\n            for (Category category : detection.getCategories()) {\n                resultText.append(category.getLabel()).append(\": \").append(category.getScore()).append(\"\\n\");\n            }\n        }\n\n        resultTextView.setText(resultText.toString());\n    }\n\n    @Override\n    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults);\n        if (requestCode == CAMERA_PERMISSION_CODE && grantResults.length > 0 && grantResults[0] == PackageManager.PERMISSION_GRANTED) {\n            initializeModel();\n        } else {\n            Log.e(\"TFLite\", \"Camera permission denied\");\n        }\n    }\n\n    @Override\n    protected void onDestroy() {\n        super.onDestroy();\n        if (interpreter != null) {\n            interpreter.close();\n        }\n    }\n}\n```\n\n```xml\n<!-- activity_main.xml -->\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n    <ImageView\n        android:id=\"@+id/imageView\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:layout_centerInParent=\"true\"\n        android:src=\"@drawable/sample_image\" />\n\n    <TextView\n        android:id=\"@+id/resultTextView\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:layout_below=\"@id/imageView\"\n        android:layout_marginTop=\"16dp\"\n        android:text=\"Detection Results\"\n        android:textSize=\"18sp\" />\n</RelativeLayout>\n```\n\n```gradle\n// build.gradle (Module: app)\ndependencies {\n    implementation 'org.tensorflow:tensorflow-lite-task-vision:0.3.1'\n    implementation 'org.tensorflow:tensorflow-lite-support:0.3.1'\n}\n```\n\n```gradle\n// build.gradle (Project)\nallprojects {\n    repositories {\n        google()\n        mavenCentral()\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glue) BatchGetJobsWithContext(ctx aws.Context, input *BatchGetJobsInput, opts ...request.Option) (*BatchGetJobsOutput, error) {\n\treq, out := c.BatchGetJobsRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```csharp\nusing Microsoft.AspNetCore.Http;\nusing Microsoft.AspNetCore.Mvc;\nusing Microsoft.AspNetCore.Mvc.Filters;\nusing System.IdentityModel.Tokens.Jwt;\nusing System.Linq;\nusing System.Net;\n\npublic class ValidateJwtClaimAttribute : Attribute, IAuthorizationFilter\n{\n    private readonly string _claimType;\n    private readonly string _claimValue;\n\n    public ValidateJwtClaimAttribute(string claimType, string claimValue)\n    {\n        _claimType = claimType;\n        _claimValue = claimValue;\n    }\n\n    public void OnAuthorization(AuthorizationFilterContext context)\n    {\n        var authHeader = context.HttpContext.Request.Headers[\"Authorization\"].FirstOrDefault();\n        if (authHeader == null || !authHeader.StartsWith(\"Bearer \"))\n        {\n            context.Result = new JsonResult(new { message = \"Unauthorized\" }) { StatusCode = (int)HttpStatusCode.Unauthorized };\n            return;\n        }\n\n        var token = authHeader.Substring(\"Bearer \".Length).Trim();\n        var jwtToken = new JwtSecurityTokenHandler().ReadJwtToken(token);\n\n        var claim = jwtToken.Claims.FirstOrDefault(c => c.Type == _claimType);\n        if (claim == null || claim.Value != _claimValue)\n        {\n            context.Result = new JsonResult(new { message = \"Forbidden\" }) { StatusCode = (int)HttpStatusCode.Forbidden };\n        }\n    }\n}\n\n// Example usage in a controller:\n[ValidateJwtClaim(ClaimTypes.Role, \"Admin\")]\npublic class AdminController : ControllerBase\n{\n    // Actions requiring the specified claim\n}\n```",
    "label": 1
  },
  {
    "codes": "```typescript\n// Import necessary Cypress commands\nimport { cy, describe, it, beforeEach } from 'cypress';\n\ndescribe('E-commerce Checkout Functionality', () => {\n  beforeEach(() => {\n    // Visit the e-commerce website before each test\n    cy.visit('https://example-ecommerce.com');\n  });\n\n  it('should successfully complete the checkout process', () => {\n    // Add a product to the cart\n    cy.get('.product').first().click();\n    cy.get('.add-to-cart').click();\n\n    // Navigate to the cart\n    cy.get('.cart-icon').click();\n\n    // Verify the product is in the cart\n    cy.get('.cart-item').should('have.length', 1);\n\n    // Proceed to checkout\n    cy.get('.checkout-button').click();\n\n    // Fill in shipping information\n    cy.get('#shipping-name').type('John Doe');\n    cy.get('#shipping-address').type('123 Main St');\n    cy.get('#shipping-city').type('New York');\n    cy.get('#shipping-zip').type('10001');\n    cy.get('#shipping-country').select('United States');\n\n    // Continue to payment\n    cy.get('.continue-to-payment').click();\n\n    // Fill in payment information\n    cy.get('#card-number').type('4111111111111111');\n    cy.get('#expiration-date').type('12/25');\n    cy.get('#cvv').type('123');\n\n    // Place the order\n    cy.get('.place-order').click();\n\n    // Verify the order confirmation page\n    cy.url().should('include', '/order-confirmation');\n    cy.get('.order-confirmation').should('contain', 'Thank you for your order!');\n  });\n\n  it('should display an error for invalid payment information', () => {\n    // Add a product to the cart\n    cy.get('.product').first().click();\n    cy.get('.add-to-cart').click();\n\n    // Navigate to the cart\n    cy.get('.cart-icon').click();\n\n    // Proceed to checkout\n    cy.get('.checkout-button').click();\n\n    // Fill in shipping information\n    cy.get('#shipping-name').type('John Doe');\n    cy.get('#shipping-address').type('123 Main St');\n    cy.get('#shipping-city').type('New York');\n    cy.get('#shipping-zip').type('10001');\n    cy.get('#shipping-country').select('United States');\n\n    // Continue to payment\n    cy.get('.continue-to-payment').click();\n\n    // Fill in invalid payment information\n    cy.get('#card-number').type('1234567812345678'); // Invalid card number\n    cy.get('#expiration-date').type('12/25');\n    cy.get('#cvv').type('123');\n\n    // Attempt to place the order\n    cy.get('.place-order').click();\n\n    // Verify the error message is displayed\n    cy.get('.payment-error').should('contain', 'Invalid payment information');\n  });\n});\n```",
    "label": 1
  },
  {
    "codes": "```verilog\n// PWM Generator Module\nmodule pwm_generator (\n    input wire clk,          // Clock signal\n    input wire rst,          // Reset signal\n    input wire [7:0] duty,   // Duty cycle input (8-bit)\n    output reg pwm_out       // PWM output signal\n);\n\n    reg [7:0] counter;       // 8-bit counter for PWM generation\n\n    // Counter logic\n    always @(posedge clk or posedge rst) begin\n        if (rst) begin\n            counter <= 8'b0; // Reset counter on reset\n            pwm_out <= 1'b0; // Set PWM output to 0\n        end else begin\n            counter <= counter + 1; // Increment counter on each clock cycle\n        end\n    end\n\n    // PWM output logic\n    always @(posedge clk) begin\n        if (counter < duty) begin\n            pwm_out <= 1'b1; // Set PWM output high if counter < duty\n        end else begin\n            pwm_out <= 1'b0; // Set PWM output low otherwise\n        end\n    end\n\nendmodule\n```",
    "label": 1
  },
  {
    "codes": "function(data){\n\n\t\t\t//Declare segments as a static property to prevent inheriting across the Chart type prototype\n\t\t\tthis.segments = [];\n\t\t\tthis.outerRadius = (helpers.min([this.chart.width,this.chart.height]) -\tthis.options.segmentStrokeWidth/2)/2;\n\n\t\t\tthis.SegmentArc = Chart.Arc.extend({\n\t\t\t\tctx : this.chart.ctx,\n\t\t\t\tx : this.chart.width/2,\n\t\t\t\ty : this.chart.height/2\n\t\t\t});\n\n\t\t\t//Set up tooltip events on the chart\n\t\t\tif (this.options.showTooltips){\n\t\t\t\thelpers.bindEvents(this, this.options.tooltipEvents, function(evt){\n\t\t\t\t\tvar activeSegments = (evt.type !== 'mouseout') ? this.getSegmentsAtEvent(evt) : [];\n\n\t\t\t\t\thelpers.each(this.segments,function(segment){\n\t\t\t\t\t\tsegment.restore([\"fillColor\"]);\n\t\t\t\t\t});\n\t\t\t\t\thelpers.each(activeSegments,function(activeSegment){\n\t\t\t\t\t\tactiveSegment.fillColor = activeSegment.highlightColor;\n\t\t\t\t\t});\n\t\t\t\t\tthis.showTooltip(activeSegments);\n\t\t\t\t});\n\t\t\t}\n\t\t\tthis.calculateTotal(data);\n\n\t\t\thelpers.each(data,function(datapoint, index){\n\t\t\t\tthis.addData(datapoint, index, true);\n\t\t\t},this);\n\n\t\t\tthis.render();\n\t\t}",
    "label": 0
  },
  {
    "codes": "function createParallelIfNeeded(option) {\n    if (option.parallel) {\n        return;\n    }\n\n    var hasParallelSeries = false;\n\n    zrUtil.each(option.series, function (seriesOpt) {\n        if (seriesOpt && seriesOpt.type === 'parallel') {\n            hasParallelSeries = true;\n        }\n    });\n\n    if (hasParallelSeries) {\n        option.parallel = [{}];\n    }\n}",
    "label": 0
  },
  {
    "codes": "function extractVars(path) {\n    const declarators = [];\n\n    if (path.isVariableDeclaration({ kind: \"var\" })) {\n      for (const decl of path.node.declarations) {\n        const bindingIds = Object.keys(t.getBindingIdentifiers(decl.id));\n\n        declarators.push(\n          ...bindingIds.map(name => t.variableDeclarator(t.identifier(name)))\n        );\n      }\n    } else {\n      path.traverse({\n        VariableDeclaration(varPath) {\n          if (!varPath.isVariableDeclaration({ kind: \"var\" })) return;\n\n          if (!isSameFunctionScope(varPath, path)) return;\n\n          for (const decl of varPath.node.declarations) {\n            const bindingIds = Object.keys(t.getBindingIdentifiers(decl.id));\n            declarators.push(\n              ...bindingIds.map(name =>\n                t.variableDeclarator(t.identifier(name))\n              )\n            );\n          }\n        }\n      });\n    }\n\n    if (declarators.length <= 0) return [];\n\n    return [t.variableDeclaration(\"var\", declarators)];\n  }",
    "label": 0
  },
  {
    "codes": "```verilog\nmodule pwm_generator(\n    input wire clk,             // Clock input\n    input wire rst_n,           // Active low reset\n    input wire [7:0] duty_cycle, // 8-bit Duty cycle input (0-255)\n    output reg pwm_out          // PWM output\n);\n\n    reg [7:0] counter;          // 8-bit counter\n\n    // Counter and PWM generation logic\n    always @(posedge clk or negedge rst_n) begin\n        if (!rst_n) begin\n            counter <= 8'b0;\n            pwm_out <= 1'b0;\n        end else begin\n            counter <= counter + 8'b1;\n            if (counter < duty_cycle)\n                pwm_out <= 1'b1;\n            else\n                pwm_out <= 1'b0;\n        end\n    end\n\nendmodule\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\n# Bubble Sort Algorithm\ndef bubble_sort(arr):\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n-i-1):\n            if arr[j] > arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n            yield arr\n\n# Generate initial unsorted data\ndata = np.random.rand(50)\ngenerator = bubble_sort(data.copy())\n\n# Setup the plot\nfig, ax = plt.subplots()\nax.set_title(\"Bubble Sort Visualization\")\nbar_rects = ax.bar(range(len(data)), data, align=\"edge\")\nax.set_xlim(0, len(data))\nax.set_ylim(0, 1)\ntext = ax.text(0.02, 0.95, \"\", transform=ax.transAxes)\n\n# Update function for animation\ndef update_fig(array, rects, iteration):\n    for rect, val in zip(rects, array):\n        rect.set_height(val)\n    text.set_text(f\"Number of operations: {iteration}\")\n\n# Run the animation\nanim = animation.FuncAnimation(fig, func=update_fig, fargs=(bar_rects, 0), \n                               frames=generator, repeat=False, blit=False, \n                               interval=100, save_count=900)\n\n# Display the plot\nplt.show()\n```",
    "label": 1
  },
  {
    "codes": "```powershell\n# ServiceDependencyManager.psm1\n\n# Function to add a service dependency\nfunction Add-ServiceDependency {\n    param (\n        [string]$ServiceName,\n        [string]$Dependency\n    )\n    # Get the current service configuration\n    $service = Get-Service -Name $ServiceName\n    $config = Get-WmiObject -Query \"SELECT * FROM Win32_Service WHERE Name='$($service.Name)'\"\n\n    # Get the current dependencies\n    $dependencies = $config.Dependencies\n\n    # Check if the dependency already exists\n    if ($dependencies -contains $Dependency) {\n        Write-Host \"Dependency already exists.\"\n        return\n    }\n\n    # Add the new dependency\n    $dependencies += $Dependency\n\n    # Update the service configuration\n    $config.Change($null, $null, $null, $null, $null, $null, $null, $null, $dependencies)\n    Write-Host \"Dependency added successfully.\"\n}\n\n# Function to remove a service dependency\nfunction Remove-ServiceDependency {\n    param (\n        [string]$ServiceName,\n        [string]$Dependency\n    )\n    # Get the current service configuration\n    $service = Get-Service -Name $ServiceName\n    $config = Get-WmiObject -Query \"SELECT * FROM Win32_Service WHERE Name='$($service.Name)'\"\n\n    # Get the current dependencies\n    $dependencies = $config.Dependencies\n\n    # Check if the dependency exists\n    if ($dependencies -notcontains $Dependency) {\n        Write-Host \"Dependency does not exist.\"\n        return\n    }\n\n    # Remove the dependency\n    $dependencies = $dependencies | Where-Object { $_ -ne $Dependency }\n\n    # Update the service configuration\n    $config.Change($null, $null, $null, $null, $null, $null, $null, $null, $dependencies)\n    Write-Host \"Dependency removed successfully.\"\n}\n\n# Function to list service dependencies\nfunction Get-ServiceDependencies {\n    param (\n        [string]$ServiceName\n    )\n    # Get the current service configuration\n    $service = Get-Service -Name $ServiceName\n    $config = Get-WmiObject -Query \"SELECT * FROM Win32_Service WHERE Name='$($service.Name)'\"\n\n    # Return the dependencies\n    return $config.Dependencies\n}\n\n# Export the functions\nExport-ModuleMember -Function Add-ServiceDependency, Remove-ServiceDependency, Get-ServiceDependencies\n```",
    "label": 1
  },
  {
    "codes": "```rust\n// Cargo.toml\n[dependencies]\nzip = \"0.6\"\naes = \"0.7\"\nrand = \"0.8\"\naes-gcm = \"0.10\"\nhex = \"0.4\"\n\n/// lib.rs\nuse std::fs::File;\nuse std::io::{Read, Write, Seek};\nuse std::path::Path;\nuse zip::write::FileOptions;\nuse zip::{ZipWriter, ZipReader};\nuse aes_gcm::{Aes256Gcm, Key, KeyInit, Nonce}; // Or Aes128Gcm, Aes192Gcm\nuse aes_gcm::aes::cipher::{NewCipher, StreamCipher};\nuse rand::Rng;\nuse hex::FromHex;\n\nconst NONCE_SIZE: usize = 12;\n\nfn create_zip_with_encryption<P: AsRef<Path>>(\n    file_path: P,\n    files: Vec<&str>,\n    password: &str,\n) -> zip::result::ZipResult<()> {\n    let file = File::create(file_path)?;\n    let mut zip = ZipWriter::new(file);\n    \n    let key = Key::<Aes256Gcm>::from_slice(&derive_key(password));\n    let cipher = Aes256Gcm::new(key);\n\n    for &filename in files.iter() {\n        let mut f = File::open(filename)?;\n        let mut buffer: Vec<u8> = vec![];\n        f.read_to_end(&mut buffer)?;\n\n        let nonce = generate_nonce();\n        let encrypted_data = encrypt(&cipher, &nonce, &buffer);\n\n        zip.start_file(\n            filename,\n            FileOptions::default().compression_method(zip::CompressionMethod::Stored),\n        )?;\n        zip.write_all(&nonce)?;\n        zip.write_all(&encrypted_data)?;\n    }\n    \n    zip.finish()?;\n    Ok(())\n}\n\nfn read_zip_with_decryption<P: AsRef<Path>>(\n    file_path: P,\n    password: &str,\n) -> zip::result::ZipResult<()> {\n    let file = File::open(file_path)?;\n    let mut zip = ZipReader::new(file)?;\n\n    let mut keys_bytes = derive_key(password);\n    let key = Key::<Aes256Gcm>::from_slice(&keys_bytes);\n\n    for i in 0..zip.len() {\n        let mut file = zip.by_index(i)?;\n        let mut buffer: Vec<u8> = vec![];\n\n        let mut nonce = vec![0u8; NONCE_SIZE];\n        file.read_exact(&mut nonce)?;\n\n        file.read_to_end(&mut buffer)?;\n        let deciphered = decrypt(&key, &nonce, &buffer)?;\n\n        println!(\n            \"Filename: {}\",\n            file.name()\n        );\n        std::io::stdout().write_all(&deciphered)?;\n    }\n    \n    Ok(())\n}\n\nfn derive_key(password: &str) -> Vec<u8> {\n    let hash = md5::compute(password);\n    let hex_hash = format!(\"{:x}\", hash);\n    Vec::from_hex(&hex_hash).unwrap()\n}\n\nfn generate_nonce() -> Vec<u8> {\n    let mut nonce = vec![0u8; NONCE_SIZE];\n    rand::thread_rng().fill(&mut nonce[..]);\n    nonce\n}\n\nfn encrypt(cipher: &Aes256Gcm, nonce: &[u8], data: &[u8]) -> Vec<u8> {\n    let nonce = Nonce::from_slice(nonce);\n    cipher.encrypt(nonce, data).expect(\"encryption failure!\")\n}\n\nfn decrypt(cipher: &Aes256Gcm, nonce: &[u8], data: &[u8]) -> Result<Vec<u8>, ()> {\n    let nonce = Nonce::from_slice(nonce);\n    cipher.decrypt(nonce, data).map_err(|_| ())\n}\n```\n\n",
    "label": 1
  },
  {
    "codes": "```java\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.ServerSocketChannel;\nimport java.nio.channels.SocketChannel;\nimport java.util.Iterator;\nimport java.util.Set;\n\npublic class NonBlockingNioServer {\n\n    public static void main(String[] args) {\n        try {\n            // Open a selector\n            Selector selector = Selector.open();\n            \n            // Open a server socket channel\n            ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();\n            \n            // Configure it to be non-blocking\n            serverSocketChannel.configureBlocking(false);\n            \n            // Bind the server socket to a local address\n            serverSocketChannel.bind(new InetSocketAddress(8080));\n            \n            // Register the server socket channel with the selector\n            serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);\n\n            while (true) {\n                // Wait for an event\n                selector.select();\n                \n                // Get keys corresponding to events detected\n                Set<SelectionKey> selectionKeys = selector.selectedKeys();\n                Iterator<SelectionKey> iterator = selectionKeys.iterator();\n                \n                while (iterator.hasNext()) {\n                    SelectionKey key = iterator.next();\n                    iterator.remove();\n                    \n                    if (key.isAcceptable()) {\n                        // Accept connection\n                        ServerSocketChannel serverChannel = (ServerSocketChannel) key.channel();\n                        SocketChannel socketChannel = serverChannel.accept();\n                        socketChannel.configureBlocking(false);\n                        // Register the new socket channel for read operations\n                        socketChannel.register(selector, SelectionKey.OP_READ);\n                    }\n                    \n                    if (key.isReadable()) {\n                        // Read data\n                        SocketChannel socketChannel = (SocketChannel) key.channel();\n                        ByteBuffer buffer = ByteBuffer.allocate(256);\n                        int bytesRead = socketChannel.read(buffer);\n                        if (bytesRead == -1) {\n                            socketChannel.close();\n                        } else {\n                            // Flip the buffer to read mode\n                            buffer.flip();\n                            // Echo the data back to the client\n                            socketChannel.write(buffer);\n                        }\n                    }\n                }\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```python\nimport xml.etree.ElementTree as ET\n\ndef large_xml_element_iterator(file_path, tag=None):\n    \"\"\"\n    Generator function to yield elements from a large XML file one-by-one.\n\n    :param file_path: Path to the XML file\n    :param tag: Specific tag to filter the elements, if None, yields all elements\n    :yield: XML element\n    \"\"\"\n    # Create an XML context and iteratively parse through the file\n    context = ET.iterparse(file_path, events=(\"start\", \"end\"))\n    _, root = next(context)  # Get the root element\n\n    # Yield each element at 'end' event, and clear memory of the element\n    for event, elem in context:\n        if event == \"end\" and (tag is None or elem.tag == tag):\n            yield elem\n            root.clear()  # Clear elements that are no longer needed to free memory\n```\n",
    "label": 1
  },
  {
    "codes": "```javascript\n// Initialize the Phaser game\nvar config = {\n    type: Phaser.AUTO,\n    width: 800,\n    height: 600,\n    scene: {\n        preload: preload,\n        create: create,\n        update: update\n    },\n    physics: {\n        default: 'arcade',\n        arcade: {\n            debug: false\n        }\n    }\n};\n\nvar game = new Phaser.Game(config);\nvar player;\nvar cursors;\nvar bullets;\nvar lastFired = 0;\nvar particles;\n\nfunction preload() {\n    // Load assets\n    this.load.image('space', 'assets/space.png');\n    this.load.image('player', 'assets/player.png');\n    this.load.image('bullet', 'assets/bullet.png');\n    this.load.image('particle', 'assets/particle.png');\n}\n\nfunction create() {\n    // Add space background\n    this.add.image(400, 300, 'space');\n\n    // Add player ship\n    player = this.physics.add.sprite(400, 500, 'player').setCollideWorldBounds(true);\n\n    // Setup keyboard controls\n    cursors = this.input.keyboard.createCursorKeys();\n\n    // Add bullets group with physics\n    bullets = this.physics.add.group({\n        classType: Phaser.Physics.Arcade.Image,\n        defaultKey: 'bullet',\n        maxSize: 10\n    });\n\n    // Particle effects for shot firing\n    particles = this.add.particles('particle');\n    particles.createEmitter({\n        lifespan: 500,\n        speed: { min: 100, max: 200 },\n        scale: { start: 0.6, end: 0 },\n        blendMode: 'ADD'\n    });\n}\n\nfunction update(time, delta) {\n    // Player movement\n    if (cursors.left.isDown) {\n        player.setVelocityX(-200);\n    } else if (cursors.right.isDown) {\n        player.setVelocityX(200);\n    } else {\n        player.setVelocityX(0);\n    }\n\n    // Firing bullets\n    if (cursors.space.isDown && time > lastFired) {\n        var bullet = bullets.get();\n\n        if (bullet) {\n            bullet.enableBody(true, player.x, player.y - 20, true, true);\n            bullet.setVelocityY(-300);\n            particles.emitParticleAt(player.x, player.y - 20);\n            lastFired = time + 100;\n        }\n    }\n\n    // Deactivate bullets that are out of bounds\n    bullets.children.each(function(b) {\n        if (b.active && b.y < 0) {\n            b.setActive(false);\n            b.setVisible(false);\n        }\n    }, this);\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport traceback\nimport logging\n\n# Configure logging to output to a file with a specific format\nlogging.basicConfig(filename='errors.log',\n                    level=logging.ERROR,\n                    format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef log_exceptions(func):\n    \"\"\"Decorator to log exceptions and their stack traces.\"\"\"\n    def wrapper(*args, **kwargs):\n        try:\n            # Execute the decorated function\n            return func(*args, **kwargs)\n        except Exception as e:\n            # Log the exception with its stack trace\n            logging.error(\"Exception occurred in %s: %s\", func.__name__, traceback.format_exc())\n            # Re-raise the exception to allow normal error handling\n            raise\n    return wrapper\n```\n",
    "label": 1
  },
  {
    "codes": "func (s *UploadMultipartPartInput) SetChecksum(v string) *UploadMultipartPartInput {\n\ts.Checksum = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```javascript\n// Import necessary libraries\nconst xss = require('xss');\n\n// Middleware to sanitize user input\nconst sanitizeInput = (req, res, next) => {\n    // Sanitize request body\n    if (req.body) {\n        for (let key in req.body) {\n            if (typeof req.body[key] === 'string') {\n                req.body[key] = xss(req.body[key]);\n            }\n        }\n    }\n\n    // Sanitize request query parameters\n    if (req.query) {\n        for (let key in req.query) {\n            if (typeof req.query[key] === 'string') {\n                req.query[key] = xss(req.query[key]);\n            }\n        }\n    }\n\n    // Sanitize request params\n    if (req.params) {\n        for (let key in req.params) {\n            if (typeof req.params[key] === 'string') {\n                req.params[key] = xss(req.params[key]);\n            }\n        }\n    }\n\n    // Proceed to the next middleware or route handler\n    next();\n};\n\nmodule.exports = sanitizeInput;\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<DeletedVaultInner>> getDeletedWithServiceResponseAsync(String vaultName, String location) {\n        if (vaultName == null) {\n            throw new IllegalArgumentException(\"Parameter vaultName is required and cannot be null.\");\n        }\n        if (location == null) {\n            throw new IllegalArgumentException(\"Parameter location is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        return service.getDeleted(vaultName, location, this.client.subscriptionId(), this.client.apiVersion(), this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<DeletedVaultInner>>>() {\n                @Override\n                public Observable<ServiceResponse<DeletedVaultInner>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<DeletedVaultInner> clientResponse = getDeletedDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```rust\nuse serde_json::{Value};\nuse std::io::{self, Read};\nuse std::process;\n\nfn main() {\n    let mut buffer = String::new();\n    io::stdin().read_to_string(&mut buffer).unwrap();\n\n    let json: Value = serde_json::from_str(&buffer).unwrap_or_else(|err| {\n        eprintln!(\"Error parsing JSON: {}\", err);\n        process::exit(1);\n    });\n\n    let query = std::env::args().nth(1).expect(\"No query provided\");\n\n    match parse_query(&json, &query) {\n        Some(result) => println!(\"{}\", result),\n        None => eprintln!(\"Query did not match any elements\"),\n    }\n}\n\n// Parses a simple dot-separated query (e.g., 'key1.key2') on a JSON Value\nfn parse_query<'a>(value: &'a Value, query: &str) -> Option<&'a Value> {\n    let keys: Vec<&str> = query.split('.').collect();\n    keys.iter().fold(Some(value), |acc, &key| acc.and_then(|v| v.get(key)))\n}\n```\n\n// Dependencies in Cargo.toml:\n// [dependencies]\n// serde = { version = \"1.0\", features = [\"derive\"] }\n// serde_json = \"1.0\"\n\n// Instructions: Compile this code using `cargo build --release`. Run it and provide JSON input through stdin and a query argument.\n```",
    "label": 1
  },
  {
    "codes": "```typescript\ntype RGBTuple = [number, number, number];\n\nfunction isRGBTuple(value: any): value is RGBTuple {\n  // Check if value is an array of length 3\n  return (\n    Array.isArray(value) &&\n    value.length === 3 &&\n    value.every(\n      (item) => typeof item === 'number' && item >= 0 && item <= 255\n    )\n  );\n}\n\n// Example usage:\nconst color: unknown = [255, 0, 128]; // Replace with any value to test\nif (isRGBTuple(color)) {\n  console.log(\"It's a valid RGB tuple.\");\n} else {\n  console.log(\"It's not a valid RGB tuple.\");\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\n// MainActivity.java\npackage com.example.objectdetection;\n\nimport android.Manifest;\nimport android.content.pm.PackageManager;\nimport android.graphics.Bitmap;\nimport android.graphics.BitmapFactory;\nimport android.os.Bundle;\nimport android.util.Log;\nimport android.widget.ImageView;\nimport android.widget.TextView;\nimport androidx.annotation.NonNull;\nimport androidx.appcompat.app.AppCompatActivity;\nimport androidx.core.app.ActivityCompat;\nimport androidx.core.content.ContextCompat;\nimport org.tensorflow.lite.support.image.TensorImage;\nimport org.tensorflow.lite.task.vision.detector.ObjectDetector;\nimport org.tensorflow.lite.task.vision.detector.ObjectDetector.ObjectDetectionResult;\nimport org.tensorflow.lite.task.vision.detector.ObjectDetector.ObjectDetectorOptions;\nimport java.io.IOException;\nimport java.util.List;\n\npublic class MainActivity extends AppCompatActivity {\n    private static final int CAMERA_PERMISSION_REQUEST_CODE = 100;\n    private ImageView imageView;\n    private TextView resultTextView;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        imageView = findViewById(R.id.imageView);\n        resultTextView = findViewById(R.id.resultTextView);\n\n        // Check and request camera permission\n        if (ContextCompat.checkSelfPermission(this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {\n            ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.CAMERA}, CAMERA_PERMISSION_REQUEST_CODE);\n        } else {\n            loadAndDetectObjects();\n        }\n    }\n\n    @Override\n    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults);\n        if (requestCode == CAMERA_PERMISSION_REQUEST_CODE && grantResults.length > 0 && grantResults[0] == PackageManager.PERMISSION_GRANTED) {\n            loadAndDetectObjects();\n        } else {\n            Log.e(\"MainActivity\", \"Camera permission denied\");\n        }\n    }\n\n    private void loadAndDetectObjects() {\n        // Load image from assets\n        Bitmap bitmap = BitmapFactory.decodeStream(getAssets().open(\"sample_image.jpg\"));\n        imageView.setImageBitmap(bitmap);\n\n        // Convert Bitmap to TensorImage\n        TensorImage tensorImage = TensorImage.fromBitmap(bitmap);\n\n        // Initialize ObjectDetector with options\n        ObjectDetectorOptions options = ObjectDetectorOptions.builder()\n                .setMaxResults(5)\n                .setScoreThreshold(0.5f)\n                .build();\n\n        try {\n            ObjectDetector objectDetector = ObjectDetector.createFromFileAndOptions(this, \"model.tflite\", options);\n\n            // Perform object detection\n            List<ObjectDetectionResult> results = objectDetector.detect(tensorImage);\n\n            // Display results\n            StringBuilder resultText = new StringBuilder();\n            for (ObjectDetectionResult result : results) {\n                resultText.append(result.getCategories().get(0).getLabel())\n                        .append(\": \")\n                        .append(result.getCategories().get(0).getScore())\n                        .append(\"\\n\");\n            }\n            resultTextView.setText(resultText.toString());\n        } catch (IOException e) {\n            Log.e(\"MainActivity\", \"Error loading model\", e);\n        }\n    }\n}\n```\n\n```xml\n<!-- activity_main.xml -->\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:orientation=\"vertical\"\n    android:padding=\"16dp\">\n\n    <ImageView\n        android:id=\"@+id/imageView\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"300dp\"\n        android:scaleType=\"centerCrop\" />\n\n    <TextView\n        android:id=\"@+id/resultTextView\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"Detection Results\"\n        android:textSize=\"18sp\"\n        android:padding=\"16dp\" />\n</LinearLayout>\n```\n\n```xml\n<!-- AndroidManifest.xml -->\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.example.objectdetection\">\n\n    <uses-permission android:name=\"android.permission.CAMERA\" />\n\n    <application\n        android:allowBackup=\"true\"\n        android:icon=\"@mipmap/ic_launcher\"\n        android:label=\"@string/app_name\"\n        android:roundIcon=\"@mipmap/ic_launcher_round\"\n        android:supportsRtl=\"true\"\n        android:theme=\"@style/Theme.ObjectDetection\">\n        <activity android:name=\".MainActivity\">\n            <intent-filter>\n                <action android:name=\"android.intent.action.MAIN\" />\n                <category android:name=\"android.intent.category.LAUNCHER\" />\n            </intent-filter>\n        </activity>\n    </application>\n</manifest>\n```\n\n```gradle\n// build.gradle (Module: app)\ndependencies {\n    implementation 'org.tensorflow:tensorflow-lite-task-vision:0.3.1'\n    implementation 'org.tensorflow:tensorflow-lite-support:0.3.1'\n    implementation 'androidx.appcompat:appcompat:1.3.1'\n    implementation 'androidx.core:core-ktx:1.6.0'\n    implementation 'com.google.android.material:material:1.4.0'\n    implementation 'androidx.constraintlayout:constraintlayout:2.1.0'\n}\n```\n\n```gradle\n// build.gradle (Project: ObjectDetection)\nallprojects {\n    repositories {\n        google()\n        mavenCentral()\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public LongTermRetentionBackupInner get(String locationName, String longTermRetentionServerName, String longTermRetentionDatabaseName, String backupName) {\n        return getWithServiceResponseAsync(locationName, longTermRetentionServerName, longTermRetentionDatabaseName, backupName).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<SubscriptionQuotasListResultInner>> listQuotasWithServiceResponseAsync(String location) {\n        if (location == null) {\n            throw new IllegalArgumentException(\"Parameter location is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        return service.listQuotas(location, this.client.subscriptionId(), this.client.apiVersion(), this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<SubscriptionQuotasListResultInner>>>() {\n                @Override\n                public Observable<ServiceResponse<SubscriptionQuotasListResultInner>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<SubscriptionQuotasListResultInner> clientResponse = listQuotasDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```kotlin\nimport kotlinx.coroutines.*\nimport kotlinx.coroutines.flow.*\nimport kotlin.random.Random\n\n// Sensor data class\ndata class SensorData(val id: Int, val value: Double)\n\n// Function to simulate a stream of incoming sensor data\nfun generateSensorData(): Flow<SensorData> = flow {\n    repeat(100) { index ->\n        delay(100) // Simulate delay between data points\n        emit(SensorData(index, Random.nextDouble(0.0, 100.0)))\n    }\n}\n\n// Function to process batches of sensor data\nsuspend fun processBatch(batch: List<SensorData>) {\n    println(\"Processing batch of size ${batch.size}\")\n    batch.forEach {\n        // Here you can process each sensor data\n        println(\"Sensor ID: ${it.id}, Value: ${it.value}\")\n    }\n}\n\nfun main() = runBlocking {\n    // Setup batch size\n    val batchSize = 10\n\n    // Create a flow pipeline\n    generateSensorData()\n        .chunked(batchSize) // Batch data into chunks of batchSize\n        .collect { batch ->\n            processBatch(batch)\n        }\n\n    println(\"All data processed.\")\n}\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<Void> failoverAsync(String resourceGroupName, String serverName, String databaseName, String linkId, final ServiceCallback<Void> serviceCallback) {\n        return ServiceFuture.fromResponse(failoverWithServiceResponseAsync(resourceGroupName, serverName, databaseName, linkId), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"CustomerClustering\") \\\n    .getOrCreate()\n\n# Load customer data into DataFrame\ndata = spark.read.csv(\"customer_data.csv\", header=True, inferSchema=True)\n\n# Select features for clustering\nfeature_columns = [\"feature1\", \"feature2\", \"feature3\", \"featureN\"]  # Replace with actual feature columns\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\ndataset = assembler.transform(data)\n\n# Truncate columns to the first few rows for display\ndataset.select('features').show(truncate=False)\n\n# Define number of clusters\nnum_clusters = 5\n\n# Train KMeans model\nkmeans = KMeans().setK(num_clusters).setSeed(1).setFeaturesCol(\"features\")\nmodel = kmeans.fit(dataset)\n\n# Make predictions\npredictions = model.transform(dataset)\n\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator(featuresCol='features', metricName='silhouette', distanceMeasure='squaredEuclidean')\nsilhouette = evaluator.evaluate(predictions)\nprint(f\"Silhouette with squared euclidean distance = {silhouette}\")\n\n# Show the result of clustering\npredictions.select('customer_id', 'prediction').show()  # Assuming there is a 'customer_id' column in the data\n\n# Stop the Spark session\nspark.stop()\n```",
    "label": 1
  },
  {
    "codes": "```python\ndef greedy_tsp(graph, start=0):\n    \"\"\"\n    Function to solve the TSP using a greedy approach.\n    \n    :param graph: A 2D list where graph[i][j] holds the distance from node i to node j\n    :param start: The starting node\n    :return: A tuple containing the total cost of the tour and the tour itself\n    \"\"\"\n    n = len(graph)  # Number of nodes\n    visited = [False] * n  # Track visitation status of each node\n    path = [start]  # The path to be followed\n    total_cost = 0  # Total cost initialized to 0\n    current_node = start  # Start the journey from the start node\n\n    # Mark the start node as visited\n    visited[start] = True\n\n    # Perform the greedy algorithm until a complete tour is formed\n    for _ in range(n - 1):\n        next_node = None\n        next_cost = float('inf')\n\n        # Find the nearest unvisited node from the current node\n        for neighbor in range(n):\n            if not visited[neighbor] and graph[current_node][neighbor] < next_cost:\n                next_cost = graph[current_node][neighbor]\n                next_node = neighbor\n\n        # Add the nearest node to the path and mark it as visited\n        path.append(next_node)\n        visited[next_node] = True\n        total_cost += next_cost\n        current_node = next_node\n\n    # Return to the starting node to complete the tour\n    total_cost += graph[current_node][start]\n    path.append(start)\n\n    return total_cost, path\n\n\n# Example usage:\ngraph = [\n    [0, 29, 20, 21],\n    [29, 0, 15, 17],\n    [20, 15, 0, 28],\n    [21, 17, 28, 0]\n]\n\ntotal_cost, path = greedy_tsp(graph, start=0)\nprint(f\"Total cost: {total_cost}\")\nprint(f\"Path: {path}\")\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<DeletedSecretBundle> getDeletedSecretAsync(String vaultBaseUrl, String secretName, final ServiceCallback<DeletedSecretBundle> serviceCallback) {\n        return ServiceFuture.fromResponse(getDeletedSecretWithServiceResponseAsync(vaultBaseUrl, secretName), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```kotlin\n// Extension function to reverse words in a string while preserving whitespace\nfun String.reverseWords(): String {\n    // Split the string into words and whitespace using regex\n    val parts = this.split(\"(?<=\\\\s)|(?=\\\\s)\".toRegex())\n    \n    // Reverse each word while keeping whitespace unchanged\n    return parts.joinToString(\"\") { part ->\n        if (part.isNotBlank()) part.reversed() else part\n    }\n}\n```\n\n```kotlin\n// Example usage\nfun main() {\n    val input = \"Hello  world   Kotlin\"\n    val reversed = input.reverseWords()\n    println(reversed) // Output: \"olleH  dlrow   niltoK\"\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<List<HostingEnvironmentDiagnosticsInner>>> listDiagnosticsWithServiceResponseAsync(String resourceGroupName, String name) {\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (name == null) {\n            throw new IllegalArgumentException(\"Parameter name is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        return service.listDiagnostics(resourceGroupName, name, this.client.subscriptionId(), this.client.apiVersion(), this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<List<HostingEnvironmentDiagnosticsInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<List<HostingEnvironmentDiagnosticsInner>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<List<HostingEnvironmentDiagnosticsInner>> clientResponse = listDiagnosticsDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "func (c *Glacier) RemoveTagsFromVault(input *RemoveTagsFromVaultInput) (*RemoveTagsFromVaultOutput, error) {\n\treq, out := c.RemoveTagsFromVaultRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```ruby\n# lib/error_reporting.rb\n\nrequire 'sentry-ruby'\n\nmodule ErrorReporting\n  Sentry.init do |config|\n    # Set your DSN here\n    config.dsn = 'your_sentry_dsn'\n    # Additional configuration can be added here\n  end\n\n  class << self\n    # Central method to report errors to Sentry\n    # @param [Exception] error The exception to report\n    # @param [Hash] context Additional context information\n    def report(error, context = {})\n      Sentry.capture_exception(error, extra: context)\n    end\n\n    # Centrally report messages to Sentry\n    # @param [String] message The message to report\n    # @param [Hash] context Additional context information\n    def report_message(message, context = {})\n      Sentry.capture_message(message, extra: context)\n    end\n  end\nend\n```\n\n",
    "label": 1
  },
  {
    "codes": "func (s *PhysicalConnectionRequirements) Validate() error {\n\tinvalidParams := request.ErrInvalidParams{Context: \"PhysicalConnectionRequirements\"}\n\tif s.AvailabilityZone != nil && len(*s.AvailabilityZone) < 1 {\n\t\tinvalidParams.Add(request.NewErrParamMinLen(\"AvailabilityZone\", 1))\n\t}\n\tif s.SubnetId != nil && len(*s.SubnetId) < 1 {\n\t\tinvalidParams.Add(request.NewErrParamMinLen(\"SubnetId\", 1))\n\t}\n\n\tif invalidParams.Len() > 0 {\n\t\treturn invalidParams\n\t}\n\treturn nil\n}",
    "label": 0
  },
  {
    "codes": "func (s *DeleteTriggerInput) SetName(v string) *DeleteTriggerInput {\n\ts.Name = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<SecretItem>>> getSecretVersionsSinglePageAsync(final String vaultBaseUrl, final String secretName) {\n        if (vaultBaseUrl == null) {\n            throw new IllegalArgumentException(\"Parameter vaultBaseUrl is required and cannot be null.\");\n        }\n        if (secretName == null) {\n            throw new IllegalArgumentException(\"Parameter secretName is required and cannot be null.\");\n        }\n        if (this.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.apiVersion() is required and cannot be null.\");\n        }\n        final Integer maxresults = null;\n        String parameterizedHost = Joiner.on(\", \").join(\"{vaultBaseUrl}\", vaultBaseUrl);\n        return service.getSecretVersions(secretName, maxresults, this.apiVersion(), this.acceptLanguage(), parameterizedHost, this.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<Page<SecretItem>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<SecretItem>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<PageImpl<SecretItem>> result = getSecretVersionsDelegate(response);\n                        return Observable.just(new ServiceResponse<Page<SecretItem>>(result.body(), result.response()));\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Ensure both signature and artifact files are passed as arguments\nif [ \"$#\" -ne 2 ]; then\n  echo \"Usage: $0 <artifact-file> <signature-file>\"\n  exit 1\nfi\n\nartifact_file=$1\nsignature_file=$2\n\n# Verify that GPG is installed\nif ! command -v gpg &>/dev/null; then\n  echo \"GPG is required but not installed. Exiting.\"\n  exit 1\nfi\n\n# Verify the PGP signature\ngpg --verify \"$signature_file\" \"$artifact_file\" 2>/dev/null\n\n# Check the exit status of the gpg verification\nif [ \"$?\" -eq 0 ]; then\n  echo \"Signature is valid.\"\nelse\n  echo \"Signature verification failed.\"\n  exit 1\nfi\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<Page<WorkerPoolResourceInner>>> listMultiRolePoolsNextWithServiceResponseAsync(final String nextPageLink) {\n        return listMultiRolePoolsNextSinglePageAsync(nextPageLink)\n            .concatMap(new Func1<ServiceResponse<Page<WorkerPoolResourceInner>>, Observable<ServiceResponse<Page<WorkerPoolResourceInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<WorkerPoolResourceInner>>> call(ServiceResponse<Page<WorkerPoolResourceInner>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(listMultiRolePoolsNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *Job) SetCreatedOn(v time.Time) *Job {\n\ts.CreatedOn = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function determineUnitForAutoTicks(minUnit, min, max, capacity) {\n\tvar ilen = UNITS.length;\n\tvar i, interval, factor;\n\n\tfor (i = UNITS.indexOf(minUnit); i < ilen - 1; ++i) {\n\t\tinterval = INTERVALS[UNITS[i]];\n\t\tfactor = interval.steps ? interval.steps[interval.steps.length - 1] : MAX_INTEGER;\n\n\t\tif (interval.common && Math.ceil((max - min) / (factor * interval.size)) <= capacity) {\n\t\t\treturn UNITS[i];\n\t\t}\n\t}\n\n\treturn UNITS[ilen - 1];\n}",
    "label": 0
  },
  {
    "codes": "function addPercent(change, goodEmoji = '', badEmooji = ':small_red_triangle:') {\n  const formatted = (change * 100).toFixed(2);\n  if (/^-|^0(?:\\.0+)$/.test(formatted)) {\n    return `${formatted}% ${goodEmoji}`;\n  }\n  return `+${formatted}% ${badEmooji}`;\n}",
    "label": 0
  },
  {
    "codes": "def validate(self):\n        \"\"\"Validate the contents of the object.\n\n        This calls ``setattr`` for each of the class's grammar properties. It\n        will catch ``ValueError``s raised by the grammar property's setters\n        and re-raise them as :class:`ValidationError`.\n        \"\"\"\n        for key, val in self.grammar.items():\n            try:\n                setattr(self, key, val)\n            except ValueError as e:\n                raise ValidationError('invalid contents: ' + e.args[0])",
    "label": 0
  },
  {
    "codes": "function parse_SerAr(blob, biff) {\n\tvar val = [blob.read_shift(1)];\n\tif(biff == 12) switch(val[0]) {\n\t\tcase 0x02: val[0] = 0x04; break; /* SerBool */\n\t\tcase 0x04: val[0] = 0x10; break; /* SerErr */\n\t\tcase 0x00: val[0] = 0x01; break; /* SerNum */\n\t\tcase 0x01: val[0] = 0x02; break; /* SerStr */\n\t}\n\tswitch(val[0]) {\n\t\tcase 0x04: /* SerBool -- boolean */\n\t\t\tval[1] = parsebool(blob, 1) ? 'TRUE' : 'FALSE';\n\t\t\tif(biff != 12) blob.l += 7; break;\n\t\tcase 0x25: /* appears to be an alias */\n\t\tcase 0x10: /* SerErr -- error */\n\t\t\tval[1] = BErr[blob[blob.l]];\n\t\t\tblob.l += ((biff == 12) ? 4 : 8); break;\n\t\tcase 0x00: /* SerNil -- honestly, I'm not sure how to reproduce this */\n\t\t\tblob.l += 8; break;\n\t\tcase 0x01: /* SerNum -- Xnum */\n\t\t\tval[1] = parse_Xnum(blob, 8); break;\n\t\tcase 0x02: /* SerStr -- XLUnicodeString (<256 chars) */\n\t\t\tval[1] = parse_XLUnicodeString2(blob, 0, {biff:biff > 0 && biff < 8 ? 2 : biff}); break;\n\t\tdefault: throw new Error(\"Bad SerAr: \" + val[0]); /* Unreachable */\n\t}\n\treturn val;\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n)\n\nfunc handleConnection(conn net.Conn, wg *sync.WaitGroup) {\n\tdefer wg.Done() // Decrement the WaitGroup counter when the function exits\n\tdefer conn.Close() // Close the connection when done\n\n\t// Create a buffered reader for the connection\n\treader := bufio.NewReader(conn)\n\tfor {\n\t\t// Read data from the client\n\t\tmessage, err := reader.ReadString('\\n')\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error reading:\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Print the received message\n\t\tfmt.Print(\"Received: \", string(message))\n\n\t\t// Echo the message back to the client\n\t\t_, err = conn.Write([]byte(\"Echo: \" + message))\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error writing:\", err)\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc main() {\n\t// Create a WaitGroup to manage concurrent connections\n\tvar wg sync.WaitGroup\n\n\t// Start listening on port 8080\n\tlistener, err := net.Listen(\"tcp\", \":8080\")\n\tif err != nil {\n\t\tfmt.Println(\"Error listening:\", err)\n\t\treturn\n\t}\n\tdefer listener.Close() // Close the listener when done\n\n\tfmt.Println(\"Server is listening on port 8080...\")\n\n\tfor {\n\t\t// Accept incoming connections\n\t\tconn, err := listener.Accept()\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error accepting connection:\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Increment the WaitGroup counter for each new connection\n\t\twg.Add(1)\n\n\t\t// Handle the connection in a new goroutine\n\t\tgo handleConnection(conn, &wg)\n\t}\n\n\t// Wait for all connections to finish\n\twg.Wait()\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *RemoveTagsFromVaultInput) SetTagKeys(v []*string) *RemoveTagsFromVaultInput {\n\ts.TagKeys = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "async function _main([buildNumber, compareUrl = '', circleToken = '']) {\n  try {\n    if (!buildNumber || isNaN(buildNumber)) {\n      throw new Error(\n          'Missing or invalid arguments.\\n' +\n          'Expected: buildNumber (number), compareUrl? (string), circleToken? (string)');\n    }\n\n    if (!compareUrl) {\n      compareUrl = await getCompareUrl(buildNumber, circleToken);\n    }\n\n    const commitRangeMatch = COMPARE_URL_RE.exec(compareUrl)\n    const commitRange = commitRangeMatch ? commitRangeMatch[1] : '';\n\n    console.log(commitRange);\n  } catch (err) {\n    console.error(err);\n    process.exit(1);\n  }\n}",
    "label": 0
  },
  {
    "codes": "func (s *JobDescription) SetVaultARN(v string) *JobDescription {\n\ts.VaultARN = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<DeletedVaultInner> getDeletedAsync(String vaultName, String location, final ServiceCallback<DeletedVaultInner> serviceCallback) {\n        return ServiceFuture.fromResponse(getDeletedWithServiceResponseAsync(vaultName, location), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "func (s *GetVaultAccessPolicyOutput) SetPolicy(v *VaultAccessPolicy) *GetVaultAccessPolicyOutput {\n\ts.Policy = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```kotlin\nimport kotlinx.coroutines.*\nimport kotlinx.coroutines.flow.*\nimport kotlin.random.Random\n\ndata class SensorData(val id: Int, val value: Double, val timestamp: Long)\n\n// Simulate a stream of sensor data\nfun sensorDataStream(): Flow<SensorData> = flow {\n    while (true) {\n        emit(SensorData(Random.nextInt(1, 10), Random.nextDouble(20.0, 30.0), System.currentTimeMillis()))\n        delay(100) // Simulate data coming every 100 ms\n    }\n}\n\n// Process the sensor batches\nsuspend fun processBatches(batches: List<SensorData>) {\n    // Placeholder for processing logic\n    println(\"Processing batch: ${batches.size} entries, Avg temp: ${batches.map { it.value }.average()}\")\n}\n\nfun main() = runBlocking {\n    sensorDataStream() // Get the data stream\n        .buffer() // Use a buffer to handle backpressure\n        .chunked(10) // Collect data in batches of 10\n        .collect { batch ->\n            processBatches(batch)\n        }\n}\n```",
    "label": 1
  },
  {
    "codes": "public void beginPurgeDeleted(String vaultName, String location) {\n        beginPurgeDeletedWithServiceResponseAsync(vaultName, location).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "func (c *Glue) GetJobs(input *GetJobsInput) (*GetJobsOutput, error) {\n\treq, out := c.GetJobsRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "public Observable<Page<UsageInner>> listWebWorkerUsagesNextAsync(final String nextPageLink) {\n        return listWebWorkerUsagesNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<UsageInner>>, Page<UsageInner>>() {\n                @Override\n                public Page<UsageInner> call(ServiceResponse<Page<UsageInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "public DeletedSecretBundle getDeletedSecret(String vaultBaseUrl, String secretName) {\n        return getDeletedSecretWithServiceResponseAsync(vaultBaseUrl, secretName).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import make_grid, save_image\n\n# Define the Generator\nclass Generator(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, output_dim),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Define the Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self, input_dim):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.LeakyReLU(0.2),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Hyperparameters\nlatent_dim = 100\nimg_shape = (1, 28, 28)  # Assuming grayscale 28x28 images\nlr = 0.0002\nbatch_size = 64\nepochs = 200\n\n# Transformations\ntransform = transforms.Compose([\n    transforms.Resize(28),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\n# Dataset and DataLoader\ndataset = datasets.FakeData(transform=transform, size=1000, image_size=(1, 28, 28), num_classes=10)\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Initialize Generator and Discriminator\ngenerator = Generator(latent_dim, int(torch.prod(torch.tensor(img_shape))))\ndiscriminator = Discriminator(int(torch.prod(torch.tensor(img_shape))))\n\n# Loss and Optimizers\ncriterion = nn.BCELoss()\noptimizer_G = optim.Adam(generator.parameters(), lr=lr)\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n\n# Training Loop\nfor epoch in range(epochs):\n    for imgs, _ in data_loader:\n        # Train Discriminator\n        real_imgs = imgs.view(imgs.size(0), -1)\n        valid = torch.ones(imgs.size(0), 1)\n        fake = torch.zeros(imgs.size(0), 1)\n\n        optimizer_D.zero_grad()\n        z = torch.randn(imgs.size(0), latent_dim)\n        gen_imgs = generator(z)\n        real_loss = criterion(discriminator(real_imgs), valid)\n        fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n        d_loss = (real_loss + fake_loss) / 2\n        d_loss.backward()\n        optimizer_D.step()\n\n        # Train Generator\n        optimizer_G.zero_grad()\n        g_loss = criterion(discriminator(gen_imgs), valid)\n        g_loss.backward()\n        optimizer_G.step()\n\n    # Print progress\n    print(f\"Epoch [{epoch}/{epochs}]  D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n\n    # Save generated images\n    if epoch % 10 == 0:\n        save_image(gen_imgs.view(gen_imgs.size(0), *img_shape), f\"images/{epoch}.png\", nrow=8, normalize=True)\n\nprint(\"Training complete.\")\n```",
    "label": 1
  },
  {
    "codes": "```kotlin\nimport kotlinx.coroutines.flow.*\n\n// Function to simulate incoming sensor data stream\nfun getSensorDataStream(): Flow<SensorData> = flow {\n    // Emit sensor data at regular intervals\n    while (true) {\n        emit(SensorData(System.currentTimeMillis(), generateRandomSensorValue()))\n        delay(100) // Simulate data arrival every 100 ms\n    }\n}\n\n// Data class representing a sensor data point\ndata class SensorData(val timestamp: Long, val value: Double)\n\nfun main() = runBlocking {\n    getSensorDataStream()\n        .buffer() // Buffer the incoming sensor data\n        .chunked(10) // Batch process every 10 elements\n        .map { batch ->\n            // Process each batch and calculate average\n            val averageValue = batch.map { it.value }.average()\n            SensorData(batch.last().timestamp, averageValue)\n        }\n        .collect { processedData ->\n            // Collect the batched and processed data\n            println(\"Batch processed data: Timestamp=${processedData.timestamp}, Average Value=${processedData.value}\")\n        }\n}\n\n// Helper function to generate random sensor value\nfun generateRandomSensorValue(): Double = (0..100).random().toDouble()\n```",
    "label": 1
  },
  {
    "codes": "```powershell\n# PowerShell Module: ServiceDependencyManager.psm1\n\n# Get service dependencies of a specific service\nfunction Get-ServiceDependencies {\n    [CmdletBinding()]\n    Param (\n        [Parameter(Mandatory = $true, ValueFromPipelineByPropertyName = $true)]\n        [string]$ServiceName\n    )\n\n    process {\n        try {\n            $service = Get-WmiObject -Class Win32_Service -Filter \"Name='$ServiceName'\"\n            if($service) {\n                $service.DependentServices\n            } else {\n                Write-Error \"Service '$ServiceName' not found.\"\n            }\n        } catch {\n            Write-Error $_.Exception.Message\n        }\n    }\n}\n\n# Add a dependency to a service\nfunction Add-ServiceDependency {\n    [CmdletBinding()]\n    Param (\n        [Parameter(Mandatory = $true)]\n        [string]$ServiceName,\n\n        [Parameter(Mandatory = $true)]\n        [string]$DependencyName\n    )\n\n    process {\n        try {\n            $service = Get-WmiObject -Class Win32_Service -Filter \"Name='$ServiceName'\"\n            if($service) {\n                $newDependencies = $service.DependentServices + $DependencyName\n                $service.ChangeStartMode(\"Auto\")  # Ensuring service mode can be changed if needed\n                $service.Change($null, $null, $null, $newDependencies)\n                Write-Output \"Dependency '$DependencyName' added to service '$ServiceName'.\"\n            } else {\n                Write-Error \"Service '$ServiceName' not found.\"\n            }\n        } catch {\n            Write-Error $_.Exception.Message\n        }\n    }\n}\n\n# Remove a dependency from a service\nfunction Remove-ServiceDependency {\n    [CmdletBinding()]\n    Param (\n        [Parameter(Mandatory = $true)]\n        [string]$ServiceName,\n\n        [Parameter(Mandatory = $true)]\n        [string]$DependencyName\n    )\n\n    process {\n        try {\n            $service = Get-WmiObject -Class Win32_Service -Filter \"Name='$ServiceName'\"\n            if($service) {\n                $dependencies = $service.DependentServices - $DependencyName\n                $service.ChangeStartMode(\"Auto\")  # Ensuring service mode can be changed if needed\n                $service.Change($null, $null, $null, $dependencies)\n                Write-Output \"Dependency '$DependencyName' removed from service '$ServiceName'.\"\n            } else {\n                Write-Error \"Service '$ServiceName' not found.\"\n            }\n        } catch {\n            Write-Error $_.Exception.Message\n        }\n    }\n}\n\n# Export Functions\nExport-ModuleMember -Function Get-ServiceDependencies, Add-ServiceDependency, Remove-ServiceDependency\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n// Header file: DynamicWeatherSystem.h\n\n#pragma once\n\n#include \"CoreMinimal.h\"\n#include \"GameFramework/Actor.h\"\n#include \"DynamicWeatherSystem.generated.h\"\n\nUCLASS()\nclass YOURGAME_API ADynamicWeatherSystem : public AActor\n{\n    GENERATED_BODY()\n    \npublic:    \n    // Sets default values for this actor's properties\n    ADynamicWeatherSystem();\n\nprotected:\n    // Called when the game starts or when spawned\n    virtual void BeginPlay() override;\n\npublic:    \n    // Called every frame\n    virtual void Tick(float DeltaTime) override;\n\n    // Function to transition weather\n    UFUNCTION(BlueprintCallable, Category = \"Weather\")\n    void TransitionWeather(FName NewWeatherType, float TransitionDuration);\n\nprivate:\n    // Current weather type\n    UPROPERTY(VisibleAnywhere, Category = \"Weather\")\n    FName CurrentWeatherType;\n\n    // Target weather type\n    UPROPERTY(VisibleAnywhere, Category = \"Weather\")\n    FName TargetWeatherType;\n\n    // Transition progress (0 to 1)\n    UPROPERTY(VisibleAnywhere, Category = \"Weather\")\n    float TransitionProgress;\n\n    // Duration of the transition\n    UPROPERTY(EditAnywhere, Category = \"Weather\")\n    float TransitionTime;\n\n    // Function to update weather effects\n    void UpdateWeatherEffects(float DeltaTime);\n};\n\n// Source file: DynamicWeatherSystem.cpp\n\n#include \"DynamicWeatherSystem.h\"\n\n// Sets default values\nADynamicWeatherSystem::ADynamicWeatherSystem()\n{\n    // Set this actor to call Tick() every frame\n    PrimaryActorTick.bCanEverTick = true;\n\n    // Initialize default values\n    CurrentWeatherType = \"Clear\";\n    TargetWeatherType = \"Clear\";\n    TransitionProgress = 0.0f;\n    TransitionTime = 5.0f; // Default transition time\n}\n\n// Called when the game starts or when spawned\nvoid ADynamicWeatherSystem::BeginPlay()\n{\n    Super::BeginPlay();\n}\n\n// Called every frame\nvoid ADynamicWeatherSystem::Tick(float DeltaTime)\n{\n    Super::Tick(DeltaTime);\n\n    // Update weather effects during transition\n    if (TransitionProgress < 1.0f)\n    {\n        TransitionProgress += DeltaTime / TransitionTime;\n        UpdateWeatherEffects(DeltaTime);\n    }\n    else\n    {\n        // Transition complete\n        CurrentWeatherType = TargetWeatherType;\n        TransitionProgress = 0.0f;\n    }\n}\n\n// Function to transition weather\nvoid ADynamicWeatherSystem::TransitionWeather(FName NewWeatherType, float TransitionDuration)\n{\n    TargetWeatherType = NewWeatherType;\n    TransitionTime = TransitionDuration;\n    TransitionProgress = 0.0f;\n}\n\n// Function to update weather effects\nvoid ADynamicWeatherSystem::UpdateWeatherEffects(float DeltaTime)\n{\n    // Interpolate between current and target weather effects\n    float LerpValue = FMath::Clamp(TransitionProgress, 0.0f, 1.0f);\n\n    // Example: Adjust fog density based on weather type\n    if (CurrentWeatherType == \"Clear\" && TargetWeatherType == \"Rain\")\n    {\n        float FogDensity = FMath::Lerp(0.0f, 0.5f, LerpValue);\n        // Apply fog density to the world\n    }\n    else if (CurrentWeatherType == \"Rain\" && TargetWeatherType == \"Clear\")\n    {\n        float FogDensity = FMath::Lerp(0.5f, 0.0f, LerpValue);\n        // Apply fog density to the world\n    }\n\n    // Add more weather effects (e.g., rain intensity, wind speed) as needed\n}\n```\n\n### Comments:\n- **DynamicWeatherSystem.h**: Defines the class and its properties, including weather types and transition logic.\n- **DynamicWeatherSystem.cpp**: Implements the weather transition logic, including interpolation between weather states.\n- **TransitionWeather**: Blueprint-callable function to initiate a weather transition.\n- **UpdateWeatherEffects**: Handles the interpolation of weather effects during transitions.",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary Selenium modules\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# Initialize the WebDriver (e.g., Chrome)\ndriver = webdriver.Chrome()\n\n# Open the login page\ndriver.get(\"https://example.com/login\")\n\n# Wait for the username field to be present and enter the username\nusername_field = WebDriverWait(driver, 10).until(\n    EC.presence_of_element_located((By.ID, \"username\"))\n)\nusername_field.send_keys(\"testuser\")\n\n# Wait for the password field to be present and enter the password\npassword_field = WebDriverWait(driver, 10).until(\n    EC.presence_of_element_located((By.ID, \"password\"))\n)\npassword_field.send_keys(\"testpassword\")\n\n# Click the login button\nlogin_button = WebDriverWait(driver, 10).until(\n    EC.element_to_be_clickable((By.ID, \"login-button\"))\n)\nlogin_button.click()\n\n# Wait for the dashboard to load and verify the URL\nWebDriverWait(driver, 10).until(\n    EC.url_to_be(\"https://example.com/dashboard\")\n)\n\n# Close the browser\ndriver.quit()\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\n# Define the Generator\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, img_shape):\n        super(Generator, self).__init__()\n        self.img_shape = img_shape\n        \n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n        \n        self.model = nn.Sequential(\n            *block(latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(torch.prod(torch.tensor(img_shape)))),\n            nn.Tanh()\n        )\n    \n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.size(0), *self.img_shape)\n        return img\n\n# Define the Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self, img_shape):\n        super(Discriminator, self).__init__()\n        \n        self.model = nn.Sequential(\n            nn.Linear(int(torch.prod(torch.tensor(img_shape))), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, img):\n        img_flat = img.view(img.size(0), -1)\n        validity = self.model(img_flat)\n        return validity\n\n# Hyperparameters\nlatent_dim = 100\nimg_shape = (1, 28, 28)  # Example for MNIST, adjust for face images\nlr = 0.0002\nb1 = 0.5\nb2 = 0.999\nn_epochs = 200\nbatch_size = 64\n\n# Initialize generator and discriminator\ngenerator = Generator(latent_dim, img_shape)\ndiscriminator = Discriminator(img_shape)\n\n# Optimizers\noptimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n\n# Loss function\nadversarial_loss = nn.BCELoss()\n\n# Configure data loader\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ndataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Training loop\nfor epoch in range(n_epochs):\n    for i, (imgs, _) in enumerate(dataloader):\n        \n        # Adversarial ground truths\n        valid = torch.ones(imgs.size(0), 1)\n        fake = torch.zeros(imgs.size(0), 1)\n        \n        # Configure input\n        real_imgs = imgs\n        \n        # Train Generator\n        optimizer_G.zero_grad()\n        \n        # Sample noise as generator input\n        z = torch.randn(imgs.size(0), latent_dim)\n        \n        # Generate a batch of images\n        gen_imgs = generator(z)\n        \n        # Loss measures generator's ability to fool the discriminator\n        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n        \n        g_loss.backward()\n        optimizer_G.step()\n        \n        # Train Discriminator\n        optimizer_D.zero_grad()\n        \n        # Measure discriminator's ability to classify real from generated samples\n        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n        d_loss = (real_loss + fake_loss) / 2\n        \n        d_loss.backward()\n        optimizer_D.step()\n        \n        # Print progress\n        if i % 100 == 0:\n            print(f\"[Epoch {epoch}/{n_epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n    \n    # Save generated images\n    if epoch % 10 == 0:\n        with torch.no_grad():\n            sample_z = torch.randn(16, latent_dim)\n            gen_imgs = generator(sample_z)\n            plt.figure(figsize=(4, 4))\n            for i in range(gen_imgs.size(0)):\n                plt.subplot(4, 4, i+1)\n                plt.imshow(gen_imgs[i].squeeze(), cmap='gray')\n                plt.axis('off')\n            plt.show()\n```",
    "label": 1
  },
  {
    "codes": "func (s *CreateDevEndpointOutput) SetRoleArn(v string) *CreateDevEndpointOutput {\n\ts.RoleArn = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (c *Glacier) DeleteVaultRequest(input *DeleteVaultInput) (req *request.Request, output *DeleteVaultOutput) {\n\top := &request.Operation{\n\t\tName:       opDeleteVault,\n\t\tHTTPMethod: \"DELETE\",\n\t\tHTTPPath:   \"/{accountId}/vaults/{vaultName}\",\n\t}\n\n\tif input == nil {\n\t\tinput = &DeleteVaultInput{}\n\t}\n\n\toutput = &DeleteVaultOutput{}\n\treq = c.newRequest(op, input, output)\n\treq.Handlers.Unmarshal.Swap(restjson.UnmarshalHandler.Name, protocol.UnmarshalDiscardBodyHandler)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.springframework.stereotype.Component;\n\n@Aspect\n@Component\npublic class CircuitBreakerAspect {\n\n    private static final int MAX_FAILURES = 3;\n    private int failureCount = 0;\n    private boolean circuitOpen = false;\n\n    @Around(\"@annotation(CircuitBreaker)\") // Assuming @CircuitBreaker annotation exists\n    public Object circuitBreaker(ProceedingJoinPoint joinPoint) throws Throwable {\n        if (circuitOpen) {\n            throw new RuntimeException(\"Circuit is open. Service unavailable.\");\n        }\n\n        try {\n            Object result = joinPoint.proceed();\n            failureCount = 0; // Reset failure count on success\n            return result;\n        } catch (Exception e) {\n            failureCount++;\n            if (failureCount >= MAX_FAILURES) {\n                circuitOpen = true; // Open the circuit\n            }\n            throw e; // Re-throw the exception\n        }\n    }\n}\n```\n\n```java\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\n@Target(ElementType.METHOD)\n@Retention(RetentionPolicy.RUNTIME)\npublic @interface CircuitBreaker {\n    // Marker annotation for methods to apply circuit breaker\n}\n```\n\n```java\n@Service\npublic class MyService {\n\n    @CircuitBreaker\n    public String performOperation() {\n        // Simulate a service call that might fail\n        if (Math.random() > 0.5) {\n            throw new RuntimeException(\"Service failure\");\n        }\n        return \"Operation successful\";\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport javafx.application.Application;\nimport javafx.scene.Scene;\nimport javafx.scene.canvas.Canvas;\nimport javafx.scene.canvas.GraphicsContext;\nimport javafx.scene.control.Slider;\nimport javafx.scene.layout.BorderPane;\nimport javafx.scene.layout.VBox;\nimport javafx.scene.paint.Color;\nimport javafx.stage.Stage;\n\npublic class FractalApp extends Application {\n\n    private static final int WIDTH = 800;\n    private static final int HEIGHT = 600;\n    private static final int MAX_ITERATIONS = 1000;\n\n    private Slider zoomSlider;\n    private Slider iterationSlider;\n    private Canvas canvas;\n    private GraphicsContext gc;\n\n    @Override\n    public void start(Stage primaryStage) {\n        // Initialize UI components\n        canvas = new Canvas(WIDTH, HEIGHT);\n        gc = canvas.getGraphicsContext2D();\n\n        zoomSlider = new Slider(1, 10, 1);\n        zoomSlider.setShowTickLabels(true);\n        zoomSlider.setShowTickMarks(true);\n\n        iterationSlider = new Slider(100, MAX_ITERATIONS, 100);\n        iterationSlider.setShowTickLabels(true);\n        iterationSlider.setShowTickMarks(true);\n\n        // Event listeners for sliders\n        zoomSlider.valueProperty().addListener((obs, oldVal, newVal) -> drawFractal());\n        iterationSlider.valueProperty().addListener((obs, oldVal, newVal) -> drawFractal());\n\n        // Layout setup\n        VBox sliders = new VBox(10, zoomSlider, iterationSlider);\n        BorderPane root = new BorderPane();\n        root.setCenter(canvas);\n        root.setBottom(sliders);\n\n        // Scene setup\n        Scene scene = new Scene(root, WIDTH, HEIGHT);\n        primaryStage.setTitle(\"Interactive Fractal Patterns\");\n        primaryStage.setScene(scene);\n        primaryStage.show();\n\n        // Initial fractal drawing\n        drawFractal();\n    }\n\n    private void drawFractal() {\n        double zoom = zoomSlider.getValue();\n        int maxIterations = (int) iterationSlider.getValue();\n\n        for (int x = 0; x < WIDTH; x++) {\n            for (int y = 0; y < HEIGHT; y++) {\n                double zx = 0, zy = 0;\n                double cx = (x - WIDTH / 2.0) / (0.5 * zoom * WIDTH);\n                double cy = (y - HEIGHT / 2.0) / (0.5 * zoom * HEIGHT);\n\n                int iteration = 0;\n                while (zx * zx + zy * zy < 4 && iteration < maxIterations) {\n                    double temp = zx * zx - zy * zy + cx;\n                    zy = 2 * zx * zy + cy;\n                    zx = temp;\n                    iteration++;\n                }\n\n                // Set color based on iteration count\n                if (iteration < maxIterations) {\n                    gc.setFill(Color.hsb(iteration % 360, 1.0, 1.0));\n                } else {\n                    gc.setFill(Color.BLACK);\n                }\n\n                gc.fillRect(x, y, 1, 1);\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        launch(args);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *CreateDevEndpointInput) Validate() error {\n\tinvalidParams := request.ErrInvalidParams{Context: \"CreateDevEndpointInput\"}\n\tif s.EndpointName == nil {\n\t\tinvalidParams.Add(request.NewErrParamRequired(\"EndpointName\"))\n\t}\n\tif s.RoleArn == nil {\n\t\tinvalidParams.Add(request.NewErrParamRequired(\"RoleArn\"))\n\t}\n\tif s.SecurityConfiguration != nil && len(*s.SecurityConfiguration) < 1 {\n\t\tinvalidParams.Add(request.NewErrParamMinLen(\"SecurityConfiguration\", 1))\n\t}\n\n\tif invalidParams.Len() > 0 {\n\t\treturn invalidParams\n\t}\n\treturn nil\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport org.apache.pdfbox.multipdf.PDFMergerUtility;\nimport org.apache.pdfbox.pdmodel.PDDocument;\n\nimport java.io.File;\nimport java.io.IOException;\n\npublic class PDFMerger {\n\n    // Method to merge multiple PDFs into a single PDF\n    public static void mergePDFs(String[] inputPaths, String outputPath) throws IOException {\n        // Create a PDFMergerUtility instance\n        PDFMergerUtility pdfMerger = new PDFMergerUtility();\n\n        // Add each PDF file to the merger utility\n        for (String inputPath : inputPaths) {\n            File file = new File(inputPath);\n            if (file.exists()) {\n                pdfMerger.addSource(file);\n            } else {\n                System.err.println(\"File not found: \" + inputPath);\n            }\n        }\n\n        // Set the destination file path\n        pdfMerger.setDestinationFileName(outputPath);\n\n        // Perform the merge operation\n        pdfMerger.mergeDocuments(null);\n\n        System.out.println(\"PDFs merged successfully into: \" + outputPath);\n    }\n\n    public static void main(String[] args) {\n        try {\n            // Example usage\n            String[] pdfFiles = {\"file1.pdf\", \"file2.pdf\", \"file3.pdf\"};\n            String outputFile = \"merged.pdf\";\n            mergePDFs(pdfFiles, outputFile);\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nfunction deepClone(obj, cache = new WeakMap()) {\n    // Handle non-object types and null\n    if (obj === null || typeof obj !== 'object') {\n        return obj;\n    }\n\n    // Handle circular references\n    if (cache.has(obj)) {\n        return cache.get(obj);\n    }\n\n    // Handle Date objects\n    if (obj instanceof Date) {\n        return new Date(obj);\n    }\n\n    // Handle Array objects\n    if (Array.isArray(obj)) {\n        const clonedArray = [];\n        cache.set(obj, clonedArray);\n        obj.forEach((item, index) => {\n            clonedArray[index] = deepClone(item, cache);\n        });\n        return clonedArray;\n    }\n\n    // Handle regular objects\n    const clonedObj = {};\n    cache.set(obj, clonedObj);\n    for (let key in obj) {\n        if (obj.hasOwnProperty(key)) {\n            clonedObj[key] = deepClone(obj[key], cache);\n        }\n    }\n\n    return clonedObj;\n}\n```",
    "label": 1
  },
  {
    "codes": "```typescript\ninterface GeometricShape {\n    // Method to calculate the area of the shape\n    calculateArea(): number;\n    // A descriptive name of the shape\n    name: string;\n}\n\n// Example implementation for a circle\nclass Circle implements GeometricShape {\n    // Circle's radius property\n    private radius: number;\n    // Circle's name property\n    name: string;\n\n    constructor(radius: number) {\n        this.radius = radius;\n        this.name = 'Circle';\n    }\n\n    // Calculate area of the circle using \u03c0r\u00b2\n    calculateArea(): number {\n        return Math.PI * this.radius * this.radius;\n    }\n}\n\n// Example usage\nconst myCircle = new Circle(5);\nconsole.log(myCircle.name);            // Output: Circle\nconsole.log(myCircle.calculateArea()); // Output: 78.53981633974483\n```\n",
    "label": 1
  },
  {
    "codes": "```rust\nuse std::collections::HashMap;\n\nfn fibonacci(n: u64, memo: &mut HashMap<u64, u64>) -> u64 {\n    if n <= 1 {\n        return n;\n    }\n\n    // Check if the result is already computed and stored in the memo\n    if let Some(&result) = memo.get(&n) {\n        return result;\n    }\n\n    // Compute the nth Fibonacci number\n    let result = fibonacci(n - 1, memo) + fibonacci(n - 2, memo);\n\n    // Store the result in the memo before returning\n    memo.insert(n, result);\n    result\n}\n\nfn main() {\n    let n = 10; // Change this value for different Fibonacci numbers\n    let mut memo = HashMap::new();\n    let fib_n = fibonacci(n, &mut memo);\n    println!(\"The {}th Fibonacci number is {}\", n, fib_n);\n}\n```",
    "label": 1
  },
  {
    "codes": "def add_platforms(in_wheel, platforms, out_path=None, clobber=False):\n    \"\"\" Add platform tags `platforms` to `in_wheel` filename and WHEEL tags\n\n    Add any platform tags in `platforms` that are missing from `in_wheel`\n    filename.\n\n    Add any platform tags in `platforms` that are missing from `in_wheel`\n    ``WHEEL`` file.\n\n    Parameters\n    ----------\n    in_wheel : str\n        Filename of wheel to which to add platform tags\n    platforms : iterable\n        platform tags to add to wheel filename and WHEEL tags - e.g.\n        ``('macosx_10_9_intel', 'macosx_10_9_x86_64')\n    out_path : None or str, optional\n        Directory to which to write new wheel.  Default is directory containing\n        `in_wheel`\n    clobber : bool, optional\n        If True, overwrite existing output filename, otherwise raise error\n\n    Returns\n    -------\n    out_wheel : None or str\n        Absolute path of wheel file written, or None if no wheel file written.\n    \"\"\"\n    in_wheel = abspath(in_wheel)\n    out_path = dirname(in_wheel) if out_path is None else abspath(out_path)\n    wf = WheelFile(in_wheel)\n    info_fname = _get_wheelinfo_name(wf)\n    # Check what tags we have\n    in_fname_tags = wf.parsed_filename.groupdict()['plat'].split('.')\n    extra_fname_tags = [tag for tag in platforms if tag not in in_fname_tags]\n    in_wheel_base, ext = splitext(basename(in_wheel))\n    out_wheel_base = '.'.join([in_wheel_base] + list(extra_fname_tags))\n    out_wheel = pjoin(out_path, out_wheel_base + ext)\n    if exists(out_wheel) and not clobber:\n        raise WheelToolsError('Not overwriting {0}; set clobber=True '\n                              'to overwrite'.format(out_wheel))\n    with InWheelCtx(in_wheel) as ctx:\n        info = read_pkg_info(info_fname)\n        if info['Root-Is-Purelib'] == 'true':\n            raise WheelToolsError('Cannot add platforms to pure wheel')\n        in_info_tags = [tag for name, tag in info.items() if name == 'Tag']\n        # Python version, C-API version combinations\n        pyc_apis = ['-'.join(tag.split('-')[:2]) for tag in in_info_tags]\n        # unique Python version, C-API version combinations\n        pyc_apis = unique_by_index(pyc_apis)\n        # Add new platform tags for each Python version, C-API combination\n        required_tags = ['-'.join(tup) for tup in product(pyc_apis, platforms)]\n        needs_write = False\n        for req_tag in required_tags:\n            if req_tag in in_info_tags: continue\n            needs_write = True\n            info.add_header('Tag', req_tag)\n        if needs_write:\n            write_pkg_info(info_fname, info)\n            # Tell context manager to write wheel on exit by setting filename\n            ctx.out_wheel = out_wheel\n    return ctx.out_wheel",
    "label": 0
  },
  {
    "codes": "func (s *JsonClassifier) SetLastUpdated(v time.Time) *JsonClassifier {\n\ts.LastUpdated = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: custom-metrics-config\n  namespace: custom-metrics\ndata:\n  config.yaml: |\n    rules:\n      - seriesQuery: 'queue_depth{namespace=\"default\"}'\n        resources:\n          overrides:\n            namespace:\n              resource: \"namespace\"\n        name:\n          matches: \"^(.*)_total$\"\n          as: \"${1}_per_second\"\n        metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[1m])) by (<<.GroupBy>>)'\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: custom-metrics-sa\n  namespace: default\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: custom-metrics-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  verbs: [\"get\", \"list\"]\n- apiGroups: [\"extensions\"]\n  resources: [\"thirdpartyresources\"]\n  verbs: [\"get\", \"list\"]\n- apiGroups: [\"custom.metrics.k8s.io\"]\n  resources: [\"*\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: custom-metrics-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: custom-metrics-reader\nsubjects:\n- kind: ServiceAccount\n  name: custom-metrics-sa\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pytorch-inference\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pytorch-inference\n  template:\n    metadata:\n      labels:\n        app: pytorch-inference\n    spec:\n      containers:\n      - name: pytorch-inference\n        image: pytorch/torchserve:latest\n        ports:\n        - containerPort: 8080\n---\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: pytorch-hpa\n  namespace: default\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: pytorch-inference\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Pods\n    pods:\n      metric:\n        name: queue_depth_per_second\n      target:\n        type: AverageValue\n        averageValue: 10\n```\n\nThis Kubernetes manifest sets up a horizontal pod autoscaler (HPA) for PyTorch inference based on the custom metric \"queue_depth_per_second\". It includes definitions for necessary roles, service accounts, and custom metric configuration.",
    "label": 1
  },
  {
    "codes": "func (s *GetUserDefinedFunctionOutput) SetUserDefinedFunction(v *UserDefinedFunction) *GetUserDefinedFunctionOutput {\n\ts.UserDefinedFunction = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<StorageBundle> setStorageAccountAsync(String vaultBaseUrl, String storageAccountName, String resourceId, String activeKeyName, boolean autoRegenerateKey, final ServiceCallback<StorageBundle> serviceCallback) {\n        return ServiceFuture.fromResponse(setStorageAccountWithServiceResponseAsync(vaultBaseUrl, storageAccountName, resourceId, activeKeyName, autoRegenerateKey), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```ruby\n# frozen_string_literal: true\n\nrequire 'sentry-ruby'\n\nmodule ErrorReporter\n  # Configure Sentry with your DSN and other settings\n  Sentry.init do |config|\n    config.dsn = 'your_sentry_dsn_here'\n    # Additional configuration can be added here, such as environment, release, etc.\n  end\n\n  # This method captures and reports exceptions to Sentry\n  def self.report(exception)\n    # Sends the exception to Sentry\n    Sentry.capture_exception(exception)\n  rescue StandardError => capture_exception_error\n    # Log or handle errors that occur during the reporting to Sentry itself\n    puts \"Failed to report exception to Sentry: #{capture_exception_error.message}\"\n  end\n\n  # Optionally, a method to capture messages can be added\n  def self.capture_message(message)\n    # Sends a custom message to Sentry\n    Sentry.capture_message(message)\n  rescue StandardError => capture_message_error\n    # Handle errors that occur during the message capture\n    puts \"Failed to report message to Sentry: #{capture_message_error.message}\"\n  end\nend\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<IssuerBundle>> setCertificateIssuerWithServiceResponseAsync(String vaultBaseUrl, String issuerName, String provider) {\n        if (vaultBaseUrl == null) {\n            throw new IllegalArgumentException(\"Parameter vaultBaseUrl is required and cannot be null.\");\n        }\n        if (issuerName == null) {\n            throw new IllegalArgumentException(\"Parameter issuerName is required and cannot be null.\");\n        }\n        if (this.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.apiVersion() is required and cannot be null.\");\n        }\n        if (provider == null) {\n            throw new IllegalArgumentException(\"Parameter provider is required and cannot be null.\");\n        }\n        final IssuerCredentials credentials = null;\n        final OrganizationDetails organizationDetails = null;\n        final IssuerAttributes attributes = null;\n        CertificateIssuerSetParameters parameter = new CertificateIssuerSetParameters();\n        parameter.withProvider(provider);\n        parameter.withCredentials(null);\n        parameter.withOrganizationDetails(null);\n        parameter.withAttributes(null);\n        String parameterizedHost = Joiner.on(\", \").join(\"{vaultBaseUrl}\", vaultBaseUrl);\n        return service.setCertificateIssuer(issuerName, this.apiVersion(), this.acceptLanguage(), parameter, parameterizedHost, this.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<IssuerBundle>>>() {\n                @Override\n                public Observable<ServiceResponse<IssuerBundle>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<IssuerBundle> clientResponse = setCertificateIssuerDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "public void start(String resourceGroupName, String clusterName) {\n        startWithServiceResponseAsync(resourceGroupName, clusterName).toBlocking().last().body();\n    }",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <opencv2/opencv.hpp>\n\nusing namespace cv;\n\n// Main function to preprocess video frames\nvoid preprocessVideoFrames(const std::string& videoPath) {\n    // Open the video file\n    VideoCapture cap(videoPath);\n    if (!cap.isOpened()) {\n        throw std::runtime_error(\"Cannot open the video file\");\n    }\n\n    Mat frame;\n    while (true) {\n        // Capture each frame\n        if (!cap.read(frame)) {\n            break; // Exit loop if done\n        }\n\n        // Resize frame to a fixed size (e.g., 256x256)\n        Mat resizedFrame;\n        resize(frame, resizedFrame, Size(256, 256));\n\n        // Convert the frame to grayscale\n        Mat grayFrame;\n        cvtColor(resizedFrame, grayFrame, COLOR_BGR2GRAY);\n\n        // Normalize pixel values to range [0, 1]\n        Mat normalizedFrame;\n        grayFrame.convertTo(normalizedFrame, CV_32F, 1.0 / 255.0);\n\n        // Optionally, apply Gaussian blur to reduce noise\n        Mat blurredFrame;\n        GaussianBlur(normalizedFrame, blurredFrame, Size(3, 3), 0);\n\n        // Now the frame is ready to be fed into a machine learning model\n        // Suppose you have a function called preprocessForModel for further processing:\n        // preprocessForModel(blurredFrame);\n\n        // For visualization (optional):\n        imshow(\"Processed Frame\", blurredFrame);\n        if (waitKey(30) >= 0) break; // Exit on any key press\n    }\n\n    // Release the video capture object and destroy all OpenCV windows\n    cap.release();\n    destroyAllWindows();\n}\n\nint main() {\n    std::string videoFilePath = \"path/to/your/video.mp4\";\n\n    try {\n        preprocessVideoFrames(videoFilePath);\n    } catch (const std::exception& e) {\n        std::cerr << \"Error: \" << e.what() << std::endl;\n        return -1;\n    }\n    \n    return 0;\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```python\nimport asyncio\nimport logging\nfrom collections import deque\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Connection pool to manage active connections\nconnection_pool = deque(maxlen=10)  # Adjust maxlen as needed\n\nasync def handle_client(reader, writer):\n    \"\"\"\n    Handle incoming client connections and proxy data to the target server.\n    \"\"\"\n    client_addr = writer.get_extra_info('peername')\n    logger.info(f\"New connection from {client_addr}\")\n\n    try:\n        # Get a connection from the pool or create a new one\n        if connection_pool:\n            target_reader, target_writer = connection_pool.pop()\n            logger.info(f\"Reusing connection from pool for {client_addr}\")\n        else:\n            target_reader, target_writer = await asyncio.open_connection('target.server.com', 8080)\n            logger.info(f\"Created new connection for {client_addr}\")\n\n        # Forward data between client and target server\n        await asyncio.gather(\n            forward_data(reader, target_writer, f\"{client_addr} -> target\"),\n            forward_data(target_reader, writer, f\"target -> {client_addr}\")\n        )\n\n    except Exception as e:\n        logger.error(f\"Error handling connection from {client_addr}: {e}\")\n    finally:\n        # Return the connection to the pool\n        if 'target_writer' in locals() and not target_writer.is_closing():\n            connection_pool.append((target_reader, target_writer))\n            logger.info(f\"Connection returned to pool for {client_addr}\")\n        writer.close()\n        await writer.wait_closed()\n        logger.info(f\"Connection closed for {client_addr}\")\n\nasync def forward_data(reader, writer, label):\n    \"\"\"\n    Forward data between two streams.\n    \"\"\"\n    try:\n        while not reader.at_eof():\n            data = await reader.read(4096)\n            if not data:\n                break\n            writer.write(data)\n            await writer.drain()\n            logger.debug(f\"Data forwarded: {label} - {len(data)} bytes\")\n    except Exception as e:\n        logger.error(f\"Error forwarding data for {label}: {e}\")\n    finally:\n        writer.close()\n        await writer.wait_closed()\n\nasync def start_proxy_server(host, port):\n    \"\"\"\n    Start the TCP proxy server.\n    \"\"\"\n    server = await asyncio.start_server(handle_client, host, port)\n    logger.info(f\"Proxy server started on {host}:{port}\")\n    async with server:\n        await server.serve_forever()\n\nif __name__ == \"__main__\":\n    asyncio.run(start_proxy_server('0.0.0.0', 8888))\n```",
    "label": 1
  },
  {
    "codes": "def remove(self, member):\n        \"\"\"\n        Identical to __delitem__, but returns whether a member was removed.\n        \"\"\"\n        if member not in self:\n            return False\n        score = self._members[member]\n        score_index = bisect_left(self._scores, (score, member))\n        del self._scores[score_index]\n        del self._members[member]\n        return True",
    "label": 0
  },
  {
    "codes": "function makeLocalDevConfig(EXAMPLE_DIR = LIB_DIR, linkToLuma) {\n  const LUMA_LINK_ALIASES = {\n    '@luma.gl/constants': `${ROOT_DIR}/../luma.gl/modules/constants/src`,\n    '@luma.gl/core': `${ROOT_DIR}/../luma.gl/modules/core/src`,\n    '@luma.gl/debug': `${ROOT_DIR}/../luma.gl/modules/debug/src`,\n    '@luma.gl/webgl': `${ROOT_DIR}/../luma.gl/modules/webgl/src`,\n    '@luma.gl/webgl-state-tracker': `${ROOT_DIR}/../luma.gl/modules/webgl-state-tracker/src`,\n    '@luma.gl/webgl2-polyfill': `${ROOT_DIR}/../luma.gl/modules/webgl2-polyfill/src`,\n    '@luma.gl/shadertools': `${ROOT_DIR}/../luma.gl/modules/shadertools/src`,\n    '@luma.gl/addons': `${ROOT_DIR}/../luma.gl/modules/addons/src`\n  };\n  const LUMA_LOCAL_ALIASES = {\n    '@luma.gl/core': `${ROOT_DIR}/node_modules/@luma.gl/core`,\n    '@luma.gl/webgl': `${ROOT_DIR}/node_modules/@luma.gl/webgl`,\n    '@luma.gl/webgl-state-tracker': `${ROOT_DIR}/node_modules/@luma.gl/webgl-state-tracker`,\n    '@luma.gl/webgl2-polyfill': `${ROOT_DIR}/node_modules/@luma.gl/webgl2-polyfill`,\n    '@luma.gl/shadertools': `${ROOT_DIR}/node_modules/@luma.gl/shadertools`,\n    // @luma.gl/addons is not available in the root node_modules, must be imported\n    // where required.\n    '@loaders.gl/core': `${ROOT_DIR}/node_modules/@loaders.gl/core`\n  };\n\n  const LUMA_ALIASES = linkToLuma ? LUMA_LINK_ALIASES : LUMA_LOCAL_ALIASES;\n  // console.warn(JSON.stringify(LUMA_ALIASES, null, 2)); // uncomment to debug config\n  // require('fs').writeFileSync('/tmp/ocular.log', JSON.stringify(config, null, 2));\n\n  return {\n    // TODO - Uncomment when all examples use webpack 4 for faster bundling\n    // mode: 'development',\n\n    // suppress warnings about bundle size\n    devServer: {\n      stats: {\n        warnings: false\n      }\n    },\n\n    devtool: 'source-map',\n\n    resolve: {\n      // mainFields: ['esnext', 'module', 'main'],\n\n      alias: Object.assign({}, ALIASES, LUMA_ALIASES, {\n        // Use luma.gl installed in parallel with deck.gl\n        // Important: ensure shared dependencies come from the main node_modules dir\n        // Versions will be controlled by the deck.gl top level package.json\n        'math.gl': resolve(LIB_DIR, './node_modules/math.gl'),\n        'viewport-mercator-project': resolve(LIB_DIR, './node_modules/viewport-mercator-project'),\n        seer: resolve(LIB_DIR, './node_modules/seer'),\n        react: resolve(LIB_DIR, './node_modules/react')\n      })\n    },\n    module: {\n      rules: [\n        {\n          // Unfortunately, webpack doesn't import library sourcemaps on its own...\n          test: /\\.js$/,\n          use: ['source-map-loader'],\n          enforce: 'pre'\n        },\n        {\n          // Compile source using babel. This is not necessary for src to run in the browser\n          // However class inheritance cannot happen between transpiled/non-transpiled code\n          // Which affects some examples\n          test: /\\.js$/,\n          loader: 'babel-loader',\n          options: {\n            presets: ['@babel/env']\n          },\n          include: [`${ROOT_DIR}/modules`, `${ROOT_DIR}/../luma.gl/modules`]\n        }\n      ]\n    }\n  };\n}",
    "label": 0
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model)\n\n# Save the CoreML model to a file\ncoreml_model.save('your_model.mlmodel')\n\n# Print success message\nprint(\"Keras model successfully converted to CoreML format and saved as 'your_model.mlmodel'.\")\n```\n\n```python\n# Alternative method using TensorFlow 2.x and coremltools 4.0+\nimport tensorflow as tf\nimport coremltools as ct\n\n# Load the Keras model\nkeras_model = tf.keras.models.load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model, convert_to='mlprogram')\n\n# Save the CoreML model to a file\ncoreml_model.save('your_model.mlpackage')\n\n# Print success message\nprint(\"Keras model successfully converted to CoreML format and saved as 'your_model.mlpackage'.\")\n```\n\n```python\n# Another alternative method with additional metadata\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format with metadata\ncoreml_model = ct.convert(\n    keras_model,\n    inputs=[ct.ImageType(name='input', scale=1/255.0)],  # Example for image input\n    classifier_config=ct.ClassifierConfig(class_labels=['class1', 'class2'])  # Example for classification\n)\n\n# Save the CoreML model to a file\ncoreml_model.save('your_model_with_metadata.mlmodel')\n\n# Print success message\nprint(\"Keras model with metadata successfully converted to CoreML format and saved as 'your_model_with_metadata.mlmodel'.\")\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/rwcarlsen/goexif/exif\" // Import the goexif package\n)\n\nfunc main() {\n\t// Open the JPEG file\n\tfile, err := os.Open(\"image.jpg\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer file.Close()\n\n\t// Decode the EXIF metadata\n\tx, err := exif.Decode(file)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Extract specific EXIF fields\n\tdateTime, err := x.DateTime()\n\tif err != nil {\n\t\tlog.Println(\"DateTime not found:\", err)\n\t} else {\n\t\tfmt.Println(\"DateTime:\", dateTime)\n\t}\n\n\tlat, long, err := x.LatLong()\n\tif err != nil {\n\t\tlog.Println(\"LatLong not found:\", err)\n\t} else {\n\t\tfmt.Printf(\"Latitude: %f, Longitude: %f\\n\", lat, long)\n\t}\n\n\t// Extract and print all available EXIF tags\n\tx.Walk(func(field exif.FieldName, val exif.Tag) error {\n\t\tfmt.Printf(\"%s: %s\\n\", field, val)\n\t\treturn nil\n\t})\n}\n```",
    "label": 1
  },
  {
    "codes": "```dart\nimport 'dart:math';\n\nString formatCompactNumber(num number) {\n  if (number < 1000) {\n    return number.toString();\n  } else if (number < 1000000) {\n    return (number / 1000).toStringAsFixed(1) + 'K';\n  } else if (number < 1000000000) {\n    return (number / 1000000).toStringAsFixed(1) + 'M';\n  } else {\n    return (number / 1000000000).toStringAsFixed(1) + 'B';\n  }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```go\n// main.go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"sync\"\n\t\"time\"\n)\n\n// PredictionRequest represents a single prediction request\ntype PredictionRequest struct {\n\tID       int\n\tInput    []float32\n\tResponse chan float32\n}\n\n// GPUWorker simulates a GPU worker processing predictions\nfunc GPUWorker(id int, requests <-chan PredictionRequest, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\tfor req := range requests {\n\t\t// Simulate GPU processing\n\t\ttime.Sleep(100 * time.Millisecond) // Simulate computation time\n\t\tresult := req.Input[0] * 2         // Dummy prediction logic\n\t\treq.Response <- result\n\t}\n}\n\n// BatchProcessor handles batching and dispatching requests to GPU workers\nfunc BatchProcessor(ctx context.Context, workerCount int, batchSize int) (chan PredictionRequest, *sync.WaitGroup) {\n\trequests := make(chan PredictionRequest, batchSize)\n\tvar wg sync.WaitGroup\n\n\t// Start GPU workers\n\tfor i := 0; i < workerCount; i++ {\n\t\twg.Add(1)\n\t\tgo GPUWorker(i, requests, &wg)\n\t}\n\n\t// Close the channel and wait for workers to finish when context is done\n\tgo func() {\n\t\t<-ctx.Done()\n\t\tclose(requests)\n\t\twg.Wait()\n\t}()\n\n\treturn requests, &wg\n}\n\nfunc main() {\n\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n\tdefer cancel()\n\n\tworkerCount := 4\n\tbatchSize := 10\n\trequests, wg := BatchProcessor(ctx, workerCount, batchSize)\n\n\t// Simulate sending prediction requests\n\tfor i := 0; i < 20; i++ {\n\t\treq := PredictionRequest{\n\t\t\tID:       i,\n\t\t\tInput:    []float32{float32(i)},\n\t\t\tResponse: make(chan float32),\n\t\t}\n\t\trequests <- req\n\n\t\t// Handle response asynchronously\n\t\tgo func(req PredictionRequest) {\n\t\t\tresult := <-req.Response\n\t\t\tfmt.Printf(\"Request %d: Result = %f\\n\", req.ID, result)\n\t\t}(req)\n\t}\n\n\t// Wait for all workers to finish\n\twg.Wait()\n\tlog.Println(\"All predictions processed\")\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport asyncio\nimport logging\nfrom collections import deque\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Connection pool to manage active connections\nclass ConnectionPool:\n    def __init__(self, max_size):\n        self.max_size = max_size\n        self.pool = deque()\n\n    async def get_connection(self, host, port):\n        \"\"\"Get a connection from the pool or create a new one.\"\"\"\n        while self.pool:\n            reader, writer = self.pool.popleft()\n            if not writer.is_closing():\n                return reader, writer\n        return await asyncio.open_connection(host, port)\n\n    def release_connection(self, reader, writer):\n        \"\"\"Release a connection back to the pool.\"\"\"\n        if len(self.pool) < self.max_size and not writer.is_closing():\n            self.pool.append((reader, writer))\n        else:\n            writer.close()\n\n# TCP Proxy Server\nclass TCPProxyServer:\n    def __init__(self, listen_host, listen_port, target_host, target_port, pool_size=10):\n        self.listen_host = listen_host\n        self.listen_port = listen_port\n        self.target_host = target_host\n        self.target_port = target_port\n        self.pool = ConnectionPool(pool_size)\n\n    async def handle_client(self, client_reader, client_writer):\n        \"\"\"Handle incoming client connections.\"\"\"\n        try:\n            # Get a connection from the pool or create a new one\n            target_reader, target_writer = await self.pool.get_connection(self.target_host, self.target_port)\n            \n            # Forward data between client and target\n            await asyncio.gather(\n                self.forward_data(client_reader, target_writer),\n                self.forward_data(target_reader, client_writer)\n        except Exception as e:\n            logger.error(f\"Error handling client: {e}\")\n        finally:\n            # Release the connection back to the pool\n            self.pool.release_connection(target_reader, target_writer)\n            client_writer.close()\n            await client_writer.wait_closed()\n\n    async def forward_data(self, reader, writer):\n        \"\"\"Forward data from reader to writer.\"\"\"\n        try:\n            while not reader.at_eof():\n                data = await reader.read(4096)\n                if not data:\n                    break\n                writer.write(data)\n                await writer.drain()\n        except Exception as e:\n            logger.error(f\"Error forwarding data: {e}\")\n        finally:\n            writer.close()\n            await writer.wait_closed()\n\n    async def start(self):\n        \"\"\"Start the TCP proxy server.\"\"\"\n        server = await asyncio.start_server(\n            self.handle_client, self.listen_host, self.listen_port)\n        logger.info(f\"Proxy server started on {self.listen_host}:{self.listen_port}\")\n        async with server:\n            await server.serve_forever()\n\n# Example usage\nif __name__ == \"__main__\":\n    proxy = TCPProxyServer(\"127.0.0.1\", 8888, \"example.com\", 80)\n    asyncio.run(proxy.start())\n```",
    "label": 1
  },
  {
    "codes": "```zig\nconst std = @import(\"std\");\nconst nrf = @import(\"nrf52\");\n\n// Function to enter low-power sleep mode\nfn enterLowPowerSleep() void {\n    // Configure the system to enter low-power mode\n    nrf.POWER.TASKS_LOWPWR = 1;\n\n    // Wait for the system to enter low-power mode\n    while (nrf.POWER.EVENTS_LOWPWR == 0) {}\n\n    // Clear the event flag\n    nrf.POWER.EVENTS_LOWPWR = 0;\n\n    // Enter sleep mode\n    asm volatile (\"wfi\");\n}\n\n// Function to wake up from low-power sleep mode\nfn wakeUpFromSleep() void {\n    // Configure the system to wake up\n    nrf.POWER.TASKS_CONSTLAT = 1;\n\n    // Wait for the system to wake up\n    while (nrf.POWER.EVENTS_CONSTLAT == 0) {}\n\n    // Clear the event flag\n    nrf.POWER.EVENTS_CONSTLAT = 0;\n}\n\n// Main function\npub fn main() void {\n    // Enter low-power sleep mode\n    enterLowPowerSleep();\n\n    // Perform some operations after waking up\n    wakeUpFromSleep();\n\n    // Continue with normal operation\n    std.debug.print(\"System woke up from low-power sleep mode.\\n\", .{});\n}\n```\n\nThis code demonstrates how to manage low-power sleep modes on an nRF52 microcontroller using Zig. The `enterLowPowerSleep` function puts the system into a low-power state, while the `wakeUpFromSleep` function wakes it up. The main function demonstrates the usage of these functions.",
    "label": 1
  },
  {
    "codes": "```java\n// Import necessary packages\nimport static org.mockito.Mockito.*;\nimport org.junit.jupiter.api.Test;\nimport org.mockito.Mock;\nimport org.mockito.InjectMocks;\nimport org.mockito.junit.jupiter.MockitoExtension;\nimport org.junit.jupiter.api.extension.ExtendWith;\nimport javax.sql.DataSource;\nimport java.sql.Connection;\nimport java.sql.SQLException;\n\n@ExtendWith(MockitoExtension.class)\npublic class DatabaseServiceTest {\n\n    @Mock\n    private DataSource dataSource; // Mock the DataSource\n\n    @Mock\n    private Connection connection; // Mock the Connection\n\n    @InjectMocks\n    private DatabaseService databaseService; // Inject mocks into the service\n\n    @Test\n    public void testTransactionRollback() throws SQLException {\n        // Arrange\n        when(dataSource.getConnection()).thenReturn(connection); // Return mock connection\n        doThrow(new SQLException(\"Simulated DB error\")).when(connection).commit(); // Simulate commit failure\n\n        // Act\n        try {\n            databaseService.performTransaction(); // Call the method that performs the transaction\n        } catch (SQLException e) {\n            // Expected exception\n        }\n\n        // Assert\n        verify(connection, times(1)).rollback(); // Verify that rollback was called\n        verify(connection, never()).commit(); // Verify that commit was never called\n    }\n}\n\n// DatabaseService class (for reference)\nclass DatabaseService {\n    private DataSource dataSource;\n\n    public DatabaseService(DataSource dataSource) {\n        this.dataSource = dataSource;\n    }\n\n    public void performTransaction() throws SQLException {\n        Connection connection = dataSource.getConnection();\n        try {\n            connection.setAutoCommit(false);\n            // Perform database operations here\n            connection.commit(); // Commit the transaction\n        } catch (SQLException e) {\n            connection.rollback(); // Rollback on error\n            throw e;\n        } finally {\n            connection.close(); // Close the connection\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<List<JobResponseInner>> listJobsAsync(final String resourceGroupName, final String resourceName, final ListOperationCallback<JobResponseInner> serviceCallback) {\n        return AzureServiceFuture.fromPageResponse(\n            listJobsSinglePageAsync(resourceGroupName, resourceName),\n            new Func1<String, Observable<ServiceResponse<Page<JobResponseInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<JobResponseInner>>> call(String nextPageLink) {\n                    return listJobsNextSinglePageAsync(nextPageLink);\n                }\n            },\n            serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"github.com/json-iterator/go\"\n    \"testing\"\n)\n\n// Sample JSON data for parsing\nconst jsonData = `{\"name\":\"John\", \"age\":30, \"city\":\"New York\"}`\n\n// Struct to represent the parsed JSON\ntype Person struct {\n    Name string `json:\"name\"`\n    Age  int    `json:\"age\"`\n    City string `json:\"city\"`\n}\n\n// Benchmark function for the standard library JSON parsing\nfunc BenchmarkStdJSON(b *testing.B) {\n    for i := 0; i < b.N; i++ {\n        var person Person\n        if err := json.Unmarshal([]byte(jsonData), &person); err != nil {\n            b.Error(err)\n        }\n    }\n}\n\n// Benchmark function for the json-iterator library parsing\nfunc BenchmarkJsonIter(b *testing.B) {\n    for i := 0; i < b.N; i++ {\n        var person Person\n        if err := jsoniter.Unmarshal([]byte(jsonData), &person); err != nil {\n            b.Error(err)\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\n// Import D3.js library (ensure it's included in your HTML file)\n// <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n\n// Set up SVG dimensions\nconst width = 800;\nconst height = 400;\nconst margin = { top: 20, right: 30, bottom: 30, left: 40 };\n\n// Create SVG container\nconst svg = d3.select(\"body\")\n  .append(\"svg\")\n  .attr(\"width\", width)\n  .attr(\"height\", height);\n\n// Define scales\nconst xScale = d3.scaleTime().range([margin.left, width - margin.right]);\nconst yScale = d3.scaleLinear().range([height - margin.bottom, margin.top]);\n\n// Create line generator\nconst line = d3.line()\n  .x(d => xScale(d.timestamp))\n  .y(d => yScale(d.price));\n\n// Initialize data array\nlet data = [];\n\n// Function to fetch real-time cryptocurrency price\nasync function fetchPrice() {\n  const response = await fetch('https://api.coingecko.com/api/v3/simple/price?ids=bitcoin&vs_currencies=usd');\n  const result = await response.json();\n  const price = result.bitcoin.usd;\n  const timestamp = new Date();\n  return { timestamp, price };\n}\n\n// Function to update the chart\nfunction updateChart(newData) {\n  // Update scales\n  xScale.domain(d3.extent(newData, d => d.timestamp));\n  yScale.domain([0, d3.max(newData, d => d.price)]);\n\n  // Bind data to the line\n  svg.selectAll(\".price-line\")\n    .data([newData])\n    .join(\"path\")\n    .attr(\"class\", \"price-line\")\n    .attr(\"d\", line)\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"steelblue\")\n    .attr(\"stroke-width\", 2);\n\n  // Add x-axis\n  svg.selectAll(\".x-axis\")\n    .data([newData])\n    .join(\"g\")\n    .attr(\"class\", \"x-axis\")\n    .attr(\"transform\", `translate(0,${height - margin.bottom})`)\n    .call(d3.axisBottom(xScale));\n\n  // Add y-axis\n  svg.selectAll(\".y-axis\")\n    .data([newData])\n    .join(\"g\")\n    .attr(\"class\", \"y-axis\")\n    .attr(\"transform\", `translate(${margin.left},0)`)\n    .call(d3.axisLeft(yScale));\n}\n\n// Function to simulate real-time updates\nasync function simulateRealTime() {\n  const newPrice = await fetchPrice();\n  data.push(newPrice);\n\n  // Keep only the last 50 data points\n  if (data.length > 50) {\n    data.shift();\n  }\n\n  // Update the chart\n  updateChart(data);\n\n  // Schedule the next update\n  setTimeout(simulateRealTime, 1000);\n}\n\n// Start the real-time simulation\nsimulateRealTime();\n```",
    "label": 1
  },
  {
    "codes": "def send_message(self, body):\n        \"\"\"\n        Send a message to the conversation.\n\n        :param msg: The message to send.\n        :type msg: :class:`aioxmpp.Message`\n        :return: The stanza token obtained from sending.\n        :rtype: :class:`~aioxmpp.stream.StanzaToken`\n\n        The default implementation simply calls :meth:`send_message_tracked`\n        and immediately cancels the tracking object, returning only the stanza\n        token.\n\n        There is no need to provide proper address attributes on `msg`.\n        Implementations will override those attributes with the values\n        appropriate for the conversation. Some implementations may allow the\n        user to choose a :attr:`~aioxmpp.Message.type_`, but others may simply\n        stamp it over.\n\n        Subclasses may override this method with a more specialised\n        implementation. Subclasses which do not provide tracked message sending\n        **must** override this method to provide untracked message sending.\n\n        .. seealso::\n\n           The corresponding feature is\n           :attr:`.ConversationFeature.SEND_MESSAGE`. See :attr:`features` for\n           details.\n\n        \"\"\"\n        token, tracker = self.send_message_tracked(body)\n        tracker.cancel()\n        return token",
    "label": 0
  },
  {
    "codes": "```python\nimport xml.etree.ElementTree as ET\n\ndef large_xml_element_generator(file_path):\n    \"\"\"\n    A generator function to yield elements from a large XML file one at a time.\n\n    Parameters:\n    - file_path: str, path to the XML file.\n\n    Yields:\n    - An `ElementTree.Element` object for each XML element in the file.\n    \"\"\"\n    # Open the XML file for reading\n    context = ET.iterparse(file_path, events=('start', 'end'))\n    # This extracts the root element (useful for dealing with namespaces)\n    _, root = next(context)  \n\n    # Iterate over the XML events\n    for event, elem in context:\n        # Yield the element if it's an 'end' event\n        if event == 'end':\n            yield elem\n            # Clear the element to free memory\n            root.clear()\n```\n\nThis generator processes XML files using `xml.etree.ElementTree`, iterating with `iterparse`. It yields elements at the 'end' event and clears elements to manage memory usage efficiently.",
    "label": 1
  },
  {
    "codes": "```dart\nString formatCompactNumber(double number) {\n  // Define the suffixes for thousands, millions, etc.\n  const suffixes = [\"\", \"K\", \"M\", \"B\", \"T\"];\n\n  // Determine the appropriate suffix index\n  int suffixIndex = 0;\n  while (number >= 1000 && suffixIndex < suffixes.length - 1) {\n    number /= 1000;\n    suffixIndex++;\n  }\n\n  // Format the number to one decimal place and append the suffix\n  return '${number.toStringAsFixed(1)}${suffixes[suffixIndex]}';\n}\n\n// Example usage:\n// print(formatCompactNumber(1234567)); // Output: 1.2M\n```",
    "label": 1
  },
  {
    "codes": "def ping(client, peer):\n    \"\"\"\n    Ping a peer.\n\n    :param peer: The peer to ping.\n    :type peer: :class:`aioxmpp.JID`\n    :raises aioxmpp.errors.XMPPError: as received\n\n    Send a :xep:`199` ping IQ to `peer` and wait for the reply.\n\n    This is a low-level version of :meth:`aioxmpp.PingService.ping`.\n\n    **When to use this function vs. the service method:** See\n    :meth:`aioxmpp.PingService.ping`.\n\n    .. note::\n\n        If the peer does not support :xep:`199`, they will respond with\n        a ``cancel`` ``service-unavailable`` error. However, some\n        implementations return a ``cancel`` ``feature-not-implemented``\n        error instead. Callers should be prepared for the\n        :class:`aioxmpp.XMPPCancelError` exceptions in those cases.\n\n    .. versionchanged:: 0.11\n\n        Extracted this helper from :class:`aioxmpp.PingService`.\n    \"\"\"\n\n    iq = aioxmpp.IQ(\n        to=peer,\n        type_=aioxmpp.IQType.GET,\n        payload=ping_xso.Ping()\n    )\n\n    yield from client.send(iq)",
    "label": 0
  },
  {
    "codes": "public Observable<Void> getPropertiesFromComputeNodeAsync(String poolId, String nodeId, String filePath) {\n        return getPropertiesFromComputeNodeWithServiceResponseAsync(poolId, nodeId, filePath).map(new Func1<ServiceResponseWithHeaders<Void, FileGetPropertiesFromComputeNodeHeaders>, Void>() {\n            @Override\n            public Void call(ServiceResponseWithHeaders<Void, FileGetPropertiesFromComputeNodeHeaders> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "def _get_group_dataframes(self):\n        \"\"\"\n        Get group dataframes\n\n        Returns\n        -------\n        out : tuple or generator\n            Group dataframes\n        \"\"\"\n        if isinstance(self.data, GroupedDataFrame):\n            grouper = self.data.groupby()\n            # groupby on categorical columns uses the categories\n            # even if they are not present in the data. This\n            # leads to empty groups. We exclude them.\n            return (gdf for _, gdf in grouper if not gdf.empty)\n        else:\n            return (self.data, )",
    "label": 0
  },
  {
    "codes": "```python\nimport os\nfrom contextlib import contextmanager\n\n@contextmanager\ndef modified_env(**kwargs):\n    \"\"\"Temporarily modify environment variables.\"\"\"\n    original_env = {key: os.getenv(key) for key in kwargs}  # Save original values\n    try:\n        for key, value in kwargs.items():\n            if value is None:\n                os.environ.pop(key, None)  # Remove if value is None\n            else:\n                os.environ[key] = value  # Set new value\n        yield  # Allow the block to run with modified environment\n    finally:\n        for key, value in original_env.items():\n            if value is None:\n                os.environ.pop(key, None)  # Restore by removing if originally None\n            else:\n                os.environ[key] = value  # Restore original value\n```",
    "label": 1
  },
  {
    "codes": "def occlusion_heatmap(net, x, target, square_length=7):\n    \"\"\"An occlusion test that checks an image for its critical parts.\n\n    In this function, a square part of the image is occluded (i.e. set\n    to 0) and then the net is tested for its propensity to predict the\n    correct label. One should expect that this propensity shrinks of\n    critical parts of the image are occluded. If not, this indicates\n    overfitting.\n\n    Depending on the depth of the net and the size of the image, this\n    function may take awhile to finish, since one prediction for each\n    pixel of the image is made.\n\n    Currently, all color channels are occluded at the same time. Also,\n    this does not really work if images are randomly distorted by the\n    batch iterator.\n\n    See paper: Zeiler, Fergus 2013\n\n    Parameters\n    ----------\n    net : NeuralNet instance\n      The neural net to test.\n\n    x : np.array\n      The input data, should be of shape (1, c, x, y). Only makes\n      sense with image data.\n\n    target : int\n      The true value of the image. If the net makes several\n      predictions, say 10 classes, this indicates which one to look\n      at.\n\n    square_length : int (default=7)\n      The length of the side of the square that occludes the image.\n      Must be an odd number.\n\n    Results\n    -------\n    heat_array : np.array (with same size as image)\n      An 2D np.array that at each point (i, j) contains the predicted\n      probability of the correct class if the image is occluded by a\n      square with center (i, j).\n\n    \"\"\"\n    if (x.ndim != 4) or x.shape[0] != 1:\n        raise ValueError(\"This function requires the input data to be of \"\n                         \"shape (1, c, x, y), instead got {}\".format(x.shape))\n    if square_length % 2 == 0:\n        raise ValueError(\"Square length has to be an odd number, instead \"\n                         \"got {}.\".format(square_length))\n\n    num_classes = get_output_shape(net.layers_[-1])[1]\n    img = x[0].copy()\n    bs, col, s0, s1 = x.shape\n\n    heat_array = np.zeros((s0, s1))\n    pad = square_length // 2 + 1\n    x_occluded = np.zeros((s1, col, s0, s1), dtype=img.dtype)\n    probs = np.zeros((s0, s1, num_classes))\n\n    # generate occluded images\n    for i in range(s0):\n        # batch s1 occluded images for faster prediction\n        for j in range(s1):\n            x_pad = np.pad(img, ((0, 0), (pad, pad), (pad, pad)), 'constant')\n            x_pad[:, i:i + square_length, j:j + square_length] = 0.\n            x_occluded[j] = x_pad[:, pad:-pad, pad:-pad]\n        y_proba = net.predict_proba(x_occluded)\n        probs[i] = y_proba.reshape(s1, num_classes)\n\n    # from predicted probabilities, pick only those of target class\n    for i in range(s0):\n        for j in range(s1):\n            heat_array[i, j] = probs[i, j, target]\n    return heat_array",
    "label": 0
  },
  {
    "codes": "func (c *Glue) UpdateCrawler(input *UpdateCrawlerInput) (*UpdateCrawlerOutput, error) {\n\treq, out := c.UpdateCrawlerRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```go\nfunc RecoveryMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        defer func() {\n            if err := recover(); err != nil {\n                // Log the error (optional)\n                // log.Printf(\"Recovered from panic: %v\", err)\n\n                // Respond with a 500 Internal Server Error\n                http.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError)\n            }\n        }()\n        \n        // Call the next handler\n        next.ServeHTTP(w, r)\n    })\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <opencv2/opencv.hpp>\n\nint main() {\n    // Open video file\n    cv::VideoCapture cap(\"input_video.mp4\");\n    if (!cap.isOpened()) {\n        std::cerr << \"Error: Could not open video file.\" << std::endl;\n        return -1;\n    }\n\n    cv::Mat frame;\n    while (cap.read(frame)) {\n        // Resize frame to model input size (e.g., 224x224)\n        cv::resize(frame, frame, cv::Size(224, 224));\n\n        // Convert frame to grayscale (optional, depending on model requirements)\n        cv::cvtColor(frame, frame, cv::COLOR_BGR2GRAY);\n\n        // Normalize pixel values to [0, 1]\n        frame.convertTo(frame, CV_32F, 1.0 / 255.0);\n\n        // Apply Gaussian blur for noise reduction\n        cv::GaussianBlur(frame, frame, cv::Size(5, 5), 0);\n\n        // Perform histogram equalization for contrast enhancement\n        cv::equalizeHist(frame, frame);\n\n        // Display the preprocessed frame (optional)\n        cv::imshow(\"Preprocessed Frame\", frame);\n        if (cv::waitKey(30) >= 0) break;\n    }\n\n    cap.release();\n    cv::destroyAllWindows();\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "public IntegrationAccountSchemaInner createOrUpdate(String resourceGroupName, String integrationAccountName, String schemaName, IntegrationAccountSchemaInner schema) {\n        return createOrUpdateWithServiceResponseAsync(resourceGroupName, integrationAccountName, schemaName, schema).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<Page<InterfaceEndpointInner>> listByResourceGroupNextAsync(final String nextPageLink) {\n        return listByResourceGroupNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<InterfaceEndpointInner>>, Page<InterfaceEndpointInner>>() {\n                @Override\n                public Page<InterfaceEndpointInner> call(ServiceResponse<Page<InterfaceEndpointInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n)\n\n// handleConnection processes client requests\nfunc handleConnection(conn net.Conn, wg *sync.WaitGroup) {\n\tdefer wg.Done() // Signal that this goroutine is done\n\tdefer conn.Close() // Close the connection when done\n\n\treader := bufio.NewReader(conn)\n\tfor {\n\t\tmessage, err := reader.ReadString('\\n')\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Client disconnected:\", err)\n\t\t\treturn\n\t\t}\n\t\tfmt.Print(\"Received: \", string(message))\n\t\tconn.Write([]byte(\"Echo: \" + message)) // Echo back the message\n\t}\n}\n\nfunc main() {\n\tlistener, err := net.Listen(\"tcp\", \":8080\")\n\tif err != nil {\n\t\tfmt.Println(\"Error starting server:\", err)\n\t\treturn\n\t}\n\tdefer listener.Close() // Close the listener when main exits\n\n\tfmt.Println(\"Server started on :8080\")\n\n\tvar wg sync.WaitGroup // WaitGroup to manage goroutines\n\n\tfor {\n\t\tconn, err := listener.Accept()\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error accepting connection:\", err)\n\t\t\tcontinue\n\t\t}\n\t\twg.Add(1) // Increment WaitGroup counter\n\t\tgo handleConnection(conn, &wg) // Handle each connection in a new goroutine\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "function getDayPeriodRules(localeData) {\n  const dayPeriodRules = localeData.get(`supplemental/dayPeriodRuleSet/${localeData.attributes.language}`);\n  const rules = {};\n  if (dayPeriodRules) {\n    Object.keys(dayPeriodRules).forEach(key => {\n      if (dayPeriodRules[key]._at) {\n        rules[key] = dayPeriodRules[key]._at;\n      } else {\n        rules[key] = [dayPeriodRules[key]._from, dayPeriodRules[key]._before];\n      }\n    });\n  }\n\n  return rules;\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutionException;\n\npublic class CompletableFutureChain {\n    public static void main(String[] args) throws ExecutionException, InterruptedException {\n        CompletableFuture.supplyAsync(() -> {\n            // Simulate a task\n            if (Math.random() > 0.75) throw new RuntimeException(\"Task 1 failed\");\n            return \"Result of Task 1\";\n        }).thenApplyAsync(result1 -> {\n            // Next async task depending on result1\n            if (Math.random() > 0.75) throw new RuntimeException(\"Task 2 failed\");\n            return \"Result of Task 2 with \" + result1;\n        }).exceptionally(ex -> {\n            // Fallback action for any previous failure\n            return \"Fallback result due to: \" + ex.getMessage();\n        }).thenApplyAsync(result2 -> {\n            // Task following fallback if necessary\n            if (Math.random() > 0.75) throw new RuntimeException(\"Task 3 failed\");\n            return \"Result of Task 3 with \" + result2;\n        }).exceptionally(ex -> {\n            // Final fallback action\n            return \"Final fallback due to: \" + ex.getMessage();\n        }).thenAcceptAsync(finalResult -> {\n            // Consuming the result\n            System.out.println(finalResult);\n        }).get();  // Wait for completion\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```java\nimport org.apache.pdfbox.multipdf.PDFMergerUtility;\nimport org.apache.pdfbox.pdmodel.PDDocument;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.List;\n\npublic class PDFMergerUtil {\n\n    /**\n     * Merges multiple PDFs into a single PDF file.\n     *\n     * @param pdfFiles List of PDF files to be merged\n     * @param outputFile The resulting merged PDF file\n     * @throws IOException If there is an error reading the input or writing the output files\n     */\n    public static void mergePDFs(List<File> pdfFiles, File outputFile) throws IOException {\n        PDFMergerUtility pdfMerger = new PDFMergerUtility();\n        \n        // Append each file to the PDF merger utility\n        for (File pdfFile : pdfFiles) {\n            pdfMerger.addSource(pdfFile);\n        }\n        \n        // Set the destination file\n        pdfMerger.setDestinationFileName(outputFile.getAbsolutePath());\n        \n        // Merge the documents\n        pdfMerger.mergeDocuments(null);\n    }\n    \n    public static void main(String[] args) {\n        try {\n            // Example usage: Merge two PDF files\n            List<File> pdfFiles = List.of(\n                    new File(\"document1.pdf\"),\n                    new File(\"document2.pdf\")\n            );\n            File outputFile = new File(\"mergedDocument.pdf\");\n            \n            mergePDFs(pdfFiles, outputFile);\n            System.out.println(\"PDFs have been merged successfully!\");\n\n        } catch (IOException e) {\n            e.printStackTrace();\n            System.err.println(\"An error occurred during the PDF merge process.\");\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport time\nfrom functools import wraps\n\ndef measure_execution_time(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        # Record start time in nanoseconds\n        start_time = time.time_ns()\n        \n        # Execute the function\n        result = func(*args, **kwargs)\n        \n        # Record end time in nanoseconds\n        end_time = time.time_ns()\n        \n        # Calculate and print execution time\n        execution_time = end_time - start_time\n        print(f\"Function '{func.__name__}' executed in {execution_time} nanoseconds\")\n        \n        return result\n    return wrapper\n```\n\n```python\n# Example usage\n@measure_execution_time\ndef example_function():\n    time.sleep(1)  # Simulate a time-consuming task\n\nexample_function()\n```",
    "label": 1
  },
  {
    "codes": "```powershell\n# Import the necessary module\nImport-Module ActiveDirectory\n\n# Define a function to check for weak permissions\nFunction Get-WeakRegistryPermissions {\n    param (\n        [string]$RegistryRoot = \"HKEY_LOCAL_MACHINE\\SOFTWARE\"\n    )\n\n    # Get the subkeys of the registry path\n    $subKeys = Get-ChildItem -Path Registry::$RegistryRoot\n\n    foreach ($subKey in $subKeys) {\n        # Get the ACL (Access Control List) for each subkey\n        $acl = Get-Acl -Path $subKey.PSPath\n\n        foreach ($access in $acl.Access) {\n            # Identify entries with wide permissions\n            if ($access.FileSystemRights -match \"FullControl|Write\" -and $access.IsInherited -eq $false) {\n                [PSCustomObject]@{\n                    Key     = $subKey.PSChildName\n                    IdentityReference = $access.IdentityReference\n                    AccessControlType = $access.AccessControlType\n                    Rights  = $access.FileSystemRights\n                }\n            }\n        }\n    }\n}\n\n# Execute the function to audit weak permissions\nGet-WeakRegistryPermissions\n```\n",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define the directory containing log files\nLOG_DIR=\"/var/log\"\n\n# Define the archive directory\nARCHIVE_DIR=\"/var/log/archive\"\n\n# Create the archive directory if it doesn't exist\nmkdir -p \"$ARCHIVE_DIR\"\n\n# Find log files older than 30 days and archive them\nfind \"$LOG_DIR\" -type f -name \"*.log\" -mtime +30 -exec tar -czf \"$ARCHIVE_DIR/logs_$(date +%Y%m%d).tar.gz\" {} +\n\n# Optionally, delete the archived log files\n# find \"$LOG_DIR\" -type f -name \"*.log\" -mtime +30 -exec rm {} \\;\n\n# Print completion message\necho \"Log files older than 30 days have been archived.\"\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.ServiceProcess;\nusing System.Management;\nusing System.Threading;\n\npublic class USBMonitorService : ServiceBase\n{\n    private ManagementEventWatcher watcher;\n\n    protected override void OnStart(string[] args)\n    {\n        // Initialize the WMI event watcher to monitor USB device connections\n        var query = new WqlEventQuery(\"SELECT * FROM Win32_DeviceChangeEvent WHERE EventType = 2\");\n        watcher = new ManagementEventWatcher(query);\n        watcher.EventArrived += new EventArrivedEventHandler(USBDeviceConnected);\n        watcher.Start();\n    }\n\n    private void USBDeviceConnected(object sender, EventArrivedEventArgs e)\n    {\n        // Log or handle the USB device connection event\n        Log(\"USB device connected.\");\n    }\n\n    protected override void OnStop()\n    {\n        // Stop the WMI event watcher\n        watcher.Stop();\n        watcher.Dispose();\n    }\n\n    private void Log(string message)\n    {\n        // Log the message to a file or other logging mechanism\n        // Example: File.AppendAllText(\"log.txt\", $\"{DateTime.Now}: {message}\\n\");\n    }\n\n    public static void Main()\n    {\n        // Run the service\n        ServiceBase.Run(new USBMonitorService());\n    }\n}\n```\n\n```csharp\n// Install the service using the following command in the Developer Command Prompt:\n// sc create USBMonitorService binPath= \"path\\to\\your\\service.exe\"\n```\n\n```csharp\n// Uninstall the service using the following command:\n// sc delete USBMonitorService\n```",
    "label": 1
  },
  {
    "codes": "```dart\nString formatNumberCompact(int number) {\n  if (number < 1000) {\n    return number.toString(); // Return as is if less than 1000\n  } else if (number < 1000000) {\n    double result = number / 1000;\n    return '${result.toStringAsFixed(result.truncateToDouble() == result ? 0 : 1)}K'; // Format as K\n  } else if (number < 1000000000) {\n    double result = number / 1000000;\n    return '${result.toStringAsFixed(result.truncateToDouble() == result ? 0 : 1)}M'; // Format as M\n  } else {\n    double result = number / 1000000000;\n    return '${result.toStringAsFixed(result.truncateToDouble() == result ? 0 : 1)}B'; // Format as B\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "public CompletableFuture<RuleDescription> createRuleAsync(String topicName, String subscriptionName, RuleDescription ruleDescription) {\n        return putRuleAsync(topicName, subscriptionName, ruleDescription, false);\n    }",
    "label": 0
  },
  {
    "codes": "def filter(self, obj, *args, **kwargs):\n        \"\"\"\n        Filter the given object through the filter chain.\n\n        :param obj: The object to filter\n        :param args: Additional arguments to pass to each filter function.\n        :param kwargs: Additional keyword arguments to pass to each filter\n                       function.\n        :return: The filtered object or :data:`None`\n\n        See the documentation of :class:`Filter` on how filtering operates.\n\n        Returns the object returned by the last function in the filter chain or\n        :data:`None` if any function returned :data:`None`.\n        \"\"\"\n        for _, _, func in self._filter_order:\n            obj = func(obj, *args, **kwargs)\n            if obj is None:\n                return None\n        return obj",
    "label": 0
  },
  {
    "codes": "def get_client_info(self):\n        \"\"\"\n        A query is sent to the server to obtain the client's data stored at the\n        server.\n\n        :return: :class:`~aioxmpp.ibr.Query`\n        \"\"\"\n        iq = aioxmpp.IQ(\n            to=self.client.local_jid.bare().replace(localpart=None),\n            type_=aioxmpp.IQType.GET,\n            payload=xso.Query()\n        )\n\n        reply = (yield from self.client.send(iq))\n        return reply",
    "label": 0
  },
  {
    "codes": "```java\nimport java.util.regex.Pattern;\n\npublic class MarkdownToPlainText {\n\n    // Method to convert Markdown to plain text\n    public static String markdownToPlainText(String markdown) {\n        // Remove headers (e.g., #, ##, ###)\n        markdown = markdown.replaceAll(\"#+\\\\s*\", \"\");\n        \n        // Remove bold and italic markdown (e.g., **bold**, *italic*)\n        markdown = markdown.replaceAll(\"\\\\*\\\\*|\\\\*\", \"\");\n        \n        // Remove links (e.g., [text](url))\n        markdown = markdown.replaceAll(\"\\\\[([^\\\\]]+)\\\\]\\\\([^\\\\)]+\\\\)\", \"$1\");\n        \n        // Remove inline code (e.g., `code`)\n        markdown = markdown.replaceAll(\"`([^`]+)`\", \"$1\");\n        \n        // Remove blockquotes (e.g., > quote)\n        markdown = markdown.replaceAll(\"^>\\\\s*\", \"\");\n        \n        // Remove unordered list items (e.g., - item, * item)\n        markdown = markdown.replaceAll(\"^[-*+]\\\\s*\", \"\");\n        \n        // Remove ordered list items (e.g., 1. item)\n        markdown = markdown.replaceAll(\"^\\\\d+\\\\.\\\\s*\", \"\");\n        \n        // Remove horizontal rules (e.g., ---, ***)\n        markdown = markdown.replaceAll(\"^[-*]{3,}\\\\s*\", \"\");\n        \n        // Remove HTML tags if any\n        markdown = markdown.replaceAll(\"<[^>]+>\", \"\");\n        \n        // Trim leading and trailing whitespace\n        markdown = markdown.trim();\n        \n        return markdown;\n    }\n\n    // Example usage\n    public static void main(String[] args) {\n        String markdown = \"# Header\\n**Bold** and *italic* text with [link](http://example.com).\";\n        System.out.println(markdownToPlainText(markdown));\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```powershell\n# Module: ManageServiceDependencies\n\n# Exported functions\nExport-ModuleMember -Function Add-ServiceDependency, Get-ServiceDependencies, Remove-ServiceDependency\n\n# Function to add a dependency to a service\nfunction Add-ServiceDependency {\n    param (\n        [Parameter(Mandatory = $true)]\n        [string]$ServiceName,\n\n        [Parameter(Mandatory = $true)]\n        [string]$Dependency\n    )\n    \n    # Retrieve the service object\n    $service = Get-WmiObject -Class Win32_Service -Filter \"Name='$ServiceName'\"\n    \n    # Add the new dependency\n    $currentDeps = $service.DependentServices\n    $newDeps = @($currentDeps + $Dependency)\n    $service.Change($null, $null, $null, $null, [string[]]$newDeps, $null, $null, $null)\n}\n\n# Function to get dependencies of a service\nfunction Get-ServiceDependencies {\n    param (\n        [Parameter(Mandatory = $true)]\n        [string]$ServiceName\n    )\n\n    # Retrieve the service object\n    $service = Get-WmiObject -Class Win32_Service -Filter \"Name='$ServiceName'\"\n    \n    # Output the dependent services\n    return $service.DependentServices\n}\n\n# Function to remove a dependency from a service\nfunction Remove-ServiceDependency {\n    param (\n        [Parameter(Mandatory = $true)]\n        [string]$ServiceName,\n\n        [Parameter(Mandatory = $true)]\n        [string]$Dependency\n    )\n    \n    # Retrieve the service object\n    $service = Get-WmiObject -Class Win32_Service -Filter \"Name='$ServiceName'\"\n    \n    # Remove the specified dependency\n    $currentDeps = $service.DependentServices\n    $newDeps = $currentDeps | Where-Object { $_ -ne $Dependency }\n    $service.Change($null, $null, $null, $null, [string[]]$newDeps, $null, $null, $null)\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *PublicKey) SetValidityStartTime(v time.Time) *PublicKey {\n\ts.ValidityStartTime = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function isHostObject(value) {\n  // Many host objects are `Object` objects that can coerce to strings\n  // despite having improperly defined `toString` methods.\n  var result = false\n  if (value != null && typeof value.toString !== 'function') {\n    try {\n      result = !!(value + '')\n    } catch (e) {}\n  }\n  return result\n}",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <fstream>\n#include <sstream>\n#include <map>\n#include <vector>\n#include <algorithm>\n\n// Structure to hold INI data\nstruct INI {\n    std::map<std::string, std::map<std::string, std::string>> data;\n    std::map<std::string, std::vector<std::string>> inheritance;\n\n    // Function to parse INI file\n    void parse(const std::string& filename) {\n        std::ifstream file(filename);\n        std::string line, currentSection;\n        while (std::getline(file, line)) {\n            line.erase(std::remove_if(line.begin(), line.end(), ::isspace), line.end());\n            if (line.empty() || line[0] == ';' || line[0] == '#') continue;\n\n            if (line[0] == '[' && line.back() == ']') {\n                currentSection = line.substr(1, line.size() - 2);\n                size_t colonPos = currentSection.find(':');\n                if (colonPos != std::string::npos) {\n                    std::string parentSection = currentSection.substr(colonPos + 1);\n                    currentSection = currentSection.substr(0, colonPos);\n                    inheritance[currentSection].push_back(parentSection);\n                }\n            } else {\n                size_t equalsPos = line.find('=');\n                if (equalsPos != std::string::npos) {\n                    std::string key = line.substr(0, equalsPos);\n                    std::string value = line.substr(equalsPos + 1);\n                    data[currentSection][key] = value;\n                }\n            }\n        }\n    }\n\n    // Function to get value with inheritance\n    std::string get(const std::string& section, const std::string& key) {\n        if (data[section].count(key)) return data[section][key];\n        for (const auto& parent : inheritance[section]) {\n            std::string value = get(parent, key);\n            if (!value.empty()) return value;\n        }\n        return \"\";\n    }\n};\n\nint main() {\n    INI ini;\n    ini.parse(\"config.ini\");\n\n    std::cout << \"Database Host: \" << ini.get(\"database\", \"host\") << std::endl;\n    std::cout << \"Database Port: \" << ini.get(\"database\", \"port\") << std::endl;\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <opencv2/opencv.hpp>\n#include <iostream>\n\nint main() {\n    // Open video file\n    cv::VideoCapture cap(\"input_video.mp4\");\n    if (!cap.isOpened()) {\n        std::cerr << \"Error opening video file\" << std::endl;\n        return -1;\n    }\n\n    cv::Mat frame, resizedFrame, grayFrame, normalizedFrame;\n\n    while (cap.read(frame)) {\n        // Resize frame to model input size (e.g., 224x224)\n        cv::resize(frame, resizedFrame, cv::Size(224, 224));\n\n        // Convert to grayscale (if required by model)\n        cv::cvtColor(resizedFrame, grayFrame, cv::COLOR_BGR2GRAY);\n\n        // Normalize pixel values to [0, 1] (if required by model)\n        grayFrame.convertTo(normalizedFrame, CV_32F, 1.0 / 255.0);\n\n        // Optional: Add additional preprocessing steps (e.g., mean subtraction)\n        // cv::subtract(normalizedFrame, meanValues, normalizedFrame);\n\n        // Display processed frame (for debugging)\n        cv::imshow(\"Processed Frame\", normalizedFrame);\n        if (cv::waitKey(30) >= 0) break;\n    }\n\n    cap.release();\n    cv::destroyAllWindows();\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "public PagedList<StorageAccountItem> getStorageAccounts(final String vaultBaseUrl, final Integer maxresults) {\n        ServiceResponse<Page<StorageAccountItem>> response = getStorageAccountsSinglePageAsync(vaultBaseUrl, maxresults).toBlocking().single();\n        return new PagedList<StorageAccountItem>(response.body()) {\n            @Override\n            public Page<StorageAccountItem> nextPage(String nextPageLink) {\n                return getStorageAccountsNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "def user_follows(uid, offset='0', limit=30):\n    \"\"\"\u83b7\u53d6\u7528\u6237\u5173\u6ce8\u5217\u8868\n\n    :param uid: \u7528\u6237\u7684ID\uff0c\u53ef\u901a\u8fc7\u767b\u5f55\u6216\u8005\u5176\u4ed6\u63a5\u53e3\u83b7\u53d6\n    :param offset: (optional) \u5206\u6bb5\u8d77\u59cb\u4f4d\u7f6e\uff0c\u9ed8\u8ba4 0\n    :param limit: (optional) \u6570\u636e\u4e0a\u9650\u591a\u5c11\u884c\uff0c\u9ed8\u8ba4 30\n    \"\"\"\n    if uid is None:\n        raise ParamsError()\n    r = NCloudBot()\n    r.method = 'USER_FOLLOWS'\n    r.params = {'uid': uid}\n    r.data = {'offset': offset, 'limit': limit, 'order': True}\n    r.send()\n\n    return r.response",
    "label": 0
  },
  {
    "codes": "def rename_style(self, old_name, new_name):\n        \"\"\"\n        Rename a style, including references to it.\n\n        Arguments:\n            old_name (str): Style to be renamed.\n            new_name (str): New name for the style (must be unused).\n\n        Raises:\n            KeyError: No style named old_name.\n            ValueError: new_name is not a legal name (cannot use commas)\n                or new_name is taken.\n\n        \"\"\"\n        if old_name not in self.styles:\n            raise KeyError(\"Style %r not found\" % old_name)\n        if new_name in self.styles:\n            raise ValueError(\"There is already a style called %r\" % new_name)\n        if not is_valid_field_content(new_name):\n            raise ValueError(\"%r is not a valid name\" % new_name)\n\n        self.styles[new_name] = self.styles[old_name]\n        del self.styles[old_name]\n\n        for line in self:\n            # XXX also handle \\r override tag\n            if line.style == old_name:\n                line.style = new_name",
    "label": 0
  },
  {
    "codes": "def capture(cls, eval_env=0, reference=0):\n        \"\"\"Capture an execution environment from the stack.\n        If `eval_env` is already an :class:`EvalEnvironment`, it is returned\n        unchanged. Otherwise, we walk up the stack by ``eval_env + reference``\n        steps and capture that function's evaluation environment.\n        For ``eval_env=0`` and ``reference=0``, the default, this captures the\n        stack frame of the function that calls :meth:`capture`. If ``eval_env\n        + reference`` is 1, then we capture that function's caller, etc.\n        This somewhat complicated calling convention is designed to be\n        convenient for functions which want to capture their caller's\n        environment by default, but also allow explicit environments to be\n        specified. See the second example.\n        Example::\n          x = 1\n          this_env = EvalEnvironment.capture()\n          assert this_env.namespace[\"x\"] == 1\n          def child_func():\n              return EvalEnvironment.capture(1)\n          this_env_from_child = child_func()\n          assert this_env_from_child.namespace[\"x\"] == 1\n        Example::\n          # This function can be used like:\n          #   my_model(formula_like, data)\n          #     -> evaluates formula_like in caller's environment\n          #   my_model(formula_like, data, eval_env=1)\n          #     -> evaluates formula_like in caller's caller's environment\n          #   my_model(formula_like, data, eval_env=my_env)\n          #     -> evaluates formula_like in environment 'my_env'\n          def my_model(formula_like, data, eval_env=0):\n              eval_env = EvalEnvironment.capture(eval_env, reference=1)\n              return model_setup_helper(formula_like, data, eval_env)\n        This is how :func:`dmatrix` works.\n        .. versionadded: 0.2.0\n           The ``reference`` argument.\n        \"\"\"\n        if isinstance(eval_env, cls):\n            return eval_env\n        elif isinstance(eval_env, numbers.Integral):\n            depth = eval_env + reference\n        else:\n            raise TypeError(\"Parameter 'eval_env' must be either an integer \"\n                            \"or an instance of patsy.EvalEnvironment.\")\n        frame = inspect.currentframe()\n        try:\n            for i in range(depth + 1):\n                if frame is None:\n                    raise ValueError(\"call-stack is not that deep!\")\n                frame = frame.f_back\n            return cls([frame.f_locals, frame.f_globals],\n                       frame.f_code.co_flags & _ALL_FUTURE_FLAGS)\n        # The try/finally is important to avoid a potential reference cycle --\n        # any exception traceback will carry a reference to *our* frame, which\n        # contains a reference to our local variables, which would otherwise\n        # carry a reference to some parent frame, where the exception was\n        # caught...:\n        finally:\n            del frame",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponseWithHeaders<Page<NodeFile>, FileListFromTaskHeaders>> listFromTaskNextSinglePageAsync(final String nextPageLink, final FileListFromTaskNextOptions fileListFromTaskNextOptions) {\n        if (nextPageLink == null) {\n            throw new IllegalArgumentException(\"Parameter nextPageLink is required and cannot be null.\");\n        }\n        Validator.validate(fileListFromTaskNextOptions);\n        UUID clientRequestId = null;\n        if (fileListFromTaskNextOptions != null) {\n            clientRequestId = fileListFromTaskNextOptions.clientRequestId();\n        }\n        Boolean returnClientRequestId = null;\n        if (fileListFromTaskNextOptions != null) {\n            returnClientRequestId = fileListFromTaskNextOptions.returnClientRequestId();\n        }\n        DateTime ocpDate = null;\n        if (fileListFromTaskNextOptions != null) {\n            ocpDate = fileListFromTaskNextOptions.ocpDate();\n        }\n        DateTimeRfc1123 ocpDateConverted = null;\n        if (ocpDate != null) {\n            ocpDateConverted = new DateTimeRfc1123(ocpDate);\n        }\n        String nextUrl = String.format(\"%s\", nextPageLink);\n        return service.listFromTaskNext(nextUrl, this.client.acceptLanguage(), clientRequestId, returnClientRequestId, ocpDateConverted, this.client.userAgent())\n                .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponseWithHeaders<Page<NodeFile>, FileListFromTaskHeaders>>>() {\n                    @Override\n                    public Observable<ServiceResponseWithHeaders<Page<NodeFile>, FileListFromTaskHeaders>> call(Response<ResponseBody> response) {\n                        try {\n                            ServiceResponseWithHeaders<PageImpl<NodeFile>, FileListFromTaskHeaders> result = listFromTaskNextDelegate(response);\n                            return Observable.just(new ServiceResponseWithHeaders<Page<NodeFile>, FileListFromTaskHeaders>(result.body(), result.headers(), result.response()));\n                        } catch (Throwable t) {\n                            return Observable.error(t);\n                        }\n                    }\n                });\n    }",
    "label": 0
  },
  {
    "codes": "```python\n#!/usr/bin/env python3\n\nimport RPi.GPIO as GPIO\nimport time\nimport signal\nimport sys\n\n# Configure GPIO pin\nGPIO_PIN = 17\n\ndef signal_handler(sig, frame):\n    # Clean up GPIO and exit\n    GPIO.cleanup()\n    sys.exit(0)\n\ndef gpio_callback(channel):\n    # This function will be called on GPIO interrupt\n    print(f\"Interrupt detected on GPIO {channel}!\")\n\ndef main():\n    # Set up GPIO\n    GPIO.setmode(GPIO.BCM)\n    GPIO.setup(GPIO_PIN, GPIO.IN, pull_up_down=GPIO.PUD_DOWN)\n    \n    # Add event detect on GPIO pin\n    GPIO.add_event_detect(GPIO_PIN, GPIO.RISING, callback=gpio_callback)\n\n    # Set up signal handler for clean exit\n    signal.signal(signal.SIGINT, signal_handler)\n\n    # Daemon loop\n    try:\n        while True:\n            time.sleep(1)  # Sleep to reduce CPU usage\n    except Exception as e:\n        print(\"An error occurred:\", e)\n        GPIO.cleanup()\n\nif __name__ == \"__main__\":\n    main()\n```\n",
    "label": 1
  },
  {
    "codes": "public Observable<IotHubNameAvailabilityInfoInner> checkNameAvailabilityAsync(String name) {\n        return checkNameAvailabilityWithServiceResponseAsync(name).map(new Func1<ServiceResponse<IotHubNameAvailabilityInfoInner>, IotHubNameAvailabilityInfoInner>() {\n            @Override\n            public IotHubNameAvailabilityInfoInner call(ServiceResponse<IotHubNameAvailabilityInfoInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```python\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\nfrom werkzeug.utils import secure_filename\nfrom PIL import Image\nimport os\nimport uuid\nimport threading\n\napp = Flask(__name__)\nCORS(app)\n\n# Configuration for upload folder and allowed extensions\nUPLOAD_FOLDER = 'uploads'\nALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\n# Ensure the upload folder exists\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\n\ndef allowed_file(filename):\n    \"\"\"Check if the file has an allowed extension.\"\"\"\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef resize_image_async(file_path, output_path, size):\n    \"\"\"Resize the image asynchronously.\"\"\"\n    with Image.open(file_path) as img:\n        img.thumbnail(size)\n        img.save(output_path)\n\n@app.route('/upload', methods=['POST'])\ndef upload_image():\n    \"\"\"Endpoint to upload and resize an image asynchronously.\"\"\"\n    if 'file' not in request.files:\n        return jsonify({\"error\": \"No file part\"}), 400\n\n    file = request.files['file']\n\n    if file.filename == '':\n        return jsonify({\"error\": \"No selected file\"}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({\"error\": \"File type not allowed\"}), 400\n\n    # Generate a unique filename\n    filename = secure_filename(file.filename)\n    unique_filename = f\"{uuid.uuid4().hex}_{filename}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], unique_filename)\n    file.save(file_path)\n\n    # Define the output path for the resized image\n    resized_filename = f\"resized_{unique_filename}\"\n    resized_file_path = os.path.join(app.config['UPLOAD_FOLDER'], resized_filename)\n\n    # Resize the image asynchronously\n    threading.Thread(target=resize_image_async, args=(file_path, resized_file_path, (300, 300))).start()\n\n    return jsonify({\"message\": \"Image uploaded and is being resized\", \"filename\": unique_filename}), 202\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse reqwest::blocking::Client;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n\n// Define the structure for the token response\n#[derive(Serialize, Deserialize, Debug)]\nstruct TokenResponse {\n    access_token: String,\n    token_type: String,\n    expires_in: u64,\n    scope: Option<String>,\n}\n\n// Function to request an access token using client credentials flow\nfn get_access_token(client_id: &str, client_secret: &str, token_url: &str) -> Result<TokenResponse, reqwest::Error> {\n    let client = Client::new();\n\n    // Prepare the form data for the token request\n    let mut form_data = HashMap::new();\n    form_data.insert(\"grant_type\", \"client_credentials\");\n    form_data.insert(\"client_id\", client_id);\n    form_data.insert(\"client_secret\", client_secret);\n\n    // Send the POST request to the token endpoint\n    let response = client\n        .post(token_url)\n        .form(&form_data)\n        .send()?;\n\n    // Parse the JSON response into the TokenResponse struct\n    let token_response: TokenResponse = response.json()?;\n    Ok(token_response)\n}\n\nfn main() {\n    // Example usage\n    let client_id = \"your_client_id\";\n    let client_secret = \"your_client_secret\";\n    let token_url = \"https://example.com/oauth2/token\";\n\n    match get_access_token(client_id, client_secret, token_url) {\n        Ok(token) => println!(\"Access Token: {:?}\", token),\n        Err(e) => eprintln!(\"Error fetching token: {}\", e),\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Variables to hold the file and signature paths\nFILE_NAME=\"artifact.tar.gz\"\nSIGNATURE_FILE=\"${FILE_NAME}.asc\"\n\n# Import the public key associated with the signature.\n# This assumes the public key file is named \"publickey.asc\".\ngpg --import publickey.asc\n\n# Verify the PGP signature of the downloaded artifact.\ngpg --verify \"$SIGNATURE_FILE\" \"$FILE_NAME\"\n\n# Check the result of the verification process.\nif [ $? -eq 0 ]; then\n  echo \"The PGP signature is valid.\"\nelse\n  echo \"The PGP signature verification failed.\"\nfi\n```\n",
    "label": 1
  },
  {
    "codes": "func (s *GetTriggerOutput) SetTrigger(v *Trigger) *GetTriggerOutput {\n\ts.Trigger = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function softwareUpdate_getElement(spec) {\n    var elem = null;\n\n    switch(spec.type) {\n      /**\n       * subtype: subtype to match\n       * value: value to match\n       */\n      case \"button\":\n        elem = new elementslib.Lookup(this._controller.window.document,\n                                      WIZARD_BUTTONS_BOX + WIZARD_BUTTON[spec.subtype]);\n        break;\n      case \"wizard\":\n        elem = new elementslib.Lookup(this._controller.window.document, WIZARD);\n        break;\n      case \"wizard_page\":\n        elem = new elementslib.Lookup(this._controller.window.document, WIZARD_DECK +\n                                      '/id(' + spec.subtype + ')');\n        break;\n      case \"download_progress\":\n        elem = new elementslib.ID(this._controller.window.document, \"downloadProgress\");\n        break;\n      default:\n        throw new Error(arguments.callee.name + \": Unknown element type - \" + spec.type);\n    }\n\n    return elem;\n  }",
    "label": 0
  },
  {
    "codes": "```yaml\napiVersion: litmuschaos.io/v1alpha1\nkind: ChaosEngine\nmetadata:\n  name: pod-delete-chaos\n  namespace: default\nspec:\n  annotationCheck: 'false'\n  appinfo:\n    appns: 'default'            # Specify the application namespace\n    applabel: 'app=myapp'       # Specify the application label\n    appkind: 'deployment'       # Specify the application kind\n  chaosServiceAccount: pod-delete-sa\n  experiments:\n    - name: pod-delete\n      spec:\n        components:\n          env:\n            # Duration of the chaos experiment (in seconds)\n            - name: TOTAL_CHAOS_DURATION\n              value: '60'\n            # Time interval between pod deletions (in seconds)\n            - name: CHAOS_INTERVAL\n              value: '10'\n            # Number of pods to be affected by the chaos\n            - name: PODS_AFFECTED_PERC\n              value: '50'\n```\n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: pod-delete-sa\n  namespace: default\n```\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-delete-role\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"delete\"]\n```\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: pod-delete-rolebinding\n  namespace: default\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: pod-delete-role\nsubjects:\n- kind: ServiceAccount\n  name: pod-delete-sa\n  namespace: default\n```\n\n# This script is a LitmusChaos chaos experiment for deleting pods in a Kubernetes cluster. \n# Configure `TOTAL_CHAOS_DURATION`, `CHAOS_INTERVAL`, and `PODS_AFFECTED_PERC` to set the duration, interval, and percentage of pods affected.",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<Page<ResourceMetricInner>>> listMultiRoleMetricsSinglePageAsync(final String resourceGroupName, final String name) {\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (name == null) {\n            throw new IllegalArgumentException(\"Parameter name is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        final String startTime = null;\n        final String endTime = null;\n        final String timeGrain = null;\n        final Boolean details = null;\n        final String filter = null;\n        return service.listMultiRoleMetrics(resourceGroupName, name, this.client.subscriptionId(), startTime, endTime, timeGrain, details, filter, this.client.apiVersion(), this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<Page<ResourceMetricInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<ResourceMetricInner>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<PageImpl<ResourceMetricInner>> result = listMultiRoleMetricsDelegate(response);\n                        return Observable.just(new ServiceResponse<Page<ResourceMetricInner>>(result.body(), result.response()));\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```zig\nconst std = @import(\"std\");\nconst nrf = @import(\"nrf52\");\n\n// Function to enter low-power sleep mode\nfn enterLowPowerSleep() void {\n    // Configure the system to enter low-power mode\n    nrf.POWER.SYSTEMOFF = 1; // Enable SYSTEMOFF mode\n    nrf.POWER.TASKS_LOWPWR = 1; // Trigger low-power mode\n\n    // Wait for the system to enter sleep mode\n    while (nrf.POWER.SYSTEMOFF == 0) {\n        // Busy-wait until the system enters sleep mode\n    }\n}\n\n// Function to wake up from low-power sleep mode\nfn wakeUpFromSleep() void {\n    // Trigger a wake-up event\n    nrf.POWER.EVENTS_LOWPWR = 1; // Clear the low-power event\n    nrf.POWER.TASKS_CONSTLAT = 1; // Enable constant latency mode for faster wake-up\n\n    // Wait for the system to wake up\n    while (nrf.POWER.EVENTS_LOWPWR == 0) {\n        // Busy-wait until the system wakes up\n    }\n}\n\n// Main function to demonstrate sleep mode management\npub fn main() void {\n    std.debug.print(\"Entering low-power sleep mode...\\n\", .{});\n    enterLowPowerSleep();\n\n    std.debug.print(\"Waking up from low-power sleep mode...\\n\", .{});\n    wakeUpFromSleep();\n\n    std.debug.print(\"System is now awake.\\n\", .{});\n}\n```\n\nThis Zig program demonstrates how to manage low-power sleep modes on an nRF52 microcontroller. The `enterLowPowerSleep` function puts the system into a low-power state, while the `wakeUpFromSleep` function wakes it up. The main function demonstrates the usage of these functions.",
    "label": 1
  },
  {
    "codes": "```dart\nString formatRelativeTime(DateTime timestamp) {\n  final now = DateTime.now();\n  final difference = now.difference(timestamp);\n\n  if (difference.inDays > 7) {\n    return '${difference.inDays ~/ 7} weeks ago';\n  } else if (difference.inDays >= 1) {\n    return '${difference.inDays} days ago';\n  } else if (difference.inHours >= 1) {\n    return '${difference.inHours} hours ago';\n  } else if (difference.inMinutes >= 1) {\n    return '${difference.inMinutes} minutes ago';\n  } else {\n    return 'just now';\n  }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```python\nfrom machine import Pin, I2C\nimport time\n\n# Setup I2C for communication\ni2c = I2C(0, scl=Pin(22), sda=Pin(21), freq=100000)\n\n# I2C address of the temperature sensor (e.g., TMP102)\nsensor_address = 0x48\n\n# Register to read temperature\ntemp_reg = 0x00\n\ndef read_temperature():\n    # Request 2 bytes from the temperature register\n    raw_data = i2c.readfrom_mem(sensor_address, temp_reg, 2)\n    # Convert the data to temperature (TMP102 specific conversion)\n    temp_raw = (raw_data[0] << 4) | (raw_data[1] >> 4)\n    if temp_raw & (1 << 11):\n        temp_raw -= 1 << 12\n    temperature_celsius = temp_raw * 0.0625\n    return temperature_celsius\n\nwhile True:\n    # Read and print temperature data\n    temperature = read_temperature()\n    print('Temperature: {:.2f} C'.format(temperature))\n    time.sleep(2)\n```",
    "label": 1
  },
  {
    "codes": "```scala\nimport org.apache.kafka.clients.consumer.{KafkaConsumer, ConsumerRecords, ConsumerConfig}\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport java.util.Properties\nimport scala.collection.JavaConverters._\n\nobject KafkaEventStreamConsumer extends App {\n\n  // Configuration settings for the Kafka consumer\n  val config: Properties = {\n    val props = new Properties()\n    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\") // Kafka broker address\n    props.put(ConsumerConfig.GROUP_ID_CONFIG, \"example-group\") // Consumer group ID\n    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName) // Key deserializer\n    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName) // Value deserializer\n    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\") // Automatically reset offset to the earliest timestamp\n    props\n  }\n\n  // Initialize Kafka consumer with the specified configurations\n  val consumer = new KafkaConsumer[String, String](config)\n\n  // Subscribe to the desired Kafka topic\n  consumer.subscribe(List(\"example-topic\").asJava)\n\n  // Continuously poll for new data\n  try {\n    while (true) {\n      // Fetch records from Kafka\n      val records: ConsumerRecords[String, String] = consumer.poll(java.time.Duration.ofMillis(1000))\n      for (record <- records.asScala) {\n        // Process each record: print the key and value of the record\n        println(s\"Key: ${record.key()}, Value: ${record.value()}, Partition: ${record.partition()}, Offset: ${record.offset()}\")\n      }\n    }\n  } catch {\n    // Handle any exception that occurs during consuming\n    case e: Exception => e.printStackTrace()\n  } finally {\n    // Close the consumer to release resources when done\n    consumer.close()\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *GetTableVersionsInput) Validate() error {\n\tinvalidParams := request.ErrInvalidParams{Context: \"GetTableVersionsInput\"}\n\tif s.CatalogId != nil && len(*s.CatalogId) < 1 {\n\t\tinvalidParams.Add(request.NewErrParamMinLen(\"CatalogId\", 1))\n\t}\n\tif s.DatabaseName == nil {\n\t\tinvalidParams.Add(request.NewErrParamRequired(\"DatabaseName\"))\n\t}\n\tif s.DatabaseName != nil && len(*s.DatabaseName) < 1 {\n\t\tinvalidParams.Add(request.NewErrParamMinLen(\"DatabaseName\", 1))\n\t}\n\tif s.MaxResults != nil && *s.MaxResults < 1 {\n\t\tinvalidParams.Add(request.NewErrParamMinValue(\"MaxResults\", 1))\n\t}\n\tif s.TableName == nil {\n\t\tinvalidParams.Add(request.NewErrParamRequired(\"TableName\"))\n\t}\n\tif s.TableName != nil && len(*s.TableName) < 1 {\n\t\tinvalidParams.Add(request.NewErrParamMinLen(\"TableName\", 1))\n\t}\n\n\tif invalidParams.Len() > 0 {\n\t\treturn invalidParams\n\t}\n\treturn nil\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n    \"sync\"\n    \"time\"\n)\n\n// Simulating a prediction task\ntype PredictionTask struct {\n    ID   int\n    Data string\n}\n\n// Worker to process prediction tasks\ntype GPUWorker struct {\n    ID        int\n    TaskQueue <-chan PredictionTask\n}\n\nfunc (w *GPUWorker) Start(wg *sync.WaitGroup) {\n    defer wg.Done()\n    for task := range w.TaskQueue {\n        // Simulated process of task using GPU\n        fmt.Printf(\"Worker %d processing task ID %d\\n\", w.ID, task.ID)\n        time.Sleep(1 * time.Second) // Simulating work\n        fmt.Printf(\"Worker %d finished task ID %d\\n\", w.ID, task.ID)\n    }\n}\n\n// Service to manage prediction tasks\ntype PredictionService struct {\n    TaskQueue chan PredictionTask\n    Workers   []GPUWorker\n}\n\nfunc NewPredictionService(workerCount int, taskQueueSize int) *PredictionService {\n    taskQueue := make(chan PredictionTask, taskQueueSize)\n    workers := make([]GPUWorker, workerCount)\n    for i := range workers {\n        workers[i] = GPUWorker{\n            ID:        i,\n            TaskQueue: taskQueue,\n        }\n    }\n    return &PredictionService{\n        TaskQueue: taskQueue,\n        Workers:   workers,\n    }\n}\n\nfunc (s *PredictionService) Start(ctx context.Context) {\n    var wg sync.WaitGroup\n    for _, worker := range s.Workers {\n        wg.Add(1)\n        go worker.Start(&wg)\n    }\n\n    go func() {\n        <-ctx.Done() // Wait for the context to be canceled\n        close(s.TaskQueue)\n        wg.Wait()\n    }()\n}\n\nfunc (s *PredictionService) AddTask(task PredictionTask) {\n    s.TaskQueue <- task\n}\n\nfunc main() {\n    // Create a new prediction service with 3 workers\n    service := NewPredictionService(3, 10)\n    \n    // Web server to handle incoming prediction requests\n    http.HandleFunc(\"/predict\", func(w http.ResponseWriter, r *http.Request) {\n        // Simulate receiving a new task\n        newTask := PredictionTask{ID: time.Now().Nanosecond(), Data: \"Prediction Data\"}\n        service.AddTask(newTask)\n        fmt.Fprintln(w, \"Added task ID\", newTask.ID)\n    })\n\n    ctx, cancel := context.WithCancel(context.Background())\n    defer cancel()\n\n    // Start the prediction service\n    service.Start(ctx)\n\n    log.Println(\"Server started on :8080\")\n    if err := http.ListenAndServe(\":8080\", nil); err != nil {\n        log.Fatal(err)\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom transformers import pipeline\n\n# Initialize the summarization pipeline\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n# Example text to summarize\ntext = \"\"\"\nThe Hugging Face Transformers library provides a simple and efficient way to perform various NLP tasks, \nincluding text summarization. It supports a wide range of pre-trained models that can be easily fine-tuned \nfor specific tasks. The library is widely used in both research and industry due to its flexibility and ease of use.\n\"\"\"\n\n# Generate the summary\nsummary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n\n# Print the summary\nprint(summary[0]['summary_text'])\n```",
    "label": 1
  },
  {
    "codes": "function setTextStyleCommon(textStyle, textStyleModel, opt, isEmphasis) {\n    // Consider there will be abnormal when merge hover style to normal style if given default value.\n    opt = opt || EMPTY_OBJ;\n\n    if (opt.isRectText) {\n        var textPosition = textStyleModel.getShallow('position')\n            || (isEmphasis ? null : 'inside');\n        // 'outside' is not a valid zr textPostion value, but used\n        // in bar series, and magric type should be considered.\n        textPosition === 'outside' && (textPosition = 'top');\n        textStyle.textPosition = textPosition;\n        textStyle.textOffset = textStyleModel.getShallow('offset');\n        var labelRotate = textStyleModel.getShallow('rotate');\n        labelRotate != null && (labelRotate *= Math.PI / 180);\n        textStyle.textRotation = labelRotate;\n        textStyle.textDistance = zrUtil.retrieve2(\n            textStyleModel.getShallow('distance'), isEmphasis ? null : 5\n        );\n    }\n\n    var ecModel = textStyleModel.ecModel;\n    var globalTextStyle = ecModel && ecModel.option.textStyle;\n\n    // Consider case:\n    // {\n    //     data: [{\n    //         value: 12,\n    //         label: {\n    //             rich: {\n    //                 // no 'a' here but using parent 'a'.\n    //             }\n    //         }\n    //     }],\n    //     rich: {\n    //         a: { ... }\n    //     }\n    // }\n    var richItemNames = getRichItemNames(textStyleModel);\n    var richResult;\n    if (richItemNames) {\n        richResult = {};\n        for (var name in richItemNames) {\n            if (richItemNames.hasOwnProperty(name)) {\n                // Cascade is supported in rich.\n                var richTextStyle = textStyleModel.getModel(['rich', name]);\n                // In rich, never `disableBox`.\n                // FIXME: consider `label: {formatter: '{a|xx}', color: 'blue', rich: {a: {}}}`,\n                // the default color `'blue'` will not be adopted if no color declared in `rich`.\n                // That might confuses users. So probably we should put `textStyleModel` as the\n                // root ancestor of the `richTextStyle`. But that would be a break change.\n                setTokenTextStyle(richResult[name] = {}, richTextStyle, globalTextStyle, opt, isEmphasis);\n            }\n        }\n    }\n    textStyle.rich = richResult;\n\n    setTokenTextStyle(textStyle, textStyleModel, globalTextStyle, opt, isEmphasis, true);\n\n    if (opt.forceRich && !opt.textStyle) {\n        opt.textStyle = {};\n    }\n\n    return textStyle;\n}",
    "label": 0
  },
  {
    "codes": "function filterByThreshold(nodeModel, totalArea, sum, orderBy, orderedChildren) {\n\n    // visibleMin is not supported yet when no option.sort.\n    if (!orderBy) {\n        return sum;\n    }\n\n    var visibleMin = nodeModel.get('visibleMin');\n    var len = orderedChildren.length;\n    var deletePoint = len;\n\n    // Always travel from little value to big value.\n    for (var i = len - 1; i >= 0; i--) {\n        var value = orderedChildren[\n            orderBy === 'asc' ? len - i - 1 : i\n        ].getValue();\n\n        if (value / sum * totalArea < visibleMin) {\n            deletePoint = i;\n            sum -= value;\n        }\n    }\n\n    orderBy === 'asc'\n        ? orderedChildren.splice(0, len - deletePoint)\n        : orderedChildren.splice(deletePoint, len - deletePoint);\n\n    return sum;\n}",
    "label": 0
  },
  {
    "codes": "```dart\n// Import necessary Flutter and Dart packages\nimport 'dart:async';\nimport 'package:flutter/services.dart';\n\n// Define a channel to communicate with native code\nconst MethodChannel _channel = MethodChannel('ml_accelerator_plugin');\n\n// Class to interact with device-native ML accelerators\nclass MLAcceleratorPlugin {\n  // Method to check if ML accelerator is available\n  static Future<bool> isMLAcceleratorAvailable() async {\n    try {\n      final bool isAvailable = await _channel.invokeMethod('isMLAcceleratorAvailable');\n      return isAvailable;\n    } on PlatformException catch (e) {\n      print(\"Failed to check ML accelerator availability: ${e.message}\");\n      return false;\n    }\n  }\n\n  // Method to perform inference using the ML accelerator\n  static Future<List<double>> performInference(List<double> inputData) async {\n    try {\n      final List<dynamic> result = await _channel.invokeMethod('performInference', inputData);\n      return result.cast<double>();\n    } on PlatformException catch (e) {\n      print(\"Failed to perform inference: ${e.message}\");\n      return [];\n    }\n  }\n\n  // Method to load a model onto the ML accelerator\n  static Future<bool> loadModel(String modelPath) async {\n    try {\n      final bool isLoaded = await _channel.invokeMethod('loadModel', {'modelPath': modelPath});\n      return isLoaded;\n    } on PlatformException catch (e) {\n      print(\"Failed to load model: ${e.message}\");\n      return false;\n    }\n  }\n\n  // Method to release resources used by the ML accelerator\n  static Future<void> releaseResources() async {\n    try {\n      await _channel.invokeMethod('releaseResources');\n    } on PlatformException catch (e) {\n      print(\"Failed to release resources: ${e.message}\");\n    }\n  }\n}\n```\n\n```kotlin\n// Android Native Code (MainActivity.kt)\npackage com.example.mlacceleratorplugin\n\nimport android.content.Context\nimport android.os.Build\nimport io.flutter.embedding.android.FlutterActivity\nimport io.flutter.embedding.engine.FlutterEngine\nimport io.flutter.plugin.common.MethodChannel\n\nclass MainActivity: FlutterActivity() {\n    private val CHANNEL = \"ml_accelerator_plugin\"\n\n    override fun configureFlutterEngine(flutterEngine: FlutterEngine) {\n        super.configureFlutterEngine(flutterEngine)\n        MethodChannel(flutterEngine.dartExecutor.binaryMessenger, CHANNEL).setMethodCallHandler { call, result ->\n            when (call.method) {\n                \"isMLAcceleratorAvailable\" -> {\n                    result.success(isMLAcceleratorAvailable())\n                }\n                \"performInference\" -> {\n                    val inputData = call.arguments as List<Double>\n                    result.success(performInference(inputData))\n                }\n                \"loadModel\" -> {\n                    val modelPath = call.argument<String>(\"modelPath\")\n                    result.success(loadModel(modelPath))\n                }\n                \"releaseResources\" -> {\n                    releaseResources()\n                    result.success(null)\n                }\n                else -> {\n                    result.notImplemented()\n                }\n            }\n        }\n    }\n\n    private fun isMLAcceleratorAvailable(): Boolean {\n        return Build.VERSION.SDK_INT >= Build.VERSION_CODES.P\n    }\n\n    private fun performInference(inputData: List<Double>): List<Double> {\n        // Placeholder for actual inference logic\n        return inputData.map { it * 2.0 }\n    }\n\n    private fun loadModel(modelPath: String?): Boolean {\n        // Placeholder for actual model loading logic\n        return modelPath != null\n    }\n\n    private fun releaseResources() {\n        // Placeholder for resource release logic\n    }\n}\n```\n\n```swift\n// iOS Native Code (AppDelegate.swift)\nimport UIKit\nimport Flutter\n\n@UIApplicationMain\n@objc class AppDelegate: FlutterAppDelegate {\n    override func application(\n        _ application: UIApplication,\n        didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?\n    ) -> Bool {\n        let controller : FlutterViewController = window?.rootViewController as! FlutterViewController\n        let mlAcceleratorChannel = FlutterMethodChannel(name: \"ml_accelerator_plugin\",\n                                                      binaryMessenger: controller.binaryMessenger)\n        mlAcceleratorChannel.setMethodCallHandler({\n            (call: FlutterMethodCall, result: @escaping FlutterResult) -> Void in\n            switch call.method {\n            case \"isMLAcceleratorAvailable\":\n                result(self.isMLAcceleratorAvailable())\n            case \"performInference\":\n                if let inputData = call.arguments as? [Double] {\n                    result(self.performInference(inputData: inputData))\n                } else {\n                    result(FlutterError(code: \"INVALID_ARGUMENT\", message: \"Invalid input data\", details: nil))\n                }\n            case \"loadModel\":\n                if let modelPath = call.arguments as? String {\n                    result(self.loadModel(modelPath: modelPath))\n                } else {\n                    result(FlutterError(code: \"INVALID_ARGUMENT\", message: \"Invalid model path\", details: nil))\n                }\n            case \"releaseResources\":\n                self.releaseResources()\n                result(nil)\n            default:\n                result(FlutterMethodNotImplemented)\n            }\n        })\n\n        GeneratedPluginRegistrant.register(with: self)\n        return super.application(application, didFinishLaunchingWithOptions: launchOptions)\n    }\n\n    private func isMLAcceleratorAvailable() -> Bool {\n        return true // Placeholder for actual availability check\n    }\n\n    private func performInference(inputData: [Double]) -> [Double] {\n        return inputData.map { $0 * 2.0 } // Placeholder for actual inference logic\n    }\n\n    private func loadModel(modelPath: String) -> Bool {\n        return !modelPath.isEmpty // Placeholder for actual model loading logic\n    }\n\n    private func releaseResources() {\n        // Placeholder for resource release logic\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport hmac\nimport secrets\n\ndef secure_compare_hmac(digest1, digest2):\n    \"\"\"\n    Securely compare two HMAC digests in constant time to avoid timing attacks.\n    \n    :param digest1: First HMAC digest (bytes)\n    :param digest2: Second HMAC digest (bytes)\n    :return: True if digests are equal, False otherwise\n    \"\"\"\n    # Generate a random key for comparison\n    random_key = secrets.token_bytes(32)\n    \n    # Create HMAC objects for both digests using the random key\n    hmac1 = hmac.new(random_key, digest1, digestmod='sha256')\n    hmac2 = hmac.new(random_key, digest2, digestmod='sha256')\n    \n    # Compare the HMAC digests in constant time\n    return hmac.compare_digest(hmac1.digest(), hmac2.digest())\n```",
    "label": 1
  },
  {
    "codes": "```typescript\nimport { Injectable } from '@angular/core';\nimport { HttpClient, HttpParams } from '@angular/common/http';\nimport { Observable } from 'rxjs';\n\n@Injectable({\n  providedIn: 'root'\n})\nexport class PaginatedDataService {\n  private apiUrl = 'https://api.example.com/data'; // Replace with your API endpoint\n\n  constructor(private http: HttpClient) {}\n\n  // Fetch paginated data from the API\n  getPaginatedData(page: number, pageSize: number): Observable<any> {\n    // Set up query parameters for pagination\n    const params = new HttpParams()\n      .set('page', page.toString())\n      .set('pageSize', pageSize.toString());\n\n    // Make the HTTP GET request with the query parameters\n    return this.http.get<any>(this.apiUrl, { params });\n  }\n}\n```\n\n```typescript\n// Example usage in a component\nimport { Component, OnInit } from '@angular/core';\nimport { PaginatedDataService } from './paginated-data.service';\n\n@Component({\n  selector: 'app-data-list',\n  templateUrl: './data-list.component.html',\n  styleUrls: ['./data-list.component.css']\n})\nexport class DataListComponent implements OnInit {\n  data: any[] = [];\n  currentPage = 1;\n  pageSize = 10;\n\n  constructor(private paginatedDataService: PaginatedDataService) {}\n\n  ngOnInit(): void {\n    this.loadData();\n  }\n\n  loadData(): void {\n    this.paginatedDataService.getPaginatedData(this.currentPage, this.pageSize)\n      .subscribe(response => {\n        this.data = response.data; // Assuming the API returns data in a 'data' field\n      });\n  }\n\n  onPageChange(newPage: number): void {\n    this.currentPage = newPage;\n    this.loadData();\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "function showError({ title, description }) {\n  addons.getChannel().emit(Events.STORY_ERRORED, { title, description });\n  showErrorDisplay({\n    message: title,\n    stack: description,\n  });\n}",
    "label": 0
  },
  {
    "codes": "```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# Create a pipeline to handle missing data and feature scaling\npipeline = Pipeline([\n    # Step 1: Impute missing values using the mean strategy\n    ('imputer', SimpleImputer(strategy='mean')),\n    \n    # Step 2: Scale features to have zero mean and unit variance\n    ('scaler', StandardScaler())\n])\n\n# Example usage:\n# pipeline.fit(X_train)\n# X_train_transformed = pipeline.transform(X_train)\n# X_test_transformed = pipeline.transform(X_test)\n```",
    "label": 1
  },
  {
    "codes": "public Observable<KeyBundle> getKeyAsync(String vaultBaseUrl, String keyName, String keyVersion) {\n        return getKeyWithServiceResponseAsync(vaultBaseUrl, keyName, keyVersion).map(new Func1<ServiceResponse<KeyBundle>, KeyBundle>() {\n            @Override\n            public KeyBundle call(ServiceResponse<KeyBundle> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```java\n// Method to convert a decimal number to binary\npublic static String decimalToBinary(int decimal) {\n    // Handle edge case for 0\n    if (decimal == 0) {\n        return \"0\";\n    }\n\n    StringBuilder binary = new StringBuilder();\n    \n    // Convert decimal to binary by dividing by 2 and storing remainders\n    while (decimal > 0) {\n        int remainder = decimal % 2;\n        binary.insert(0, remainder); // Insert remainder at the beginning\n        decimal = decimal / 2;\n    }\n    \n    return binary.toString();\n}\n```",
    "label": 1
  },
  {
    "codes": "Certainly! Below are some pytest cases with different approaches to validate an API response schema, focusing on response codes:\n\n```python\nimport requests\nimport pytest\n\n@pytest.fixture\ndef api_response():\n    url = \"https://api.example.com/endpoint\"\n    response = requests.get(url)\n    return response\n\ndef test_status_code_200(api_response):\n    # Validate that the response status code is 200\n    assert api_response.status_code == 200\n\ndef test_json_content_type(api_response):\n    # Ensure that the Content-Type header is 'application/json'\n    assert api_response.headers['Content-Type'] == \"application/json\"\n\ndef test_key_presence(api_response):\n    # Check if a specific key is present in the JSON response\n    json_data = api_response.json()\n    assert 'expected_key' in json_data\n\ndef test_value_type(api_response):\n    # Ensure that a key's value is of the expected type\n    json_data = api_response.json()\n    assert isinstance(json_data.get('key_to_check'), dict)\n\ndef test_status_code_not_500(api_response):\n    # Validate that the response status code is not 500\n    assert api_response.status_code != 500\n```\n\nThese test cases cover various aspects of schema validation by checking response codes, content types, and specific data requirements.",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Collections.Concurrent;\nusing System.Threading;\nusing System.Threading.Tasks;\n\npublic class Program\n{\n    public static void Main()\n    {\n        // Create a cancellation token source\n        var cts = new CancellationTokenSource();\n        \n        // Create a collection of items to process\n        var items = new ConcurrentBag<int> { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\n\n        try\n        {\n            // Start a task to cancel after 2 seconds\n            Task.Run(() =>\n            {\n                Thread.Sleep(2000); // Simulate a delay\n                cts.Cancel(); // Cancel the operation\n            });\n\n            // Process items in parallel with cancellation support\n            Parallel.ForEach(items, new ParallelOptions { CancellationToken = cts.Token }, item =>\n            {\n                // Simulate work\n                Thread.Sleep(500);\n                Console.WriteLine($\"Processed item: {item}\");\n            });\n        }\n        catch (OperationCanceledException)\n        {\n            Console.WriteLine(\"Operation was canceled.\");\n        }\n    }\n}\n```\n\n### Explanation:\n- **CancellationTokenSource**: Used to signal cancellation.\n- **ConcurrentBag**: A thread-safe collection to hold items.\n- **Parallel.ForEach**: Processes items in parallel with cancellation support.\n- **OperationCanceledException**: Catches the exception thrown when the operation is canceled.",
    "label": 1
  },
  {
    "codes": "function(value) {\n\t\tvar t, r, b, l;\n\n\t\tif (helpers.isObject(value)) {\n\t\t\tt = +value.top || 0;\n\t\t\tr = +value.right || 0;\n\t\t\tb = +value.bottom || 0;\n\t\t\tl = +value.left || 0;\n\t\t} else {\n\t\t\tt = r = b = l = +value || 0;\n\t\t}\n\n\t\treturn {\n\t\t\ttop: t,\n\t\t\tright: r,\n\t\t\tbottom: b,\n\t\t\tleft: l,\n\t\t\theight: t + b,\n\t\t\twidth: l + r\n\t\t};\n\t}",
    "label": 0
  },
  {
    "codes": "func (s *GetTableOutput) SetTable(v *Table) *GetTableOutput {\n\ts.Table = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n)\n\n// Matrix represents a 2D matrix\ntype Matrix [][]int\n\n// Multiply two matrices\nfunc multiply(a, b Matrix) Matrix {\n\tn := len(a)\n\tresult := make(Matrix, n)\n\tfor i := range result {\n\t\tresult[i] = make([]int, n)\n\t\tfor j := range result[i] {\n\t\t\tfor k := range a {\n\t\t\t\tresult[i][j] += a[i][k] * b[k][j]\n\t\t\t}\n\t\t}\n\t}\n\treturn result\n}\n\n// Identity matrix of size n\nfunc identity(n int) Matrix {\n\tI := make(Matrix, n)\n\tfor i := range I {\n\t\tI[i] = make([]int, n)\n\t\tI[i][i] = 1\n\t}\n\treturn I\n}\n\n// Matrix exponentiation using fast exponentiation method\nfunc matrixExponentiation(mat Matrix, power int) Matrix {\n\tn := len(mat)\n\tresult := identity(n)\n\n\tfor power > 0 {\n\t\tif power%2 == 1 {\n\t\t\tresult = multiply(result, mat)\n\t\t}\n\t\tmat = multiply(mat, mat)\n\t\tpower /= 2\n\t}\n\treturn result\n}\n\nfunc main() {\n\tmat := Matrix{\n\t\t{1, 1},\n\t\t{1, 0},\n\t}\n\tpower := 5\n\tresult := matrixExponentiation(mat, power)\n\tfmt.Println(result)\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nfrom flask import Flask, request, jsonify\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom PIL import Image\nimport io\n\napp = Flask(__name__)\n\n# Load your pre-trained model\nmodel = keras.models.load_model('path_to_your_model.h5')\n\n# Function to preprocess the input image for the model\ndef preprocess_image(image):\n    image = image.resize((224, 224))  # Resize to match model's input shape\n    image_array = np.array(image) / 255.0  # Normalize the image\n    return np.expand_dims(image_array, axis=0)  # Add batch dimension\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file part'})\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No selected file'})\n\n    # Read the file and preprocess\n    image = Image.open(io.BytesIO(file.read()))\n    processed_image = preprocess_image(image)\n    \n    # Make prediction\n    predictions = model.predict(processed_image)\n    predicted_class = np.argmax(predictions, axis=1)\n\n    return jsonify({'predicted_class': int(predicted_class[0])})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```",
    "label": 1
  },
  {
    "codes": "function (segment, step) {\n      if (segment % step === 0) {\n        return segment;\n      }\n      if (Math.round((segment % step) / step)) {\n        return (segment + (step - segment % step)) % 60;\n      } else {\n        return segment - segment % step;\n      }\n    }",
    "label": 0
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Collections.Concurrent;\nusing System.Threading;\nusing System.Threading.Tasks;\n\nclass Program\n{\n    static void Main(string[] args)\n    {\n        // Create a cancellation token source\n        var cts = new CancellationTokenSource();\n\n        // Create a collection to process\n        var items = new ConcurrentBag<int> { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\n\n        // Start a task to cancel after 2 seconds\n        Task.Run(() =>\n        {\n            Thread.Sleep(2000); // Simulate a delay\n            cts.Cancel();       // Cancel the operation\n        });\n\n        try\n        {\n            // Use Parallel.ForEach with cancellation token support\n            Parallel.ForEach(items, new ParallelOptions { CancellationToken = cts.Token }, item =>\n            {\n                // Simulate work\n                Thread.Sleep(500);\n                Console.WriteLine($\"Processed item: {item}\");\n\n                // Check for cancellation\n                cts.Token.ThrowIfCancellationRequested();\n            });\n        }\n        catch (OperationCanceledException)\n        {\n            Console.WriteLine(\"Operation was canceled.\");\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "def bads_report(bads, path_prefix=None):\n    \"\"\" Return a nice report of bad architectures in `bads`\n\n    Parameters\n    ----------\n    bads : set\n        set of length 2 or 3 tuples. A length 2 tuple is of form\n        ``(depending_lib, missing_archs)`` meaning that an arch in\n        `require_archs` was missing from ``depending_lib``.  A length 3 tuple\n        is of form ``(depended_lib, depending_lib, missing_archs)`` where\n        ``depended_lib`` is the filename of the library depended on,\n        ``depending_lib`` is the library depending on ``depending_lib`` and\n        ``missing_archs`` is a set of missing architecture strings giving\n        architectures present in ``depending_lib`` and missing in\n        ``depended_lib``.  An empty set means all architectures were present as\n        required.\n    path_prefix : None or str, optional\n        Path prefix to strip from ``depended_lib`` and ``depending_lib``. None\n        means do not strip anything.\n\n    Returns\n    -------\n    report : str\n        A nice report for printing\n    \"\"\"\n    path_processor = ((lambda x : x) if path_prefix is None\n                      else get_rp_stripper(path_prefix))\n    reports = []\n    for result in bads:\n        if len(result) == 3:\n            depended_lib, depending_lib, missing_archs = result\n            reports.append(\"{0} needs {1} {2} missing from {3}\".format(\n                path_processor(depending_lib),\n                'archs' if len(missing_archs) > 1 else 'arch',\n                ', '.join(sorted(missing_archs)),\n                path_processor(depended_lib)))\n        elif len(result) == 2:\n            depending_lib, missing_archs = result\n            reports.append(\"Required {0} {1} missing from {2}\".format(\n                'archs' if len(missing_archs) > 1 else 'arch',\n                ', '.join(sorted(missing_archs)),\n                path_processor(depending_lib)))\n        else:\n            raise ValueError('Report tuple should be length 2 or 3')\n    return '\\n'.join(sorted(reports))",
    "label": 0
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Collections.Concurrent;\nusing System.Threading;\nusing System.Threading.Tasks;\n\nclass Program\n{\n    static void Main(string[] args)\n    {\n        // Create a cancellation token source\n        var cts = new CancellationTokenSource();\n\n        // Create a collection to process\n        var items = new ConcurrentBag<int> { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\n\n        try\n        {\n            // Start a task to cancel after 2 seconds\n            Task.Run(() =>\n            {\n                Thread.Sleep(2000);\n                cts.Cancel();\n                Console.WriteLine(\"Cancellation requested.\");\n            });\n\n            // Parallel.ForEach with cancellation token support\n            Parallel.ForEach(items, new ParallelOptions { CancellationToken = cts.Token }, item =>\n            {\n                // Simulate work\n                Thread.Sleep(500);\n                Console.WriteLine($\"Processed item: {item}\");\n\n                // Check for cancellation\n                cts.Token.ThrowIfCancellationRequested();\n            });\n        }\n        catch (OperationCanceledException)\n        {\n            Console.WriteLine(\"Operation was canceled.\");\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "Here's a sample C wrapper using OpenSSL for AES-GCM encryption and decryption:\n\n```c\n#include <openssl/evp.h>\n#include <stdlib.h>\n#include <string.h>\n#include <stdio.h>\n\n#define AES_GCM_KEY_SIZE 32 // 256-bit key\n#define AES_GCM_IV_SIZE 12  // 96-bit IV\n#define AES_GCM_TAG_SIZE 16 // 128-bit tag\n\ntypedef struct {\n    unsigned char *key;\n    unsigned char *iv;\n} AES_GCM_Params;\n\n// Initialize OpenSSL library\nvoid openssl_init() {\n    OpenSSL_add_all_algorithms();\n    ERR_load_crypto_strings();\n}\n\n// Clean up OpenSSL library\nvoid openssl_cleanup() {\n    EVP_cleanup();\n    ERR_free_strings();\n}\n\n// Encrypt data using AES-GCM\nint aes_gcm_encrypt(const unsigned char *plaintext, int plaintext_len, \n                    const unsigned char *aad, int aad_len, \n                    AES_GCM_Params *params,\n                    unsigned char *ciphertext, unsigned char *tag) {\n\n    EVP_CIPHER_CTX *ctx;\n    int len, ciphertext_len;\n\n    // Create and initialize the context\n    if(!(ctx = EVP_CIPHER_CTX_new())) return -1;\n\n    // Initialize encryption operation with AES-256-GCM\n    if(1 != EVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL))\n        return -1;\n    \n    // Set IV length\n    if(1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_IVLEN, AES_GCM_IV_SIZE, NULL))\n        return -1;\n\n    // Initialize key and IV\n    if(1 != EVP_EncryptInit_ex(ctx, NULL, NULL, params->key, params->iv))\n        return -1;\n\n    // Provide any AAD data\n    if(1 != EVP_EncryptUpdate(ctx, NULL, &len, aad, aad_len))\n        return -1;\n\n    // Encrypt the plaintext\n    if(1 != EVP_EncryptUpdate(ctx, ciphertext, &len, plaintext, plaintext_len))\n        return -1;\n    ciphertext_len = len;\n\n    // Finalize encryption\n    if(1 != EVP_EncryptFinal_ex(ctx, ciphertext + len, &len))\n        return -1;\n    ciphertext_len += len;\n\n    // Get the tag\n    if(1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_GET_TAG, AES_GCM_TAG_SIZE, tag))\n        return -1;\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    return ciphertext_len;\n}\n\n// Decrypt data using AES-GCM\nint aes_gcm_decrypt(const unsigned char *ciphertext, int ciphertext_len,\n                    const unsigned char *aad, int aad_len, \n                    const unsigned char *tag,\n                    AES_GCM_Params *params,\n                    unsigned char *plaintext) {\n\n    EVP_CIPHER_CTX *ctx;\n    int len, plaintext_len, ret;\n\n    // Create and initialize the context\n    if(!(ctx = EVP_CIPHER_CTX_new())) return -1;\n\n    // Initialize decryption operation with AES-256-GCM\n    if(!EVP_DecryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL))\n        return -1;\n    \n    // Set IV length\n    if(!EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_IVLEN, AES_GCM_IV_SIZE, NULL))\n        return -1;\n\n    // Initialize key and IV\n    if(!EVP_DecryptInit_ex(ctx, NULL, NULL, params->key, params->iv))\n        return -1;\n\n    // Provide any AAD data\n    if(!EVP_DecryptUpdate(ctx, NULL, &len, aad, aad_len))\n        return -1;\n\n    // Decrypt the ciphertext\n    if(!EVP_DecryptUpdate(ctx, plaintext, &len, ciphertext, ciphertext_len))\n        return -1;\n    plaintext_len = len;\n\n    // Set expected tag value\n    if(!EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_TAG, AES_GCM_TAG_SIZE, (void *)tag))\n        return -1;\n\n    // Finalize decryption\n    ret = EVP_DecryptFinal_ex(ctx, plaintext + len, &len);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    if(ret > 0) {\n        // Successful decryption\n        plaintext_len += len;\n        return plaintext_len;\n    } else {\n        // Verification failed\n        return -1;\n    }\n}\n```\n\nThis code snippet offers fundamental encryption and decryption functions using AES-GCM with OpenSSL. Additional error handling should be incorporated for production-level code. Ensure that OpenSSL is properly initialized before use and cleaned up afterward.",
    "label": 1
  },
  {
    "codes": "```powershell\n# Define the path and filter for security logs\n$logPath = \"Security\"\n$eventFilter = @{\n    LogName = $logPath\n    # Security event IDs commonly audited. Modify as needed.\n    Id = 4624, 4625, 4634, 4647, 4697, 4720, 4722, 4723, 4724, 4725, 4726, 4738, 4740, 4741, 4776\n}\n\n# Retrieve events from the Security log\n$securityEvents = Get-WinEvent -FilterHashtable $eventFilter -MaxEvents 1000 \n\n# Process and output results\nforeach ($event in $securityEvents) {\n    $eventDetails = @{\n        TimeCreated = $event.TimeCreated\n        ID = $event.Id\n        LevelDisplayName = $event.LevelDisplayName\n        Message = $event.Message\n    }\n    \n    # Output event details\n    Write-Output $eventDetails\n}\n\n# Optionally, export to CSV file for further analysis\n$outputPath = \"C:\\Audit\\SecurityEventsAudit.csv\"\n$securityEvents | Select-Object TimeCreated, Id, LevelDisplayName, Message | Export-Csv -Path $outputPath -NoTypeInformation\n\n# Comment: Modify the `$eventFilter` as needed to include or exclude specific event IDs.\n# Comment: Ensure the script runs with adequate permissions to access security logs.\n# Comment: Adjust `-MaxEvents` parameter as necessary based on log volume and needs.\n```",
    "label": 1
  },
  {
    "codes": "def publish(self, node, data, *, id_=None, access_model=None):\n        \"\"\"\n        Publish an item `data` in the PubSub node `node` on the\n        PEP service associated with the user's JID.\n\n        :param node: The PubSub node to publish to.\n        :param data: The item to publish.\n        :type data: An XSO representing the paylaod.\n        :param id_: The id the published item shall have.\n        :param access_model: The access model to enforce on the node. Defaults\n            to not enforcing any access model.\n        :returns: The PubSub id of the published item or\n            :data:`None` if it is unknown.\n        :raises RuntimeError: if PEP is not supported.\n        :raises RuntimeError: if `access_model` is set and `publish_options` is\n            not supported by the server\n\n        If no `id_` is given it is generated by the server (and may be\n        returned).\n\n        `access_model` defines a pre-condition on the access model used for the\n        `node`. The valid values depend on the service; commonly useful\n        ``\"presence\"`` (the default for PEP; allows access to anyone who can\n        receive the presence) and ``\"whitelist\"`` (allows access only to a\n        whitelist (which defaults to the own account only)).\n        \"\"\"\n        publish_options = None\n\n        def autocreate_publish_options():\n            nonlocal publish_options\n            if publish_options is None:\n                publish_options = aioxmpp.forms.Data(\n                    aioxmpp.forms.DataType.SUBMIT\n                )\n                publish_options.fields.append(\n                    aioxmpp.forms.Field(\n                        type_=aioxmpp.forms.FieldType.HIDDEN,\n                        var=\"FORM_TYPE\",\n                        values=[\n                            \"http://jabber.org/protocol/pubsub#publish-options\"\n                        ]\n                    )\n                )\n            return publish_options\n\n        if access_model is not None:\n            autocreate_publish_options()\n            publish_options.fields.append(aioxmpp.forms.Field(\n                var=\"pubsub#access_model\",\n                values=[access_model],\n            ))\n\n        yield from self._check_for_pep()\n        return (yield from self._pubsub.publish(\n            None, node, data, id_=id_,\n            publish_options=publish_options\n        ))",
    "label": 0
  },
  {
    "codes": "function (axisModel, ecModel, api, payload, force) {\n        updateAxisPointer(this, axisModel, ecModel, api, payload, false);\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport random\n\n# Define elements\nELEMENTS = ['Fire', 'Water', 'Earth', 'Air']\nWEAKNESSES = {\n    'Fire': 'Water',\n    'Water': 'Air',\n    'Earth': 'Fire',\n    'Air': 'Earth'\n}\n\n# Base class Character\nclass Character:\n    def __init__(self, name, element, health, strength):\n        self.name = name\n        self.element = element\n        self.health = health\n        self.strength = strength\n\n    def attack(self, target):\n        base_damage = random.randint(5, 10)  # Base random damage\n        # If the target's element is weak to the attacker's element, double damage\n        if WEAKNESSES[target.element] == self.element:\n            damage = base_damage + self.strength * 2\n            print(f\"{self.name}'s {self.element} attack is super effective against {target.name}'s {target.element}!\")\n        else:\n            damage = base_damage + self.strength\n        target.health -= damage\n        print(f\"{self.name} attacks {target.name} for {damage} damage.\")\n        return damage\n\n    def is_alive(self):\n        return self.health > 0\n\n    def __str__(self):\n        return f\"{self.name}({self.element}): {self.health} HP\"\n\n# Player and Enemy subclasses for now are identical, but you can customize later\nclass Player(Character):\n    pass\n\nclass Enemy(Character):\n    pass\n\ndef create_character(name):\n    element = random.choice(ELEMENTS)\n    return Character(name, element, random.randint(20, 30), random.randint(5, 7))\n\ndef battle(player, enemy):\n    print(f\"A wild {enemy.name} appears!\")\n\n    while player.is_alive() and enemy.is_alive():\n        print(\"\\nPlayer's turn:\")\n        player.attack(enemy)\n        print(enemy)\n\n        if not enemy.is_alive():\n            print(f\"{enemy.name} has been defeated!\")\n            break\n\n        print(\"\\nEnemy's turn:\")\n        enemy.attack(player)\n        print(player)\n\n        if not player.is_alive():\n            print(f\"{player.name} has been defeated!\")\n            break\n\n# Example of game play\nplayer = create_character(\"Hero\")\nenemy = create_character(\"Enemy Goblin\")\n\nprint(\"Battle Start!\")\nprint(player)\nprint(enemy)\n\nbattle(player, enemy)\n```\n\nThis Python code defines a simple text-based RPG combat system where characters have one of four elements, each with a defined weakness. Characters can attack each other, and if one character's element is the weakness of another's, they deal additional damage. The code allows for turn-based combat between characters until one is defeated.",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<List<SasDefinitionItem>> getSasDefinitionsAsync(final String vaultBaseUrl, final String storageAccountName, final Integer maxresults, final ListOperationCallback<SasDefinitionItem> serviceCallback) {\n        return AzureServiceFuture.fromPageResponse(\n            getSasDefinitionsSinglePageAsync(vaultBaseUrl, storageAccountName, maxresults),\n            new Func1<String, Observable<ServiceResponse<Page<SasDefinitionItem>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<SasDefinitionItem>>> call(String nextPageLink) {\n                    return getSasDefinitionsNextSinglePageAsync(nextPageLink);\n                }\n            },\n            serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport pandas as pd\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('your_data.csv')\n\n# Remove rows with any null values\ncleaned_df = df.dropna()\n\n# Save the cleaned DataFrame to a new CSV file\ncleaned_df.to_csv('cleaned_data.csv', index=False)\n```",
    "label": 1
  },
  {
    "codes": "def invoke_step(self, context):\n        \"\"\"Invoke 'run_step' in the dynamically loaded step module.\n\n        Don't invoke this from outside the Step class. Use\n        pypyr.dsl.Step.run_step instead.\n        invoke_step just does the bare module step invocation, it does not\n        evaluate any of the decorator logic surrounding the step. So unless\n        you really know what you're doing, use run_step if you intend on\n        executing the step the same way pypyr does.\n\n        Args:\n            context: (pypyr.context.Context) The pypyr context. This arg will\n                     mutate.\n        \"\"\"\n        logger.debug(\"starting\")\n\n        logger.debug(f\"running step {self.module}\")\n\n        self.run_step_function(context)\n\n        logger.debug(f\"step {self.module} done\")",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<CheckNameResultInner> checkNameAvailabilityAsync(String location, String name, final ServiceCallback<CheckNameResultInner> serviceCallback) {\n        return ServiceFuture.fromResponse(checkNameAvailabilityWithServiceResponseAsync(location, name), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "func (s *StorageDescriptor) SetLocation(v string) *StorageDescriptor {\n\ts.Location = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function parse_BLOB(blob) {\n\tvar size = blob.read_shift(4);\n\tvar bytes = blob.slice(blob.l,blob.l+size);\n\tblob.l += size;\n\tif((size & 3) > 0) blob.l += (4 - (size & 3)) & 3;\n\treturn bytes;\n}",
    "label": 0
  },
  {
    "codes": "```typescript\nimport { Injectable } from '@angular/core';\nimport { HttpClient, HttpParams } from '@angular/common/http';\nimport { Observable } from 'rxjs';\n\n@Injectable({\n  providedIn: 'root'\n})\nexport class DataService {\n  private apiUrl = 'https://api.example.com/data'; // Replace with your API endpoint\n\n  constructor(private http: HttpClient) {}\n\n  // Fetch paginated data from the API\n  getPaginatedData(page: number, pageSize: number): Observable<any> {\n    // Set up query parameters for pagination\n    const params = new HttpParams()\n      .set('page', page.toString())\n      .set('pageSize', pageSize.toString());\n\n    // Make the HTTP GET request with the pagination parameters\n    return this.http.get<any>(this.apiUrl, { params });\n  }\n}\n```\n\n```typescript\n// Example usage in a component\nimport { Component, OnInit } from '@angular/core';\nimport { DataService } from './data.service';\n\n@Component({\n  selector: 'app-data-list',\n  templateUrl: './data-list.component.html',\n  styleUrls: ['./data-list.component.css']\n})\nexport class DataListComponent implements OnInit {\n  data: any[] = [];\n  currentPage = 1;\n  pageSize = 10;\n\n  constructor(private dataService: DataService) {}\n\n  ngOnInit(): void {\n    this.loadData();\n  }\n\n  loadData(): void {\n    this.dataService.getPaginatedData(this.currentPage, this.pageSize).subscribe(\n      (response) => {\n        this.data = response; // Assuming the response contains the data array\n      },\n      (error) => {\n        console.error('Error fetching data:', error);\n      }\n    );\n  }\n\n  onPageChange(newPage: number): void {\n    this.currentPage = newPage;\n    this.loadData();\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "def load_data(self, datatype='tvc', tag=None, measure=''):\n        \"\"\"\n        Function loads time-varying connectivity estimates created by the TenetoBIDS.derive function.\n        The default grabs all data (in numpy arrays) in the teneto/../func/tvc/ directory.\n        Data is placed in teneto.tvc_data_\n\n        Parameters\n        ----------\n\n        datatype : str\n            \\'tvc\\', \\'parcellation\\', \\'participant\\', \\'temporalnetwork\\'\n\n        tag : str or list\n            any additional tag that must be in file name. After the tag there must either be a underscore or period (following bids).\n\n        measure : str\n            retquired when datatype is temporalnetwork. A networkmeasure that should be loaded.\n\n        Returns\n        -------\n\n        tvc_data_ : numpy array\n            Containing the parcellation data. Each file is appended to the first dimension of the numpy array.\n        tvc_trialinfo_ : pandas data frame\n            Containing the subject info (all BIDS tags) in the numpy array.\n        \"\"\"\n\n        if datatype == 'temporalnetwork' and not measure:\n            raise ValueError(\n                'When datatype is temporalnetwork, \\'measure\\' must also be specified.')\n\n        self.add_history(inspect.stack()[0][3], locals(), 1)\n        data_list = []\n        trialinfo_list = []\n\n        for s in self.bids_tags['sub']:\n            # Define base folder\n            base_path, file_list, datainfo = self._get_filelist(\n                datatype, s, tag, measure=measure)\n            if base_path:\n                for f in file_list:\n                    # Include only if all analysis step tags are present\n                    # Get all BIDS tags. i.e. in 'sub-AAA', get 'sub' as key and 'AAA' as item.\n                    # Ignore if tsv file is empty\n                    try:\n                        filetags = get_bids_tag(f, 'all')\n                        data_list.append(load_tabular_file(base_path + f))\n                        # Only return trialinfo if datatype is trlinfo\n                        if datainfo == 'trlinfo':\n                            trialinfo_list.append(\n                                pd.DataFrame(filetags, index=[0]))\n                    except pd.errors.EmptyDataError:\n                        pass\n        # If group data and length of output is one, don't make it a list\n        if datatype == 'group' and len(data_list) == 1:\n            data_list = data_list[0]\n        if measure:\n            data_list = {measure: data_list}\n        setattr(self, datatype + '_data_', data_list)\n        if trialinfo_list:\n            out_trialinfo = pd.concat(trialinfo_list)\n            out_trialinfo.reset_index(inplace=True, drop=True)\n            setattr(self, datatype + '_trialinfo_', out_trialinfo)",
    "label": 0
  },
  {
    "codes": "public void beginDelete(String resourceGroupName, String name, Boolean forceDelete) {\n        beginDeleteWithServiceResponseAsync(resourceGroupName, name, forceDelete).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "def msg_intro(self):\n        \"\"\"Introductory message disabled above heading.\"\"\"\n        delim = self.style.attr_minor(self.style.delimiter)\n        txt = self.intro_msg_fmt.format(delim=delim).rstrip()\n        return self.term.center(txt)",
    "label": 0
  },
  {
    "codes": "```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"CustomerClustering\") \\\n    .getOrCreate()\n\n# Load customer data\ndata = spark.read.csv(\"path/to/customer_data.csv\", header=True, inferSchema=True)\n\n# Select features and assemble into a single vector\nassembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"features\")\nassembled_data = assembler.transform(data)\n\n# Set up KMeans clustering\nkmeans = KMeans().setK(5).setSeed(42).setFeaturesCol(\"features\")\n\n# Fit the model\nmodel = kmeans.fit(assembled_data)\n\n# Make predictions\npredictions = model.transform(assembled_data)\n\n# Evaluate clustering by computing Within Set Sum of Squared Errors (WSSSE)\nevaluator = ClusteringEvaluator()\nwssse = evaluator.evaluate(predictions)\n\n# Show WSSSE\nprint(f\"Within Set Sum of Squared Errors = {wssse}\")\n\n# Show result\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)\n\n# Stop the Spark session\nspark.stop()\n```\n",
    "label": 1
  },
  {
    "codes": "public Observable<Page<PolicySetDefinitionInner>> listBuiltInNextAsync(final String nextPageLink) {\n        return listBuiltInNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<PolicySetDefinitionInner>>, Page<PolicySetDefinitionInner>>() {\n                @Override\n                public Page<PolicySetDefinitionInner> call(ServiceResponse<Page<PolicySetDefinitionInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```javascript\nfunction mergeSortedArrays(arr1, arr2) {\n    let merged = [];\n    let i = 0, j = 0;\n\n    // Traverse both arrays and push the smaller element to the merged array\n    while (i < arr1.length && j < arr2.length) {\n        if (arr1[i] < arr2[j]) {\n            merged.push(arr1[i]);\n            i++;\n        } else {\n            merged.push(arr2[j]);\n            j++;\n        }\n    }\n\n    // If there are remaining elements in arr1, add them to merged\n    while (i < arr1.length) {\n        merged.push(arr1[i]);\n        i++;\n    }\n\n    // If there are remaining elements in arr2, add them to merged\n    while (j < arr2.length) {\n        merged.push(arr2[j]);\n        j++;\n    }\n\n    return merged;\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nconst fs = require('fs');\nconst csv = require('csv-parser');\nconst { Transform } = require('stream');\n\n// Create a transform stream to convert CSV to NDJSON\nconst csvToNdjson = new Transform({\n  writableObjectMode: true,\n  readableObjectMode: true,\n  transform(chunk, encoding, callback) {\n    // Convert each CSV row (object) into a JSON string and add a newline\n    this.push(JSON.stringify(chunk) + '\\n');\n    callback();\n  },\n});\n\n// Set up the input and output file streams\nconst inputFilePath = 'input.csv';\nconst outputFilePath = 'output.ndjson';\n\nconst readStream = fs.createReadStream(inputFilePath);\nconst writeStream = fs.createWriteStream(outputFilePath);\n\n// Set up the pipeline\nreadStream\n  .pipe(csv())            // Parse CSV into JSON objects\n  .pipe(csvToNdjson)      // Convert JSON objects to NDJSON format\n  .pipe(writeStream)      // Write NDJSON to the output file\n  .on('finish', () => {\n    console.log('Conversion complete');\n  })\n  .on('error', (err) => {\n    console.error('Error during conversion', err);\n  });\n\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"CustomerClustering\") \\\n    .getOrCreate()\n\n# Load customer data\ncustomer_data = spark.read.csv(\"path/to/customer_data.csv\", header=True, inferSchema=True)\n\n# Select relevant features for clustering\nfeature_columns = [\"age\", \"annual_income\", \"spending_score\"]\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\ndata_with_features = assembler.transform(customer_data)\n\n# Standardize the features\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\nscaler_model = scaler.fit(data_with_features)\nscaled_data = scaler_model.transform(data_with_features)\n\n# Train KMeans clustering model\nkmeans = KMeans(k=5, seed=42, featuresCol=\"scaled_features\", predictionCol=\"cluster\")\nmodel = kmeans.fit(scaled_data)\n\n# Get cluster predictions\nclustered_data = model.transform(scaled_data)\n\n# Show the clustered data\nclustered_data.select(\"customer_id\", \"cluster\").show()\n\n# Stop the Spark session\nspark.stop()\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport javafx.application.Application;\nimport javafx.scene.Scene;\nimport javafx.scene.canvas.Canvas;\nimport javafx.scene.canvas.GraphicsContext;\nimport javafx.scene.layout.StackPane;\nimport javafx.scene.paint.Color;\nimport javafx.stage.Stage;\n\npublic class FractalApp extends Application {\n\n    @Override\n    public void start(Stage primaryStage) {\n        // Create a canvas for drawing the fractal\n        Canvas canvas = new Canvas(800, 600);\n        GraphicsContext gc = canvas.getGraphicsContext2D();\n\n        // Clear the canvas with a white background\n        gc.setFill(Color.WHITE);\n        gc.fillRect(0, 0, 800, 600);\n\n        // Set the initial color for the fractal\n        gc.setStroke(Color.BLACK);\n\n        // Draw a simple fractal pattern (e.g., Sierpinski Triangle)\n        drawSierpinskiTriangle(gc, 400, 50, 700, 550, 100, 550, 5);\n\n        // Set up the scene and stage\n        StackPane root = new StackPane(canvas);\n        Scene scene = new Scene(root, 800, 600);\n        primaryStage.setTitle(\"Interactive Fractal Patterns\");\n        primaryStage.setScene(scene);\n        primaryStage.show();\n    }\n\n    // Recursive function to draw the Sierpinski Triangle\n    private void drawSierpinskiTriangle(GraphicsContext gc, double x1, double y1, double x2, double y2, double x3, double y3, int depth) {\n        if (depth == 0) {\n            // Draw the triangle\n            gc.strokeLine(x1, y1, x2, y2);\n            gc.strokeLine(x2, y2, x3, y3);\n            gc.strokeLine(x3, y3, x1, y1);\n        } else {\n            // Calculate midpoints\n            double midX1 = (x1 + x2) / 2;\n            double midY1 = (y1 + y2) / 2;\n            double midX2 = (x2 + x3) / 2;\n            double midY2 = (y2 + y3) / 2;\n            double midX3 = (x3 + x1) / 2;\n            double midY3 = (y3 + y1) / 2;\n\n            // Recursively draw smaller triangles\n            drawSierpinskiTriangle(gc, x1, y1, midX1, midY1, midX3, midY3, depth - 1);\n            drawSierpinskiTriangle(gc, midX1, midY1, x2, y2, midX2, midY2, depth - 1);\n            drawSierpinskiTriangle(gc, midX3, midY3, midX2, midY2, x3, y3, depth - 1);\n        }\n    }\n\n    public static void main(String[] args) {\n        launch(args);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```swift\nimport Security\n\nfunc storeKeyInKeychain(account: String, keyData: Data) -> Bool {\n    // Define the keychain query\n    let query: [String: Any] = [\n        kSecClass as String: kSecClassGenericPassword,\n        kSecAttrAccount as String: account,\n        kSecValueData as String: keyData,\n        kSecAttrAccessible as String: kSecAttrAccessibleWhenUnlocked\n    ]\n    \n    // Delete any existing items\n    SecItemDelete(query as CFDictionary)\n    \n    // Add the new key to the keychain\n    let status = SecItemAdd(query as CFDictionary, nil)\n    \n    // Return whether the operation was successful\n    return status == errSecSuccess\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/rwcarlsen/goexif/exif\"\n)\n\n// Function to open and parse the JPEG file to extract EXIF metadata\nfunc main() {\n\tif len(os.Args) != 2 {\n\t\tfmt.Println(\"Usage: go run main.go <image-path>\")\n\t\treturn\n\t}\n\n\tfilePath := os.Args[1]\n\n\t// Open the image file\n\tf, err := os.Open(filePath)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer f.Close()\n\n\t// Decode EXIF metadata from the image\n\tx, err := exif.Decode(f)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Walk through all the tags and print them\n\terr = x.Walk(walkerFunc)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n\n// walkerFunc is used to print out the EXIF tags\nvar walkerFunc exif.WalkerFunc = func(name exif.FieldName, tag *exif.Tag) error {\n\tfmt.Printf(\"%s: %s\\n\", name, tag.String())\n\treturn nil\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```javascript\n// pages/index.js\nimport { PrismaClient } from '@prisma/client';\n\n// Initialize Prisma Client\nconst prisma = new PrismaClient();\n\n// Fetch data from PostgreSQL database\nexport async function getServerSideProps() {\n  const data = await prisma.your_table_name.findMany(); // Replace 'your_table_name' with your actual table name\n  return {\n    props: {\n      data,\n    },\n  };\n}\n\n// Render the data on the page\nexport default function Home({ data }) {\n  return (\n    <div>\n      <h1>Server-Side Rendered Data</h1>\n      <ul>\n        {data.map((item) => (\n          <li key={item.id}>{item.column_name}</li> // Replace 'column_name' with your actual column name\n        ))}\n      </ul>\n    </div>\n  );\n}\n```\n\n```javascript\n// prisma/schema.prisma\n// Define your database schema\ngenerator client {\n  provider = \"prisma-client-js\"\n}\n\ndatasource db {\n  provider = \"postgresql\"\n  url      = env(\"DATABASE_URL\") // Ensure DATABASE_URL is set in your .env file\n}\n\nmodel your_table_name {\n  id        Int      @id @default(autoincrement())\n  column_name String\n  // Add other columns as needed\n}\n```\n\n```bash\n# .env\n# Add your PostgreSQL connection string\nDATABASE_URL=\"postgresql://user:password@localhost:5432/mydb?schema=public\"\n```\n\n```bash\n# Install Prisma and dependencies\nnpm install @prisma/client\nnpx prisma generate\n```\n\n```bash\n# Run the Next.js app\nnpm run dev\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"crypto/rand\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"strings\"\n)\n\n// generatePassword generates a secure random password of a given length.\nfunc generatePassword(length int) (string, error) {\n\t// Define the character set for the password\n\tconst charset = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*()_+\"\n\n\t// Calculate the entropy of the password\n\tentropy := float64(length) * (math.Log2(float64(len(charset))))\n\n\t// Check if the entropy is sufficient (e.g., >= 80 bits)\n\tif entropy < 80 {\n\t\treturn \"\", fmt.Errorf(\"password entropy too low: %f bits\", entropy)\n\t}\n\n\t// Generate the password\n\tpassword := make([]byte, length)\n\tfor i := 0; i < length; i++ {\n\t\tnum, err := rand.Int(rand.Reader, big.NewInt(int64(len(charset))))\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tpassword[i] = charset[num.Int64()]\n\t}\n\n\treturn string(password), nil\n}\n\nfunc main() {\n\t// Generate a password of length 16\n\tpassword, err := generatePassword(16)\n\tif err != nil {\n\t\tfmt.Println(\"Error generating password:\", err)\n\t\treturn\n\t}\n\n\tfmt.Println(\"Generated Password:\", password)\n}\n```\n\n```go\n// Alternative implementation using base64 encoding for higher entropy\nfunc generateBase64Password(length int) (string, error) {\n\t// Calculate the required byte length for base64 encoding\n\tbyteLength := (length * 6) / 8\n\n\t// Generate random bytes\n\trandomBytes := make([]byte, byteLength)\n\t_, err := rand.Read(randomBytes)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Encode the bytes to base64\n\tpassword := base64.URLEncoding.EncodeToString(randomBytes)\n\n\t// Trim the password to the desired length\n\tpassword = password[:length]\n\n\treturn password, nil\n}\n\nfunc main() {\n\t// Generate a base64 encoded password of length 16\n\tpassword, err := generateBase64Password(16)\n\tif err != nil {\n\t\tfmt.Println(\"Error generating password:\", err)\n\t\treturn\n\t}\n\n\tfmt.Println(\"Generated Base64 Password:\", password)\n}\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define source and destination directories\nSRC_DIR=\"/path/to/source\"\nDEST_DIR=\"/path/to/destination\"\n\n# Check if both directories exist\nif [ ! -d \"$SRC_DIR\" ]; then\n    echo \"Source directory does not exist.\"\n    exit 1\nfi\n\nif [ ! -d \"$DEST_DIR\" ]; then\n    echo \"Destination directory does not exist.\"\n    exit 1\nfi\n\n# Compare directories using rsync in dry-run mode\necho \"Comparing directories...\"\nrsync -avun --delete \"$SRC_DIR/\" \"$DEST_DIR/\"\n\n# Prompt user to confirm sync\nread -p \"Do you want to sync changes? (y/n): \" confirm\n\n# Sync changes if confirmed\nif [ \"$confirm\" == \"y\" ]; then\n    echo \"Syncing directories...\"\n    rsync -avu --delete \"$SRC_DIR/\" \"$DEST_DIR/\"\n    echo \"Sync completed.\"\nelse\n    echo \"Sync aborted.\"\nfi\n```",
    "label": 1
  },
  {
    "codes": "```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# Create a pipeline\npipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),  # Step 1: Handle missing data with mean imputation\n    ('scaler', StandardScaler())  # Step 2: Feature scaling with StandardScaler\n])\n```",
    "label": 1
  },
  {
    "codes": "```typescript\nfunction Retry(retries: number, delay: number = 1000) {\n    return function (target: any, propertyKey: string, descriptor: PropertyDescriptor) {\n        const originalMethod = descriptor.value;\n\n        descriptor.value = async function (...args: any[]) {\n            for (let attempt = 0; attempt < retries; attempt++) {\n                try {\n                    return await originalMethod.apply(this, args);\n                } catch (error) {\n                    if (attempt < retries - 1) {\n                        console.log(`Attempt ${attempt + 1} failed. Retrying...`);\n                        await new Promise(resolve => setTimeout(resolve, delay));\n                    } else {\n                        throw error; // Rethrow the last error after all attempts\n                    }\n                }\n            }\n        };\n    };\n}\n\n// Example usage:\nclass Example {\n    @Retry(3, 500)\n    async riskyMethod() {\n        // Simulate a method that may fail\n        if (Math.random() < 0.7) {\n            throw new Error(\"Failed!\");\n        }\n        return \"Success!\";\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```ruby\n# A basic Fiber-based scheduler for lightweight concurrency in Ruby\nclass FiberScheduler\n  def initialize\n    @queue = [] # Queue to store fibers\n  end\n\n  # Method to run the scheduler until all fibers are done\n  def run\n    while fiber = @queue.shift\n      # Resume the fiber execution\n      fiber.resume\n    end\n  end\n\n  # Create a new fiber and add it to the queue\n  def schedule(&block)\n    fiber = Fiber.new do\n      block.call\n      Fiber.yield # Yield the fiber when the block finishes execution\n    end\n    @queue << fiber\n  end\n\n  # Example task method to simulate asynchronous tasks\n  def async_task(&block)\n    schedule(&block)\n  end\nend\n\n# Example usage of the FiberScheduler\nscheduler = FiberScheduler.new\n\n# Scheduling a couple of tasks\nscheduler.async_task do\n  puts \"Task 1 started\"\n  sleep 1 # Simulate a blocking task\n  puts \"Task 1 completed\"\nend\n\nscheduler.async_task do\n  puts \"Task 2 started\"\n  sleep 1 # Simulate a blocking task\n  puts \"Task 2 completed\"\nend\n\n# Run all scheduled tasks\nscheduler.run\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<List<DeletedCertificateItem>> getDeletedCertificatesNextAsync(final String nextPageLink, final ServiceFuture<List<DeletedCertificateItem>> serviceFuture, final ListOperationCallback<DeletedCertificateItem> serviceCallback) {\n        return AzureServiceFuture.fromPageResponse(\n            getDeletedCertificatesNextSinglePageAsync(nextPageLink),\n            new Func1<String, Observable<ServiceResponse<Page<DeletedCertificateItem>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<DeletedCertificateItem>>> call(String nextPageLink) {\n                    return getDeletedCertificatesNextSinglePageAsync(nextPageLink);\n                }\n            },\n            serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"net\"\n\t\"os\"\n\t\"time\"\n)\n\nfunc main() {\n\t// Seed random number generator\n\trand.Seed(time.Now().UnixNano())\n\n\t// Listen for incoming TCP connections\n\tlistener, err := net.Listen(\"tcp\", \":8080\")\n\tif err != nil {\n\t\tfmt.Println(\"Error listening:\", err.Error())\n\t\tos.Exit(1)\n\t}\n\tdefer listener.Close()\n\tfmt.Println(\"Listening on :8080\")\n\n\tfor {\n\t\t// Accept connection from client\n\t\tconn, err := listener.Accept()\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error accepting:\", err.Error())\n\t\t\tcontinue\n\t\t}\n\n\t\t// Handle connection in a new goroutine\n\t\tgo handleConnection(conn)\n\t}\n}\n\nfunc handleConnection(conn net.Conn) {\n\tdefer conn.Close()\n\n\tbuffer := make([]byte, 1024)\n\tfor {\n\t\t// Read from connection\n\t\tbytesRead, err := conn.Read(buffer)\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error reading:\", err.Error())\n\t\t\treturn\n\t\t}\n\n\t\tif shouldDropPacket() {\n\t\t\t// Simulate packet loss by not sending a response\n\t\t\tfmt.Println(\"Dropping packet\")\n\t\t\tcontinue\n\t\t}\n\n\t\t// Otherwise, send a response back to the client\n\t\t_, err = conn.Write(buffer[:bytesRead])\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error writing:\", err.Error())\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc shouldDropPacket() bool {\n\t// Simulate packet loss with a 30% probability\n\treturn rand.Float32() < 0.3\n}\n```",
    "label": 1
  },
  {
    "codes": "function parse_Window2(blob, length, opts) {\n\tif(opts && opts.biff >= 2 && opts.biff < 8) return {};\n\tvar f = blob.read_shift(2);\n\treturn { RTL: f & 0x40 };\n}",
    "label": 0
  },
  {
    "codes": "public PagedList<KeyItem> getKeys(final String vaultBaseUrl) {\n        ServiceResponse<Page<KeyItem>> response = getKeysSinglePageAsync(vaultBaseUrl).toBlocking().single();\n        return new PagedList<KeyItem>(response.body()) {\n            @Override\n            public Page<KeyItem> nextPage(String nextPageLink) {\n                return getKeysNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "```c\n#include <openssl/evp.h>\n#include <openssl/err.h>\n#include <string.h>\n#include <stdio.h>\n\n// Function to handle errors\nvoid handleErrors(void) {\n    ERR_print_errors_fp(stderr);\n    abort();\n}\n\n// Encryption function\nint aes_gcm_encrypt(const unsigned char *plaintext, int plaintext_len,\n                    const unsigned char *aad, int aad_len,\n                    const unsigned char *key,\n                    const unsigned char *iv, int iv_len,\n                    unsigned char *ciphertext,\n                    unsigned char *tag) {\n\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int ciphertext_len;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) handleErrors();\n\n    // Initialize the encryption operation.\n    if (EVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL) != 1)\n        handleErrors();\n\n    // Set IV length, only if different from default 96 bits\n    if (EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_IVLEN, iv_len, NULL) != 1)\n        handleErrors();\n\n    // Initialize key and IV\n    if (EVP_EncryptInit_ex(ctx, NULL, NULL, key, iv) != 1)\n        handleErrors();\n\n    // Provide AAD data if any\n    if (EVP_EncryptUpdate(ctx, NULL, &len, aad, aad_len) != 1)\n        handleErrors();\n\n    // Provide the message to be encrypted, and obtain the encrypted output\n    if (EVP_EncryptUpdate(ctx, ciphertext, &len, plaintext, plaintext_len) != 1)\n        handleErrors();\n\n    ciphertext_len = len;\n\n    // Finalize Encryption\n    if (EVP_EncryptFinal_ex(ctx, ciphertext + len, &len) != 1) handleErrors();\n    ciphertext_len += len;\n\n    // Get the tag\n    if (EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_GET_TAG, 16, tag) != 1)\n        handleErrors();\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n    return ciphertext_len;\n}\n\n// Decryption function\nint aes_gcm_decrypt(const unsigned char *ciphertext, int ciphertext_len,\n                    const unsigned char *aad, int aad_len,\n                    const unsigned char *tag,\n                    const unsigned char *key,\n                    const unsigned char *iv, int iv_len,\n                    unsigned char *plaintext) {\n\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int plaintext_len;\n    int ret;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) handleErrors();\n\n    // Initialize the decryption operation\n    if (EVP_DecryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL) != 1)\n        handleErrors();\n\n    // Set IV length, only if different from default 96 bits\n    if (EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_IVLEN, iv_len, NULL) != 1)\n        handleErrors();\n\n    // Initialize key and IV\n    if (EVP_DecryptInit_ex(ctx, NULL, NULL, key, iv) != 1)\n        handleErrors();\n\n    // Provide AAD data if any\n    if (EVP_DecryptUpdate(ctx, NULL, &len, aad, aad_len) != 1)\n        handleErrors();\n\n    // Provide the message to be decrypted, and obtain the plaintext\n    if (EVP_DecryptUpdate(ctx, plaintext, &len, ciphertext, ciphertext_len) != 1)\n        handleErrors();\n\n    plaintext_len = len;\n\n    // Set expected tag value\n    if (EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_TAG, 16, (void *)tag) != 1)\n        handleErrors();\n\n    // Finalize the decryption, check tag\n    ret = EVP_DecryptFinal_ex(ctx, plaintext + len, &len);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    if (ret > 0) {\n        // Successful decryption\n        plaintext_len += len;\n        return plaintext_len;\n    } else {\n        // Verification failed\n        return -1;\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Chaos Engineering Experiment: Random Pod Deletion in Kubernetes\n\n# Step 1: Get a list of all running pods in the default namespace\nPODS=$(kubectl get pods -o jsonpath='{.items[*].metadata.name}')\n\n# Step 2: Check if there are any pods running\nif [ -z \"$PODS\" ]; then\n  echo \"No pods found in the default namespace.\"\n  exit 1\nfi\n\n# Step 3: Select a random pod from the list\nRANDOM_POD=$(echo $PODS | tr ' ' '\\n' | shuf -n 1)\n\n# Step 4: Delete the selected pod\necho \"Deleting pod: $RANDOM_POD\"\nkubectl delete pod $RANDOM_POD\n\n# Step 5: Monitor the system's response (optional)\n# You can add monitoring commands here to observe the system's behavior post-deletion\n\n# Step 6: Log the experiment details\necho \"Chaos experiment completed. Deleted pod: $RANDOM_POD at $(date)\" >> chaos_experiment.log\n\n# Step 7: Exit with success status\nexit 0\n```",
    "label": 1
  },
  {
    "codes": "```labview\n// LabVIEW VI structure using VISA blocks\n\n// Initialize VISA session\nVISA Open.vi\n    // Input: VISA resource name (e.g., USB0::0x1AB1::0x0588::DS1ZA194700000::INSTR)\n    // Output: VISA session, error out\n\n// Set timeout\nVISA Set Timeout.vi\n    // Input: VISA session, timeout value (e.g., 2000 ms)\n    // Output: error out\n\n// Write SCPI command to configure oscilloscope\nVISA Write.vi\n    // Inputs: VISA session, SCPI command string (e.g., \":TIMEBASE:SCALE 0.001\")\n    // Outputs: bytes written, error out\n\n// Query oscilloscope for data\nVISA Write.vi\n    // Inputs: VISA session, SCPI query string (e.g., \":WAV:DATA?\")\n    // Outputs: bytes written, error out\n\nVISA Read.vi\n    // Inputs: VISA session\n    // Outputs: read buffer (waveform data), return count, error out\n\n// Process received data (convert binary data to waveform)\n\n// Close the VISA session\nVISA Close.vi\n    // Input: VISA session\n    // Output: error out\n\n// Handle and display any errors\nSimple Error Handler.vi\n    // Inputs: error in\n    // Outputs: error out\n\n// End of LabVIEW VI script\n```",
    "label": 1
  },
  {
    "codes": "```julia\nusing VoronoiDelaunay\nusing Plots\n\n# Number of random points\nn_points = 100\n\n# Generate random points\nx = rand(n_points)\ny = rand(n_points)\n\n# Create a Voronoi grid\nvoronoi_diagram = voronoi(x, y)\n\n# Plot Voronoi diagram\nplot(voronoi_diagram, legend=false, lw=2)\nscatter!(x, y, color=:red, label=\"Random Points\")\n\n# Optionally, save the plot\nsavefig(\"voronoi_diagram.png\")\n```\n",
    "label": 1
  },
  {
    "codes": "```rust\n// Define a macro to generate error types with thiserror-like derive\nmacro_rules! generate_error {\n    ($name:ident { $($variant:ident($ty:ty),* $(,)? }) => {\n        #[derive(Debug)]\n        pub enum $name {\n            $(\n                $variant($ty),\n            )*\n        }\n\n        impl std::fmt::Display for $name {\n            fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n                match self {\n                    $(\n                        $name::$variant(err) => write!(f, \"{}: {}\", stringify!($variant), err),\n                    )*\n                }\n            }\n        }\n\n        impl std::error::Error for $name {\n            fn source(&self) -> Option<&(dyn std::error::Error + 'static)> {\n                match self {\n                    $(\n                        $name::$variant(err) => Some(err),\n                    )*\n                }\n            }\n        }\n    };\n}\n\n// Example usage\ngenerate_error!(MyError {\n    Io(std::io::Error),\n    Parse(std::num::ParseIntError),\n});\n\n// This will generate:\n// #[derive(Debug)]\n// pub enum MyError {\n//     Io(std::io::Error),\n//     Parse(std::num::ParseIntError),\n// }\n//\n// impl std::fmt::Display for MyError {\n//     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n//         match self {\n//             MyError::Io(err) => write!(f, \"Io: {}\", err),\n//             MyError::Parse(err) => write!(f, \"Parse: {}\", err),\n//         }\n//     }\n// }\n//\n// impl std::error::Error for MyError {\n//     fn source(&self) -> Option<&(dyn std::error::Error + 'static)> {\n//         match self {\n//             MyError::Io(err) => Some(err),\n//             MyError::Parse(err) => Some(err),\n//         }\n//     }\n// }\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <Servo.h>\n#include <SoftwareSerial.h>\n\n// Define Bluetooth RX and TX pins\n#define BT_RX 10\n#define BT_TX 11\n\n// Create a SoftwareSerial object for Bluetooth communication\nSoftwareSerial BTSerial(BT_RX, BT_TX);\n\n// Create a Servo object\nServo myServo;\n\nvoid setup() {\n  // Initialize serial communication for debugging\n  Serial.begin(9600);\n  \n  // Initialize Bluetooth communication\n  BTSerial.begin(9600);\n  \n  // Attach the servo to pin 9\n  myServo.attach(9);\n  \n  // Set initial position of the servo to 90 degrees\n  myServo.write(90);\n}\n\nvoid loop() {\n  // Check if data is available from Bluetooth\n  if (BTSerial.available()) {\n    // Read the incoming byte\n    int angle = BTSerial.parseInt();\n    \n    // Ensure the angle is within valid range (0 to 180)\n    if (angle >= 0 && angle <= 180) {\n      // Move the servo to the specified angle\n      myServo.write(angle);\n      \n      // Send confirmation back via Bluetooth\n      BTSerial.print(\"Servo moved to: \");\n      BTSerial.println(angle);\n    } else {\n      // Send error message if angle is invalid\n      BTSerial.println(\"Invalid angle. Please enter a value between 0 and 180.\");\n    }\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```hcl\n# Define the SageMaker model resource\nresource \"aws_sagemaker_model\" \"ml_model\" {\n  name               = \"my-ml-model\"\n  execution_role_arn = aws_iam_role.sagemaker_execution_role.arn\n\n  primary_container {\n    image = \"<your-docker-image-uri>\"\n    model_data_url = \"s3://<your-bucket>/<model-path>.tar.gz\"\n  }\n\n  tags = {\n    Environment = \"Production\"\n  }\n}\n\n# Define the SageMaker endpoint configuration\nresource \"aws_sagemaker_endpoint_configuration\" \"endpoint_config\" {\n  name = \"my-endpoint-config\"\n\n  production_variants {\n    variant_name           = \"my-variant\"\n    model_name             = aws_sagemaker_model.ml_model.name\n    instance_type          = \"ml.m5.large\"\n    initial_instance_count = 1\n  }\n\n  tags = {\n    Environment = \"Production\"\n  }\n}\n\n# Define the SageMaker endpoint\nresource \"aws_sagemaker_endpoint\" \"endpoint\" {\n  name                 = \"my-endpoint\"\n  endpoint_config_name = aws_sagemaker_endpoint_configuration.endpoint_config.name\n\n  tags = {\n    Environment = \"Production\"\n  }\n}\n\n# Define the IAM role for SageMaker execution\nresource \"aws_iam_role\" \"sagemaker_execution_role\" {\n  name = \"sagemaker-execution-role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = \"sts:AssumeRole\"\n        Effect = \"Allow\"\n        Principal = {\n          Service = \"sagemaker.amazonaws.com\"\n        }\n      }\n    ]\n  })\n}\n\n# Attach the necessary policies to the IAM role\nresource \"aws_iam_role_policy_attachment\" \"sagemaker_policy_attachment\" {\n  role       = aws_iam_role.sagemaker_execution_role.name\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\"\n}\n\nresource \"aws_iam_role_policy_attachment\" \"s3_policy_attachment\" {\n  role       = aws_iam_role.sagemaker_execution_role.name\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\n\n# Function to create a bar plot\ndef bar_plot(array, title):\n    bars = plt.bar(range(len(array)), array, color='blue')\n    plt.title(title)\n    return bars\n\n# Function to update the bar plot for each frame\ndef update_bars(bars, array):\n    for bar, val in zip(bars, array):\n        bar.set_height(val)\n        bar.set_color('blue')\n\n# Bubble Sort algorithm with yield\ndef bubble_sort(array):\n    n = len(array)\n    for i in range(n):\n        for j in range(0, n-i-1):\n            if array[j] > array[j+1]:\n                array[j], array[j+1] = array[j+1], array[j]\n            yield array\n\n# Main function to setup and run the animation\ndef animate_sorting():\n    # Generate a random array\n    array = np.random.randint(1, 100, 50)\n    sorted_steps = bubble_sort(array)\n\n    # Create initial plot\n    fig, ax = plt.subplots()\n    bars = bar_plot(array, \"Bubble Sort Visualization\")\n\n    # Function to animate each frame\n    def animate(frame):\n        update_bars(bars, frame)\n\n    ani = animation.FuncAnimation(fig, animate, frames=sorted_steps, repeat=False, blit=False, interval=100)\n    plt.show()\n\n# Run the animation\nanimate_sorting()\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\n\n# Generate sample molecular data (atoms with x, y, z coordinates)\ndata = {\n    'Atom': ['C', 'O', 'H', 'H', 'H', 'H'],\n    'X': [0.0, 1.0, -0.5, 0.5, 0.0, 0.0],\n    'Y': [0.0, 0.0, 0.5, 0.5, -0.5, 0.5],\n    'Z': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Create a 3D scatter plot for the molecular structure\nfig = px.scatter_3d(df, x='X', y='Y', z='Z', color='Atom', size_max=10, \n                    title='3D Molecular Structure Visualization')\n\n# Customize the layout for better visualization\nfig.update_traces(marker=dict(size=12, line=dict(width=2, color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\n\n# Add lines to represent bonds between atoms\nfor i in range(len(df)):\n    for j in range(i + 1, len(df)):\n        fig.add_trace(px.line_3d(pd.DataFrame({\n            'X': [df.iloc[i]['X'], df.iloc[j]['X']],\n            'Y': [df.iloc[i]['Y'], df.iloc[j]['Y']],\n            'Z': [df.iloc[i]['Z'], df.iloc[j]['Z']]\n        }), x='X', y='Y', z='Z').data[0])\n\n# Show the plot\nfig.show()\n```",
    "label": 1
  },
  {
    "codes": "```python\ndef traveling_salesman_greedy(distance_matrix):\n    # Number of cities\n    num_cities = len(distance_matrix)\n    \n    # Track the order of cities in the path\n    path = [0]  # Start from the first city (index 0)\n    \n    # Keep track of visited cities\n    visited = set(path)\n    \n    # Tour construction\n    current_city = 0\n    while len(visited) < num_cities:\n        # Find the nearest unvisited city\n        nearest_city = None\n        min_distance = float('inf')\n        \n        for city in range(num_cities):\n            if city not in visited and 0 <= distance_matrix[current_city][city] < min_distance:\n                nearest_city = city\n                min_distance = distance_matrix[current_city][city]\n        \n        # Move to the nearest city\n        path.append(nearest_city)\n        visited.add(nearest_city)\n        current_city = nearest_city\n\n    # Return to the starting city\n    path.append(path[0])  # Closing the tour\n\n    return path\n\n# Example usage:\n# distance_matrix = [\n#     [0, 10, 15, 20],\n#     [10, 0, 35, 25],\n#     [15, 35, 0, 30],\n#     [20, 25, 30, 0]\n# ]\n# path = traveling_salesman_greedy(distance_matrix)\n# print(\"Path:\", path)\n```\n",
    "label": 1
  },
  {
    "codes": "def process_keystroke(self, inp, idx, offset):\n        \"\"\"\n        Process keystroke ``inp``, adjusting screen parameters.\n\n        :param inp: return value of Terminal.inkey().\n        :type inp: blessed.keyboard.Keystroke\n        :param idx: page index.\n        :type idx: int\n        :param offset: scrolling region offset of current page.\n        :type offset: int\n        :returns: tuple of next (idx, offset).\n        :rtype: (int, int)\n        \"\"\"\n        if inp.lower() in (u'q', u'Q'):\n            # exit\n            return (-1, -1)\n        self._process_keystroke_commands(inp)\n        idx, offset = self._process_keystroke_movement(inp, idx, offset)\n        return idx, offset",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<CertificateBundle>> updateCertificateWithServiceResponseAsync(String vaultBaseUrl, String certificateName, String certificateVersion, CertificatePolicy certificatePolicy, CertificateAttributes certificateAttributes, Map<String, String> tags) {\n        if (vaultBaseUrl == null) {\n            throw new IllegalArgumentException(\"Parameter vaultBaseUrl is required and cannot be null.\");\n        }\n        if (certificateName == null) {\n            throw new IllegalArgumentException(\"Parameter certificateName is required and cannot be null.\");\n        }\n        if (certificateVersion == null) {\n            throw new IllegalArgumentException(\"Parameter certificateVersion is required and cannot be null.\");\n        }\n        if (this.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.apiVersion() is required and cannot be null.\");\n        }\n        Validator.validate(certificatePolicy);\n        Validator.validate(certificateAttributes);\n        Validator.validate(tags);\n        CertificateUpdateParameters parameters = new CertificateUpdateParameters();\n        parameters.withCertificatePolicy(certificatePolicy);\n        parameters.withCertificateAttributes(certificateAttributes);\n        parameters.withTags(tags);\n        String parameterizedHost = Joiner.on(\", \").join(\"{vaultBaseUrl}\", vaultBaseUrl);\n        return service.updateCertificate(certificateName, certificateVersion, this.apiVersion(), this.acceptLanguage(), parameters, parameterizedHost, this.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<CertificateBundle>>>() {\n                @Override\n                public Observable<ServiceResponse<CertificateBundle>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<CertificateBundle> clientResponse = updateCertificateDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "function clone(orig) {\n    var copy;\n    switch(typeof(orig)) {\n        case 'object':\n            copy = (orig.length) ? [] : {};\n            for (var attr in orig) {\n                copy[attr] = clone(orig[attr]);\n            }\n            break;\n        default:\n            copy = orig;\n            break;\n    }\n    return copy;\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<DeletedSasDefinitionItem>>> getDeletedSasDefinitionsSinglePageAsync(final String vaultBaseUrl, final String storageAccountName, final Integer maxresults) {\n        if (vaultBaseUrl == null) {\n            throw new IllegalArgumentException(\"Parameter vaultBaseUrl is required and cannot be null.\");\n        }\n        if (storageAccountName == null) {\n            throw new IllegalArgumentException(\"Parameter storageAccountName is required and cannot be null.\");\n        }\n        if (this.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.apiVersion() is required and cannot be null.\");\n        }\n        String parameterizedHost = Joiner.on(\", \").join(\"{vaultBaseUrl}\", vaultBaseUrl);\n        return service.getDeletedSasDefinitions(storageAccountName, maxresults, this.apiVersion(), this.acceptLanguage(), parameterizedHost, this.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<Page<DeletedSasDefinitionItem>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<DeletedSasDefinitionItem>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<PageImpl<DeletedSasDefinitionItem>> result = getDeletedSasDefinitionsDelegate(response);\n                        return Observable.just(new ServiceResponse<Page<DeletedSasDefinitionItem>>(result.body(), result.response()));\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```rust\nuse bevy::prelude::*;\nuse bevy::render::camera::Camera;\nuse bevy::window::CursorMoved;\n\n#[derive(Component)]\nstruct Selectable;\n\n#[derive(Resource)]\nstruct SelectedUnits(Vec<Entity>);\n\n#[derive(Resource)]\nstruct SelectionBox {\n    start: Option<Vec2>,\n    end: Option<Vec2>,\n}\n\nfn main() {\n    App::build()\n        .add_plugins(DefaultPlugins)\n        .insert_resource(SelectedUnits(Vec::new()))\n        .insert_resource(SelectionBox { start: None, end: None })\n        .add_startup_system(setup_camera.system())\n        .add_system(start_selection_box.system())\n        .add_system(update_selection_box.system())\n        .add_system(finalize_selection_box.system())\n        .run();\n}\n\nfn setup_camera(mut commands: Commands) {\n    commands.spawn_bundle(OrthographicCameraBundle::new_2d());\n}\n\nfn start_selection_box(\n    mut selection_box: ResMut<SelectionBox>,\n    mouse_button_input: Res<Input<MouseButton>>,\n    windows: Res<Windows>,\n    q_camera: Query<(&Camera, &GlobalTransform)>,\n) {\n    if mouse_button_input.just_pressed(MouseButton::Left) {\n        if let Some(position) = get_cursor_position(windows, q_camera) {\n            selection_box.start = Some(position);\n            selection_box.end = Some(position);\n        }\n    }\n}\n\nfn update_selection_box(\n    mut selection_box: ResMut<SelectionBox>,\n    mouse_button_input: Res<Input<MouseButton>>,\n    windows: Res<Windows>,\n    q_camera: Query<(&Camera, &GlobalTransform)>,\n) {\n    if mouse_button_input.pressed(MouseButton::Left) {\n        if let (Some(start), Some(position)) = (selection_box.start, get_cursor_position(windows, q_camera)) {\n            selection_box.end = Some(position);\n        }\n    }\n}\n\nfn finalize_selection_box(\n    mut selected_units: ResMut<SelectedUnits>,\n    mut selection_box: ResMut<SelectionBox>,\n    query_units: Query<(Entity, &Transform), With<Selectable>>,\n    mouse_button_input: Res<Input<MouseButton>>,\n) {\n    if mouse_button_input.just_released(MouseButton::Left) {\n        if let (Some(start), Some(end)) = (selection_box.start, selection_box.end) {\n            let (min_x, max_x) = if start.x < end.x { (start.x, end.x) } else { (end.x, start.x) };\n            let (min_y, max_y) = if start.y < end.y { (start.y, end.y) } else { (end.y, start.y) };\n\n            selected_units.0.clear();\n            for (entity, transform) in query_units.iter() {\n                if transform.translation.x >= min_x && transform.translation.x <= max_x &&\n                   transform.translation.y >= min_y && transform.translation.y <= max_y {\n                    selected_units.0.push(entity);\n                }\n            }\n        }\n        selection_box.start = None;\n        selection_box.end = None;\n    }\n}\n\nfn get_cursor_position(windows: Res<Windows>, q_camera: Query<(&Camera, &GlobalTransform)>) -> Option<Vec2> {\n    let window = windows.get_primary()?;\n    let (camera, camera_transform) = q_camera.single();\n    let cursor_position = window.cursor_position()?;\n    let window_size = Vec2::new(window.width() as f32, window.height() as f32);\n    let ndc = (cursor_position / window_size) * 2.0 - Vec2::ONE;\n\n    let ndc_to_world = camera_transform.compute_matrix() * camera.projection_matrix().inverse();\n    let world_position = ndc_to_world.project_point3(ndc.extend(-1.0));\n\n    Some(world_position.truncate())\n}\n```\n\nNote: This code defines an RTS-style (Real-Time Strategy) unit selection system using the Bevy ECS framework and assumes you have a basic understanding of Bevy, ECS, and Rust. Adjustments may be necessary for a specific game setup.",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<List<SkuInfoInner>> listWorkerPoolSkusNextAsync(final String nextPageLink, final ServiceFuture<List<SkuInfoInner>> serviceFuture, final ListOperationCallback<SkuInfoInner> serviceCallback) {\n        return AzureServiceFuture.fromPageResponse(\n            listWorkerPoolSkusNextSinglePageAsync(nextPageLink),\n            new Func1<String, Observable<ServiceResponse<Page<SkuInfoInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<SkuInfoInner>>> call(String nextPageLink) {\n                    return listWorkerPoolSkusNextSinglePageAsync(nextPageLink);\n                }\n            },\n            serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```cpp\n// Save this script as `debug_memory_leaks.cpp`\n// Ensure the program you want to check for leaks is compiled with -g for debug symbols\n\n#include <iostream>\n\n// Sample class to demonstrate memory allocation\nclass MemoryLeakExample {\npublic:\n    MemoryLeakExample() {\n        data = new int[10];\n    }\n\n    ~MemoryLeakExample() {\n        // Destructor should normally release allocated resources:\n        // delete[] data;\n    }\n\nprivate:\n    int* data;\n};\n\nint main() {\n    // Create an instance of MemoryLeakExample\n    MemoryLeakExample* example = new MemoryLeakExample();\n\n    // Intentionally forget to delete 'example' to simulate a memory leak\n    // Normally, you'd call: delete example;\n\n    std::cout << \"Exiting program without freeing all allocated memory.\" << std::endl;\n\n    return 0;\n}\n\n// To check for memory leaks, compile this file with debugging information:\n// g++ -g -o debug_memory_leaks debug_memory_leaks.cpp\n\n// Run Valgrind to analyze memory leaks:\n// valgrind --leak-check=full ./debug_memory_leaks\n\n// Look for lines in the Valgrind output that indicate memory that was \n// \"definitely lost,\" which reflects memory leaks in your program.\n```\n",
    "label": 1
  },
  {
    "codes": "public RoleAssignmentInner create(String scope, String roleAssignmentName, RoleAssignmentCreateParameters parameters) {\n        return createWithServiceResponseAsync(scope, roleAssignmentName, parameters).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "func (s *InventoryRetrievalJobDescription) SetMarker(v string) *InventoryRetrievalJobDescription {\n\ts.Marker = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public void deleteFromTask(String jobId, String taskId, String filePath) {\n        deleteFromTaskWithServiceResponseAsync(jobId, taskId, filePath).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "```javascript\n// Import D3.js library (ensure it's included in your HTML)\n// <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n\n// Set up SVG dimensions\nconst width = 800;\nconst height = 400;\nconst margin = { top: 20, right: 30, bottom: 30, left: 40 };\n\n// Create SVG container\nconst svg = d3.select(\"body\")\n  .append(\"svg\")\n  .attr(\"width\", width)\n  .attr(\"height\", height);\n\n// Define scales for x and y axes\nconst xScale = d3.scaleTime()\n  .range([margin.left, width - margin.right]);\n\nconst yScale = d3.scaleLinear()\n  .range([height - margin.bottom, margin.top]);\n\n// Create x and y axes\nconst xAxis = d3.axisBottom(xScale);\nconst yAxis = d3.axisLeft(yScale);\n\n// Append x and y axes to the SVG\nsvg.append(\"g\")\n  .attr(\"transform\", `translate(0,${height - margin.bottom})`)\n  .call(xAxis);\n\nsvg.append(\"g\")\n  .attr(\"transform\", `translate(${margin.left},0)`)\n  .call(yAxis);\n\n// Initialize data array to store price points\nlet data = [];\n\n// Function to fetch real-time cryptocurrency price\nasync function fetchPrice() {\n  const response = await fetch('https://api.coingecko.com/api/v3/simple/price?ids=bitcoin&vs_currencies=usd');\n  const result = await response.json();\n  const price = result.bitcoin.usd;\n  const timestamp = new Date();\n\n  // Add new price point to data array\n  data.push({ timestamp, price });\n\n  // Keep only the last 50 data points\n  if (data.length > 50) {\n    data.shift();\n  }\n\n  // Update scales\n  xScale.domain(d3.extent(data, d => d.timestamp));\n  yScale.domain([d3.min(data, d => d.price) - 10, d3.max(data, d => d.price) + 10]);\n\n  // Update x and y axes\n  svg.selectAll(\".x-axis\").call(xAxis);\n  svg.selectAll(\".y-axis\").call(yAxis);\n\n  // Draw the line\n  const line = d3.line()\n    .x(d => xScale(d.timestamp))\n    .y(d => yScale(d.price));\n\n  svg.selectAll(\".price-line\")\n    .datum(data)\n    .attr(\"class\", \"price-line\")\n    .attr(\"d\", line)\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"steelblue\")\n    .attr(\"stroke-width\", 2);\n\n  // Add circles for each data point\n  svg.selectAll(\".price-point\")\n    .data(data)\n    .join(\"circle\")\n    .attr(\"class\", \"price-point\")\n    .attr(\"cx\", d => xScale(d.timestamp))\n    .attr(\"cy\", d => yScale(d.price))\n    .attr(\"r\", 3)\n    .attr(\"fill\", \"orange\");\n}\n\n// Fetch price every 5 seconds\nsetInterval(fetchPrice, 5000);\n\n// Initial fetch\nfetchPrice();\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define source and destination directories\nSRC_DIR=\"/path/to/source\"\nDEST_DIR=\"/path/to/destination\"\n\n# Check if directories exist\nif [ ! -d \"$SRC_DIR\" ]; then\n    echo \"Source directory does not exist.\"\n    exit 1\nfi\n\nif [ ! -d \"$DEST_DIR\" ]; then\n    echo \"Destination directory does not exist.\"\n    exit 1\nfi\n\n# Compare directories and sync changes using rsync\nrsync -av --dry-run \"$SRC_DIR/\" \"$DEST_DIR/\" | grep -E '^>|^<'\n\n# Prompt user to confirm sync\nread -p \"Do you want to sync changes? (y/n): \" confirm\n\nif [ \"$confirm\" == \"y\" ]; then\n    rsync -av \"$SRC_DIR/\" \"$DEST_DIR/\"\n    echo \"Sync completed.\"\nelse\n    echo \"Sync aborted.\"\nfi\n```",
    "label": 1
  },
  {
    "codes": "```kotlin\n// build.gradle (Module: app)\ndependencies {\n    implementation 'com.github.PhilJay:MPAndroidChart:v3.1.0' // Add MPAndroidChart library\n}\n\n// AndroidManifest.xml\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.example.sensordata\">\n\n    <uses-permission android:name=\"android.permission.BODY_SENSORS\" /> // Ensure necessary permissions\n    <application ... >\n        ...\n    </application>\n</manifest>\n\n// MainActivity.kt\npackage com.example.sensordata\n\nimport android.hardware.Sensor\nimport android.hardware.SensorEvent\nimport android.hardware.SensorEventListener\nimport android.hardware.SensorManager\nimport android.os.Bundle\nimport androidx.appcompat.app.AppCompatActivity\nimport com.github.mikephil.charting.charts.LineChart\nimport com.github.mikephil.charting.components.Description\nimport com.github.mikephil.charting.data.Entry\nimport com.github.mikephil.charting.data.LineData\nimport com.github.mikephil.charting.data.LineDataSet\n\nclass MainActivity : AppCompatActivity(), SensorEventListener {\n\n    private lateinit var sensorManager: SensorManager\n    private var sensor: Sensor? = null\n    private lateinit var lineChart: LineChart\n    private val dataPoints: MutableList<Entry> = mutableListOf()\n    \n    override fun onCreate(savedInstanceState: Bundle?) {\n        super.onCreate(savedInstanceState)\n        setContentView(R.layout.activity_main)\n\n        // Initialize sensor and chart\n        sensorManager = getSystemService(SENSOR_SERVICE) as SensorManager\n        sensor = sensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER)\n        lineChart = findViewById(R.id.lineChart)\n\n        // Configure the chart display\n        val description = Description()\n        description.text = \"Real-Time Sensor Data\"\n        lineChart.description = description\n    }\n\n    override fun onResume() {\n        super.onResume()\n        // Register the sensor listener\n        sensor?.also { accelerometer ->\n            sensorManager.registerListener(this, accelerometer, SensorManager.SENSOR_DELAY_NORMAL)\n        }\n    }\n\n    override fun onPause() {\n        super.onPause()\n        // Unregister the sensor listener\n        sensorManager.unregisterListener(this)\n    }\n\n    override fun onSensorChanged(event: SensorEvent?) {\n        event?.let {\n            // Update chart with new sensor data\n            val value = it.values[0] // Let's plot only one axis, e.g., x-axis\n            dataPoints.add(Entry(dataPoints.size.toFloat(), value))\n            val lineDataSet = LineDataSet(dataPoints, \"X-axis Value\")\n            val lineData = LineData(lineDataSet)\n            lineChart.data = lineData\n            lineChart.notifyDataSetChanged()\n            lineChart.invalidate() // Refresh the chart\n        }\n    }\n\n    override fun onAccuracyChanged(sensor: Sensor?, accuracy: Int) {\n        // Handle accuracy changes if needed\n    }\n}\n\n// activity_main.xml\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n    <com.github.mikephil.charting.charts.LineChart\n        android:id=\"@+id/lineChart\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"/>\n\n</RelativeLayout>\n```",
    "label": 1
  },
  {
    "codes": "func (s *ListProvisionedCapacityOutput) SetProvisionedCapacityList(v []*ProvisionedCapacityDescription) *ListProvisionedCapacityOutput {\n\ts.ProvisionedCapacityList = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model)\n\n# Save the CoreML model to a file\ncoreml_model.save('your_model.mlmodel')\n\n# Print success message\nprint(\"Keras model successfully converted to CoreML format and saved as 'your_model.mlmodel'\")\n```\n\n```python\n# Alternative approach using TensorFlow 2.x and coremltools 5.0+\nimport tensorflow as tf\nimport coremltools as ct\n\n# Load the Keras model\nkeras_model = tf.keras.models.load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model, source='tensorflow')\n\n# Save the CoreML model to a file\ncoreml_model.save('your_model.mlmodel')\n\n# Print success message\nprint(\"Keras model successfully converted to CoreML format and saved as 'your_model.mlmodel'\")\n```\n\n```python\n# Another approach using a custom input/output description\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Define input and output descriptions\ninput_description = ct.ImageType(shape=(1, 224, 224, 3), bias=[-1,-1,-1], scale=1/127)\noutput_description = ct.TensorType(name='output', shape=(1, 1000))\n\n# Convert the Keras model to CoreML format with custom descriptions\ncoreml_model = ct.convert(keras_model, inputs=[input_description], outputs=[output_description])\n\n# Save the CoreML model to a file\ncoreml_model.save('your_model.mlmodel')\n\n# Print success message\nprint(\"Keras model successfully converted to CoreML format with custom descriptions and saved as 'your_model.mlmodel'\")\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport hmac\n\ndef constant_time_compare(val1, val2):\n    # Securely compare two HMAC digests in constant time.\n    return hmac.compare_digest(val1, val2)\n\n# Example usage:\n# digest_a = hmac.new(key, message_a, hashlib.sha256).digest()\n# digest_b = hmac.new(key, message_b, hashlib.sha256).digest()\n# result = constant_time_compare(digest_a, digest_b)\n```",
    "label": 1
  },
  {
    "codes": "function applyDefaultTextStyle(textStyle) {\n    var opt = textStyle.insideRollbackOpt;\n\n    // Only `insideRollbackOpt` created (in `setTextStyleCommon`),\n    // applyDefaultTextStyle works.\n    if (!opt || textStyle.textFill != null) {\n        return;\n    }\n\n    var useInsideStyle = opt.useInsideStyle;\n    var textPosition = textStyle.insideRawTextPosition;\n    var insideRollback;\n    var autoColor = opt.autoColor;\n\n    if (useInsideStyle !== false\n        && (useInsideStyle === true\n            || (opt.isRectText\n                && textPosition\n                // textPosition can be [10, 30]\n                && typeof textPosition === 'string'\n                && textPosition.indexOf('inside') >= 0\n            )\n        )\n    ) {\n        insideRollback = {\n            textFill: null,\n            textStroke: textStyle.textStroke,\n            textStrokeWidth: textStyle.textStrokeWidth\n        };\n        textStyle.textFill = '#fff';\n        // Consider text with #fff overflow its container.\n        if (textStyle.textStroke == null) {\n            textStyle.textStroke = autoColor;\n            textStyle.textStrokeWidth == null && (textStyle.textStrokeWidth = 2);\n        }\n    }\n    else if (autoColor != null) {\n        insideRollback = {textFill: null};\n        textStyle.textFill = autoColor;\n    }\n\n    // Always set `insideRollback`, for clearing previous.\n    if (insideRollback) {\n        textStyle.insideRollback = insideRollback;\n    }\n}",
    "label": 0
  },
  {
    "codes": "```kotlin\n// Define a flow to emit sensor data\nval sensorDataFlow: Flow<SensorData> = flow {\n    while (true) {\n        emit(readSensorData()) // Simulate reading sensor data\n        delay(100) // Simulate delay between readings\n    }\n}\n\n// Batch processing pipeline\nsensorDataFlow\n    .buffer(50) // Buffer to handle backpressure\n    .map { data -> \n        // Preprocess data (e.g., normalize, filter)\n        preprocessData(data)\n    }\n    .batch(1000) // Collect data into batches of 1000 items\n    .map { batch -> \n        // Process the batch (e.g., aggregate, analyze)\n        processBatch(batch)\n    }\n    .flowOn(Dispatchers.IO) // Run on IO thread for heavy processing\n    .collect { result -> \n        // Handle the processed batch result\n        handleResult(result)\n    }\n\n// Helper functions\nsuspend fun readSensorData(): SensorData { /* ... */ }\nfun preprocessData(data: SensorData): SensorData { /* ... */ }\nfun processBatch(batch: List<SensorData>): BatchResult { /* ... */ }\nfun handleResult(result: BatchResult) { /* ... */ }\n```",
    "label": 1
  },
  {
    "codes": "public Observable<Page<LongTermRetentionBackupInner>> listByDatabaseNextAsync(final String nextPageLink) {\n        return listByDatabaseNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<LongTermRetentionBackupInner>>, Page<LongTermRetentionBackupInner>>() {\n                @Override\n                public Page<LongTermRetentionBackupInner> call(ServiceResponse<Page<LongTermRetentionBackupInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<PolicySetDefinitionInner> getAsync(String policySetDefinitionName) {\n        return getWithServiceResponseAsync(policySetDefinitionName).map(new Func1<ServiceResponse<PolicySetDefinitionInner>, PolicySetDefinitionInner>() {\n            @Override\n            public PolicySetDefinitionInner call(ServiceResponse<PolicySetDefinitionInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "function finalPropName( name ) {\n\tvar ret = jQuery.cssProps[ name ];\n\tif ( !ret ) {\n\t\tret = jQuery.cssProps[ name ] = vendorPropName( name ) || name;\n\t}\n\treturn ret;\n}",
    "label": 0
  },
  {
    "codes": "func (s *ListCrawlersInput) SetTags(v map[string]*string) *ListCrawlersInput {\n\ts.Tags = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "def event_callback(self, *event_types):\n        \"\"\"Function decorator to register a blanket event callback for the given event types. Event types can be given\n        as str (e.g.  'start-file'), integer or MpvEventID object.\n\n        WARNING: Due to the way this is filtering events, this decorator cannot be chained with itself.\n\n        To unregister the event callback, call its ``unregister_mpv_events`` function::\n\n            player = mpv.MPV()\n            @player.event_callback('shutdown')\n            def my_handler(event):\n                print('It ded.')\n\n            my_handler.unregister_mpv_events()\n        \"\"\"\n        def register(callback):\n            types = [MpvEventID.from_str(t) if isinstance(t, str) else t for t in event_types] or MpvEventID.ANY\n            @wraps(callback)\n            def wrapper(event, *args, **kwargs):\n                if event['event_id'] in types:\n                    callback(event, *args, **kwargs)\n            self._event_callbacks.append(wrapper)\n            wrapper.unregister_mpv_events = partial(self.unregister_event_callback, wrapper)\n            return wrapper\n        return register",
    "label": 0
  },
  {
    "codes": "func (s *CreateDevEndpointOutput) SetYarnEndpointAddress(v string) *CreateDevEndpointOutput {\n\ts.YarnEndpointAddress = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```typescript\n// Define the User type\ntype User = {\n  id: ID!\n  username: String!\n  email: String!\n  posts: [Post!]! // A user can have multiple posts\n  followers: [User!]! // A user can have multiple followers\n  following: [User!]! // A user can follow multiple users\n}\n\n// Define the Post type\ntype Post = {\n  id: ID!\n  content: String!\n  author: User! // Each post has one author\n  likes: [User!]! // A post can have multiple likes\n  comments: [Comment!]! // A post can have multiple comments\n}\n\n// Define the Comment type\ntype Comment = {\n  id: ID!\n  content: String!\n  author: User! // Each comment has one author\n  post: Post! // Each comment belongs to one post\n}\n\n// Define the Query type\ntype Query = {\n  user(id: ID!): User // Fetch a user by ID\n  post(id: ID!): Post // Fetch a post by ID\n  comment(id: ID!): Comment // Fetch a comment by ID\n  users: [User!]! // Fetch all users\n  posts: [Post!]! // Fetch all posts\n  comments: [Comment!]! // Fetch all comments\n}\n\n// Define the Mutation type\ntype Mutation = {\n  createUser(username: String!, email: String!): User // Create a new user\n  createPost(content: String!, authorId: ID!): Post // Create a new post\n  createComment(content: String!, authorId: ID!, postId: ID!): Comment // Create a new comment\n  followUser(followerId: ID!, followeeId: ID!): User // Follow a user\n  likePost(userId: ID!, postId: ID!): Post // Like a post\n}\n\n// Define the Subscription type\ntype Subscription = {\n  newPost: Post! // Subscribe to new posts\n  newComment(postId: ID!): Comment! // Subscribe to new comments on a specific post\n}\n\n// Export the schema\nexport const schema = `\n  ${User}\n  ${Post}\n  ${Comment}\n  ${Query}\n  ${Mutation}\n  ${Subscription}\n`;\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Management;\nusing System.ServiceProcess;\nusing System.IO;\n\npublic class UsbMonitorService : ServiceBase\n{\n    private ManagementEventWatcher _watcher;\n\n    public UsbMonitorService()\n    {\n        ServiceName = \"UsbMonitorService\";\n    }\n\n    protected override void OnStart(string[] args)\n    {\n        // Create a WMI event query to monitor USB insertions and removals\n        WqlEventQuery query = new WqlEventQuery(\"SELECT * FROM Win32_DeviceChangeEvent WHERE EventType = 2 OR EventType = 3\");\n\n        // Create a watcher for the query\n        _watcher = new ManagementEventWatcher(query);\n\n        // Subscribe to the event\n        _watcher.EventArrived += new EventArrivedEventHandler(UsbEventHandler);\n\n        // Start listening for the events\n        _watcher.Start();\n    }\n\n    private void UsbEventHandler(object sender, EventArrivedEventArgs e)\n    {\n        string eventType = (Convert.ToInt16(e.NewEvent.Properties[\"EventType\"].Value) == 2) ? \"USB Inserted\" : \"USB Removed\";\n        \n        // Log the event\n        LogEvent(eventType);\n    }\n\n    protected override void OnStop()\n    {\n        // Stop the watcher when the service stops\n        if (_watcher != null)\n        {\n            _watcher.Stop();\n            _watcher.Dispose();\n        }\n    }\n\n    private void LogEvent(string message)\n    {\n        // Log the event to a file\n        string logFilePath = Path.Combine(AppDomain.CurrentDomain.BaseDirectory, \"UsbMonitorLog.txt\");\n        \n        File.AppendAllText(logFilePath, $\"{DateTime.Now}: {message}{Environment.NewLine}\");\n    }\n\n    public static void Main()\n    {\n        ServiceBase.Run(new UsbMonitorService());\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define source and destination directories\nSOURCE_DIR=\"/path/to/source\"\nDEST_DIR=\"/path/to/destination\"\n\n# Check if directories exist\nif [ ! -d \"$SOURCE_DIR\" ]; then\n    echo \"Source directory does not exist.\"\n    exit 1\nfi\n\nif [ ! -d \"$DEST_DIR\" ]; then\n    echo \"Destination directory does not exist.\"\n    exit 1\nfi\n\n# Compare directories using rsync in dry-run mode\necho \"Comparing directories...\"\nrsync -avn --delete \"$SOURCE_DIR/\" \"$DEST_DIR/\"\n\n# Prompt user to confirm sync\nread -p \"Do you want to sync changes? (y/n): \" confirm\n\n# Sync changes if confirmed\nif [ \"$confirm\" == \"y\" ]; then\n    echo \"Syncing directories...\"\n    rsync -av --delete \"$SOURCE_DIR/\" \"$DEST_DIR/\"\n    echo \"Sync complete.\"\nelse\n    echo \"Sync cancelled.\"\nfi\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.security.KeyStore;\nimport java.security.cert.Certificate;\nimport java.security.cert.X509Certificate;\n\npublic class KeyStoreManager {\n    private KeyStore keyStore;\n    private String keyStoreFile;\n    private String keyStorePassword;\n\n    public KeyStoreManager(String keyStoreFile, String keyStorePassword) throws Exception {\n        this.keyStoreFile = keyStoreFile;\n        this.keyStorePassword = keyStorePassword;\n        loadKeyStore();\n    }\n\n    private void loadKeyStore() throws Exception {\n        keyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n        try (FileInputStream fis = new FileInputStream(keyStoreFile)) {\n            keyStore.load(fis, keyStorePassword.toCharArray());\n        }\n    }\n\n    public void saveKeyStore() throws Exception {\n        try (FileOutputStream fos = new FileOutputStream(keyStoreFile)) {\n            keyStore.store(fos, keyStorePassword.toCharArray());\n        }\n    }\n\n    public void addCertificate(String alias, X509Certificate certificate) throws Exception {\n        keyStore.setCertificateEntry(alias, certificate);\n        saveKeyStore();\n    }\n\n    public Certificate getCertificate(String alias) throws Exception {\n        return keyStore.getCertificate(alias);\n    }\n\n    public void deleteCertificate(String alias) throws Exception {\n        keyStore.deleteEntry(alias);\n        saveKeyStore();\n    }\n\n    public boolean containsCertificate(String alias) throws Exception {\n        return keyStore.containsAlias(alias);\n    }\n\n    public X509Certificate getX509Certificate(String alias) throws Exception {\n        Certificate cert = getCertificate(alias);\n        if (cert instanceof X509Certificate) {\n            return (X509Certificate) cert;\n        }\n        return null;\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function (brushOption) {\n        if (__DEV__) {\n            zrUtil.assert(this._mounted);\n        }\n\n        this._brushType && doDisableBrush(this);\n        brushOption.brushType && doEnableBrush(this, brushOption);\n\n        return this;\n    }",
    "label": 0
  },
  {
    "codes": "```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# Define the pipeline\npipeline = Pipeline([\n    # Step 1: Handle missing data using SimpleImputer\n    ('imputer', SimpleImputer(strategy='mean')),  # Use 'mean', 'median', or 'most_frequent' as needed\n    \n    # Step 2: Feature scaling using StandardScaler\n    ('scaler', StandardScaler())\n])\n\n# Now, the pipeline is ready to be used for fitting and transforming data\n```\n",
    "label": 1
  },
  {
    "codes": "```javascript\n// Import required modules\nconst jwt = require('jsonwebtoken');\nconst { SECRET_KEY } = require('./config');\n\n// Middleware to authenticate JWT tokens\nconst authenticateJWT = (req, res, next) => {\n    // Extract token from the Authorization header\n    const authHeader = req.headers.authorization;\n\n    // Check if the token exists\n    if (authHeader) {\n        const token = authHeader.split(' ')[1]; // Format: \"Bearer <token>\"\n\n        // Verify the token\n        jwt.verify(token, SECRET_KEY, (err, user) => {\n            if (err) {\n                // Token is invalid or expired\n                return res.sendStatus(403); // Forbidden\n            }\n\n            // Attach the decoded user information to the request object\n            req.user = user;\n            next(); // Proceed to the next middleware/route\n        });\n    } else {\n        // No token provided\n        res.sendStatus(401); // Unauthorized\n    }\n};\n\nmodule.exports = authenticateJWT;\n```",
    "label": 1
  },
  {
    "codes": "Certainly! Below is an example of a REST API using Express.js in JavaScript to handle user login, focusing on the response codes and comments:\n\n```javascript\nconst express = require('express');\nconst bodyParser = require('body-parser');\nconst app = express();\nconst PORT = 3000;\n\n// Middleware to parse JSON bodies\napp.use(bodyParser.json());\n\n// Dummy database for demonstration\nconst users = [\n  { id: 1, username: 'user1', password: 'password1' },\n  { id: 2, username: 'user2', password: 'password2' }\n];\n\n// Login endpoint\napp.post('/login', (req, res) => {\n  const { username, password } = req.body;\n\n  // Check for missing credentials\n  if (!username || !password) {\n    // Respond with 400 Bad Request if username or password is missing\n    return res.status(400).json({ error: 'Username and password are required.' });\n  }\n\n  // Find user with matching credentials\n  const user = users.find(u => u.username === username && u.password === password);\n  \n  // Check if user is found\n  if (user) {\n    // Respond with 200 OK if login is successful\n    return res.status(200).json({ message: 'Login successful!', userId: user.id });\n  } else {\n    // Respond with 401 Unauthorized if credentials are invalid\n    return res.status(401).json({ error: 'Invalid username or password.' });\n  }\n});\n\n// Start the server\napp.listen(PORT, () => {\n  console.log(`Server is running on port ${PORT}`);\n});\n```\n\n- `200 OK`: Login was successful and user credentials are correct.\n- `400 Bad Request`: Either the username or password is missing from the request.\n- `401 Unauthorized`: The provided username or password was invalid.",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.IO;\nusing System.Management;\nusing System.ServiceProcess;\nusing System.Text;\n\npublic partial class UsbMonitorService : ServiceBase\n{\n    public UsbMonitorService()\n    {\n        InitializeComponent();\n    }\n\n    protected override void OnStart(string[] args)\n    {\n        // Set up management event watcher for USB insert/remove events\n        ManagementEventWatcher insertWatcher = new ManagementEventWatcher();\n        insertWatcher.EventArrived += new EventArrivedEventHandler(OnUsbInserted);\n        insertWatcher.Query = new WqlEventQuery(\"SELECT * FROM __InstanceCreationEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_USBHub'\");\n        insertWatcher.Start();\n\n        ManagementEventWatcher removeWatcher = new ManagementEventWatcher();\n        removeWatcher.EventArrived += new EventArrivedEventHandler(OnUsbRemoved);\n        removeWatcher.Query = new WqlEventQuery(\"SELECT * FROM __InstanceDeletionEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_USBHub'\");\n        removeWatcher.Start();\n    }\n\n    protected override void OnStop()\n    {\n        // Clean up resources on stop\n        insertWatcher.Stop();\n        insertWatcher.Dispose();\n        removeWatcher.Stop();\n        removeWatcher.Dispose();\n    }\n\n    private void OnUsbInserted(object sender, EventArrivedEventArgs e)\n    {\n        // Handle USB inserted event\n        ManagementBaseObject usbDevice = (ManagementBaseObject)e.NewEvent[\"TargetInstance\"];\n        LogEvent(\"USB Device Inserted: \" + usbDevice[\"DeviceID\"]);\n    }\n\n    private void OnUsbRemoved(object sender, EventArrivedEventArgs e)\n    {\n        // Handle USB removed event\n        ManagementBaseObject usbDevice = (ManagementBaseObject)e.NewEvent[\"TargetInstance\"];\n        LogEvent(\"USB Device Removed: \" + usbDevice[\"DeviceID\"]);\n    }\n\n    private void LogEvent(string message)\n    {\n        // Log events to a file\n        using (StreamWriter sw = new StreamWriter(AppDomain.CurrentDomain.BaseDirectory + \"\\\\usbLog.txt\", true, Encoding.UTF8))\n        {\n            sw.WriteLine(DateTime.Now.ToString(\"yyyy-MM-dd HH:mm:ss\") + \": \" + message);\n        }\n    }\n}\n```\n\n### Install/Setup Notes:\n1. Compile the service as a Windows Service application.\n2. Install using `InstallUtil.exe` or create a setup project.\n3. Ensure your service has appropriate permissions to write to the file system.\n",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<StorageBundle>> regenerateStorageAccountKeyWithServiceResponseAsync(String vaultBaseUrl, String storageAccountName, String keyName) {\n        if (vaultBaseUrl == null) {\n            throw new IllegalArgumentException(\"Parameter vaultBaseUrl is required and cannot be null.\");\n        }\n        if (storageAccountName == null) {\n            throw new IllegalArgumentException(\"Parameter storageAccountName is required and cannot be null.\");\n        }\n        if (this.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.apiVersion() is required and cannot be null.\");\n        }\n        if (keyName == null) {\n            throw new IllegalArgumentException(\"Parameter keyName is required and cannot be null.\");\n        }\n        StorageAccountRegenerteKeyParameters parameters = new StorageAccountRegenerteKeyParameters();\n        parameters.withKeyName(keyName);\n        String parameterizedHost = Joiner.on(\", \").join(\"{vaultBaseUrl}\", vaultBaseUrl);\n        return service.regenerateStorageAccountKey(storageAccountName, this.apiVersion(), this.acceptLanguage(), parameters, parameterizedHost, this.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<StorageBundle>>>() {\n                @Override\n                public Observable<ServiceResponse<StorageBundle>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<StorageBundle> clientResponse = regenerateStorageAccountKeyDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```python\nfrom flask import Flask, request, jsonify\nfrom werkzeug.utils import secure_filename\nfrom PIL import Image\nimport os\nimport uuid\nfrom concurrent.futures import ThreadPoolExecutor\n\napp = Flask(__name__)\nexecutor = ThreadPoolExecutor(2)  # Adjust the number of threads as needed\n\n# Configuration\nUPLOAD_FOLDER = 'uploads'\nALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\n# Ensure upload folder exists\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\n\ndef allowed_file(filename):\n    \"\"\"Check if the file has an allowed extension.\"\"\"\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef resize_image(image_path, output_path, size=(128, 128)):\n    \"\"\"Resize the image to the specified size.\"\"\"\n    with Image.open(image_path) as img:\n        img.thumbnail(size)\n        img.save(output_path)\n\n@app.route('/upload', methods=['POST'])\ndef upload_image():\n    \"\"\"Endpoint to upload and resize images asynchronously.\"\"\"\n    if 'file' not in request.files:\n        return jsonify({\"error\": \"No file part\"}), 400\n\n    file = request.files['file']\n\n    if file.filename == '':\n        return jsonify({\"error\": \"No selected file\"}), 400\n\n    if file and allowed_file(file.filename):\n        # Generate a unique filename\n        filename = secure_filename(file.filename)\n        unique_filename = f\"{uuid.uuid4().hex}_{filename}\"\n        file_path = os.path.join(app.config['UPLOAD_FOLDER'], unique_filename)\n        file.save(file_path)\n\n        # Define the resized image path\n        resized_filename = f\"resized_{unique_filename}\"\n        resized_path = os.path.join(app.config['UPLOAD_FOLDER'], resized_filename)\n\n        # Resize the image asynchronously\n        executor.submit(resize_image, file_path, resized_path)\n\n        return jsonify({\"message\": \"File uploaded and resizing started\", \"filename\": unique_filename}), 202\n\n    return jsonify({\"error\": \"File type not allowed\"}), 400\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```",
    "label": 1
  },
  {
    "codes": "function getVersion(repo) {\n    const git = path.join(repo, '.git');\n    const headPath = path.join(git, 'HEAD');\n    let head;\n    try {\n        head = fs.readFileSync(headPath, 'utf8').trim();\n    }\n    catch (e) {\n        return undefined;\n    }\n    if (/^[0-9a-f]{40}$/i.test(head)) {\n        return head;\n    }\n    const refMatch = /^ref: (.*)$/.exec(head);\n    if (!refMatch) {\n        return undefined;\n    }\n    const ref = refMatch[1];\n    const refPath = path.join(git, ref);\n    try {\n        return fs.readFileSync(refPath, 'utf8').trim();\n    }\n    catch (e) {\n        // noop\n    }\n    const packedRefsPath = path.join(git, 'packed-refs');\n    let refsRaw;\n    try {\n        refsRaw = fs.readFileSync(packedRefsPath, 'utf8').trim();\n    }\n    catch (e) {\n        return undefined;\n    }\n    const refsRegex = /^([0-9a-f]{40})\\s+(.+)$/gm;\n    let refsMatch;\n    let refs = {};\n    while (refsMatch = refsRegex.exec(refsRaw)) {\n        refs[refsMatch[2]] = refsMatch[1];\n    }\n    return refs[ref];\n}",
    "label": 0
  },
  {
    "codes": "def check_against_tables(chars, tables):\n    \"\"\"\n    Perform a check against the table predicates in `tables`. `tables` must be\n    a reusable iterable containing characteristic functions of character sets,\n    that is, functions which return :data:`True` if the character is in the\n    table.\n\n    The function returns the first character occuring in any of the tables or\n    :data:`None` if no character matches.\n    \"\"\"\n\n    for c in chars:\n        if any(in_table(c) for in_table in tables):\n            return c\n\n    return None",
    "label": 0
  },
  {
    "codes": "def send_and_wait_for_sent(self, stanza):\n        \"\"\"\n        Send the given `stanza` over the given :class:`StanzaStream` `stream`.\n\n        .. deprecated:: 0.8\n\n           This method will be removed in 1.0. Use :meth:`send` instead.\n        \"\"\"\n        warnings.warn(\n            r\"send_and_wait_for_sent is deprecated and will be removed in 1.0\",\n            DeprecationWarning,\n            stacklevel=1,\n        )\n        yield from self._enqueue(stanza)",
    "label": 0
  },
  {
    "codes": "```rust\n// Cargo.toml\n[dependencies]\nzip = \"0.6\"\nzip-extensions = \"0.6\"\n\n// src/lib.rs\nuse std::fs::File;\nuse std::io::{Read, Write};\nuse std::path::Path;\nuse zip::ZipArchive;\nuse zip::write::FileOptions;\nuse zip_extensions::write::ZipWriterExtensions;\nuse zip_extensions::read::ZipArchiveExtensions;\n\n/// Encrypts and writes a file to a ZIP archive with a password.\npub fn write_zip_with_password(\n    zip_path: &Path,\n    file_path: &Path,\n    password: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let file = File::create(zip_path)?;\n    let mut zip = zip::ZipWriter::new(file);\n\n    let options = FileOptions::default()\n        .compression_method(zip::CompressionMethod::Deflated)\n        .with_deprecated_encryption(password.as_bytes());\n\n    zip.start_file(file_path.file_name().unwrap().to_str().unwrap(), options)?;\n    let mut buffer = Vec::new();\n    let mut file_to_zip = File::open(file_path)?;\n    file_to_zip.read_to_end(&mut buffer)?;\n    zip.write_all(&buffer)?;\n\n    zip.finish()?;\n    Ok(())\n}\n\n/// Reads a file from a password-protected ZIP archive.\npub fn read_zip_with_password(\n    zip_path: &Path,\n    password: &str,\n) -> Result<Vec<u8>, Box<dyn std::error::Error>> {\n    let file = File::open(zip_path)?;\n    let mut archive = ZipArchive::new(file)?;\n\n    let mut buffer = Vec::new();\n    for i in 0..archive.len() {\n        let mut file = archive.by_index_decrypt(i, password.as_bytes())??;\n        file.read_to_end(&mut buffer)?;\n    }\n\n    Ok(buffer)\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *CreateTrailInput) SetCloudWatchLogsLogGroupArn(v string) *CreateTrailInput {\n\ts.CloudWatchLogsLogGroupArn = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```javascript\n// Initialize Phaser game\nconst game = new Phaser.Game(800, 600, Phaser.AUTO, 'game-container', {\n    preload: preload,\n    create: create,\n    update: update\n});\n\n// Preload assets\nfunction preload() {\n    this.load.image('ship', 'assets/ship.png');\n    this.load.image('bullet', 'assets/bullet.png');\n    this.load.image('star', 'assets/star.png');\n}\n\n// Create game objects\nfunction create() {\n    // Add player ship\n    this.ship = this.add.sprite(400, 500, 'ship');\n    this.ship.anchor.set(0.5);\n\n    // Enable physics for the ship\n    this.physics.enable(this.ship, Phaser.Physics.ARCADE);\n\n    // Create a group for bullets\n    this.bullets = this.add.group();\n    this.bullets.enableBody = true;\n    this.bullets.physicsBodyType = Phaser.Physics.ARCADE;\n\n    // Create a particle emitter for stars\n    this.starEmitter = this.add.emitter(0, 0, 200);\n    this.starEmitter.makeParticles('star');\n    this.starEmitter.gravity = 0;\n    this.starEmitter.setAlpha(1, 0, 3000);\n    this.starEmitter.setScale(0.5, 1, 0.5, 1, 3000);\n    this.starEmitter.start(false, 5000, 100);\n\n    // Set up keyboard controls\n    this.cursors = this.input.keyboard.createCursorKeys();\n    this.fireButton = this.input.keyboard.addKey(Phaser.KeyCode.SPACEBAR);\n}\n\n// Update game state\nfunction update() {\n    // Move ship left and right\n    if (this.cursors.left.isDown) {\n        this.ship.body.velocity.x = -200;\n    } else if (this.cursors.right.isDown) {\n        this.ship.body.velocity.x = 200;\n    } else {\n        this.ship.body.velocity.x = 0;\n    }\n\n    // Fire bullets\n    if (this.fireButton.isDown) {\n        this.fireBullet();\n    }\n\n    // Update star emitter position\n    this.starEmitter.x = this.ship.x;\n    this.starEmitter.y = this.ship.y;\n}\n\n// Fire a bullet\nfunction fireBullet() {\n    if (this.time.now > this.nextFire) {\n        this.nextFire = this.time.now + 100; // Cooldown between shots\n\n        const bullet = this.bullets.getFirstExists(false);\n        if (bullet) {\n            bullet.reset(this.ship.x, this.ship.y - 20);\n            bullet.body.velocity.y = -400;\n        } else {\n            const bullet = this.bullets.create(this.ship.x, this.ship.y - 20, 'bullet');\n            bullet.anchor.set(0.5);\n            bullet.body.velocity.y = -400;\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function addonsManager_addons(aSpec) {\n    var spec = aSpec || {};\n\n    return this.getElements({\n      type: \"addons\",\n      subtype: spec.attribute,\n      value: spec.value,\n      parent: this.selectedView\n    });\n  }",
    "label": 0
  },
  {
    "codes": "public void setData(String key, Object value) {\n        this.data = this.data.addData(key, value);\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<Page<SkuInfoInner>> listMultiRolePoolSkusAsync(final String resourceGroupName, final String name) {\n        return listMultiRolePoolSkusWithServiceResponseAsync(resourceGroupName, name)\n            .map(new Func1<ServiceResponse<Page<SkuInfoInner>>, Page<SkuInfoInner>>() {\n                @Override\n                public Page<SkuInfoInner> call(ServiceResponse<Page<SkuInfoInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def connected(self, *, presence=structs.PresenceState(False), **kwargs):\n        \"\"\"\n        Return a :class:`.node.UseConnected` context manager which does not\n        modify the presence settings.\n\n        The keyword arguments are passed to the :class:`.node.UseConnected`\n        context manager constructor.\n\n        .. versionadded:: 0.8\n        \"\"\"\n        return UseConnected(self, presence=presence, **kwargs)",
    "label": 0
  },
  {
    "codes": "function engineManager_getElement(spec) {\n    var elem = null;\n\n    switch(spec.type) {\n      /**\n       * subtype: subtype to match\n       * value: value to match\n       */\n      case \"more_engines\":\n        elem = new elementslib.ID(this._controller.window.document, \"addEngines\");\n        break;\n      case \"button\":\n        elem = new elementslib.Lookup(this._controller.window.document, MANAGER_BUTTONS +\n                                      '/{\"dlgtype\":\"' + spec.subtype + '\"}');\n        break;\n      case \"engine_button\":\n        switch(spec.subtype) {\n          case \"down\":\n            elem = new elementslib.ID(this._controller.window.document, \"dn\");\n            break;\n          case \"edit\":\n            elem = new elementslib.ID(this._controller.window.document, \"edit\");\n            break;\n          case \"remove\":\n            elem = new elementslib.ID(this._controller.window.document, \"remove\");\n            break;\n          case \"up\":\n            elem = new elementslib.ID(this._controller.window.document, \"up\");\n            break;\n        }\n        break;\n      case \"engine_list\":\n        elem = new elementslib.ID(this._controller.window.document, \"engineList\");\n        break;\n      case \"suggest\":\n        elem = new elementslib.ID(this._controller.window.document, \"enableSuggest\");\n        break;\n      default:\n        throw new Error(arguments.callee.name + \": Unknown element type - \" + spec.type);\n    }\n\n    return elem;\n  }",
    "label": 0
  },
  {
    "codes": "```cpp\n// Define a custom Unreal Engine Blueprint Function Library for managing the dynamic weather system\nUCLASS()\nclass YOURPROJECT_API UWeatherSystemBlueprintLibrary : public UBlueprintFunctionLibrary\n{\n    GENERATED_BODY()\n\npublic:\n    // Function to smoothly transition weather based on the intensity\n    UFUNCTION(BlueprintCallable, Category = \"Weather System\")\n    static void TransitionWeather(UWorld* WorldContext, EWeatherType NewWeather, float TransitionTime);\n};\n\n// Implementation file (.cpp)\nvoid UWeatherSystemBlueprintLibrary::TransitionWeather(UWorld* WorldContext, EWeatherType NewWeather, float TransitionTime)\n{\n    if(!WorldContext)\n    {\n        UE_LOG(LogTemp, Warning, TEXT(\"Invalid World Context\"));\n        return;\n    }\n\n    // Get the current weather system actor in your scene\n    AWeatherSystemActor* CurrentWeatherSystem = Cast<AWeatherSystemActor>(UGameplayStatics::GetActorOfClass(WorldContext, AWeatherSystemActor::StaticClass()));\n    if(!CurrentWeatherSystem)\n    {\n        UE_LOG(LogTemp, Warning, TEXT(\"Weather System Actor not found\"));\n        return;\n    }\n\n    // Begin transitioning weather\n    CurrentWeatherSystem->BeginWeatherTransition(NewWeather, TransitionTime);\n}\n\n// Your Weather System Actor implementation\nvoid AWeatherSystemActor::BeginWeatherTransition(EWeatherType NewWeather, float TransitionTime)\n{\n    // Set target weather and store initial state\n    TargetWeather = NewWeather;\n    InitialWeather = CurrentWeather;\n    TransitionElapsedTime = 0.0f;\n    bIsTransitioning = true;\n    WeatherTransitionTime = TransitionTime;\n\n    // Prepare to gradually change conditions, such as lighting, effects, etc.\n}\n\nvoid AWeatherSystemActor::Tick(float DeltaTime)\n{\n    Super::Tick(DeltaTime);\n\n    if(bIsTransitioning)\n    {\n        TransitionElapsedTime += DeltaTime;\n        float Progress = FMath::Clamp(TransitionElapsedTime / WeatherTransitionTime, 0.0f, 1.0f);\n\n        // Lerp between initial and target weather conditions\n        UpdateWeatherParameters(FMath::Lerp(InitialWeather, TargetWeather, Progress));\n\n        if(TransitionElapsedTime >= WeatherTransitionTime)\n        {\n            CurrentWeather = TargetWeather;\n            bIsTransitioning = false;\n        }\n    }\n}\n\nvoid AWeatherSystemActor::UpdateWeatherParameters(EWeatherType CurrentWeather)\n{\n    // Update rendering features, particle systems, audio, etc., based on current weather\n    // For example:\n    switch (CurrentWeather)\n    {\n        case EWeatherType::Sunny:\n            // Set sunny skybox, lighting, etc.\n            break;\n        case EWeatherType::Rainy:\n            // Activate rain particle systems, darken sky, etc.\n            break;\n        case EWeatherType::Snowy:\n            // Activate snow particle systems, adjust lighting, etc.\n            break;\n        // Add more weather types as needed\n    }\n}\n\n// Assuming enumeration for weather types\nUENUM(BlueprintType)\nenum class EWeatherType : uint8\n{\n    Sunny,\n    Rainy,\n    Snowy,\n    // Add more weather types as needed\n};\n```\n\n```cpp\n// Blueprint Integration: \n// 1. Use \"Add Code to Project\" in Unreal to add the above classes.\n// 2. Create a Weather System actor in your level.\n// 3. Use the `TransitionWeather` node in Blueprints to initiate transitions specifying NewWeather and TransitionTime.\n```",
    "label": 1
  },
  {
    "codes": "```julia\n# Import necessary libraries\nusing VoronoiDelaunay\nusing Plots\n\n# Function to generate Voronoi diagram from random points\nfunction generate_voronoi(n_points::Int)\n    # Generate random points\n    points = [(rand(), rand()) for _ in 1:n_points]\n\n    # Create a Voronoi diagram\n    vd = voronoi(points)\n\n    # Plot the Voronoi diagram\n    plot()\n    for edge in eachedge(vd)\n        plot!([points[edge.src][1], points[edge.dst][1]], [points[edge.src][2], points[edge.dst][2]], \n              seriestype = :line, lw = 2, color = :blue)\n    end\n\n    # Plot points\n    scatter!([p[1] for p in points], [p[2] for p in points], color=:red, label=\"Points\")\n\n    # Display the plot\n    display(plot)\nend\n\n# Generate a Voronoi diagram for 10 random points\ngenerate_voronoi(10)\n```\n",
    "label": 1
  },
  {
    "codes": "```python\nimport os\nimport tempfile\nfrom contextlib import contextmanager\n\n@contextmanager\ndef atomic_write(file_path, mode='w', **kwargs):\n    # Create a temporary file with a .tmp suffix\n    temp_file_path = f\"{file_path}.tmp\"\n    try:\n        # Open the temporary file\n        with open(temp_file_path, mode, **kwargs) as temp_file:\n            yield temp_file  # Yield the file object to the caller\n        # If no exceptions occurred, rename the temp file to the target file\n        os.replace(temp_file_path, file_path)\n    except Exception:\n        # If an exception occurred, remove the temporary file\n        if os.path.exists(temp_file_path):\n            os.remove(temp_file_path)\n        raise  # Re-raise the exception\n```",
    "label": 1
  },
  {
    "codes": "function fixOnBandTicksCoords(axis, ticksCoords, tickCategoryInterval, alignWithLabel, clamp) {\n    var ticksLen = ticksCoords.length;\n\n    if (!axis.onBand || alignWithLabel || !ticksLen) {\n        return;\n    }\n\n    var axisExtent = axis.getExtent();\n    var last;\n    if (ticksLen === 1) {\n        ticksCoords[0].coord = axisExtent[0];\n        last = ticksCoords[1] = {coord: axisExtent[0]};\n    }\n    else {\n        var shift = (ticksCoords[1].coord - ticksCoords[0].coord);\n        each(ticksCoords, function (ticksItem) {\n            ticksItem.coord -= shift / 2;\n            var tickCategoryInterval = tickCategoryInterval || 0;\n            // Avoid split a single data item when odd interval.\n            if (tickCategoryInterval % 2 > 0) {\n                ticksItem.coord -= shift / ((tickCategoryInterval + 1) * 2);\n            }\n        });\n        last = {coord: ticksCoords[ticksLen - 1].coord + shift};\n        ticksCoords.push(last);\n    }\n\n    var inverse = axisExtent[0] > axisExtent[1];\n\n    if (littleThan(ticksCoords[0].coord, axisExtent[0])) {\n        clamp ? (ticksCoords[0].coord = axisExtent[0]) : ticksCoords.shift();\n    }\n    if (clamp && littleThan(axisExtent[0], ticksCoords[0].coord)) {\n        ticksCoords.unshift({coord: axisExtent[0]});\n    }\n    if (littleThan(axisExtent[1], last.coord)) {\n        clamp ? (last.coord = axisExtent[1]) : ticksCoords.pop();\n    }\n    if (clamp && littleThan(last.coord, axisExtent[1])) {\n        ticksCoords.push({coord: axisExtent[1]});\n    }\n\n    function littleThan(a, b) {\n        return inverse ? a > b : a < b;\n    }\n}",
    "label": 0
  },
  {
    "codes": "def _incr_exceptions(self):\n        \"\"\"Increment the number of exceptions for the current connection.\"\"\"\n        self._pool_manager.get_connection(self.pid, self._conn).exceptions += 1",
    "label": 0
  },
  {
    "codes": "```csharp\npublic static class StringExtensions\n{\n    // Truncate the string to a maximum length, appending ellipsis if trimmed.\n    public static string TruncateWithEllipsis(this string value, int maxLength)\n    {\n        if (maxLength < 0)\n            throw new ArgumentOutOfRangeException(nameof(maxLength), \"maxLength must be greater than or equal to 0.\");\n        \n        // Check if truncation is necessary\n        if (string.IsNullOrEmpty(value) || value.Length <= maxLength)\n            return value;\n        \n        // Determine the length for the ellipsis\n        const string ellipsis = \"...\";\n        int truncatedLength = maxLength - ellipsis.Length;\n        \n        // Ensure truncatedLength is not negative\n        if (truncatedLength < 0)\n            throw new ArgumentOutOfRangeException(nameof(maxLength), \"maxLength is too small to contain even the ellipsis.\");\n        \n        // Return the truncated string with ellipsis\n        return value.Substring(0, truncatedLength) + ellipsis;\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function getThemeProps(params) {\n  const { theme, name, props } = params;\n\n  if (!theme || !theme.props || !theme.props[name]) {\n    return props;\n  }\n\n  // Resolve default props, code borrow from React source.\n  // https://github.com/facebook/react/blob/15a8f031838a553e41c0b66eb1bcf1da8448104d/packages/react/src/ReactElement.js#L221\n  const defaultProps = theme.props[name];\n  let propName;\n\n  for (propName in defaultProps) {\n    if (props[propName] === undefined) {\n      props[propName] = defaultProps[propName];\n    }\n  }\n\n  return props;\n}",
    "label": 0
  },
  {
    "codes": "func (s *GetPartitionInput) SetTableName(v string) *GetPartitionInput {\n\ts.TableName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n)\n\n// Function to perform matrix multiplication\nfunc multiplyMatrices(a, b [][]int) [][]int {\n\tn := len(a)\n\tresult := make([][]int, n)\n\tfor i := range result {\n\t\tresult[i] = make([]int, n)\n\t\tfor j := range result[i] {\n\t\t\tfor k := 0; k < n; k++ {\n\t\t\t\tresult[i][j] += a[i][k] * b[k][j]\n\t\t\t}\n\t\t}\n\t}\n\treturn result\n}\n\n// Function to perform matrix exponentiation\nfunc matrixExponentiation(matrix [][]int, power int) [][]int {\n\tn := len(matrix)\n\t// Create identity matrix of the same size\n\tresult := make([][]int, n)\n\tfor i := range result {\n\t\tresult[i] = make([]int, n)\n\t\tresult[i][i] = 1\n\t}\n\n\t// Copy the matrix to avoid modifying the original\n\tbase := make([][]int, n)\n\tfor i := range matrix {\n\t\tbase[i] = make([]int, n)\n\t\tcopy(base[i], matrix[i])\n\t}\n\n\t// Exponentiation by squaring\n\tfor power > 0 {\n\t\tif power%2 == 1 {\n\t\t\tresult = multiplyMatrices(result, base)\n\t\t}\n\t\tbase = multiplyMatrices(base, base)\n\t\tpower /= 2\n\t}\n\treturn result\n}\n\nfunc main() {\n\tmatrix := [][]int{\n\t\t{1, 1},\n\t\t{1, 0},\n\t}\n\tpower := 5\n\tresult := matrixExponentiation(matrix, power)\n\tfmt.Println(\"Resultant matrix:\")\n\tfor _, row := range result {\n\t\tfmt.Println(row)\n\t}\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```cpp\n// debug_memory_leak.cpp\n#include <iostream>\n\nint main() {\n    // Simulating a memory leak\n    int* leakArray = new int[10];\n    // Forgot to delete leakArray, causing a memory leak\n\n    std::cout << \"Memory leak example\" << std::endl;\n\n    return 0;\n}\n```\n\n```shell\n# Build the program with debugging information\ng++ -g -o debug_memory_leak debug_memory_leak.cpp\n\n# Run Valgrind to detect memory leaks\nvalgrind --leak-check=full --track-origins=yes --show-leak-kinds=all ./debug_memory_leak\n\n# Expected Valgrind output:\n# ...\n# ==XXXX== LEAK SUMMARY:\n# ==XXXX==    definitely lost: N bytes in 1 blocks\n# ==XXXX==    indirectly lost: 0 bytes in 0 blocks\n# ==XXXX==      possibly lost: 0 bytes in 0 blocks\n# ==XXXX==    still reachable: 0 bytes in 0 blocks\n# ==XXXX==         suppressed: 0 bytes in 0 blocks\n# ...\n```",
    "label": 1
  },
  {
    "codes": "```ini\n; PlatformIO configuration for cross-compiling to multiple MCUs\n\n[env]\nplatform = ststm32\nframework = stm32cube\n\n; Common settings for all environments\nbuild_flags =\n    -DDEBUG\n    -DUSE_FULL_LL_DRIVER\nlib_deps =\n    stm32/STM32CubeF4@^1.25.0\n\n; Environment for STM32F4 Discovery board\n[env:discovery_f4]\nboard = disco_f407vg\nplatform = ststm32\nframework = stm32cube\nbuild_flags =\n    ${env.build_flags}\n    -DSTM32F407xx\nlib_deps =\n    ${env.lib_deps}\n\n; Environment for STM32F7 Discovery board\n[env:discovery_f7]\nboard = disco_f746ng\nplatform = ststm32\nframework = stm32cube\nbuild_flags =\n    ${env.build_flags}\n    -DSTM32F746xx\nlib_deps =\n    ${env.lib_deps}\n\n; Environment for STM32H7 Discovery board\n[env:discovery_h7]\nboard = disco_h743zi\nplatform = ststm32\nframework = stm32cube\nbuild_flags =\n    ${env.build_flags}\n    -DSTM32H743xx\nlib_deps =\n    ${env.lib_deps}\n\n; Environment for STM32G0 Nucleo board\n[env:nucleo_g0]\nboard = nucleo_g071rb\nplatform = ststm32\nframework = stm32cube\nbuild_flags =\n    ${env.build_flags}\n    -DSTM32G071xx\nlib_deps =\n    ${env.lib_deps}\n\n; Environment for STM32L4 Nucleo board\n[env:nucleo_l4]\nboard = nucleo_l476rg\nplatform = ststm32\nframework = stm32cube\nbuild_flags =\n    ${env.build_flags}\n    -DSTM32L476xx\nlib_deps =\n    ${env.lib_deps}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nconst fs = require('fs');\nconst { Transform } = require('stream');\nconst csv = require('csv-parser');\n\n// Create a readable stream from the CSV file\nconst readStream = fs.createReadStream('input.csv');\n\n// Create a writable stream to save the NDJSON output\nconst writeStream = fs.createWriteStream('output.ndjson');\n\n// Transform stream to convert CSV rows to NDJSON\nconst csvToNdjson = new Transform({\n  objectMode: true,\n  transform(chunk, encoding, callback) {\n    // Convert the CSV row (chunk) to NDJSON format\n    const ndjson = JSON.stringify(chunk) + '\\n';\n    this.push(ndjson);\n    callback();\n  }\n});\n\n// Pipeline: Read CSV -> Parse CSV -> Convert to NDJSON -> Write to file\nreadStream\n  .pipe(csv()) // Parse CSV into objects\n  .pipe(csvToNdjson) // Convert each object to NDJSON\n  .pipe(writeStream) // Write NDJSON to output file\n  .on('finish', () => console.log('CSV to NDJSON conversion complete!'))\n  .on('error', (err) => console.error('Error in pipeline:', err));\n```",
    "label": 1
  },
  {
    "codes": "```rust\n// Function to calculate the Levenshtein distance between two strings\nfn levenshtein_distance(s1: &str, s2: &str) -> usize {\n    let len1 = s1.chars().count();\n    let len2 = s2.chars().count();\n    \n    // Create a 2D vector to store distances\n    let mut dp = vec![vec![0; len2 + 1]; len1 + 1];\n    \n    // Initialize the first row and column\n    for i in 0..=len1 {\n        dp[i][0] = i;\n    }\n    for j in 0..=len2 {\n        dp[0][j] = j;\n    }\n    \n    // Fill the DP table\n    for (i, char1) in s1.chars().enumerate() {\n        for (j, char2) in s2.chars().enumerate() {\n            let cost = if char1 == char2 { 0 } else { 1 };\n            dp[i + 1][j + 1] = (dp[i][j + 1] + 1)\n                .min(dp[i + 1][j] + 1)\n                .min(dp[i][j] + cost);\n        }\n    }\n    \n    // Return the final distance\n    dp[len1][len2]\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom flask import Flask, request, jsonify\nfrom PIL import Image\nimport numpy as np\nimport tensorflow as tf\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Load pre-trained model\nmodel = tf.keras.models.load_model('path_to_pretrained_model.h5')\n\n# Define image preprocessing function\ndef preprocess_image(image):\n    image = image.resize((224, 224))  # Resize to model's expected input size\n    image = np.array(image) / 255.0  # Normalize pixel values\n    image = np.expand_dims(image, axis=0)  # Add batch dimension\n    return image\n\n# Define API endpoint for image classification\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n    \n    file = request.files['file']\n    try:\n        image = Image.open(file.stream)\n        processed_image = preprocess_image(image)\n        predictions = model.predict(processed_image)\n        predicted_class = np.argmax(predictions, axis=1)[0]\n        return jsonify({'class': int(predicted_class)})\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n# Run the Flask app\nif __name__ == '__main__':\n    app.run(debug=True)\n```",
    "label": 1
  },
  {
    "codes": "```typescript\nimport { Injectable } from '@angular/core';\nimport { HttpClient, HttpParams } from '@angular/common/http';\nimport { Observable } from 'rxjs';\n\n@Injectable({\n  providedIn: 'root',\n})\nexport class DataService {\n  private apiUrl = 'https://api.example.com/data'; // Base URL of the API\n\n  constructor(private http: HttpClient) {}\n\n  /**\n   * Fetch paginated data from the API.\n   * @param page The page number to fetch.\n   * @param size The number of items per page.\n   * @returns An Observable of the paginated data.\n   */\n  fetchPaginatedData(page: number, size: number): Observable<any> {\n    // Create HttpParams object to send page and size as query parameters\n    const params = new HttpParams()\n      .set('page', page.toString())\n      .set('size', size.toString());\n\n    // Send GET request to the API with pagination parameters\n    return this.http.get<any>(this.apiUrl, { params });\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "def handle_internal_commands(command):\n    \"\"\"Run repl-internal commands.\n\n    Repl-internal commands are all commands starting with \":\".\n\n    \"\"\"\n    if command.startswith(\":\"):\n        target = _get_registered_target(command[1:], default=None)\n        if target:\n            return target()",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<SyncAgentLinkedDatabaseInner>>> listLinkedDatabasesNextSinglePageAsync(final String nextPageLink) {\n        if (nextPageLink == null) {\n            throw new IllegalArgumentException(\"Parameter nextPageLink is required and cannot be null.\");\n        }\n        String nextUrl = String.format(\"%s\", nextPageLink);\n        return service.listLinkedDatabasesNext(nextUrl, this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<Page<SyncAgentLinkedDatabaseInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<SyncAgentLinkedDatabaseInner>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<PageImpl<SyncAgentLinkedDatabaseInner>> result = listLinkedDatabasesNextDelegate(response);\n                        return Observable.just(new ServiceResponse<Page<SyncAgentLinkedDatabaseInner>>(result.body(), result.response()));\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom flask import Flask, request, jsonify\nimport torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport io\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Load pre-trained model (e.g., ResNet18)\nmodel = models.resnet18(pretrained=True)\nmodel.eval()  # Set model to evaluation mode\n\n# Define image preprocessing\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load ImageNet class labels\nwith open('imagenet_classes.txt') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Define API endpoint for image classification\n@app.route('/predict', methods=['POST'])\ndef predict():\n    # Check if an image file is provided\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    # Read the image file\n    file = request.files['file']\n    image = Image.open(io.BytesIO(file.read()))\n\n    # Preprocess the image\n    input_tensor = preprocess(image)\n    input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n\n    # Perform inference\n    with torch.no_grad():\n        output = model(input_batch)\n\n    # Get predicted class\n    _, predicted_idx = torch.max(output, 1)\n    predicted_label = labels[predicted_idx.item()]\n\n    # Return prediction as JSON\n    return jsonify({'prediction': predicted_label})\n\n# Run the Flask app\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n```",
    "label": 1
  },
  {
    "codes": "public Observable<WorkflowRunActionRepetitionDefinitionInner> getAsync(String resourceGroupName, String workflowName, String runName, String actionName, String repetitionName) {\n        return getWithServiceResponseAsync(resourceGroupName, workflowName, runName, actionName, repetitionName).map(new Func1<ServiceResponse<WorkflowRunActionRepetitionDefinitionInner>, WorkflowRunActionRepetitionDefinitionInner>() {\n            @Override\n            public WorkflowRunActionRepetitionDefinitionInner call(ServiceResponse<WorkflowRunActionRepetitionDefinitionInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "def get_active_pitch_range(self):\n        \"\"\"\n        Return the active pitch range as a tuple (lowest, highest).\n\n        Returns\n        -------\n        lowest : int\n            The lowest active pitch in the pianoroll.\n        highest : int\n            The highest active pitch in the pianoroll.\n\n        \"\"\"\n        if self.pianoroll.shape[1] < 1:\n            raise ValueError(\"Cannot compute the active pitch range for an \"\n                             \"empty pianoroll\")\n        lowest = 0\n        highest = 127\n        while lowest < highest:\n            if np.any(self.pianoroll[:, lowest]):\n                break\n            lowest += 1\n        if lowest == highest:\n            raise ValueError(\"Cannot compute the active pitch range for an \"\n                             \"empty pianoroll\")\n        while not np.any(self.pianoroll[:, highest]):\n            highest -= 1\n\n        return lowest, highest",
    "label": 0
  },
  {
    "codes": "```julia\n# Import necessary packages\nusing VoronoiDelaunay\nusing Plots\n\n# Generate random points\nn = 50  # Number of points\npoints = rand(n, 2)  # Random 2D points\n\n# Create Voronoi diagram\ntess = VoronoiTessellation(points)\n\n# Plot the Voronoi diagram\nplot(tess, legend=false, aspect_ratio=:equal, title=\"Voronoi Diagram from Random Points\")\n```\n\n```julia\n# Alternative approach using VoronoiCells package\nusing VoronoiCells\nusing Plots\n\n# Generate random points\nn = 50  # Number of points\npoints = rand(n, 2)  # Random 2D points\n\n# Create Voronoi diagram\nvoronoi = voronoicells(points)\n\n# Plot the Voronoi diagram\nplot(voronoi, legend=false, aspect_ratio=:equal, title=\"Voronoi Diagram from Random Points\")\n```\n\n```julia\n# Another approach using GeometryBasics and VoronoiDelaunay\nusing GeometryBasics\nusing VoronoiDelaunay\nusing Plots\n\n# Generate random points\nn = 50  # Number of points\npoints = [Point2(rand(), rand()) for _ in 1:n]\n\n# Create Voronoi diagram\ntess = VoronoiTessellation(points)\n\n# Plot the Voronoi diagram\nplot(tess, legend=false, aspect_ratio=:equal, title=\"Voronoi Diagram from Random Points\")\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nconst express = require('express');\nconst app = express();\napp.use(express.json());\n\n// Mock user data\nconst users = [\n    { id: 1, username: 'user1', password: 'pass1' },\n    { id: 2, username: 'user2', password: 'pass2' }\n];\n\n// Login endpoint\napp.post('/login', (req, res) => {\n    const { username, password } = req.body;\n\n    // Check if username and password are provided\n    if (!username || !password) {\n        return res.status(400).json({ message: 'Username and password are required' });\n    }\n\n    // Find user in mock data\n    const user = users.find(u => u.username === username && u.password === password);\n\n    // If user not found\n    if (!user) {\n        return res.status(401).json({ message: 'Invalid credentials' });\n    }\n\n    // Successful login\n    res.status(200).json({ message: 'Login successful', user });\n});\n\n// Start server\napp.listen(3000, () => {\n    console.log('Server is running on port 3000');\n});\n```",
    "label": 1
  },
  {
    "codes": "def update_from_json(self, data):\n        \"\"\"\n        Update the attributes of this :class:`Item` using the values obtained\n        from the dictionary `data`.\n\n        The format of `data` should be the same as the format returned by\n        :meth:`export_as_json`.\n        \"\"\"\n        self.subscription = data.get(\"subscription\", \"none\")\n        self.approved = bool(data.get(\"approved\", False))\n        self.ask = data.get(\"ask\", None)\n        self.name = data.get(\"name\", None)\n        self.groups = set(data.get(\"groups\", []))",
    "label": 0
  },
  {
    "codes": "func (s *ListPartsOutput) SetParts(v []*PartListElement) *ListPartsOutput {\n\ts.Parts = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function ignoreMomentLocale(webpackConfig) {\n  delete webpackConfig.module.noParse;\n  webpackConfig.plugins.push(new webpack.IgnorePlugin(/^\\.\\/locale$/, /moment$/));\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define a custom CNN model\nclass CustomCNN(nn.Module):\n    def __init__(self):\n        super(CustomCNN, self).__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        \n        # Max pooling layers\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 10)  # 10 classes for CIFAR-10\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(0.25)\n    \n    def forward(self, x):\n        # Apply convolutional layers with ReLU and pooling\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        \n        # Flatten the tensor\n        x = x.view(-1, 128 * 4 * 4)\n        \n        # Apply fully connected layers with dropout\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        \n        return x\n\n# Instantiate the model\nmodel = CustomCNN()\n\n# Print the model architecture\nprint(model)\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse serde_json::Value;\nuse apache_avro::Schema;\nuse apache_avro::types::Record;\nuse apache_avro::Writer;\nuse std::fs::File;\nuse std::io::BufReader;\n\nfn main() {\n    // Load JSON file\n    let file = File::open(\"data.json\").expect(\"Failed to open JSON file\");\n    let reader = BufReader::new(file);\n    let json_value: Value = serde_json::from_reader(reader).expect(\"Failed to parse JSON\");\n\n    // Define Avro schema (example schema, adjust as needed)\n    let schema_str = r#\"\n    {\n        \"type\": \"record\",\n        \"name\": \"example\",\n        \"fields\": [\n            {\"name\": \"field1\", \"type\": \"string\"},\n            {\"name\": \"field2\", \"type\": \"int\"}\n        ]\n    }\"#;\n    let schema = Schema::parse_str(schema_str).expect(\"Failed to parse Avro schema\");\n\n    // Convert JSON to Avro Record\n    let mut record = Record::new(&schema).expect(\"Failed to create Avro record\");\n    if let Value::Object(map) = json_value {\n        for (key, value) in map {\n            match key.as_str() {\n                \"field1\" => {\n                    if let Value::String(s) = value {\n                        record.put(\"field1\", s).expect(\"Failed to put field1\");\n                    }\n                }\n                \"field2\" => {\n                    if let Value::Number(n) = value {\n                        if let Some(i) = n.as_i64() {\n                            record.put(\"field2\", i).expect(\"Failed to put field2\");\n                        }\n                    }\n                }\n                _ => {}\n            }\n        }\n    }\n\n    // Write Avro record to file\n    let avro_file = File::create(\"output.avro\").expect(\"Failed to create Avro file\");\n    let mut writer = Writer::new(&schema, avro_file);\n    writer.append(record).expect(\"Failed to append record\");\n    writer.flush().expect(\"Failed to flush writer\");\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<LuisResult>> resolveWithServiceResponseAsync(String appId, String query, ResolveOptionalParameter resolveOptionalParameter) {\n        if (this.client.endpoint() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.endpoint() is required and cannot be null.\");\n        }\n        if (appId == null) {\n            throw new IllegalArgumentException(\"Parameter appId is required and cannot be null.\");\n        }\n        if (query == null) {\n            throw new IllegalArgumentException(\"Parameter query is required and cannot be null.\");\n        }\n        final Double timezoneOffset = resolveOptionalParameter != null ? resolveOptionalParameter.timezoneOffset() : null;\n        final Boolean verbose = resolveOptionalParameter != null ? resolveOptionalParameter.verbose() : null;\n        final Boolean staging = resolveOptionalParameter != null ? resolveOptionalParameter.staging() : null;\n        final Boolean spellCheck = resolveOptionalParameter != null ? resolveOptionalParameter.spellCheck() : null;\n        final String bingSpellCheckSubscriptionKey = resolveOptionalParameter != null ? resolveOptionalParameter.bingSpellCheckSubscriptionKey() : null;\n        final Boolean log = resolveOptionalParameter != null ? resolveOptionalParameter.log() : null;\n\n        return resolveWithServiceResponseAsync(appId, query, timezoneOffset, verbose, staging, spellCheck, bingSpellCheckSubscriptionKey, log);\n    }",
    "label": 0
  },
  {
    "codes": "```typescript\ninterface GeometricShape {\n  // Method to calculate the area of the shape\n  calculateArea(): number;\n}\n\n// Example of a rectangle implementing GeometricShape\nclass Rectangle implements GeometricShape {\n  constructor(public width: number, public height: number) {}\n\n  // Calculate the area of the rectangle\n  calculateArea(): number {\n    return this.width * this.height;\n  }\n}\n\n// Example of a circle implementing GeometricShape\nclass Circle implements GeometricShape {\n  constructor(public radius: number) {}\n\n  // Calculate the area of the circle\n  calculateArea(): number {\n    return Math.PI * this.radius * this.radius;\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "def close(self, connection, reason='Closed via management api'):\n        \"\"\"Close Connection.\n\n        :param str connection: Connection name\n        :param str reason: Reason for closing connection.\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: None\n        \"\"\"\n        close_payload = json.dumps({\n            'name': connection,\n            'reason': reason\n        })\n        connection = quote(connection, '')\n        return self.http_client.delete(API_CONNECTION % connection,\n                                       payload=close_payload,\n                                       headers={\n                                           'X-Reason': reason\n                                       })",
    "label": 0
  },
  {
    "codes": "function commit() {\n\t\tconsole.log( \"Adding files to dist...\" );\n\t\tRelease.exec( \"git add -A\", \"Error adding files.\" );\n\t\tRelease.exec(\n\t\t\t\"git commit -m \\\"Release \" + Release.newVersion + \"\\\"\",\n\t\t\t\"Error committing files.\"\n\t\t);\n\t\tconsole.log();\n\n\t\tconsole.log( \"Tagging release on dist...\" );\n\t\tRelease.exec( \"git tag \" + Release.newVersion,\n\t\t\t\"Error tagging \" + Release.newVersion + \" on dist repo.\" );\n\t\tRelease.tagTime = Release.exec( \"git log -1 --format=\\\"%ad\\\"\",\n\t\t\t\"Error getting tag timestamp.\" ).trim();\n\t}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"testing\"\n\n\t\"github.com/tidwall/gjson\"\n)\n\n// Sample JSON data\nvar jsonData = []byte(`{\"name\":\"John\",\"age\":30,\"city\":\"New York\"}`)\n\n// Benchmark for standard library encoding/json\nfunc BenchmarkStdJSON(b *testing.B) {\n\tfor i := 0; i < b.N; i++ {\n\t\tvar result map[string]interface{}\n\t\t_ = json.Unmarshal(jsonData, &result)\n\t}\n}\n\n// Benchmark for gjson library\nfunc BenchmarkGJSON(b *testing.B) {\n\tfor i := 0; i < b.N; i++ {\n\t\t_ = gjson.GetBytes(jsonData, \"name\")\n\t}\n}\n```\n\n```go\n// Run benchmarks with: go test -bench=.\n```",
    "label": 1
  },
  {
    "codes": "def xml(self, attribs = None,elements = None, skipchildren = False):\n        \"\"\"See :meth:`AbstractElement.xml`\"\"\"\n        if not attribs: attribs = {}\n        E = ElementMaker(namespace=\"http://ilk.uvt.nl/folia\",nsmap={None: \"http://ilk.uvt.nl/folia\", 'xml' : \"http://www.w3.org/XML/1998/namespace\"})\n        e = super(AbstractSpanAnnotation,self).xml(attribs, elements, True)\n        for child in self:\n            if isinstance(child, (Word, Morpheme, Phoneme)):\n                #Include REFERENCES to word items instead of word items themselves\n                attribs['{' + NSFOLIA + '}id'] = child.id\n                if child.PRINTABLE and child.hastext(self.textclass):\n                    attribs['{' + NSFOLIA + '}t'] = child.text(self.textclass)\n                e.append( E.wref(**attribs) )\n            elif not (isinstance(child, Feature) and child.SUBSET): #Don't add pre-defined features, they are already added as attributes\n                e.append( child.xml() )\n        return e",
    "label": 0
  },
  {
    "codes": "```yaml\n---\n- name: Deploy web application\n  hosts: web_servers\n  become: yes\n\n  vars:\n    app_name: my_web_app\n    app_repo_url: \"https://repository.example.com/my_web_app.git\"\n    app_dest: \"/var/www/my_web_app\"\n\n  tasks:\n\n    - name: Ensure git is present\n      ansible.builtin.package:\n        name: git\n        state: present\n\n    - name: Clone the web application repo\n      ansible.builtin.git:\n        repo: \"{{ app_repo_url }}\"\n        dest: \"{{ app_dest }}\"\n        version: master\n        force: yes\n\n    - name: Install application dependencies\n      ansible.builtin.shell:\n        cmd: \"pip install -r requirements.txt\"\n        chdir: \"{{ app_dest }}\"\n\n    - name: Configure application\n      ansible.builtin.template:\n        src: templates/config.j2\n        dest: \"{{ app_dest }}/config.py\"\n\n    - name: Ensure web service is started and enabled\n      ansible.builtin.systemd:\n        name: \"{{ app_name }}\"\n        state: started\n        enabled: true\n\n    - name: Open firewall port for HTTP\n      ansible.builtin.firewalld:\n        port: 80/tcp\n        permanent: true\n        state: enabled\n        immediate: yes\n\n    - name: Reload firewall to apply changes\n      ansible.builtin.firewalld:\n        state: reloaded\n```",
    "label": 1
  },
  {
    "codes": "```dart\n// Function to format timestamps into human-readable relative time\nString formatRelativeTime(DateTime timestamp) {\n  final now = DateTime.now();\n  final difference = now.difference(timestamp);\n\n  if (difference.inDays > 365) {\n    final years = (difference.inDays / 365).floor();\n    return '$years year${years > 1 ? 's' : ''} ago';\n  } else if (difference.inDays > 30) {\n    final months = (difference.inDays / 30).floor();\n    return '$months month${months > 1 ? 's' : ''} ago';\n  } else if (difference.inDays > 0) {\n    return '${difference.inDays} day${difference.inDays > 1 ? 's' : ''} ago';\n  } else if (difference.inHours > 0) {\n    return '${difference.inHours} hour${difference.inHours > 1 ? 's' : ''} ago';\n  } else if (difference.inMinutes > 0) {\n    return '${difference.inMinutes} minute${difference.inMinutes > 1 ? 's' : ''} ago';\n  } else {\n    return 'just now';\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "def select(self):\n        \"\"\"Enable standard transaction mode.\n\n            This will enable transaction mode on the channel. Meaning that\n            messages will be kept in the remote server buffer until such a\n            time that either commit or rollback is called.\n\n        :return:\n        \"\"\"\n        self._tx_active = True\n        return self._channel.rpc_request(specification.Tx.Select())",
    "label": 0
  },
  {
    "codes": "async function markVersions () {\n  const packages = JSON.parse(\n    (await execa(require.resolve('lerna/cli'), ['list', '--json'])).stdout\n  ).filter(p => !p.private)\n  const markerPath = path.resolve(__dirname, '../packages/vue-cli-version-marker/package.json')\n  const marker = JSON.parse(fs.readFileSync(markerPath))\n\n  const curVersion = marker.version\n  const mainVersion = require('../lerna.json').version\n\n  if (semver.prerelease(mainVersion)) {\n    marker.version = mainVersion\n  } else {\n    const releaseType = semver.diff(curVersion, mainVersion) || 'patch'\n    marker.version = semver.inc(marker.version, releaseType)\n  }\n\n  marker.devDependencies = packages.reduce((prev, pkg) => {\n    prev[pkg.name] = pkg.version\n    return prev\n  }, {})\n  fs.writeFileSync(markerPath, JSON.stringify(marker, null, 2))\n}",
    "label": 0
  },
  {
    "codes": "```kotlin\n// Extension function to reverse words in a string while preserving whitespace\nfun String.reverseWordsPreservingWhitespace(): String {\n    // Using regex to match words and collect non-word segments like whitespaces\n    val segments = Regex(\"(\\\\S+)|(\\\\s+)\").findAll(this).map { it.value }.toList()\n    \n    // Reverse only the word segments while preserving the order of non-word segments\n    val reversedSegments = segments.map { segment ->\n        if (segment.isNotBlank()) segment.reversed() else segment\n    }\n    \n    // Join the segments back together to form the final string\n    return reversedSegments.joinToString(\"\")\n}\n\n// Example usage\nfun main() {\n    val original = \"Hello   World  from Kotlin\"\n    val reversed = original.reverseWordsPreservingWhitespace()\n    println(reversed) // Output: \"olleH   dlroW  morf niltoK\"\n}\n```",
    "label": 1
  },
  {
    "codes": "def save_as_json(total: list,\n                 name='data.json',\n                 sort_by: str = None,\n                 no_duplicate=False,\n                 order='asc'):\n    \"\"\"Save what you crawled as a json file.\n\n    Args:\n        total (list): Total of data you crawled.\n        name (str, optional): Defaults to 'data.json'. The name of the file.\n        sort_by (str, optional): Defaults to None. Sort items by a specific key.\n        no_duplicate (bool, optional): Defaults to False. If True, it will remove duplicated data.\n        order (str, optional): Defaults to 'asc'. The opposite option is 'desc'.\n    \"\"\"\n    if sort_by:\n        reverse = order == 'desc'\n        total = sorted(total, key=itemgetter(sort_by), reverse=reverse)\n    if no_duplicate:\n        total = [key for key, _ in groupby(total)]\n    data = json.dumps(total, ensure_ascii=False)\n    Path(name).write_text(data, encoding='utf-8')",
    "label": 0
  },
  {
    "codes": "```java\n// Method to convert a decimal number to binary\npublic static String decimalToBinary(int decimal) {\n    // Handle edge case for 0\n    if (decimal == 0) {\n        return \"0\";\n    }\n    \n    StringBuilder binary = new StringBuilder();\n    \n    // Convert decimal to binary by dividing by 2\n    while (decimal > 0) {\n        int remainder = decimal % 2;\n        binary.insert(0, remainder); // Insert remainder at the beginning\n        decimal = decimal / 2;\n    }\n    \n    return binary.toString();\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *UpdateTrailOutput) SetSnsTopicName(v string) *UpdateTrailOutput {\n\ts.SnsTopicName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function getAddress(loopback, family) {\n  let interfaces;\n  if (loopback) {\n    let lo = getLoInterface();\n    interfaces = lo ? [lo] : null;\n  }\n  interfaces = interfaces || os.networkInterfaces();\n  for (let key in interfaces) {\n    if (!interfaces.hasOwnProperty(key)) {\n      continue;\n    }\n\n    for (let ipAddress of interfaces[key]) {\n      if (ipAddress.family === family && ipAddress.internal === loopback) {\n        return ipAddress.address;\n      }\n    }\n  }\n  return undefined;\n}",
    "label": 0
  },
  {
    "codes": "def error_future(self):\n        \"\"\"\n        Return a future which will receive the next XML stream error as\n        exception.\n\n        It is safe to cancel the future at any time.\n        \"\"\"\n        fut = asyncio.Future(loop=self._loop)\n        self._error_futures.append(fut)\n        return fut",
    "label": 0
  },
  {
    "codes": "```python\nimport hmac\nimport hashlib\n\ndef secure_compare_hmac(digest1, digest2):\n    \"\"\"\n    Securely compare two HMAC digests in constant time to avoid timing attacks.\n    \n    :param digest1: First HMAC digest (bytes or str)\n    :param digest2: Second HMAC digest (bytes or str)\n    :return: True if digests match, False otherwise\n    \"\"\"\n    # Ensure both inputs are bytes\n    if isinstance(digest1, str):\n        digest1 = digest1.encode('utf-8')\n    if isinstance(digest2, str):\n        digest2 = digest2.encode('utf-8')\n    \n    # Use hmac.compare_digest for constant-time comparison\n    return hmac.compare_digest(digest1, digest2)\n```",
    "label": 1
  },
  {
    "codes": "def make_parser():\n    \"\"\"\n    Create a parser which is suitably configured for parsing an XMPP XML\n    stream. It comes equipped with :class:`XMPPLexicalHandler`.\n    \"\"\"\n    p = xml.sax.make_parser()\n    p.setFeature(xml.sax.handler.feature_namespaces, True)\n    p.setFeature(xml.sax.handler.feature_external_ges, False)\n    p.setProperty(xml.sax.handler.property_lexical_handler,\n                  XMPPLexicalHandler)\n    return p",
    "label": 0
  },
  {
    "codes": "def get_upscaled_value_tuple(self):\n        \"\"\"\n        Scales an RGB color object from decimal 0.0-1.0 to int 0-255.\n        \"\"\"\n        # Scale up to 0-255 values.\n        rgb_r = int(math.floor(0.5 + self.rgb_r * 255))\n        rgb_g = int(math.floor(0.5 + self.rgb_g * 255))\n        rgb_b = int(math.floor(0.5 + self.rgb_b * 255))\n\n        return rgb_r, rgb_g, rgb_b",
    "label": 0
  },
  {
    "codes": "```csharp\n// Assuming 'products' is a collection of Product objects with properties 'Price' and 'Category'\n\n// Filter products by price range (e.g., between 50 and 200) and category (e.g., \"Electronics\")\nvar filteredProducts = products\n    .Where(p => p.Price >= 50 && p.Price <= 200 && p.Category == \"Electronics\")\n    .ToList();\n\n// filteredProducts now contains products within the specified price range and category\n```",
    "label": 1
  },
  {
    "codes": "public Observable<Page<BillingMeterInner>> listAsync() {\n        return listWithServiceResponseAsync()\n            .map(new Func1<ServiceResponse<Page<BillingMeterInner>>, Page<BillingMeterInner>>() {\n                @Override\n                public Page<BillingMeterInner> call(ServiceResponse<Page<BillingMeterInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "Certainly! Below is a simple implementation of a REST API in JavaScript using Express.js that handles user login with only response codes and comments:\n\n```javascript\nconst express = require('express');\nconst bodyParser = require('body-parser');\n\nconst app = express();\napp.use(bodyParser.json());\n\n// Mock user data\nconst users = [\n  { username: 'user1', password: 'pass1' }\n];\n\n// Login endpoint\napp.post('/login', (req, res) => {\n  const { username, password } = req.body;\n\n  // Check if request body contains username and password\n  if (!username || !password) {\n    // 400 Bad Request\n    return res.status(400).send();\n  }\n\n  // Find user by username\n  const user = users.find(u => u.username === username);\n\n  if (!user) {\n    // 404 Not Found if user does not exist\n    return res.status(404).send();\n  }\n\n  // Check if password matches\n  if (user.password !== password) {\n    // 401 Unauthorized if password is incorrect\n    return res.status(401).send();\n  }\n\n  // 200 OK if login is successful\n  res.status(200).send();\n});\n\n// Set up server\nconst PORT = process.env.PORT || 3000;\napp.listen(PORT, () => {\n  console.log(`Server running on port ${PORT}`);\n});\n```\n\nPlease make sure to install Express by running `npm install express` before executing the code.",
    "label": 1
  },
  {
    "codes": "```csharp\n// Assuming we have a Product class and a list of products\npublic class Product\n{\n    public string Name { get; set; }\n    public decimal Price { get; set; }\n    public string Category { get; set; }\n}\n\n// Sample list of products\nList<Product> products = new List<Product>\n{\n    new Product { Name = \"Product1\", Price = 20.0m, Category = \"Electronics\" },\n    new Product { Name = \"Product2\", Price = 35.5m, Category = \"Clothing\" },\n    new Product { Name = \"Product3\", Price = 15.0m, Category = \"Electronics\" },\n    new Product { Name = \"Product4\", Price = 50.0m, Category = \"Clothing\" },\n};\n\n// Define the price range and category\ndecimal minPrice = 10.0m;\ndecimal maxPrice = 40.0m;\nstring category = \"Electronics\";\n\n// LINQ query to filter products\nvar filteredProducts = from product in products\n                       where product.Price >= minPrice && product.Price <= maxPrice\n                             && product.Category == category\n                       select product;\n\n// Output the filtered products\nforeach (var product in filteredProducts)\n{\n    Console.WriteLine($\"{product.Name}: {product.Price} - {product.Category}\");\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nfunction mergeSortedArrays(arr1, arr2) {\n    let merged = [];\n    let i = 0, j = 0;\n\n    // Traverse both arrays and push the smaller element to the merged array\n    while (i < arr1.length && j < arr2.length) {\n        if (arr1[i] < arr2[j]) {\n            merged.push(arr1[i]);\n            i++;\n        } else {\n            merged.push(arr2[j]);\n            j++;\n        }\n    }\n\n    // If there are remaining elements in arr1, add them to merged\n    while (i < arr1.length) {\n        merged.push(arr1[i]);\n        i++;\n    }\n\n    // If there are remaining elements in arr2, add them to merged\n    while (j < arr2.length) {\n        merged.push(arr2[j]);\n        j++;\n    }\n\n    return merged;\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model)\n\n# Save the CoreML model to a file\ncoreml_model.save('your_coreml_model.mlmodel')\n```\n\n```python\n# Alternative method using TensorFlow 2.x and Keras\nimport tensorflow as tf\nimport coremltools as ct\n\n# Load the Keras model\nkeras_model = tf.keras.models.load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.converters.tensorflow.convert(keras_model)\n\n# Save the CoreML model to a file\ncoreml_model.save('your_coreml_model.mlmodel')\n```\n\n```python\n# Another approach using a custom input shape and output labels\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Define input and output descriptions\ninput_shape = (1, 224, 224, 3)  # Example input shape\noutput_labels = ['class1', 'class2', 'class3']  # Example output labels\n\n# Convert the Keras model to CoreML format with custom descriptions\ncoreml_model = ct.convert(\n    keras_model,\n    inputs=[ct.ImageType(shape=input_shape)],\n    classifier_config=ct.ClassifierConfig(output_labels)\n)\n\n# Save the CoreML model to a file\ncoreml_model.save('your_coreml_model.mlmodel')\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.ServiceProcess;\nusing System.Management;\nusing System.Threading;\n\npublic class USBMonitorService : ServiceBase\n{\n    private ManagementEventWatcher watcher;\n\n    public USBMonitorService()\n    {\n        this.ServiceName = \"USBMonitorService\";\n        this.CanStop = true;\n        this.CanPauseAndContinue = false;\n        this.AutoLog = true;\n    }\n\n    protected override void OnStart(string[] args)\n    {\n        // Set up a WMI query to monitor USB device connections\n        WqlEventQuery query = new WqlEventQuery(\n            \"SELECT * FROM Win32_DeviceChangeEvent WHERE EventType = 2\");\n\n        watcher = new ManagementEventWatcher(query);\n        watcher.EventArrived += new EventArrivedEventHandler(USBDeviceConnected);\n        watcher.Start();\n    }\n\n    private void USBDeviceConnected(object sender, EventArrivedEventArgs e)\n    {\n        // Log or handle the USB device connection event\n        Log(\"USB device connected.\");\n    }\n\n    protected override void OnStop()\n    {\n        watcher.Stop();\n        watcher.Dispose();\n    }\n\n    private void Log(string message)\n    {\n        // Implement logging mechanism (e.g., write to a file or event log)\n        // Example: EventLog.WriteEntry(message);\n    }\n\n    public static void Main()\n    {\n        ServiceBase.Run(new USBMonitorService());\n    }\n}\n```\n\n### Comments:\n- **WMI Query**: Monitors USB device connections using `Win32_DeviceChangeEvent`.\n- **Event Handling**: Logs or handles USB connection events in the `USBDeviceConnected` method.\n- **Logging**: Implement a logging mechanism to track USB connection events.\n- **Service Lifecycle**: Properly starts and stops the WMI event watcher.",
    "label": 1
  },
  {
    "codes": "function softwareUpdate_waitForDialogOpen(browserController) {\n    this._controller = utils.handleWindow(\"type\", \"Update:Wizard\",\n                                                   null, true);\n    this._wizard = this.getElement({type: \"wizard\"});\n\n    this._controller.waitFor(function() {\n      return this.currentPage != WIZARD_PAGES.dummy;\n    }, \"Dummy wizard page has been made invisible.\", undefined, undefined, this);\n\n    this._controller.window.focus();\n  }",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <vector>\n#include <thread>\n#include <queue>\n#include <future>\n#include <mutex>\n#include <condition_variable>\n#include <functional>\n#include <atomic>\n#include <random>\n#include <stdexcept>\n\nclass ThreadPool {\npublic:\n    ThreadPool(size_t numThreads) : stop(false) {\n        // Launch worker threads\n        for (size_t i = 0; i < numThreads; ++i) {\n            workers.emplace_back([this, i] {\n                while (true) {\n                    std::function<void()> task;\n                    {\n                        std::unique_lock<std::mutex> lock(queueMutex);\n                        condition.wait(lock, [this, i] { \n                            // Attempt to steal work if necessary\n                            return stop.load() || !tasks.empty() || tryStealWork(i, task); \n                        });\n\n                        if (stop && tasks.empty() && !task) {\n                            return; // Stop the thread\n                        }\n\n                        if (!task && !tasks.empty()) {\n                            task = std::move(tasks.front());\n                            tasks.pop();\n                        }\n                    }\n\n                    if (task) {\n                        task();\n                    }\n                }\n            });\n        }\n    }\n\n    template <typename F, typename... Args>\n    auto enqueue(F&& f, Args&&... args) -> std::future<std::result_of_t<F(Args...)>> {\n        using returnType = std::result_of_t<F(Args...)>;\n        auto task = std::make_shared<std::packaged_task<returnType()>>(\n            std::bind(std::forward<F>(f), std::forward<Args>(args)...)\n        );\n\n        std::future<returnType> res = task->get_future();\n        {\n            std::unique_lock<std::mutex> lock(queueMutex);\n            if (stop) {\n                throw std::runtime_error(\"enqueue on stopped ThreadPool\");\n            }\n            tasks.emplace([task](){ (*task)(); });\n        }\n        condition.notify_one();\n        return res;\n    }\n\n    ~ThreadPool() {\n        stop.store(true);\n        condition.notify_all();\n        for (std::thread &worker : workers) {\n            worker.join();\n        }\n    }\n\nprivate:\n    std::vector<std::thread> workers;       // Worker threads\n    std::vector<std::queue<std::function<void()>>> workerTasks; // Task queues\n    std::queue<std::function<void()>> tasks; // Main task queue\n    std::mutex queueMutex;\n    std::condition_variable condition;\n    std::atomic<bool> stop; // Indicates whether to stop the pool\n    \n    bool tryStealWork(size_t thiefIndex, std::function<void()>& task) {\n        for (size_t i = 0; i < workers.size(); ++i) {\n            if (i == thiefIndex) continue;\n            std::unique_lock<std::mutex> lock(queueMutex);\n            if (!workerTasks[i].empty()) {\n                task = std::move(workerTasks[i].front());\n                workerTasks[i].pop();\n                return true;\n            }\n        }\n        return false;\n    }\n};\n\nint main() {\n    ThreadPool pool(4);\n    auto res = pool.enqueue([](int answer) { return answer; }, 42);\n    std::cout << res.get() << std::endl;\n    return 0;\n}\n```\n",
    "label": 1
  },
  {
    "codes": "func (s *JobDescription) SetJobOutputPath(v string) *JobDescription {\n\ts.JobOutputPath = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"os\"\n    \"os/signal\"\n    \"sync\"\n    \"time\"\n\n    \"github.com/segmentio/kafka-go\"\n)\n\nconst (\n    broker1Address = \"localhost:9092\"\n    topic = \"example-topic\"\n    groupID = \"example-group\"\n)\n\n// messageProcessor simulates processing of a Kafka message\nfunc messageProcessor(message kafka.Message) error {\n    // Simulate processing time\n    time.Sleep(time.Second)\n    // For simplicity just print out the message\n    fmt.Printf(\"Processed message: %v\\n\", string(message.Value))\n    return nil\n}\n\nfunc worker(id int, messagesCh <-chan kafka.Message, wg *sync.WaitGroup) {\n    defer wg.Done()\n    for msg := range messagesCh {\n        log.Printf(\"Worker %d processing message: %s\\n\", id, string(msg.Value))\n        if err := messageProcessor(msg); err != nil {\n            log.Printf(\"Worker %d failed to process message: %v\\n\", id, err)\n        }\n    }\n}\n\nfunc main() {\n    // Setup signal handling to terminate program gracefully\n    sigChan := make(chan os.Signal, 1)\n    signal.Notify(sigChan, os.Interrupt)\n\n    // Kafka reader configuration\n    r := kafka.NewReader(kafka.ReaderConfig{\n        Brokers: []string{broker1Address},\n        GroupID: groupID,\n        Topic:   topic,\n    })\n    defer r.Close()\n\n    workers := 5\n    messagesCh := make(chan kafka.Message, 100)\n    wg := sync.WaitGroup{}\n\n    // Launching worker routines\n    for i := 0; i < workers; i++ {\n        wg.Add(1)\n        go worker(i, messagesCh, &wg)\n    }\n\n    // Main loop to read messages from Kafka and distribute to workers\n    go func() {\n        for {\n            msg, err := r.FetchMessage(context.Background())\n            if err != nil {\n                log.Println(\"Failed to fetch message:\", err)\n                break\n            }\n            messagesCh <- msg\n            if err := r.CommitMessages(context.Background(), msg); err != nil {\n                log.Println(\"Failed to commit message:\", err)\n            }\n        }\n        close(messagesCh)\n    }()\n\n    <-sigChan // Wait for a termination signal\n    close(messagesCh)\n    wg.Wait() // Wait for workers to finish processing\n}\n```",
    "label": 1
  },
  {
    "codes": "```hcl\n# Define the SageMaker model resource\nresource \"aws_sagemaker_model\" \"ml_model\" {\n  name               = \"my-ml-model\"\n  execution_role_arn = aws_iam_role.sagemaker_execution_role.arn\n\n  primary_container {\n    image = \"123456789012.dkr.ecr.us-west-2.amazonaws.com/my-ml-image:latest\"\n    model_data_url = \"s3://my-bucket/path/to/model.tar.gz\"\n  }\n}\n\n# Define the SageMaker endpoint configuration\nresource \"aws_sagemaker_endpoint_configuration\" \"endpoint_config\" {\n  name = \"my-endpoint-config\"\n\n  production_variants {\n    variant_name           = \"my-variant\"\n    model_name             = aws_sagemaker_model.ml_model.name\n    instance_type          = \"ml.m5.large\"\n    initial_instance_count = 1\n  }\n}\n\n# Define the SageMaker endpoint\nresource \"aws_sagemaker_endpoint\" \"endpoint\" {\n  name                 = \"my-endpoint\"\n  endpoint_config_name = aws_sagemaker_endpoint_configuration.endpoint_config.name\n}\n\n# Define the IAM role for SageMaker execution\nresource \"aws_iam_role\" \"sagemaker_execution_role\" {\n  name = \"sagemaker-execution-role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = \"sts:AssumeRole\"\n        Effect = \"Allow\"\n        Principal = {\n          Service = \"sagemaker.amazonaws.com\"\n        }\n      }\n    ]\n  })\n}\n\n# Attach the necessary policies to the IAM role\nresource \"aws_iam_role_policy_attachment\" \"sagemaker_policy_attachment\" {\n  role       = aws_iam_role.sagemaker_execution_role.name\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\"\n}\n\nresource \"aws_iam_role_policy_attachment\" \"s3_policy_attachment\" {\n  role       = aws_iam_role.sagemaker_execution_role.name\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport time\nfrom functools import wraps\n\ndef measure_execution_time(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        # Record start time in nanoseconds\n        start_time = time.perf_counter_ns()\n        \n        # Execute the function\n        result = func(*args, **kwargs)\n        \n        # Record end time in nanoseconds\n        end_time = time.perf_counter_ns()\n        \n        # Calculate and print execution time\n        execution_time = end_time - start_time\n        print(f\"{func.__name__} executed in {execution_time} nanoseconds\")\n        \n        return result\n    return wrapper\n```",
    "label": 1
  },
  {
    "codes": "func (s *BatchGetCrawlersOutput) SetCrawlers(v []*Crawler) *BatchGetCrawlersOutput {\n\ts.Crawlers = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public Observable<Page<HubVirtualNetworkConnectionInner>> listAsync(final String resourceGroupName, final String virtualHubName) {\n        return listWithServiceResponseAsync(resourceGroupName, virtualHubName)\n            .map(new Func1<ServiceResponse<Page<HubVirtualNetworkConnectionInner>>, Page<HubVirtualNetworkConnectionInner>>() {\n                @Override\n                public Page<HubVirtualNetworkConnectionInner> call(ServiceResponse<Page<HubVirtualNetworkConnectionInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<Page<ResourceMetricInner>> listWebWorkerMetricsAsync(final String resourceGroupName, final String name, final String workerPoolName, final Boolean details, final String filter) {\n        return listWebWorkerMetricsWithServiceResponseAsync(resourceGroupName, name, workerPoolName, details, filter)\n            .map(new Func1<ServiceResponse<Page<ResourceMetricInner>>, Page<ResourceMetricInner>>() {\n                @Override\n                public Page<ResourceMetricInner> call(ServiceResponse<Page<ResourceMetricInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```yaml\n# Horizontal Pod Autoscaler (HPA) to scale based on custom metrics (queue depth)\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: pytorch-inference-hpa\n  namespace: default\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: pytorch-inference\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Pods\n    pods:\n      metric:\n        name: queue_depth_metric # Custom metric for queue depth\n      target:\n        type: AverageValue\n        averageValue: 10 # Scale up if average queue depth per pod exceeds 10\n---\n# Deployment for PyTorch inference pods\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pytorch-inference\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pytorch-inference\n  template:\n    metadata:\n      labels:\n        app: pytorch-inference\n    spec:\n      containers:\n      - name: pytorch-inference\n        image: pytorch-inference:latest\n        resources:\n          requests:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n          limits:\n            cpu: \"1\"\n            memory: \"1Gi\"\n---\n# Service to expose the PyTorch inference pods\napiVersion: v1\nkind: Service\nmetadata:\n  name: pytorch-inference-service\n  namespace: default\nspec:\n  selector:\n    app: pytorch-inference\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n---\n# Custom Metrics API (requires a metrics server or Prometheus adapter)\n# Ensure the custom metric \"queue_depth_metric\" is exposed by your metrics solution\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import the necessary library\nfrom transformers import pipeline\n\n# Create a summarization pipeline using a pre-trained model\nsummarization_pipeline = pipeline(\"summarization\")\n\n# Example text to summarize\ntext = \"\"\"\n    Artificial Intelligence (AI) is a rapidly advancing field of technology with the potential to radically alter the\n    socioeconomic landscape. It involves the use of complex algorithms and datasets to enable machines to perform tasks\n    that typically require human intelligence. AI applications are pervasive, ranging from intelligent virtual assistants\n    like Siri and Alexa to advanced systems like autonomous vehicles and medical diagnosis tools. Researchers and\n    technologists are continually exploring new AI methodologies to improve machine learning outcomes and expand the\n    potential uses of AI in various industries.\n\"\"\"\n\n# Summarize the text using the created pipeline\nsummary = summarization_pipeline(text, max_length=50, min_length=25, do_sample=False)\n\n# Output the summary\nprint(summary[0]['summary_text'])\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport javafx.application.Application;\nimport javafx.scene.Scene;\nimport javafx.scene.canvas.Canvas;\nimport javafx.scene.canvas.GraphicsContext;\nimport javafx.scene.control.Slider;\nimport javafx.scene.layout.BorderPane;\nimport javafx.stage.Stage;\nimport javafx.scene.paint.Color;\n\npublic class FractalViewer extends Application {\n\n    private Canvas fractalCanvas;\n\n    @Override\n    public void start(Stage primaryStage) {\n        primaryStage.setTitle(\"Fractal Viewer\");\n        BorderPane root = new BorderPane();\n\n        fractalCanvas = new Canvas(800, 800);\n        drawFractal(6); // Default complexity level\n\n        Slider complexitySlider = new Slider(1, 10, 6);\n        complexitySlider.setShowTickLabels(true);\n        complexitySlider.setShowTickMarks(true);\n        complexitySlider.setMajorTickUnit(1);\n        complexitySlider.setBlockIncrement(1);\n        complexitySlider.valueProperty().addListener((obs, oldVal, newVal) -> {\n            drawFractal(newVal.intValue());\n        });\n\n        root.setCenter(fractalCanvas);\n        root.setBottom(complexitySlider);\n\n        Scene scene = new Scene(root);\n        primaryStage.setScene(scene);\n        primaryStage.show();\n    }\n\n    private void drawFractal(int complexity) {\n        GraphicsContext gc = fractalCanvas.getGraphicsContext2D();\n        gc.clearRect(0, 0, fractalCanvas.getWidth(), fractalCanvas.getHeight());\n        gc.setStroke(Color.BLACK);\n        drawSnowflake(gc, complexity, 400, 400, 300);\n    }\n\n    private void drawSnowflake(GraphicsContext gc, int level, double x, double y, double size) {\n        if (level == 0) {\n            return;\n        }\n\n        drawTriangle(gc, x, y, size);\n        double newSize = size / 2;\n\n        drawSnowflake(gc, level - 1, x, y - newSize, newSize);\n        drawSnowflake(gc, level - 1, x - newSize, y + newSize, newSize);\n        drawSnowflake(gc, level - 1, x + newSize, y + newSize, newSize);\n    }\n\n    private void drawTriangle(GraphicsContext gc, double x, double y, double size) {\n        double height = Math.sqrt(3) / 2 * size;\n        double x1 = x - size / 2;\n        double y1 = y + height / 2;\n        double x2 = x + size / 2;\n        double y2 = y + height / 2;\n        double x3 = x;\n        double y3 = y - height / 2;\n        \n        gc.strokeLine(x1, y1, x2, y2);\n        gc.strokeLine(x2, y2, x3, y3);\n        gc.strokeLine(x3, y3, x1, y1);\n    }\n\n    public static void main(String[] args) {\n        launch(args);\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<AgreementTermsInner>> getWithServiceResponseAsync(String publisherId, String offerId, String planId) {\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (publisherId == null) {\n            throw new IllegalArgumentException(\"Parameter publisherId is required and cannot be null.\");\n        }\n        if (offerId == null) {\n            throw new IllegalArgumentException(\"Parameter offerId is required and cannot be null.\");\n        }\n        if (planId == null) {\n            throw new IllegalArgumentException(\"Parameter planId is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        final String offerType = \"virtualmachine\";\n        return service.get(this.client.subscriptionId(), offerType, publisherId, offerId, planId, this.client.apiVersion(), this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<AgreementTermsInner>>>() {\n                @Override\n                public Observable<ServiceResponse<AgreementTermsInner>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<AgreementTermsInner> clientResponse = getDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```csharp\n// Assuming 'products' is a collection of Product objects with properties 'Price' and 'Category'\n\n// LINQ query to filter products by price range and category\nvar filteredProducts = from product in products\n                       where product.Price >= minPrice && product.Price <= maxPrice\n                       where product.Category == targetCategory\n                       select product;\n\n// Alternative using method syntax\nvar filteredProducts = products\n    .Where(product => product.Price >= minPrice && product.Price <= maxPrice)\n    .Where(product => product.Category == targetCategory);\n```",
    "label": 1
  },
  {
    "codes": "```gdscript\nshader_type canvas_item;\n\nuniform vec2 screen_size;\nuniform sampler2D texture_normal_map;\nuniform vec2 normal_map_scale = vec2(0.05, 0.05);\nuniform float refraction_strength = 0.1;\n\nvoid fragment() {\n    // Get normalized screen coordinates\n    vec2 uv = SCREEN_UV;\n\n    // Sample the normal map to simulate water refraction\n    vec2 normal_uv = uv * normal_map_scale;\n    vec3 normal_sample = texture(texture_normal_map, normal_uv).rgb;\n    \n    // Convert the sampled normal to [-1,1] range\n    vec2 normal_offset = (normal_sample.rg * 2.0 - 1.0) * refraction_strength;\n\n    // Offset the texture coordinates using the normal map\n    vec2 refracted_uv = uv + normal_offset / screen_size;\n\n    // Sample the screen texture at the refracted coordinates\n    vec4 refracted_color = textureLod(SCREEN_TEXTURE, refracted_uv, 0.0);\n\n    // Output the final color with some attenuation for a water-like effect\n    COLOR = refracted_color * vec4(0.9, 0.9, 0.95, 1.0);\n}\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <cstdlib>\n\n// Function to allocate memory and intentionally leak it\nvoid createMemoryLeak() {\n    int* leakyInt = new int(42); // Allocate memory and assign value\n    // Intentionally not deleting to create a memory leak\n}\n\nint main() {\n    // Call the function that creates a memory leak\n    createMemoryLeak();\n\n    // Print a message to indicate the program is running\n    std::cout << \"Program is running. Check for memory leaks using Valgrind.\" << std::endl;\n\n    return 0;\n}\n```\n\n```bash\n# Compile the program with debugging symbols\ng++ -g -o memory_leak_debug memory_leak_debug.cpp\n\n# Run the program with Valgrind to detect memory leaks\nvalgrind --leak-check=full ./memory_leak_debug\n```\n\n```bash\n# Expected Valgrind output indicating a memory leak\n==12345== HEAP SUMMARY:\n==12345==     in use at exit: 4 bytes in 1 blocks\n==12345==   total heap usage: 1 allocs, 0 frees, 4 bytes allocated\n==12345== \n==12345== 4 bytes in 1 blocks are definitely lost in loss record 1 of 1\n==12345==    at 0x4C2BBAF: operator new(unsigned long) (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==12345==    by 0x4006F6: createMemoryLeak() (memory_leak_debug.cpp:6)\n==12345==    by 0x40071A: main (memory_leak_debug.cpp:13)\n==12345== \n==12345== LEAK SUMMARY:\n==12345==    definitely lost: 4 bytes in 1 blocks\n==12345==    indirectly lost: 0 bytes in 0 blocks\n==12345==      possibly lost: 0 bytes in 0 blocks\n==12345==    still reachable: 0 bytes in 0 blocks\n==12345==         suppressed: 0 bytes in 0 blocks\n==12345== \n==12345== For counts of detected and suppressed errors, rerun with: -v\n==12345== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)\n```",
    "label": 1
  },
  {
    "codes": "public Observable<Page<CertificateIssuerItem>> getCertificateIssuersAsync(final String vaultBaseUrl, final Integer maxresults) {\n        return getCertificateIssuersWithServiceResponseAsync(vaultBaseUrl, maxresults)\n            .map(new Func1<ServiceResponse<Page<CertificateIssuerItem>>, Page<CertificateIssuerItem>>() {\n                @Override\n                public Page<CertificateIssuerItem> call(ServiceResponse<Page<CertificateIssuerItem>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def _connect(self):\n        \"\"\"Connect to PostgreSQL, either by reusing a connection from the pool\n        if possible, or by creating the new connection.\n\n        :rtype: psycopg2.extensions.connection\n        :raises: pool.NoIdleConnectionsError\n\n        \"\"\"\n        future = concurrent.Future()\n\n        # Attempt to get a cached connection from the connection pool\n        try:\n            connection = self._pool_manager.get(self.pid, self)\n            self._connections[connection.fileno()] = connection\n            future.set_result(connection)\n\n            # Add the connection to the IOLoop\n            self._ioloop.add_handler(connection.fileno(),\n                                     self._on_io_events,\n                                     ioloop.IOLoop.WRITE)\n        except pool.NoIdleConnectionsError:\n            self._create_connection(future)\n\n        return future",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Specify the log file location\nLOG_FILE=\"/path/to/apache_log.log\"\n\n# Define the time window (e.g., last 1 hour)\nTIME_WINDOW=$(date --date='1 hour ago' '+%d/%b/%Y:%H')\n\n# Extract 5xx error codes and count occurrences\nawk -v time=\"$TIME_WINDOW\" '$9 ~ /^5/ && $4 > \"[\"time {print $0}' \"$LOG_FILE\" | \\\nawk '{codes[$9]++} END {for(code in codes) if(codes[code] > 10) print code, codes[code]}'\n```\n\n### Comments:\n# This script checks for 5xx errors within the last hour and counts how many occurrences there are.",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponseWithHeaders<Void, FileGetPropertiesFromTaskHeaders>> getPropertiesFromTaskWithServiceResponseAsync(String jobId, String taskId, String filePath) {\n        if (this.client.batchUrl() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.batchUrl() is required and cannot be null.\");\n        }\n        if (jobId == null) {\n            throw new IllegalArgumentException(\"Parameter jobId is required and cannot be null.\");\n        }\n        if (taskId == null) {\n            throw new IllegalArgumentException(\"Parameter taskId is required and cannot be null.\");\n        }\n        if (filePath == null) {\n            throw new IllegalArgumentException(\"Parameter filePath is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        final FileGetPropertiesFromTaskOptions fileGetPropertiesFromTaskOptions = null;\n        Integer timeout = null;\n        UUID clientRequestId = null;\n        Boolean returnClientRequestId = null;\n        DateTime ocpDate = null;\n        DateTime ifModifiedSince = null;\n        DateTime ifUnmodifiedSince = null;\n        String parameterizedHost = Joiner.on(\", \").join(\"{batchUrl}\", this.client.batchUrl());\n        DateTimeRfc1123 ocpDateConverted = null;\n        if (ocpDate != null) {\n            ocpDateConverted = new DateTimeRfc1123(ocpDate);\n        }\n        DateTimeRfc1123 ifModifiedSinceConverted = null;\n        if (ifModifiedSince != null) {\n            ifModifiedSinceConverted = new DateTimeRfc1123(ifModifiedSince);\n        }\n        DateTimeRfc1123 ifUnmodifiedSinceConverted = null;\n        if (ifUnmodifiedSince != null) {\n            ifUnmodifiedSinceConverted = new DateTimeRfc1123(ifUnmodifiedSince);\n        }\n        return service.getPropertiesFromTask(jobId, taskId, filePath, this.client.apiVersion(), this.client.acceptLanguage(), timeout, clientRequestId, returnClientRequestId, ocpDateConverted, ifModifiedSinceConverted, ifUnmodifiedSinceConverted, parameterizedHost, this.client.userAgent())\n                .flatMap(new Func1<Response<Void>, Observable<ServiceResponseWithHeaders<Void, FileGetPropertiesFromTaskHeaders>>>() {\n                    @Override\n                    public Observable<ServiceResponseWithHeaders<Void, FileGetPropertiesFromTaskHeaders>> call(Response<Void> response) {\n                        try {\n                            ServiceResponseWithHeaders<Void, FileGetPropertiesFromTaskHeaders> clientResponse = getPropertiesFromTaskDelegate(response);\n                            return Observable.just(clientResponse);\n                        } catch (Throwable t) {\n                            return Observable.error(t);\n                        }\n                    }\n                });\n    }",
    "label": 0
  },
  {
    "codes": "```scala\n// Import necessary Kafka libraries\nimport java.time.Duration\nimport java.util.{Collections, Properties}\nimport org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}\nimport org.apache.kafka.common.serialization.StringDeserializer\n\n// Define Kafka consumer properties\nval props = new Properties()\nprops.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\")\nprops.put(ConsumerConfig.GROUP_ID_CONFIG, \"scala-consumer-group\")\nprops.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)\nprops.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)\nprops.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\")\n\n// Create Kafka consumer instance\nval consumer = new KafkaConsumer[String, String](props)\n\n// Subscribe to the topic\nval topic = \"real-time-events\"\nconsumer.subscribe(Collections.singletonList(topic))\n\n// Poll for new messages\nwhile (true) {\n  val records = consumer.poll(Duration.ofMillis(1000))\n  records.forEach { record =>\n    // Process each record\n    println(s\"Received message: key=${record.key()}, value=${record.value()}\")\n  }\n}\n\n// Close the consumer (this won't be reached in the infinite loop)\nconsumer.close()\n```",
    "label": 1
  },
  {
    "codes": "function Plugin(option) {\n    return this.each(function () {\n      var $this = $(this);\n      var data  = $this.data(DataKey);\n\n      if (!data) {\n        var options = $.extend({}, Default, $this.data(), typeof option == 'object' && option);\n        $this.data(DataKey, (data = new BoxRefresh($this, options)));\n      }\n\n      if (typeof data == 'string') {\n        if (typeof data[option] == 'undefined') {\n          throw new Error('No method named ' + option);\n        }\n        data[option]();\n      }\n    });\n  }",
    "label": 0
  },
  {
    "codes": "```python\nfrom fastapi import FastAPI, Depends, HTTPException, Security\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom pydantic import BaseModel\nfrom jwt import JWT, ExpiredSignatureError\nfrom datetime import datetime, timedelta\nfrom typing import Optional\n\napp = FastAPI()\n\n# Mock database\nfake_users_db = {\n    \"johndoe\": {\n        \"username\": \"johndoe\",\n        \"full_name\": \"John Doe\",\n        \"email\": \"johndoe@example.com\",\n        \"hashed_password\": \"fakehashedpassword\",\n        \"disabled\": False,\n    }\n}\n\n# JWT configuration\nSECRET_KEY = \"mysecretkey\"\nALGORITHM = \"HS256\"\nACCESS_TOKEN_EXPIRE_MINUTES = 30\n\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\nclass Token(BaseModel):\n    access_token: str\n    token_type: str\n\nclass TokenData(BaseModel):\n    username: str | None = None\n\n# Function to create a JWT token\ndef create_access_token(data: dict, expires_delta: Optional[timedelta] = None):\n    to_encode = data.copy()\n    if expires_delta:\n        expire = datetime.utcnow() + expires_delta\n    else:\n        expire = datetime.utcnow() + timedelta(minutes=15)\n    to_encode.update({\"exp\": expire})\n    token = JWT.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n    return token\n\n# Function to verify the token\ndef get_current_user(token: str = Depends(oauth2_scheme)):\n    credentials_exception = HTTPException(\n        status_code=401,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": \"Bearer\"},\n    )\n    try:\n        payload = JWT.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n        token_data = TokenData(username=username)\n    except (ExpiredSignatureError, Exception):\n        raise credentials_exception\n    return token_data\n\n# Endpoint to log in and get the JWT token\n@app.post(\"/token\", response_model=Token)\nasync def login(form_data: OAuth2PasswordRequestForm = Depends()):\n    user = fake_users_db.get(form_data.username)\n    if not user or not user['hashed_password'] == form_data.password:\n        raise HTTPException(status_code=400, detail=\"Incorrect username or password\")\n    \n    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n    access_token = create_access_token(\n        data={\"sub\": user['username']}, expires_delta=access_token_expires\n    )\n    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n\n# Model prediction endpoint that requires JWT authentication\n@app.post(\"/predict\")\nasync def predict(data: dict, current_user: TokenData = Depends(get_current_user)):\n    # Here you would include the logic for your model prediction.\n    prediction = {\"prediction\": \"dummy prediction\"}  # Replace with actual prediction logic\n    return prediction\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"crypto/rand\"\n\t\"fmt\"\n\t\"math/big\"\n)\n\nfunc main() {\n\tlength := 12 // Specify the length of the password\n\tcharSet := \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!@#$%^&*()-_+=\"\n\n\tpassword, err := generatePassword(length, charSet)\n\tif err != nil {\n\t\tfmt.Println(\"Error generating password:\", err)\n\t\treturn\n\t}\n\n\tfmt.Println(\"Generated Password:\", password)\n}\n\n// generatePassword creates a random password of a specified length\nfunc generatePassword(length int, charSet string) (string, error) {\n\tvar password string\n\tcharSetLength := big.NewInt(int64(len(charSet)))\n\n\tfor i := 0; i < length; i++ {\n\t\t// Generate secure random index\n\t\tindex, err := rand.Int(rand.Reader, charSetLength)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\t// Append character at the random index to the password\n\t\tpassword += string(charSet[index.Int64()])\n\t}\n\n\treturn password, nil\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *ResourceTag) SetTagsList(v []*Tag) *ResourceTag {\n\ts.TagsList = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```elixir\ndefmodule DistributedState do\n  use GenServer\n\n  ## Client API\n\n  @doc \"\"\"\n  Starts the GenServer with initial state.\n  \"\"\"\n  def start_link(initial_state) do\n    GenServer.start_link(__MODULE__, initial_state, name: __MODULE__)\n  end\n\n  @doc \"\"\"\n  Gets the current state.\n  \"\"\"\n  def get() do\n    GenServer.call(__MODULE__, :get)\n  end\n\n  @doc \"\"\"\n  Updates the state with a new value.\n  \"\"\"\n  def update(new_state) do\n    GenServer.cast(__MODULE__, {:update, new_state})\n  end\n\n  ## Server Callbacks\n\n  @impl true\n  def init(initial_state) do\n    {:ok, initial_state}\n  end\n\n  @impl true\n  def handle_call(:get, _from, state) do\n    {:reply, state, state}\n  end\n\n  @impl true\n  def handle_cast({:update, new_state}, _state) do\n    {:noreply, new_state}\n  end\nend\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glue) GetTableVersionsPagesWithContext(ctx aws.Context, input *GetTableVersionsInput, fn func(*GetTableVersionsOutput, bool) bool, opts ...request.Option) error {\n\tp := request.Pagination{\n\t\tNewRequest: func() (*request.Request, error) {\n\t\t\tvar inCpy *GetTableVersionsInput\n\t\t\tif input != nil {\n\t\t\t\ttmp := *input\n\t\t\t\tinCpy = &tmp\n\t\t\t}\n\t\t\treq, _ := c.GetTableVersionsRequest(inCpy)\n\t\t\treq.SetContext(ctx)\n\t\t\treq.ApplyOptions(opts...)\n\t\t\treturn req, nil\n\t\t},\n\t}\n\n\tcont := true\n\tfor p.Next() && cont {\n\t\tcont = fn(p.Page().(*GetTableVersionsOutput), !p.HasNextPage())\n\t}\n\treturn p.Err()\n}",
    "label": 0
  },
  {
    "codes": "public QueueDescription getQueue(String path) throws ServiceBusException, InterruptedException {\n        return Utils.completeFuture(this.asyncClient.getQueueAsync(path));\n    }",
    "label": 0
  },
  {
    "codes": "func (c *Glue) CreateDevEndpoint(input *CreateDevEndpointInput) (*CreateDevEndpointOutput, error) {\n\treq, out := c.CreateDevEndpointRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "function _main() {\n  const srcIndexPath = join(DIST_DIR, 'index.html');\n  const src404BodyPath = join(SRC_DIR, '404-body.html');\n  const dst404PagePath = join(DIST_DIR, '404.html');\n\n  const srcIndexContent = readFileSync(srcIndexPath, 'utf8');\n  const src404BodyContent = readFileSync(src404BodyPath, 'utf8');\n  const dst404PageContent = srcIndexContent.replace(/<body>[\\s\\S]+<\\/body>/, src404BodyContent);\n\n  if (dst404PageContent === srcIndexContent) {\n    throw new Error(\n        'Failed to generate \\'404.html\\'. ' +\n        'The content of \\'index.html\\' does not match the expected pattern.');\n  }\n\n  writeFileSync(dst404PagePath, dst404PageContent);\n}",
    "label": 0
  },
  {
    "codes": "def spop(self, name):\n        \"\"\"Emulate spop.\"\"\"\n        redis_set = self._get_set(name, 'SPOP')\n        if not redis_set:\n            return None\n        member = choice(list(redis_set))\n        redis_set.remove(member)\n        if len(redis_set) == 0:\n            self.delete(name)\n        return member",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<BackupShortTermRetentionPolicyInner> createOrUpdateAsync(String resourceGroupName, String serverName, String databaseName, Integer retentionDays, final ServiceCallback<BackupShortTermRetentionPolicyInner> serviceCallback) {\n        return ServiceFuture.fromResponse(createOrUpdateWithServiceResponseAsync(resourceGroupName, serverName, databaseName, retentionDays), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<Page<JobResponseInner>> listJobsAsync(final String resourceGroupName, final String resourceName) {\n        return listJobsWithServiceResponseAsync(resourceGroupName, resourceName)\n            .map(new Func1<ServiceResponse<Page<JobResponseInner>>, Page<JobResponseInner>>() {\n                @Override\n                public Page<JobResponseInner> call(ServiceResponse<Page<JobResponseInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "function generateLocaleCurrencies(localeData, baseCurrencies) {\n  const currenciesData = localeData.main('numbers/currencies');\n  const currencies = {};\n  Object.keys(currenciesData).forEach(code => {\n    let symbolsArray = [];\n    const symbol = currenciesData[code].symbol;\n    const symbolNarrow = currenciesData[code]['symbol-alt-narrow'];\n    if (symbol && symbol !== code) {\n      symbolsArray.push(symbol);\n    }\n    if (symbolNarrow && symbolNarrow !== symbol) {\n      if (symbolsArray.length > 0) {\n        symbolsArray.push(symbolNarrow);\n      } else {\n        symbolsArray = [undefined, symbolNarrow];\n      }\n    }\n\n    // if locale data are different, set the value\n    if ((baseCurrencies[code] || []).toString() !== symbolsArray.toString()) {\n      currencies[code] = symbolsArray;\n    }\n  });\n  return currencies;\n}",
    "label": 0
  },
  {
    "codes": "func (s *CreateJobInput) SetMaxRetries(v int64) *CreateJobInput {\n\ts.MaxRetries = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```java\n@RestController\n@RequestMapping(\"/api/inventory\")\npublic class InventoryController {\n\n    @Autowired\n    private InventoryService inventoryService;\n\n    // Retrieve all inventory items\n    @GetMapping\n    public ResponseEntity<List<InventoryItem>> getAllItems() {\n        List<InventoryItem> items = inventoryService.getAllItems();\n        return ResponseEntity.ok(items); // Return 200 OK with the list of items\n    }\n\n    // Retrieve a specific inventory item by ID\n    @GetMapping(\"/{id}\")\n    public ResponseEntity<InventoryItem> getItemById(@PathVariable Long id) {\n        InventoryItem item = inventoryService.getItemById(id);\n        return item != null ? ResponseEntity.ok(item) : ResponseEntity.notFound().build(); // Return 200 OK or 404 Not Found\n    }\n\n    // Add a new inventory item\n    @PostMapping\n    public ResponseEntity<InventoryItem> addItem(@RequestBody InventoryItem item) {\n        InventoryItem newItem = inventoryService.addItem(item);\n        return ResponseEntity.status(HttpStatus.CREATED).body(newItem); // Return 201 Created with the new item\n    }\n\n    // Update an existing inventory item\n    @PutMapping(\"/{id}\")\n    public ResponseEntity<InventoryItem> updateItem(@PathVariable Long id, @RequestBody InventoryItem item) {\n        InventoryItem updatedItem = inventoryService.updateItem(id, item);\n        return updatedItem != null ? ResponseEntity.ok(updatedItem) : ResponseEntity.notFound().build(); // Return 200 OK or 404 Not Found\n    }\n\n    // Delete an inventory item by ID\n    @DeleteMapping(\"/{id}\")\n    public ResponseEntity<Void> deleteItem(@PathVariable Long id) {\n        boolean isDeleted = inventoryService.deleteItem(id);\n        return isDeleted ? ResponseEntity.noContent().build() : ResponseEntity.notFound().build(); // Return 204 No Content or 404 Not Found\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *DataCatalogEncryptionSettings) SetConnectionPasswordEncryption(v *ConnectionPasswordEncryption) *DataCatalogEncryptionSettings {\n\ts.ConnectionPasswordEncryption = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function _deleteDir(path) {\n  if (fs.existsSync(path)) {\n    var subpaths = fs.readdirSync(path);\n    subpaths.forEach(function(subpath) {\n      var curPath = path + '/' + subpath;\n      if (fs.lstatSync(curPath).isDirectory()) {\n        _deleteDir(curPath);\n      } else {\n        fs.unlinkSync(curPath);\n      }\n    });\n    fs.rmdirSync(path);\n  }\n}",
    "label": 0
  },
  {
    "codes": "function parse_kwargs(kwargs)\n{\n    var args = new Object();\n    var pairs = kwargs.split(/,/);\n    for (var i = 0; i < pairs.length;) {\n        if (i > 0 && pairs[i].indexOf('=') == -1) {\n            // the value string contained a comma. Glue the parts back together.\n            pairs[i-1] += ',' + pairs.splice(i, 1)[0];\n        }\n        else {\n            ++i;\n        }\n    }\n    for (var i = 0; i < pairs.length; ++i) {\n        var splits = pairs[i].split(/=/);\n        if (splits.length == 1) {\n            continue;\n        }\n        var key = splits.shift();\n        var value = splits.join('=');\n        args[key.trim()] = value.trim();\n    }\n    return args;\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<HostingEnvironmentDiagnosticsInner> getDiagnosticsItemAsync(String resourceGroupName, String name, String diagnosticsName, final ServiceCallback<HostingEnvironmentDiagnosticsInner> serviceCallback) {\n        return ServiceFuture.fromResponse(getDiagnosticsItemWithServiceResponseAsync(resourceGroupName, name, diagnosticsName), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```java\nimport static org.mockito.Mockito.*;\n\nimport org.junit.jupiter.api.Test;\nimport org.mockito.InjectMocks;\nimport org.mockito.Mock;\nimport org.mockito.MockitoAnnotations;\nimport org.springframework.transaction.PlatformTransactionManager;\nimport org.springframework.transaction.TransactionStatus;\nimport org.springframework.transaction.TransactionDefinition;\nimport org.springframework.transaction.support.DefaultTransactionDefinition;\n\npublic class AccountServiceTest {\n\n    @Mock\n    private AccountRepository accountRepository;\n\n    @Mock\n    private PlatformTransactionManager transactionManager;\n\n    @InjectMocks\n    private AccountService accountService;\n\n    public AccountServiceTest() {\n        MockitoAnnotations.openMocks(this);\n    }\n\n    @Mock\n    TransactionStatus transactionStatus;\n\n    @Test\n    public void testTransactionRollbackOnException() {\n        // Arrange\n        when(transactionManager.getTransaction(any(TransactionDefinition.class))).thenReturn(transactionStatus);\n        doThrow(new RuntimeException(\"Error\")).when(accountRepository).updateAccount(any(Account.class));\n\n        // Act\n        try {\n            accountService.transferAmount(100L, 200L, 50.0);\n        } catch (RuntimeException e) {\n            // Expected exception\n        }\n\n        // Assert\n        verify(transactionManager).rollback(transactionStatus);\n        verify(transactionManager, never()).commit(transactionStatus);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```powershell\n# ServiceDependencyManager.psm1\n\n# Function to add a service dependency\nfunction Add-ServiceDependency {\n    param (\n        [string]$ServiceName,\n        [string]$Dependency\n    )\n    # Get the current service configuration\n    $service = Get-Service -Name $ServiceName\n    $serviceConfig = Get-WmiObject -Query \"SELECT * FROM Win32_Service WHERE Name='$ServiceName'\"\n\n    # Get the current dependencies\n    $dependencies = $serviceConfig.Dependencies\n\n    # Check if the dependency already exists\n    if ($dependencies -contains $Dependency) {\n        Write-Host \"Dependency '$Dependency' already exists for service '$ServiceName'.\"\n        return\n    }\n\n    # Add the new dependency\n    $newDependencies = @($dependencies) + $Dependency\n\n    # Update the service configuration\n    $serviceConfig.Change($null, $null, $null, $null, $null, $null, $null, $null, $null, $null, $newDependencies)\n    Write-Host \"Dependency '$Dependency' added to service '$ServiceName'.\"\n}\n\n# Function to remove a service dependency\nfunction Remove-ServiceDependency {\n    param (\n        [string]$ServiceName,\n        [string]$Dependency\n    )\n    # Get the current service configuration\n    $serviceConfig = Get-WmiObject -Query \"SELECT * FROM Win32_Service WHERE Name='$ServiceName'\"\n\n    # Get the current dependencies\n    $dependencies = $serviceConfig.Dependencies\n\n    # Check if the dependency exists\n    if ($dependencies -notcontains $Dependency) {\n        Write-Host \"Dependency '$Dependency' does not exist for service '$ServiceName'.\"\n        return\n    }\n\n    # Remove the dependency\n    $newDependencies = $dependencies | Where-Object { $_ -ne $Dependency }\n\n    # Update the service configuration\n    $serviceConfig.Change($null, $null, $null, $null, $null, $null, $null, $null, $null, $null, $newDependencies)\n    Write-Host \"Dependency '$Dependency' removed from service '$ServiceName'.\"\n}\n\n# Function to list service dependencies\nfunction Get-ServiceDependencies {\n    param (\n        [string]$ServiceName\n    )\n    # Get the current service configuration\n    $serviceConfig = Get-WmiObject -Query \"SELECT * FROM Win32_Service WHERE Name='$ServiceName'\"\n\n    # Return the dependencies\n    return $serviceConfig.Dependencies\n}\n\n# Export the functions to make them available when the module is imported\nExport-ModuleMember -Function Add-ServiceDependency, Remove-ServiceDependency, Get-ServiceDependencies\n```",
    "label": 1
  },
  {
    "codes": "def close(self):\n        \"\"\"\n        Close the XML stream and the underlying transport.\n\n        This gracefully shuts down the XML stream and the transport, if\n        possible by writing the eof using :meth:`asyncio.Transport.write_eof`\n        after sending the stream footer.\n\n        After a call to :meth:`close`, no other stream manipulating or sending\n        method can be called; doing so will result in a\n        :class:`ConnectionError` exception or any exception caused by the\n        transport during shutdown.\n\n        Calling :meth:`close` while the stream is closing or closed is a\n        no-op.\n        \"\"\"\n        if (self._smachine.state == State.CLOSING or\n                self._smachine.state == State.CLOSED):\n            return\n        self._writer.close()\n        if self._transport.can_write_eof():\n            self._transport.write_eof()\n        if self._smachine.state == State.STREAM_HEADER_SENT:\n            # at this point, we cannot wait for the peer to send\n            # </stream:stream>\n            self._close_transport()\n        self._smachine.state = State.CLOSING",
    "label": 0
  },
  {
    "codes": "```rust\n#[macro_export]\nmacro_rules! define_error {\n    (\n        $(#[$meta:meta])*\n        pub enum $error_name:ident {\n            $(\n                $(#[$variant_meta:meta])*\n                $variant:ident $( ( $( $field:ident : $type:ty ),* ) )? $(,)? {\n                    $( $derived_error:tt )*\n                }\n            )*\n        }\n    ) => {\n        $(#[$meta])*\n        #[derive(Debug, thiserror::Error)]\n        pub enum $error_name {\n            $(\n                $(#[$variant_meta])*\n                $variant $( ( $( $field : $type ),* ) )* {\n                    $( $derived_error )*\n                }\n            )*\n        }\n    }\n}\n\n// Usage Example:\ndefine_error! {\n    /// Error type for our application.\n    pub enum AppError {\n        /// An error occurred during IO.\n        IOError(std::io::Error),\n\n        /// An error occurred in a database operation.\n        DbError {\n            source: diesel::result::Error\n        },\n\n        /// A custom error with a message.\n        Custom {\n            message: String\n        } {\n            #[from]\n            source: std::string::FromUtf8Error\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<ImportExportOperationResultInner> beginExportAsync(String resourceGroupName, String serverName, String databaseName, ImportExportDatabaseDefinition parameters, final ServiceCallback<ImportExportOperationResultInner> serviceCallback) {\n        return ServiceFuture.fromResponse(beginExportWithServiceResponseAsync(resourceGroupName, serverName, databaseName, parameters), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```typescript\nimport React, { useState } from 'react';\n\ninterface TreeNode {\n  id: string;\n  name: string;\n  children?: TreeNode[];\n}\n\ninterface TreeProps {\n  data: TreeNode;\n}\n\nconst TreeNodeComponent: React.FC<{ node: TreeNode }> = ({ node }) => {\n  const [isExpanded, setIsExpanded] = useState(false);\n\n  const toggleExpand = () => setIsExpanded(!isExpanded);\n\n  return (\n    <div style={{ marginLeft: 20 }}>\n      <div onClick={toggleExpand} style={{ cursor: 'pointer' }}>\n        {node.children && (isExpanded ? '-' : '+')} {node.name}\n      </div>\n      {isExpanded && node.children && (\n        <div>\n          {node.children.map((childNode) => (\n            <TreeNodeComponent key={childNode.id} node={childNode} />\n          ))}\n        </div>\n      )}\n    </div>\n  );\n};\n\nconst TreeDiagram: React.FC<TreeProps> = ({ data }) => {\n  return (\n    <div>\n      <TreeNodeComponent node={data} />\n    </div>\n  );\n};\n\n// Example usage with sample data:\n\nconst sampleData: TreeNode = {\n  id: '1',\n  name: 'Root',\n  children: [\n    {\n      id: '2',\n      name: 'Child 1',\n      children: [\n        { id: '3', name: 'Grandchild 1' },\n        { id: '4', name: 'Grandchild 2' }\n      ]\n    },\n    {\n      id: '5',\n      name: 'Child 2',\n      children: [\n        { id: '6', name: 'Grandchild 3' }\n      ]\n    }\n  ]\n};\n\n// Render the TreeDiagram component with sample data:\n// <TreeDiagram data={sampleData} />\n```\n",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the MNIST dataset\nmnist = fetch_openml('mnist_784')\n\n# Extract data and labels\nX, y = mnist.data, mnist.target\n\n# Preprocess the data by scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Use t-SNE to reduce dimensionality to 2 dimensions\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X_scaled[:10000])\n\n# Create a scatter plot of the t-SNE transformed data\nplt.figure(figsize=(12, 8))\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y[:10000].astype(int), cmap='tab10', alpha=0.6)\n\n# Add a legend with labels\nplt.legend(*scatter.legend_elements(), title=\"Digits\")\nplt.title('t-SNE visualization of MNIST digit embeddings')\nplt.xlabel('t-SNE component 1')\nplt.ylabel('t-SNE component 2')\nplt.grid(True)\nplt.show()\n```\n",
    "label": 1
  },
  {
    "codes": "```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\n# Define the model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n        self.fc2 = nn.Linear(512, 10)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = self.pool(self.relu(self.conv2(x)))\n        x = x.view(-1, 64 * 8 * 8)  # Flatten the tensor\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Data preprocessing and loading\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n\n# Create the model, define loss function and optimizer\nmodel = SimpleCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(10):  # loop over the dataset multiple times\n    model.train()  # Set the model to training mode\n    running_loss = 0.0\n    for inputs, labels in trainloader:\n        optimizer.zero_grad()  # Zero the parameter gradients\n        outputs = model(inputs)  # Forward pass\n        loss = criterion(outputs, labels)  # Compute the loss\n        loss.backward()  # Backward pass\n        optimizer.step()  # Optimize the weights\n        running_loss += loss.item()\n    print(f'Epoch [{epoch + 1}], Loss: {running_loss / len(trainloader):.4f}')\n\n# Evaluation on the test set\nmodel.eval()  # Set the model to evaluate mode\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in testloader:\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n```",
    "label": 1
  },
  {
    "codes": "```hcl\nprovider \"aws\" {\n  region = \"us-west-2\"\n}\n\nresource \"aws_sagemaker_model\" \"ml_model\" {\n  name    = \"example-ml-model\"\n  execution_role_arn = aws_iam_role.sagemaker_exec_role.arn\n\n  primary_container {\n    image   = \"123456789012.dkr.ecr.us-west-2.amazonaws.com/your-image:latest\"\n    model_data_url = \"s3://your-bucket/model.tar.gz\"\n\n    environment = {\n      SAGEMAKER_CONTAINER_LOG_LEVEL = \"20\" # Debug level logs\n      SAGEMAKER_PROGRAM             = \"train.py\"\n      SAGEMAKER_REGION              = \"us-west-2\"\n    }\n  }\n}\n\nresource \"aws_iam_role\" \"sagemaker_exec_role\" {\n  name = \"sagemaker-exec-role\"\n\n  assume_role_policy = jsonencode({\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n      \"Action\": \"sts:AssumeRole\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"sagemaker.amazonaws.com\"\n      }\n    }]\n  })\n}\n\nresource \"aws_iam_role_policy\" \"sagemaker_policy\" {\n  name = \"sagemaker-policy\"\n  role = aws_iam_role.sagemaker_exec_role.id\n\n  policy = jsonencode({\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n      {\n        \"Action\": [\n          \"sagemaker:*\",\n          \"ecr:GetAuthorizationToken\",\n          \"ecr:BatchGetImage\",\n          \"ecr:GetDownloadUrlForLayer\",\n          \"cloudwatch:PutMetricData\",\n          \"logs:CreateLogStream\",\n          \"logs:PutLogEvents\"\n        ],\n        \"Effect\": \"Allow\",\n        \"Resource\": \"*\"\n      },\n      {\n        \"Action\": [\n          \"s3:GetObject\",\n          \"s3:PutObject\"\n        ],\n        \"Effect\": \"Allow\",\n        \"Resource\": \"arn:aws:s3:::your-bucket/*\"\n      }\n    ]\n  })\n}\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glacier) DescribeJobRequest(input *DescribeJobInput) (req *request.Request, output *JobDescription) {\n\top := &request.Operation{\n\t\tName:       opDescribeJob,\n\t\tHTTPMethod: \"GET\",\n\t\tHTTPPath:   \"/{accountId}/vaults/{vaultName}/jobs/{jobId}\",\n\t}\n\n\tif input == nil {\n\t\tinput = &DescribeJobInput{}\n\t}\n\n\toutput = &JobDescription{}\n\treq = c.newRequest(op, input, output)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "def stroke_opacity(value):\n        \"\"\"ValueRef : number, opacity of the stroke (0 to 1)\n        \"\"\"\n        if value.value:\n            _assert_is_type('stroke_opacity.value', value.value,\n                            (float, int))\n            if value.value < 0 or value.value > 1:\n                raise ValueError(\n                    'stroke_opacity must be between 0 and 1')",
    "label": 0
  },
  {
    "codes": "function getInstanceDetails (id) {\n  const instance = instanceMap.get(id)\n  if (!instance) {\n    const vnode = findInstanceOrVnode(id)\n\n    if (!vnode) return {}\n\n    const data = {\n      id,\n      name: getComponentName(vnode.fnOptions),\n      file: vnode.fnOptions.__file || null,\n      state: processProps({ $options: vnode.fnOptions, ...(vnode.devtoolsMeta && vnode.devtoolsMeta.renderContext.props) }),\n      functional: true\n    }\n\n    return data\n  } else {\n    const data = {\n      id: id,\n      name: getInstanceName(instance),\n      state: getInstanceState(instance)\n    }\n\n    let i\n    if ((i = instance.$vnode) && (i = i.componentOptions) && (i = i.Ctor) && (i = i.options)) {\n      data.file = i.__file || null\n    }\n\n    return data\n  }\n}",
    "label": 0
  },
  {
    "codes": "func (c *Glacier) GetDataRetrievalPolicyRequest(input *GetDataRetrievalPolicyInput) (req *request.Request, output *GetDataRetrievalPolicyOutput) {\n\top := &request.Operation{\n\t\tName:       opGetDataRetrievalPolicy,\n\t\tHTTPMethod: \"GET\",\n\t\tHTTPPath:   \"/{accountId}/policies/data-retrieval\",\n\t}\n\n\tif input == nil {\n\t\tinput = &GetDataRetrievalPolicyInput{}\n\t}\n\n\toutput = &GetDataRetrievalPolicyOutput{}\n\treq = c.newRequest(op, input, output)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "func (s *BatchStopJobRunInput) SetJobRunIds(v []*string) *BatchStopJobRunInput {\n\ts.JobRunIds = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<HybridRunbookWorkerGroupInner>> updateWithServiceResponseAsync(String resourceGroupName, String automationAccountName, String hybridRunbookWorkerGroupName, RunAsCredentialAssociationProperty credential) {\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (automationAccountName == null) {\n            throw new IllegalArgumentException(\"Parameter automationAccountName is required and cannot be null.\");\n        }\n        if (hybridRunbookWorkerGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter hybridRunbookWorkerGroupName is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        Validator.validate(credential);\n        final String apiVersion = \"2015-10-31\";\n        HybridRunbookWorkerGroupUpdateParameters parameters = new HybridRunbookWorkerGroupUpdateParameters();\n        parameters.withCredential(credential);\n        return service.update(resourceGroupName, automationAccountName, hybridRunbookWorkerGroupName, this.client.subscriptionId(), apiVersion, this.client.acceptLanguage(), parameters, this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<HybridRunbookWorkerGroupInner>>>() {\n                @Override\n                public Observable<ServiceResponse<HybridRunbookWorkerGroupInner>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<HybridRunbookWorkerGroupInner> clientResponse = updateDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```java\n// Method to batch-insert 10,000 records using Hibernate\npublic void batchInsert(List<MyEntity> entities) {\n    Session session = HibernateUtil.getSessionFactory().openSession();\n    Transaction transaction = null;\n\n    try {\n        transaction = session.beginTransaction();\n\n        // Set batch size for optimal performance\n        session.setJdbcBatchSize(50);\n\n        for (int i = 0; i < entities.size(); i++) {\n            session.save(entities.get(i));\n\n            // Flush and clear session periodically to manage memory\n            if (i % 50 == 0) {\n                session.flush();\n                session.clear();\n            }\n        }\n\n        transaction.commit();\n    } catch (Exception e) {\n        if (transaction != null) {\n            transaction.rollback();\n        }\n        e.printStackTrace();\n    } finally {\n        session.close();\n    }\n}\n```\n\n```java\n// HibernateUtil class for session management\npublic class HibernateUtil {\n    private static final SessionFactory sessionFactory = buildSessionFactory();\n\n    private static SessionFactory buildSessionFactory() {\n        try {\n            return new Configuration().configure().buildSessionFactory();\n        } catch (Throwable ex) {\n            throw new ExceptionInInitializerError(ex);\n        }\n    }\n\n    public static SessionFactory getSessionFactory() {\n        return sessionFactory;\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nfunction deepClone(obj, hash = new WeakMap()) {\n    // If the object is null or not an object, return it directly\n    if (obj === null || typeof obj !== 'object') {\n        return obj;\n    }\n\n    // If the object is already cloned, return the cloned version\n    if (hash.has(obj)) {\n        return hash.get(obj);\n    }\n\n    // Handle Date objects\n    if (obj instanceof Date) {\n        const clone = new Date(obj);\n        hash.set(obj, clone);\n        return clone;\n    }\n\n    // Handle Array objects\n    if (Array.isArray(obj)) {\n        const clone = [];\n        hash.set(obj, clone);\n        obj.forEach((item, index) => {\n            clone[index] = deepClone(item, hash);\n        });\n        return clone;\n    }\n\n    // Handle regular objects\n    const clone = Object.create(Object.getPrototypeOf(obj));\n    hash.set(obj, clone);\n    for (const key in obj) {\n        if (obj.hasOwnProperty(key)) {\n            clone[key] = deepClone(obj[key], hash);\n        }\n    }\n\n    return clone;\n}\n```",
    "label": 1
  },
  {
    "codes": "```csharp\n// Define a retry policy with exponential backoff\nvar retryPolicy = Policy\n    .Handle<HttpRequestException>() // Handle transient HTTP request exceptions\n    .Or<TimeoutException>()         // Handle timeout exceptions\n    .WaitAndRetryAsync(\n        retryCount: 5,              // Retry up to 5 times\n        sleepDurationProvider: retryAttempt => TimeSpan.FromSeconds(Math.Pow(2, retryAttempt)), // Exponential backoff\n        onRetry: (exception, timespan, retryCount, context) => \n        {\n            // Log retry attempts for monitoring\n            Console.WriteLine($\"Retry {retryCount} due to {exception.Message}. Waiting {timespan} before next retry.\");\n        });\n\n// Define a circuit breaker policy to stop retries after a certain number of failures\nvar circuitBreakerPolicy = Policy\n    .Handle<HttpRequestException>() // Handle transient HTTP request exceptions\n    .Or<TimeoutException>()         // Handle timeout exceptions\n    .CircuitBreakerAsync(\n        exceptionsAllowedBeforeBreaking: 3, // Break after 3 consecutive failures\n        durationOfBreak: TimeSpan.FromSeconds(30), // Break the circuit for 30 seconds\n        onBreak: (exception, timespan) => \n        {\n            // Log circuit break event\n            Console.WriteLine($\"Circuit broken due to {exception.Message}. Circuit will remain open for {timespan}.\");\n        },\n        onReset: () => \n        {\n            // Log circuit reset event\n            Console.WriteLine(\"Circuit reset. Requests will now flow through.\");\n        });\n\n// Combine retry and circuit breaker policies\nvar resiliencePolicy = Policy.WrapAsync(retryPolicy, circuitBreakerPolicy);\n\n// Example usage in a microservice call\nawait resiliencePolicy.ExecuteAsync(async () =>\n{\n    // Simulate a microservice call\n    var response = await httpClient.GetAsync(\"https://example.com/api/resource\");\n    response.EnsureSuccessStatusCode(); // Throws HttpRequestException if not successful\n});\n```",
    "label": 1
  },
  {
    "codes": "def temporal_participation_coeff(tnet, communities=None, decay=None, removeneg=False):\n    r'''\n    Temporal participation coefficient is a measure of diversity of connections across communities for individual nodes.\n\n    Parameters\n    ----------\n    tnet : array, dict\n        graphlet or contact sequence input. Only positive matrices considered.\n    communities : array\n        community vector. Either 1D (node) community index or 2D (node,time).\n    removeneg : bool (default false)\n        If true, all values < 0 are made to be 0.\n\n\n    Returns\n    -------\n    P : array\n        participation coefficient\n\n\n    Notes\n    -----\n\n    Static participatoin coefficient is:\n\n    .. math:: P_i = 1 - \\sum_s^{N_M}({{k_{is}}\\over{k_i}})^2\n\n    Where s is the index of each community (:math:`N_M`). :math:`k_i` is total degree of node. And :math:`k_{is}` is degree of connections within community.[part-1]_\n\n    This \"temporal\" version only loops through temporal snapshots and calculates :math:`P_i` for each t.\n\n    If directed, function sums axis=1, so tnet may need to be transposed before hand depending on what type of directed part_coef you are interested in.\n\n\n    References\n    ----------\n\n    .. [part-1] Guimera et al (2005) Functional cartography of complex metabolic networks. Nature. 433: 7028, p895-900. [`Link <http://doi.org/10.1038/nature03288>`_]\n    '''\n\n    if communities is None:\n        if isinstance(tnet, dict):\n            if 'communities' in tnet.keys():\n                communities = tnet['communities']\n            else:\n                raise ValueError('Community index not found')\n        else:\n            raise ValueError('Community must be provided for graphlet input')\n\n    # Get input in right format\n    tnet = process_input(tnet, ['C', 'G', 'TN'], 'TN')\n\n    if tnet.nettype[0] == 'w':\n        # TODO add contingency when hdf5 data has negative edges\n        if tnet.hdf5 == False:\n            if sum(tnet.network['weight'] < 0) > 0 and not removeneg:\n                print(\n                    'TENETO WARNING: negative edges exist when calculating participation coefficient.')\n            else:\n                tnet.network['weight'][tnet.network['weight'] < 0] = 0\n\n    part = np.zeros([tnet.netshape[0], tnet.netshape[1]])\n\n    if len(communities.shape) == 1:\n        for t in np.arange(0, tnet.netshape[1]):\n            C = communities\n            snapshot = tnet.get_network_when(t=t)\n            if tnet.nettype[1] == 'd':\n                i_at_t = snapshot['i'].values\n            else:\n                i_at_t = np.concatenate(\n                    [snapshot['i'].values, snapshot['j'].values])\n            i_at_t = np.unique(i_at_t).tolist()\n            i_at_t = list(map(int, i_at_t))\n            for i in i_at_t:\n                # Calculate degree of node\n                if tnet.nettype[1] == 'd':\n                    df = tnet.get_network_when(i=i, t=t)\n                    j_at_t = df['j'].values\n                    if tnet.nettype == 'wd':\n                        k_i = df['weight'].sum()\n                    elif tnet.nettype == 'bd':\n                        k_i = len(df)\n                elif tnet.nettype[1] == 'u':\n                    df = tnet.get_network_when(ij=i, t=t)\n                    j_at_t = np.concatenate([df['i'].values, df['j'].values])\n                    if tnet.nettype == 'wu':\n                        k_i = df['weight'].sum()\n                    elif tnet.nettype == 'bu':\n                        k_i = len(df)\n                j_at_t = list(map(int, j_at_t))\n                for c in np.unique(C[j_at_t]):\n                    ci = np.where(C == c)[0].tolist()\n                    k_is = tnet.get_network_when(i=i, j=ci, t=t)\n                    if tnet.nettype[1] == 'u':\n                        k_is2 = tnet.get_network_when(j=i, i=ci, t=t)\n                        k_is = pd.concat([k_is, k_is2])\n                    if len(k_is) > 0:\n                        if tnet.nettype[0] == 'b':\n                            k_is = len(k_is)\n                        else:\n                            k_is = k_is['weight'].sum()\n                        part[i, t] += np.square(k_is/k_i)\n            part[i_at_t, t] = 1 - part[i_at_t, t]\n            if decay is not None and t > 0:\n                part[i_at_t, t] += decay*part[i_at_t, t-1]\n    else:\n        for t in np.arange(0, tnet.netshape[1]):\n            snapshot = tnet.get_network_when(t=t)\n            if tnet.nettype[1] == 'd':\n                i_at_t = snapshot['i'].values\n            else:\n                i_at_t = np.concatenate(\n                    [snapshot['i'].values, snapshot['j'].values])\n            i_at_t = np.unique(i_at_t).tolist()\n            i_at_t = list(map(int, i_at_t))\n            for i in i_at_t:\n                for tc in np.arange(0, tnet.netshape[1]):\n                    C = communities[:, tc]\n                    # Calculate degree of node\n                    if tnet.nettype[1] == 'd':\n                        df = tnet.get_network_when(i=i, t=t)\n                        j_at_t = df['j'].values\n                        if tnet.nettype == 'wd':\n                            k_i = df['weight'].sum()\n                        elif tnet.nettype == 'bd':\n                            k_i = len(df)\n                    elif tnet.nettype[1] == 'u':\n                        df = tnet.get_network_when(ij=i, t=t)\n                        j_at_t = np.concatenate(\n                            [df['i'].values, df['j'].values])\n                        if tnet.nettype == 'wu':\n                            k_i = df['weight'].sum()\n                        elif tnet.nettype == 'bu':\n                            k_i = len(df)\n                    j_at_t = list(map(int, j_at_t))\n                    for c in np.unique(C[j_at_t]):\n                        ci = np.where(C == c)[0].tolist()\n                        k_is = tnet.get_network_when(i=i, j=ci, t=t)\n                        if tnet.nettype[1] == 'u':\n                            k_is2 = tnet.get_network_when(j=i, i=ci, t=t)\n                            k_is = pd.concat([k_is, k_is2])\n                        if tnet.nettype[0] == 'b':\n                            k_is = len(k_is)\n                        else:\n                            k_is = k_is['weight'].sum()\n                        part[i, t] += np.square(k_is/k_i)\n                part[i, t] = part[i, t] / tnet.netshape[1]\n            part[i_at_t, t] = 1 - part[i_at_t, t]\n            if decay is not None and t > 0:\n                part[i_at_t, t] += decay*part[i_at_t, t-1]\n\n    # Set any division by 0 to 0\n    part[np.isnan(part) == 1] = 0\n\n    return part",
    "label": 0
  },
  {
    "codes": "func (c *Glacier) GetJobOutput(input *GetJobOutputInput) (*GetJobOutputOutput, error) {\n\treq, out := c.GetJobOutputRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "func (s *TableInput) SetViewOriginalText(v string) *TableInput {\n\ts.ViewOriginalText = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public BackupShortTermRetentionPolicyInner beginCreateOrUpdate(String resourceGroupName, String serverName, String databaseName) {\n        return beginCreateOrUpdateWithServiceResponseAsync(resourceGroupName, serverName, databaseName).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "public BackupCertificateResult backupCertificate(String vaultBaseUrl, String certificateName) {\n        return backupCertificateWithServiceResponseAsync(vaultBaseUrl, certificateName).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Crypto Price Viewer</title>\n    <script src=\"https://d3js.org/d3.v6.min.js\"></script>\n</head>\n<body>\n    <h2>Real-time Cryptocurrency Prices</h2>\n    <svg width=\"800\" height=\"400\"></svg>\n    <script>\n        // SVG setup\n        const svg = d3.select(\"svg\");\n        const width = +svg.attr(\"width\");\n        const height = +svg.attr(\"height\");\n\n        // Setup for time series\n        const xScale = d3.scaleTime().range([0, width]);\n        const yScale = d3.scaleLinear().range([height, 0]);\n\n        const line = d3.line()\n            .x(d => xScale(new Date(d.time)))\n            .y(d => yScale(d.price));\n\n        svg.append(\"g\")\n            .attr(\"class\", \"x-axis\")\n            .attr(\"transform\", `translate(0,${height})`);\n        \n        svg.append(\"g\")\n            .attr(\"class\", \"y-axis\");\n\n        // Function to fetch crypto prices\n        const fetchCryptoPrice = async () => {\n            const response = await fetch('https://api.coingecko.com/api/v3/simple/price?ids=bitcoin&vs_currencies=usd');\n            const data = await response.json();\n            return { time: new Date(), price: data.bitcoin.usd };\n        };\n\n        // Updating the visualization\n        const updateData = async () => {\n            const newPrice = await fetchCryptoPrice();\n            prices.push(newPrice);\n\n            if (prices.length > 20) { // Limit data points for better performance\n                prices.shift();\n            }\n\n            xScale.domain(d3.extent(prices, d => new Date(d.time)));\n            yScale.domain([0, d3.max(prices, d => d.price)]);\n\n            svg.select(\".x-axis\")\n                .call(d3.axisBottom(xScale).ticks(5));\n            \n            svg.select(\".y-axis\")\n                .call(d3.axisLeft(yScale).ticks(5));\n\n            svg.selectAll(\".line\").remove();\n\n            svg.append(\"path\")\n                .datum(prices)\n                .attr(\"class\", \"line\")\n                .attr(\"fill\", \"none\")\n                .attr(\"stroke\", \"steelblue\")\n                .attr(\"stroke-width\", 2)\n                .attr(\"d\", line);\n        };\n\n        // Array to hold historical price data\n        const prices = [];\n\n        // Initial update and setup for interval\n        updateData();\n        setInterval(updateData, 60000); // update every minute\n    </script>\n</body>\n</html>\n```",
    "label": 1
  },
  {
    "codes": "```c\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <string.h>\n#include <stdio.h>\n\n// Encrypt plaintext using AES-GCM\nint aes_gcm_encrypt(const unsigned char *plaintext, size_t plaintext_len,\n                    const unsigned char *key, const unsigned char *iv,\n                    unsigned char *ciphertext, unsigned char *tag) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int ciphertext_len;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        return -1;\n    }\n\n    // Initialize the encryption operation\n    if (1 != EVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, key, iv)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Encrypt the plaintext\n    if (1 != EVP_EncryptUpdate(ctx, ciphertext, &len, plaintext, plaintext_len)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    ciphertext_len = len;\n\n    // Finalize the encryption\n    if (1 != EVP_EncryptFinal_ex(ctx, ciphertext + len, &len)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    ciphertext_len += len;\n\n    // Get the authentication tag\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_GET_TAG, 16, tag)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    return ciphertext_len;\n}\n\n// Decrypt ciphertext using AES-GCM\nint aes_gcm_decrypt(const unsigned char *ciphertext, size_t ciphertext_len,\n                    const unsigned char *key, const unsigned char *iv,\n                    const unsigned char *tag, unsigned char *plaintext) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int plaintext_len;\n    int ret;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        return -1;\n    }\n\n    // Initialize the decryption operation\n    if (1 != EVP_DecryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, key, iv)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Decrypt the ciphertext\n    if (1 != EVP_DecryptUpdate(ctx, plaintext, &len, ciphertext, ciphertext_len)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    plaintext_len = len;\n\n    // Set the authentication tag\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_TAG, 16, (void *)tag)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Finalize the decryption\n    ret = EVP_DecryptFinal_ex(ctx, plaintext + len, &len);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    if (ret > 0) {\n        plaintext_len += len;\n        return plaintext_len;\n    } else {\n        // Authentication failed\n        return -1;\n    }\n}\n\n// Example usage\nint main() {\n    unsigned char key[32]; // 256-bit key\n    unsigned char iv[12];  // 96-bit IV\n    unsigned char tag[16]; // 128-bit tag\n    unsigned char plaintext[] = \"Hello, AES-GCM!\";\n    unsigned char ciphertext[128];\n    unsigned char decryptedtext[128];\n\n    // Generate random key and IV\n    RAND_bytes(key, sizeof(key));\n    RAND_bytes(iv, sizeof(iv));\n\n    // Encrypt\n    int ciphertext_len = aes_gcm_encrypt(plaintext, strlen((char *)plaintext), key, iv, ciphertext, tag);\n    if (ciphertext_len == -1) {\n        printf(\"Encryption failed\\n\");\n        return 1;\n    }\n\n    // Decrypt\n    int decryptedtext_len = aes_gcm_decrypt(ciphertext, ciphertext_len, key, iv, tag, decryptedtext);\n    if (decryptedtext_len == -1) {\n        printf(\"Decryption failed\\n\");\n        return 1;\n    }\n\n    // Null-terminate the decrypted text\n    decryptedtext[decryptedtext_len] = '\\0';\n\n    printf(\"Decrypted text: %s\\n\", decryptedtext);\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model)\n\n# Save the converted CoreML model\ncoreml_model.save('converted_model.mlmodel')\n\n# Print success message\nprint(\"Model successfully converted to CoreML format and saved as 'converted_model.mlmodel'\")\n```\n\n```python\n# Alternative approach using TensorFlow 2.x and coremltools\nimport tensorflow as tf\nimport coremltools as ct\n\n# Load the Keras model\nkeras_model = tf.keras.models.load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model, source='tensorflow')\n\n# Save the converted CoreML model\ncoreml_model.save('converted_model.mlmodel')\n\n# Print success message\nprint(\"Model successfully converted to CoreML format and saved as 'converted_model.mlmodel'\")\n```",
    "label": 1
  },
  {
    "codes": "def textcontent(self, cls='current', correctionhandling=CorrectionHandling.CURRENT):\n        \"\"\"See :meth:`AbstractElement.textcontent`\"\"\"\n        if cls == 'original': correctionhandling = CorrectionHandling.ORIGINAL #backward compatibility\n        if correctionhandling in (CorrectionHandling.CURRENT, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, New) or isinstance(e, Current):\n                    return e.textcontent(cls,correctionhandling)\n        if correctionhandling in (CorrectionHandling.ORIGINAL, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, Original):\n                    return e.textcontent(cls,correctionhandling)\n        raise NoSuchText",
    "label": 0
  },
  {
    "codes": "func (s *DevEndpoint) SetExtraPythonLibsS3Path(v string) *DevEndpoint {\n\ts.ExtraPythonLibsS3Path = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport psutil\nimport time\nimport argparse\n\ndef get_process_network_usage():\n    process_info = {}\n    for proc in psutil.process_iter(['pid', 'name']):\n        try:\n            io_counters = proc.io_counters()\n            if io_counters:\n                process_info[proc.info['name']] = {\n                    'pid': proc.info['pid'],\n                    'bytes_sent': io_counters[1],\n                    'bytes_recv': io_counters[0],\n                }\n        except (psutil.NoSuchProcess, psutil.AccessDenied):\n            continue\n    return process_info\n\ndef print_usage(process_info):\n    print(f\"{'Process Name':<25} {'PID':<10} {'Bytes Sent':<15} {'Bytes Received':<15}\")\n    print(\"=\"*75)\n    for name, info in process_info.items():\n        print(f\"{name:<25} {info['pid']:<10} {info['bytes_sent']:<15} {info['bytes_recv']:<15}\")\n\ndef monitor_bandwidth(interval):\n    try:\n        while True:\n            process_info = get_process_network_usage()\n            print_usage(process_info)\n            time.sleep(interval)\n            print(\"\\n\" + \"-\"*75 + \"\\n\")\n    except KeyboardInterrupt:\n        print(\"Monitoring stopped.\")\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Monitor network bandwidth usage per process.')\n    parser.add_argument('--interval', type=int, default=1, help='Time interval in seconds between updates.')\n    args = parser.parse_args()\n\n    monitor_bandwidth(args.interval)\n```",
    "label": 1
  },
  {
    "codes": "function b2a(b) {\n\tvar out = new Array(b.length);\n\tfor(var i = 0; i < out.length; i++) out[i] = (b[i] < 0 ? b[i] + 256 : b[i]);\n\treturn out;\n}",
    "label": 0
  },
  {
    "codes": "Certainly! Here is an example of pytest cases to validate an API response schema using response codes and comments:\n\n```python\nimport requests\nimport pytest\nfrom jsonschema import validate\n\n# Define the expected schema\nexpected_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"id\": {\"type\": \"integer\"},\n        \"name\": {\"type\": \"string\"},\n        \"email\": {\"type\": \"string\"}\n    },\n    \"required\": [\"id\", \"name\", \"email\"]\n}\n\n# Test case to validate a successful API response schema\ndef test_api_response_schema():\n    response = requests.get(\"https://api.example.com/user/1\")\n    assert response.status_code == 200  # Check if the response code is 200 (OK)\n\n    data = response.json()\n    validate(instance=data, schema=expected_schema)  # Validate the response schema\n\n# Test case for response when resource is not found\ndef test_api_response_404():\n    response = requests.get(\"https://api.example.com/user/9999\")\n    assert response.status_code == 404  # Check if the response code is 404 (Not Found)\n\n# Test case for unauthorized access\ndef test_api_response_401():\n    response = requests.get(\"https://api.example.com/user/1\", headers={\"Authorization\": \"InvalidToken\"})\n    assert response.status_code == 401  # Check if the response code is 401 (Unauthorized)\n\n# Test case for unexpected server error\ndef test_api_response_500():\n    # Simulate a server error by sending incorrect data or hitting a problematic endpoint\n    response = requests.post(\"https://api.example.com/user\", json={\"name\": \"Bad Request\"})\n    assert response.status_code == 500  # Check if the response code is 500 (Internal Server Error)\n\n# Running the tests can be done via command line:\n# pytest -q path/to/test_file.py\n```\n\nPlease note that you should replace `\"https://api.example.com/user\"` with the actual API endpoint you intend to test. The schema should also reflect the actual expected response from your API.",
    "label": 1
  },
  {
    "codes": "public Observable<RoleDefinitionInner> deleteAsync(String scope, String roleDefinitionId) {\n        return deleteWithServiceResponseAsync(scope, roleDefinitionId).map(new Func1<ServiceResponse<RoleDefinitionInner>, RoleDefinitionInner>() {\n            @Override\n            public RoleDefinitionInner call(ServiceResponse<RoleDefinitionInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "def to_string(self, format_, fps=None, **kwargs):\n        \"\"\"\n        Get subtitle file as a string.\n\n        See :meth:`SSAFile.save()` for full description.\n\n        Returns:\n            str\n\n        \"\"\"\n        fp = io.StringIO()\n        self.to_file(fp, format_, fps=fps, **kwargs)\n        return fp.getvalue()",
    "label": 0
  },
  {
    "codes": "func (s *CreateJobInput) SetConnections(v *ConnectionsList) *CreateJobInput {\n\ts.Connections = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```powershell\n# Define the registry paths to audit\n$registryPaths = @(\n    \"HKLM:\\Software\",\n    \"HKCU:\\Software\",\n    \"HKLM:\\SYSTEM\",\n    \"HKCU:\\SYSTEM\"\n)\n\n# Function to check registry key permissions\nfunction Check-RegistryPermissions {\n    param (\n        [string]$path\n    )\n\n    try {\n        $key = Get-Acl -Path $path\n        $rules = $key.Access\n\n        foreach ($rule in $rules) {\n            # Check for insecure permissions\n            if ($rule.AccessControlType -eq \"Allow\" -and $rule.FileSystemRights -match \"FullControl|Write|Modify\") {\n                Write-Output \"$path has weak permissions: $($rule.IdentityReference) - $($rule.FileSystemRights)\"\n            }\n        }\n    } catch {\n        Write-Output \"Failed to access $path: $_\"\n    }\n}\n\n# Loop through each registry path and check permissions\nforeach ($path in $registryPaths) {\n    Check-RegistryPermissions -path $path\n}\n```",
    "label": 1
  },
  {
    "codes": "def register_child(cls, prop, child_cls):\n        \"\"\"\n        Register a new :class:`XMLStreamClass` instance `child_cls` for a given\n        :class:`Child` descriptor `prop`.\n\n        .. warning::\n\n           This method cannot be used after a class has been derived from this\n           class. This is for consistency: the method modifies the bookkeeping\n           attributes of the class. There would be two ways to deal with the\n           situation:\n\n           1. Updating all the attributes at all the subclasses and re-evaluate\n              the constraints of inheritance. This is simply not implemented,\n              although it would be the preferred way.\n\n           2. Only update the bookkeeping attributes on *this* class, hiding\n              the change from any existing subclasses. New subclasses would\n              pick the change up, however, which is inconsistent. This is the\n              way which was previously documented here and is not supported\n              anymore.\n\n           Obviously, (2) is bad, which is why it is not supported anymore. (1)\n           might be supported at some point in the future.\n\n           Attempting to use :meth:`register_child` on a class which already\n           has subclasses results in a :class:`TypeError`.\n\n        Note that *first* using :meth:`register_child` and only *then* deriving\n        clasess is a valid use: it will still lead to a consistent inheritance\n        hierarchy and is a convenient way to break reference cycles (e.g. if an\n        XSO may be its own child).\n        \"\"\"\n        if cls.__subclasses__():\n            raise TypeError(\n                \"register_child is forbidden on classes with subclasses\"\n                \" (subclasses: {})\".format(\n                    \", \".join(map(str, cls.__subclasses__()))\n                ))\n\n        if child_cls.TAG in cls.CHILD_MAP:\n            raise ValueError(\"ambiguous Child\")\n\n        prop.xq_descriptor._register(child_cls)\n        cls.CHILD_MAP[child_cls.TAG] = prop.xq_descriptor",
    "label": 0
  },
  {
    "codes": "```python\nfrom flask import Flask, request, jsonify\nfrom PIL import Image\nimport os\nimport uuid\nimport asyncio\nimport aiofiles\n\napp = Flask(__name__)\nUPLOAD_FOLDER = 'uploads'\nRESIZED_FOLDER = 'resized'\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs(RESIZED_FOLDER, exist_ok=True)\n\nasync def resize_image(image_path):\n    async with aiofiles.open(image_path, 'rb') as f:\n        contents = await f.read()\n    with Image.open(BytesIO(contents)) as img:\n        img = img.resize((200, 200))  # Resize to 200x200\n        resized_path = os.path.join(RESIZED_FOLDER, f'resized_{os.path.basename(image_path)}')\n        img.save(resized_path)\n    return resized_path\n\n@app.route('/upload', methods=['POST'])\nasync def upload_image():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file part'}), 400\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No selected file'}), 400\n    if file:\n        filename = f\"{uuid.uuid4()}.{file.filename.split('.')[-1]}\"\n        file_path = os.path.join(UPLOAD_FOLDER, filename)\n        await aiofiles.upload(file_path, file.read())\n        resized_image_path = await resize_image(file_path)\n        return jsonify({'resized_image_path': resized_image_path}), 201\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Collections.Concurrent;\nusing System.Linq;\nusing System.Threading;\nusing System.Threading.Tasks;\n\nclass Program\n{\n    static void Main()\n    {\n        // Sample data\n        var data = Enumerable.Range(1, 100);\n\n        // Initialize a cancellation token source\n        using (var cts = new CancellationTokenSource())\n        {\n            // Create a concurrent bag to store results or logs\n            ConcurrentBag<int> processedData = new ConcurrentBag<int>();\n\n            try\n            {\n                // Launch a Task to cancel after some delay (for demonstration)\n                Task.Run(() =>\n                {\n                    Thread.Sleep(500); // Adjust delay as needed\n                    cts.Cancel();\n                });\n\n                // Parallel.ForEach with cancellation support\n                Parallel.ForEach(data, new ParallelOptions { CancellationToken = cts.Token }, item =>\n                {\n                    // Simulate some work with possible cancellation\n                    cts.Token.ThrowIfCancellationRequested();\n                    Console.WriteLine($\"Processing item {item}\");\n                    \n                    // Simulate work\n                    Thread.Sleep(100);\n\n                    // Add the processed item to the results\n                    processedData.Add(item);\n                });\n            }\n            catch (OperationCanceledException)\n            {\n                Console.WriteLine(\"Operation was canceled.\");\n            }\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<CertificatePolicy> updateCertificatePolicyAsync(String vaultBaseUrl, String certificateName, CertificatePolicy certificatePolicy, final ServiceCallback<CertificatePolicy> serviceCallback) {\n        return ServiceFuture.fromResponse(updateCertificatePolicyWithServiceResponseAsync(vaultBaseUrl, certificateName, certificatePolicy), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```java\nimport static org.mockito.Mockito.*;\n\nimport org.junit.jupiter.api.Test;\nimport org.mockito.InjectMocks;\nimport org.mockito.Mock;\nimport org.mockito.MockitoAnnotations;\nimport org.springframework.dao.DataAccessException;\nimport org.springframework.jdbc.core.JdbcTemplate;\nimport org.springframework.transaction.TransactionStatus;\nimport org.springframework.transaction.support.TransactionCallbackWithoutResult;\nimport org.springframework.transaction.support.TransactionTemplate;\n\npublic class TransactionServiceTest {\n\n    @Mock\n    private JdbcTemplate jdbcTemplate; // Mocking JdbcTemplate\n\n    @Mock\n    private TransactionTemplate transactionTemplate; // Mocking TransactionTemplate\n\n    @InjectMocks\n    private TransactionService transactionService; // Injecting mocks into the service\n\n    public TransactionServiceTest() {\n        MockitoAnnotations.openMocks(this); // Initializing mocks\n    }\n\n    @Test\n    public void testTransactionRollbackOnException() {\n        doThrow(new DataAccessException(\"Simulated Exception\") {}) // Simulating exception\n            .when(jdbcTemplate).update(anyString(), any(Object[].class));\n\n        // Simulating transactionTemplate behavior with a callback\n        doAnswer(invocation -> {\n            TransactionCallbackWithoutResult callback = invocation.getArgument(0);\n            callback.doInTransactionWithoutResult(mock(TransactionStatus.class));\n            return null;\n        }).when(transactionTemplate).execute(any());\n\n        try {\n            transactionService.performDatabaseOperation(); // Execute method triggering the exception\n        } catch (DataAccessException e) {\n            // Exception expected, no action needed\n        }\n\n        // Verifying that the transaction rolled back by checking if rollback method is called\n        verify(transactionTemplate).execute(any(TransactionCallbackWithoutResult.class));\n        verify(jdbcTemplate, never()).update(anyString(), any(Object[].class)); // Ensure no updates were committed\n    }\n}\n```\n\n```java\n// TransactionService.java class that contains the method under test\npublic class TransactionService {\n\n    private final JdbcTemplate jdbcTemplate;\n    private final TransactionTemplate transactionTemplate;\n\n    public TransactionService(JdbcTemplate jdbcTemplate, TransactionTemplate transactionTemplate) {\n        this.jdbcTemplate = jdbcTemplate;\n        this.transactionTemplate = transactionTemplate;\n    }\n\n    public void performDatabaseOperation() {\n        transactionTemplate.execute(new TransactionCallbackWithoutResult() {\n            @Override\n            protected void doInTransactionWithoutResult(TransactionStatus status) {\n                jdbcTemplate.update(\"INSERT INTO example_table (column) VALUES (?)\", \"value\");\n            }\n        });\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nfrom airflow import DAG\nfrom airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator\nfrom airflow.providers.amazon.aws.operators.s3 import S3CreateObjectOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.utils.dates import days_ago\nimport pandas as pd\nimport boto3\nimport io\n\n# Set default arguments for DAG\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n}\n\n# Define a function to extract data from a source\ndef extract_data():\n    # Normally extract data from an API or database\n    # Here we simulate extracted data with a pandas DataFrame\n    data = pd.DataFrame({\n        'id': [1, 2, 3],\n        'value': ['A', 'B', 'C']\n    })\n    return data\n\n# Define a function to transform the data\ndef transform_data(ds, **kwargs):\n    # Retrieve data from previous task\n    ti = kwargs['ti']\n    data = ti.xcom_pull(task_ids='extract')\n    df = pd.DataFrame(data)\n\n    # Simulate a data transformation\n    df['value'] = df['value'].str.lower()\n    return df.to_dict()\n\n# Define a function to load data into S3\ndef load_to_s3(bucket_name, object_name, **kwargs):\n    ti = kwargs['ti']\n    df_dict = ti.xcom_pull(task_ids='transform')\n    df = pd.DataFrame(df_dict)\n\n    csv_buffer = io.StringIO()\n    df.to_csv(csv_buffer, index=False)\n    \n    s3_resource = boto3.resource('s3')\n    s3_resource.Object(bucket_name, object_name).put(Body=csv_buffer.getvalue())\n\n# Initialize the DAG\nwith DAG(\n    'etl_pipeline',\n    default_args=default_args,\n    description='An ETL pipeline in Airflow',\n    schedule_interval='@daily',\n    start_date=days_ago(1),\n    catchup=False,\n) as dag:\n\n    # ETL tasks using Python and S3ToRedshiftOperator\n    \n    extract = PythonOperator(\n        task_id='extract',\n        python_callable=extract_data,\n    )\n    \n    transform = PythonOperator(\n        task_id='transform',\n        python_callable=transform_data,\n        provide_context=True,\n    )\n    \n    load_s3 = PythonOperator(\n        task_id='load_to_s3',\n        python_callable=load_to_s3,\n        op_kwargs={\n            'bucket_name': 'my-bucket',\n            'object_name': 'data/my_data.csv',\n        },\n        provide_context=True,\n    )\n\n    load_redshift = S3ToRedshiftOperator(\n        task_id='load_to_redshift',\n        schema='public',\n        table='my_table',\n        s3_bucket='my-bucket',\n        s3_key='data/my_data.csv',\n        copy_options=['csv'],\n        aws_conn_id='aws_default',\n        redshift_conn_id='redshift_default',\n    )\n    \n    # Set task dependencies\n    extract >> transform >> load_s3 >> load_redshift\n```",
    "label": 1
  },
  {
    "codes": "```python\nclass NMEAParser:\n    \"\"\"\n    NMEAParser is a simple class to parse NMEA GPS sentences.\n    \"\"\"\n\n    def __init__(self):\n        # Initialize parsed data attributes\n        self.latitude = None\n        self.longitude = None\n        self.timestamp = None\n        self.data_valid = False\n    \n    def parse(self, nmea_sentence):\n        \"\"\"\n        Parses an NMEA sentence to extract GPS data.\n        :param nmea_sentence: A string containing an NMEA sentence.\n        \"\"\"\n        # Split the sentence into its components\n        parts = nmea_sentence.strip().split(',')\n        \n        # Validate the sentence is a GPGGA sentence\n        if parts[0] == '$GPGGA':\n            # Parse time, latitude, longitude, and validity\n            try:\n                self.timestamp = parts[1]  # Extract the timestamp\n                self.latitude = self._parse_latitude(parts[2], parts[3])  # Convert to decimal degrees\n                self.longitude = self._parse_longitude(parts[4], parts[5])  # Convert to decimal degrees\n                self.data_valid = parts[6] == '1'  # Check if data is valid\n            except (ValueError, IndexError):\n                # Handle parsing errors\n                self.data_valid = False\n    \n    @staticmethod\n    def _parse_latitude(lat_str, direction):\n        \"\"\"\n        Convert raw latitude string into decimal degrees\n        :param lat_str: Raw latitude string from NMEA sentence\n        :param direction: 'N' or 'S' for north/south\n        :return: Latitude in decimal degrees\n        \"\"\"\n        if not lat_str or not direction:\n            return None\n\n        latitude = float(lat_str[:2]) + float(lat_str[2:]) / 60\n        if direction == 'S':  # Check if latitude is south\n            latitude = -latitude\n        return latitude\n    \n    @staticmethod\n    def _parse_longitude(lon_str, direction):\n        \"\"\"\n        Convert raw longitude string into decimal degrees\n        :param lon_str: Raw longitude string from NMEA sentence\n        :param direction: 'E' or 'W' for east/west\n        :return: Longitude in decimal degrees\n        \"\"\"\n        if not lon_str or not direction:\n            return None\n\n        longitude = float(lon_str[:3]) + float(lon_str[3:]) / 60\n        if direction == 'W':  # Check if longitude is west\n            longitude = -longitude\n        return longitude\n```\n",
    "label": 1
  },
  {
    "codes": "public WorkerPoolResourceInner getMultiRolePool(String resourceGroupName, String name) {\n        return getMultiRolePoolWithServiceResponseAsync(resourceGroupName, name).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "func (s *ListDevEndpointsInput) SetMaxResults(v int64) *ListDevEndpointsInput {\n\ts.MaxResults = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *CreateDevEndpointInput) SetSubnetId(v string) *CreateDevEndpointInput {\n\ts.SubnetId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```kotlin\nsealed class DomainError {\n    object NetworkError : DomainError() // Represents a network-related error\n    object NotFoundError : DomainError() // Represents a resource not found error\n    object UnauthorizedError : DomainError() // Represents an unauthorized access error\n    data class ValidationError(val message: String) : DomainError() // Represents validation errors with a message\n    data class ServerError(val errorCode: Int, val message: String) : DomainError() // Represents server error with code and message\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<Page<SasDefinitionItem>>> getSasDefinitionsNextWithServiceResponseAsync(final String nextPageLink) {\n        return getSasDefinitionsNextSinglePageAsync(nextPageLink)\n            .concatMap(new Func1<ServiceResponse<Page<SasDefinitionItem>>, Observable<ServiceResponse<Page<SasDefinitionItem>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<SasDefinitionItem>>> call(ServiceResponse<Page<SasDefinitionItem>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(getSasDefinitionsNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "function () {\n            var thisOption = this.option;\n\n            return zrUtil.bind(\n                thisOption.mappingMethod === 'category'\n                    ? function (value, isNormalized) {\n                        !isNormalized && (value = this._normalizeData(value));\n                        return doMapCategory.call(this, value);\n                    }\n                    : function (value, isNormalized, out) {\n                        // If output rgb array\n                        // which will be much faster and useful in pixel manipulation\n                        var returnRGBArray = !!out;\n                        !isNormalized && (value = this._normalizeData(value));\n                        out = zrColor.fastLerp(value, thisOption.parsedVisual, out);\n                        return returnRGBArray ? out : zrColor.stringify(out, 'rgba');\n                    },\n                this\n            );\n        }",
    "label": 0
  },
  {
    "codes": "```powershell\n# Define the directory containing the files\n$directoryPath = \"C:\\path\\to\\your\\directory\"\n\n# Define the regex pattern to match the filenames\n$regexPattern = \"oldPattern(\\d+)\"\n\n# Define the replacement pattern\n$replacementPattern = \"newPattern$1\"\n\n# Get all files in the specified directory\nGet-ChildItem -Path $directoryPath | Where-Object { $_.PSIsContainer -eq $false } | ForEach-Object {\n    # If the filename matches the specified regex pattern\n    if ($_ -match $regexPattern) {\n        # Create the new filename using the replacement pattern\n        $newFileName = $_.Name -replace $regexPattern, $replacementPattern\n        \n        # Define the full path of the new file\n        $newFilePath = Join-Path -Path $_.DirectoryName -ChildPath $newFileName\n        \n        # Rename the file\n        Rename-Item -Path $_.FullName -NewName $newFilePath\n        \n        # Output the change to the console\n        Write-Host \"Renamed '$($_.Name)' to '$newFileName'\"\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "function parse_EncInfoStd(blob) {\n\tvar flags = blob.read_shift(4);\n\tif((flags & 0x3F) != 0x24) throw new Error(\"EncryptionInfo mismatch\");\n\tvar sz = blob.read_shift(4);\n\t//var tgt = blob.l + sz;\n\tvar hdr = parse_EncryptionHeader(blob, sz);\n\tvar verifier = parse_EncryptionVerifier(blob, blob.length - blob.l);\n\treturn { t:\"Std\", h:hdr, v:verifier };\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<Resource>>> listResourceNextWithServiceResponseAsync(final String nextPageLink) {\n        return listResourceNextSinglePageAsync(nextPageLink)\n            .concatMap(new Func1<ServiceResponse<Page<Resource>>, Observable<ServiceResponse<Page<Resource>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<Resource>>> call(ServiceResponse<Page<Resource>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(listResourceNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```javascript\nfunction mergeSortedArrays(arr1, arr2) {\n    let merged = [];\n    let i = 0, j = 0;\n\n    // Traverse both arrays and push the smaller element to the merged array\n    while (i < arr1.length && j < arr2.length) {\n        if (arr1[i] < arr2[j]) {\n            merged.push(arr1[i]);\n            i++;\n        } else {\n            merged.push(arr2[j]);\n            j++;\n        }\n    }\n\n    // If there are remaining elements in arr1, add them to merged\n    while (i < arr1.length) {\n        merged.push(arr1[i]);\n        i++;\n    }\n\n    // If there are remaining elements in arr2, add them to merged\n    while (j < arr2.length) {\n        merged.push(arr2[j]);\n        j++;\n    }\n\n    return merged;\n}\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n)\n\n// handleConnection manages the input and output for the connection\nfunc handleConnection(conn net.Conn, wg *sync.WaitGroup) {\n\tdefer wg.Done() // Signal that we're done once the function exits\n\tdefer conn.Close() // Ensure the connection is closed when done\n\n\treader := bufio.NewReader(conn)\n\tfor {\n\t\t// Read client input from the connection\n\t\tmessage, err := reader.ReadString('\\n')\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error reading:\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Clean up the input and prepare a response\n\t\tmessage = strings.TrimSpace(message)\n\t\tfmt.Println(\"Received:\", message)\n\n\t\t// If client wants to exit, break the loop\n\t\tif message == \"QUIT\" {\n\t\t\tfmt.Println(\"Client requested to close the connection\")\n\t\t\treturn\n\t\t}\n\n\t\tresponse := fmt.Sprintf(\"Echo: %s\\n\", message)\n\t\t// Send the response back to the client\n\t\t_, err = conn.Write([]byte(response))\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error writing:\", err)\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc main() {\n\tlistener, err := net.Listen(\"tcp\", \":8080\")\n\tif err != nil {\n\t\tfmt.Println(\"Error listening on port 8080:\", err)\n\t\treturn\n\t}\n\tdefer listener.Close() // Ensure the listener is closed when we're done\n\n\tvar wg sync.WaitGroup // Helps manage goroutines\n\n\tfmt.Println(\"Server listening on port 8080\")\n\n\tfor {\n\t\t// Wait for a connection from a client\n\t\tconn, err := listener.Accept()\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error accepting connection:\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\twg.Add(1) // Increment the wait group counter\n\t\t// Handle client connection concurrently\n\t\tgo handleConnection(conn, &wg)\n\t}\n\n\twg.Wait() // Wait for all connections to be handled\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *CatalogImportStatus) SetImportedBy(v string) *CatalogImportStatus {\n\ts.ImportedBy = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.security.KeyStore;\nimport java.security.cert.CertificateException;\nimport java.security.cert.X509Certificate;\nimport java.security.NoSuchAlgorithmException;\nimport java.security.KeyStoreException;\n\npublic class KeyStoreManager {\n\n    private KeyStore keyStore;\n\n    // Initialize the KeyStore\n    public KeyStoreManager(String keyStorePath, char[] password) throws Exception {\n        keyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n        try (FileInputStream fis = new FileInputStream(keyStorePath)) {\n            keyStore.load(fis, password);\n        }\n    }\n\n    // Save the KeyStore to a file\n    public void saveKeyStore(String keyStorePath, char[] password) throws Exception {\n        try (FileOutputStream fos = new FileOutputStream(keyStorePath)) {\n            keyStore.store(fos, password);\n        }\n    }\n\n    // Add a certificate to the KeyStore\n    public void addCertificate(String alias, X509Certificate cert) throws KeyStoreException {\n        keyStore.setCertificateEntry(alias, cert);\n    }\n\n    // Retrieve a certificate from the KeyStore\n    public X509Certificate getCertificate(String alias) throws KeyStoreException {\n        return (X509Certificate) keyStore.getCertificate(alias);\n    }\n\n    // Delete a certificate from the KeyStore\n    public void deleteCertificate(String alias) throws KeyStoreException {\n        keyStore.deleteEntry(alias);\n    }\n\n    // Check if an alias exists\n    public boolean aliasExists(String alias) throws KeyStoreException {\n        return keyStore.containsAlias(alias);\n    }\n}\n```\n\n- Creates a `KeyStoreManager` class for handling X.509 certificates in a Java KeyStore.\n- Initializes the KeyStore with a file path and password.\n- Provides methods to save the KeyStore, add, retrieve, and delete certificates, and check alias existence.",
    "label": 1
  },
  {
    "codes": "```javascript\n// Import necessary libraries\nconst xss = require('xss');\n\n// Middleware to sanitize user input against XSS attacks\nconst sanitizeInput = (req, res, next) => {\n    // Sanitize request body\n    if (req.body) {\n        for (let key in req.body) {\n            if (req.body.hasOwnProperty(key)) {\n                req.body[key] = xss(req.body[key]);\n            }\n        }\n    }\n\n    // Sanitize request query parameters\n    if (req.query) {\n        for (let key in req.query) {\n            if (req.query.hasOwnProperty(key)) {\n                req.query[key] = xss(req.query[key]);\n            }\n        }\n    }\n\n    // Sanitize request params\n    if (req.params) {\n        for (let key in req.params) {\n            if (req.params.hasOwnProperty(key)) {\n                req.params[key] = xss(req.params[key]);\n            }\n        }\n    }\n\n    // Proceed to the next middleware or route handler\n    next();\n};\n\nmodule.exports = sanitizeInput;\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<List<UsageInner>> listMultiRoleUsagesAsync(final String resourceGroupName, final String name, final ListOperationCallback<UsageInner> serviceCallback) {\n        return AzureServiceFuture.fromPageResponse(\n            listMultiRoleUsagesSinglePageAsync(resourceGroupName, name),\n            new Func1<String, Observable<ServiceResponse<Page<UsageInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<UsageInner>>> call(String nextPageLink) {\n                    return listMultiRoleUsagesNextSinglePageAsync(nextPageLink);\n                }\n            },\n            serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"log\"\n\t\"net/http\"\n)\n\n// RecoverMiddleware is a middleware that recovers from panics in HTTP handlers.\nfunc RecoverMiddleware(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\t// Defer a function to handle the panic and recover\n\t\tdefer func() {\n\t\t\t// Check if there was a panic\n\t\t\tif err := recover(); err != nil {\n\t\t\t\t// Log the panic\n\t\t\t\tlog.Printf(\"Recovered from panic: %v\", err)\n\t\t\t\t// Send a generic error response to the client\n\t\t\t\thttp.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError)\n\t\t\t}\n\t\t}()\n\t\t// Call the next handler in the chain\n\t\tnext.ServeHTTP(w, r)\n\t})\n}\n\n// Usage of the middleware with an example handler\nfunc exampleHandler(w http.ResponseWriter, r *http.Request) {\n\t// Example code that may panic\n\tpanic(\"something went wrong!\")\n}\n\nfunc main() {\n\thttp.Handle(\"/\", RecoverMiddleware(http.HandlerFunc(exampleHandler)))\n\thttp.ListenAndServe(\":8080\", nil)\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# CircuitPython NMEA GPS Parser Library\n\nimport re\n\nclass NMEAParser:\n    def __init__(self):\n        self.sentence_pattern = re.compile(r'^\\$([A-Z]{2}[A-Z]{3}),(.*)\\*([0-9A-Fa-f]{2})$')\n        self.checksum_pattern = re.compile(r'^\\$.*\\*[0-9A-Fa-f]{2}$')\n\n    def checksum(self, sentence):\n        \"\"\"Calculate the NMEA checksum for a sentence.\"\"\"\n        checksum = 0\n        for char in sentence[1:]:  # Skip the '$'\n            if char == '*':\n                break\n            checksum ^= ord(char)\n        return f\"{checksum:02X}\"\n\n    def validate_checksum(self, sentence):\n        \"\"\"Validate the checksum of an NMEA sentence.\"\"\"\n        if not self.checksum_pattern.match(sentence):\n            return False\n        calculated_checksum = self.checksum(sentence)\n        provided_checksum = sentence[-2:]\n        return calculated_checksum == provided_checksum\n\n    def parse(self, sentence):\n        \"\"\"Parse an NMEA sentence into its components.\"\"\"\n        if not self.validate_checksum(sentence):\n            return None\n\n        match = self.sentence_pattern.match(sentence)\n        if not match:\n            return None\n\n        talker_id = match.group(1)[:2]\n        sentence_type = match.group(1)[2:]\n        data_fields = match.group(2).split(',')\n        checksum = match.group(3)\n\n        return {\n            'talker_id': talker_id,\n            'sentence_type': sentence_type,\n            'data_fields': data_fields,\n            'checksum': checksum\n        }\n\n# Example usage:\n# parser = NMEAParser()\n# sentence = \"$GPGGA,123519,4807.038,N,01131.000,E,1,08,0.9,545.4,M,46.9,M,,*47\"\n# parsed_data = parser.parse(sentence)\n# print(parsed_data)\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport re\n\nclass NMEAParser:\n    def __init__(self):\n        self.data = {}\n\n    def parse(self, nmea_sentence):\n        sentence_type = self._get_sentence_type(nmea_sentence)\n\n        if sentence_type == 'GGA':\n            self._parse_gga(nmea_sentence)\n        elif sentence_type == 'RMC':\n            self._parse_rmc(nmea_sentence)\n        # Add more sentence types as needed\n\n    def _get_sentence_type(self, nmea_sentence):\n        try:\n            return nmea_sentence.split(',')[0][3:]\n        except IndexError:\n            return None\n\n    def _parse_gga(self, nmea_sentence):\n        fields = nmea_sentence.split(',')\n        try:\n            self.data['time'] = fields[1]\n            self.data['latitude'] = self._convert_to_decimal(fields[2], fields[3])\n            self.data['longitude'] = self._convert_to_decimal(fields[4], fields[5])\n            self.data['fix_quality'] = fields[6]\n            self.data['satellites'] = fields[7]\n            self.data['altitude'] = fields[9]\n        except (IndexError, ValueError):\n            pass  # handle invalid data\n\n    def _parse_rmc(self, nmea_sentence):\n        fields = nmea_sentence.split(',')\n        try:\n            self.data['time'] = fields[1]\n            self.data['status'] = fields[2]\n            self.data['latitude'] = self._convert_to_decimal(fields[3], fields[4])\n            self.data['longitude'] = self._convert_to_decimal(fields[5], fields[6])\n            self.data['speed'] = fields[7]\n            self.data['track_angle'] = fields[8]\n            self.data['date'] = fields[9]\n        except (IndexError, ValueError):\n            pass  # handle invalid data\n\n    def _convert_to_decimal(self, coordinate, direction):\n        try:\n            degrees = float(coordinate[:2])\n            minutes = float(coordinate[2:])\n            decimal_degrees = degrees + (minutes / 60)\n            if direction in ['S', 'W']:\n                decimal_degrees *= -1\n            return decimal_degrees\n        except ValueError:\n            return None\n\n    def get_data(self):\n        return self.data\n```\n",
    "label": 1
  },
  {
    "codes": "public PagedList<SkuInfoInner> listWorkerPoolSkus(final String resourceGroupName, final String name, final String workerPoolName) {\n        ServiceResponse<Page<SkuInfoInner>> response = listWorkerPoolSkusSinglePageAsync(resourceGroupName, name, workerPoolName).toBlocking().single();\n        return new PagedList<SkuInfoInner>(response.body()) {\n            @Override\n            public Page<SkuInfoInner> nextPage(String nextPageLink) {\n                return listWorkerPoolSkusNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\nLOG_FILE=\"/path/to/apache_log.log\"\nTEMP_FILE=\"/tmp/error_counts.txt\"\n\n# Extract 5xx errors, format: date, status\ngrep 'HTTP/1.[01]\" 5' \"$LOG_FILE\" | awk '{print $4, $5}' | sed 's/\\[//;s/\\]//;s/\"/ /g' | sort | uniq -c > \"$TEMP_FILE\"\n\n# Detect spikes (for example, more than 10 errors in a 1-minute window)\necho \"5xx Error Spikes Detected:\"\nawk '$1 > 10 {print $0}' \"$TEMP_FILE\"\n\n# Clean up\nrm \"$TEMP_FILE\"\n```",
    "label": 1
  },
  {
    "codes": "```zig\nconst std = @import(\"std\");\nconst nrf = @import(\"nrf52\");\n\n// Function to enter low-power sleep mode\nfn enterLowPowerSleep() void {\n    // Configure the system to enter low-power mode\n    nrf.POWER.TASKS_LOWPWR = 1;\n\n    // Wait for the system to enter low-power mode\n    while (nrf.POWER.EVENTS_LOWPWR == 0) {}\n\n    // Clear the event flag\n    nrf.POWER.EVENTS_LOWPWR = 0;\n\n    // Enter sleep mode\n    asm volatile (\"wfi\");\n}\n\n// Function to wake up from low-power sleep mode\nfn wakeUp() void {\n    // Trigger a wake-up event\n    nrf.POWER.TASKS_CONSTLAT = 1;\n\n    // Wait for the system to wake up\n    while (nrf.POWER.EVENTS_POFWARN == 0) {}\n\n    // Clear the event flag\n    nrf.POWER.EVENTS_POFWARN = 0;\n}\n\npub fn main() void {\n    // Example usage\n    std.debug.print(\"Entering low-power sleep mode...\\n\", .{});\n    enterLowPowerSleep();\n\n    std.debug.print(\"Waking up from low-power sleep mode...\\n\", .{});\n    wakeUp();\n}\n```\n\nThis code demonstrates how to manage low-power sleep modes on an nRF52 microcontroller using Zig. The `enterLowPowerSleep` function puts the system into a low-power state, while the `wakeUp` function brings it back to an active state. The `main` function shows an example of how to use these functions.",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<List<NodeFile>> listFromComputeNodeAsync(final String poolId, final String nodeId, final Boolean recursive, final FileListFromComputeNodeOptions fileListFromComputeNodeOptions, final ListOperationCallback<NodeFile> serviceCallback) {\n        return AzureServiceFuture.fromHeaderPageResponse(\n                listFromComputeNodeSinglePageAsync(poolId, nodeId, recursive, fileListFromComputeNodeOptions),\n                new Func1<String, Observable<ServiceResponseWithHeaders<Page<NodeFile>, FileListFromComputeNodeHeaders>>>() {\n                    @Override\n                    public Observable<ServiceResponseWithHeaders<Page<NodeFile>, FileListFromComputeNodeHeaders>> call(String nextPageLink) {\n                        FileListFromComputeNodeNextOptions fileListFromComputeNodeNextOptions = null;\n                        if (fileListFromComputeNodeOptions != null) {\n                            fileListFromComputeNodeNextOptions = new FileListFromComputeNodeNextOptions();\n                            fileListFromComputeNodeNextOptions.withClientRequestId(fileListFromComputeNodeOptions.clientRequestId());\n                            fileListFromComputeNodeNextOptions.withReturnClientRequestId(fileListFromComputeNodeOptions.returnClientRequestId());\n                            fileListFromComputeNodeNextOptions.withOcpDate(fileListFromComputeNodeOptions.ocpDate());\n                        }\n                        return listFromComputeNodeNextSinglePageAsync(nextPageLink, fileListFromComputeNodeNextOptions);\n                    }\n                },\n                serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```javascript\nimport React, { useState, useEffect } from 'react';\n\nconst Chat = () => {\n  const [messages, setMessages] = useState([]);\n  const [input, setInput] = useState('');\n  \n  // Mock function to simulate receiving messages in real time\n  useEffect(() => {\n    const receiveMessage = (msg) => {\n      setMessages(prevMessages => [...prevMessages, { text: msg, sender: 'other' }]);\n    };\n\n    const interval = setInterval(() => receiveMessage('Hello from other user!'), 5000);\n    return () => clearInterval(interval);\n  }, []);\n\n  const handleSendMessage = () => {\n    if(input.trim()) {\n      setMessages([...messages, { text: input, sender: 'me'}]);\n      setInput('');\n    }\n  };\n\n  return (\n    <div style={{ border: '1px solid #ccc', padding: '10px', width: '300px' }}>\n      <div style={{ overflowY: 'auto', height: '200px', border: '1px solid #eee', marginBottom: '10px' }}>\n        {messages.map((msg, index) => (\n          <div key={index} style={{ textAlign: msg.sender === 'me' ? 'right' : 'left' }}>\n            <span>{msg.sender === 'me' ? 'Me: ' : 'Other: '}{msg.text}</span>\n          </div>\n        ))}\n      </div>\n      <div style={{ display: 'flex' }}>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={(e) => setInput(e.target.value)}\n          style={{ flexGrow: 1, marginRight: '5px' }}\n        />\n        <button onClick={handleSendMessage}>Send</button>\n      </div>\n    </div>\n  );\n};\n\nexport default Chat;\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glue) GetCrawlerMetricsPages(input *GetCrawlerMetricsInput, fn func(*GetCrawlerMetricsOutput, bool) bool) error {\n\treturn c.GetCrawlerMetricsPagesWithContext(aws.BackgroundContext(), input, fn)\n}",
    "label": 0
  },
  {
    "codes": "```typescript\n// Interface representing a geometric shape\ninterface GeometricShape {\n    // Method to calculate the area of the shape\n    calculateArea(): number;\n}\n\n// Example implementation for a Rectangle\nclass Rectangle implements GeometricShape {\n    constructor(private width: number, private height: number) {}\n\n    // Implements the calculateArea method for a rectangle\n    calculateArea(): number {\n        return this.width * this.height;\n    }\n}\n\n// Example implementation for a Circle\nclass Circle implements GeometricShape {\n    constructor(private radius: number) {}\n\n    // Implements the calculateArea method for a circle\n    calculateArea(): number {\n        return Math.PI * this.radius * this.radius;\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Drop rows with any null values\ndf_cleaned = df.dropna()\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```\n\n```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Drop columns with any null values\ndf_cleaned = df.dropna(axis=1)\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```\n\n```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Fill null values with a specific value (e.g., 0)\ndf_cleaned = df.fillna(0)\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```\n\n```python\nimport pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Drop rows where all values are null\ndf_cleaned = df.dropna(how='all')\n\n# Save the cleaned data to a new CSV file\ndf_cleaned.to_csv('cleaned_data.csv', index=False)\n```",
    "label": 1
  },
  {
    "codes": "def alias(self, annotationtype, set, fallback=False):\n        \"\"\"Return the alias for a set (if applicable, returns the unaltered set otherwise iff fallback is enabled)\"\"\"\n        if inspect.isclass(annotationtype): annotationtype = annotationtype.ANNOTATIONTYPE\n        if annotationtype in self.set_alias and set in self.set_alias[annotationtype]:\n            return self.set_alias[annotationtype][set]\n        elif fallback:\n            return set\n        else:\n            raise KeyError(\"No alias for set \" + set)",
    "label": 0
  },
  {
    "codes": "```javascript\n// main.js\nconst workerPoolSize = navigator.hardwareConcurrency || 4; // Optimal number of workers based on available CPU cores\nlet workerPool = [];\nlet taskQueue = [];\n\n// Initialize the worker pool\nfor (let i = 0; i < workerPoolSize; i++) {\n    const worker = new Worker('worker.js');\n    worker.onmessage = handleWorkerResponse;\n    workerPool.push({\n        worker: worker,\n        busy: false\n    });\n}\n\nfunction dispatchTask(imageData) {\n    const availableWorker = workerPool.find(w => !w.busy);\n    if (availableWorker) {\n        availableWorker.busy = true;\n        availableWorker.worker.postMessage({ imageData }); // Send the task to the worker\n    } else {\n        taskQueue.push(imageData); // Queue the task if no workers are available\n    }\n}\n\nfunction handleWorkerResponse(event) {\n    const workerData = workerPool.find(w => w.worker === event.target);\n    workerData.busy = false;\n    \n    // Handle the processed image data returned by worker\n    const processedImageData = event.data;\n    // e.g., update UI with processedImageData\n\n    // Check if there are pending tasks in the queue\n    if (taskQueue.length > 0) {\n        const nextTask = taskQueue.shift();\n        dispatchTask(nextTask);\n    }\n}\n\n// Example of task dispatch\nconst imageDatas = [/* Array of image data to be processed */];\nimageDatas.forEach(imageData => dispatchTask(imageData));\n\n// worker.js\nself.onmessage = function(event) {\n    const imageData = event.data.imageData;\n    const processedImageData = processImage(imageData); // Perform CPU-intensive image processing\n\n    self.postMessage(processedImageData); // Send back the processed image data\n}\n\nfunction processImage(imageData) {\n    // Placeholder for image processing logic, e.g., applying filters\n    // Perform CPU-intensive tasks here\n\n    return imageData; // Return processed image data\n}\n```\n",
    "label": 1
  },
  {
    "codes": "def set_option(name, value):\n    \"\"\"\n    Set plydata option\n\n    Parameters\n    ----------\n    name : str\n        Name of the option\n    value : object\n        New value of the option\n\n    Returns\n    -------\n    old : object\n        Old value of the option\n\n    See also\n    --------\n    :class:`options`\n    \"\"\"\n    old = get_option(name)\n    globals()[name] = value\n    return old",
    "label": 0
  },
  {
    "codes": "def make_functional_connectivity(self, njobs=None, returngroup=False, file_hdr=None, file_idx=None):\n        \"\"\"\n        Makes connectivity matrix for each of the subjects.\n\n        Parameters\n        ----------\n        returngroup : bool, default=False\n            If true, returns the group average connectivity matrix.\n        njobs : int\n            How many parallel jobs to run\n        file_idx : bool\n            Default False, true if to ignore index column in loaded file.\n        file_hdr : bool\n            Default False, true if to ignore header row in loaded file.\n\n        Returns\n        -------\n        Saves data in derivatives/teneto_<version>/.../fc/\n        R_group : array\n            if returngroup is true, the average connectivity matrix is returned.\n\n        \"\"\"\n        if not njobs:\n            njobs = self.njobs\n        self.add_history(inspect.stack()[0][3], locals(), 1)\n        files = self.get_selected_files(quiet=1)\n\n        R_group = []\n\n        with ProcessPoolExecutor(max_workers=njobs) as executor:\n            job = {executor.submit(\n                self._run_make_functional_connectivity, f, file_hdr, file_idx) for f in files}\n            for j in as_completed(job):\n                R_group.append(j.result())\n\n        if returngroup:\n            # Fisher tranform -> mean -> inverse fisher tranform\n            R_group = np.tanh(np.mean(np.arctanh(np.array(R_group)), axis=0))\n            return np.array(R_group)",
    "label": 0
  },
  {
    "codes": "```typescript\nfunction Retry(maxRetries: number = 3, delay: number = 1000) {\n    return function (target: any, propertyKey: string, descriptor: PropertyDescriptor) {\n        const originalMethod = descriptor.value;\n\n        descriptor.value = async function (...args: any[]) {\n            let retries = 0;\n\n            while (retries < maxRetries) {\n                try {\n                    // Attempt to execute the original method\n                    const result = await originalMethod.apply(this, args);\n                    return result; // If successful, return the result\n                } catch (error) {\n                    retries++;\n                    if (retries >= maxRetries) {\n                        throw error; // If max retries reached, throw the error\n                    }\n                    await new Promise(resolve => setTimeout(resolve, delay)); // Wait before retrying\n                }\n            }\n        };\n\n        return descriptor;\n    };\n}\n\n// Example usage:\nclass MyService {\n    @Retry(5, 2000) // Retry up to 5 times with a 2-second delay\n    async fetchData() {\n        // Simulate a failing async operation\n        throw new Error(\"Failed to fetch data\");\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function parse_Format(blob, length, opts) {\n\tvar numFmtId = blob.read_shift(2);\n\tvar fmtstr = parse_XLUnicodeString2(blob, 0, opts);\n\treturn [numFmtId, fmtstr];\n}",
    "label": 0
  },
  {
    "codes": "```python\nfrom machine import Pin, I2C\nimport time\n\n# Connect GPIO 21 to SDA and GPIO 22 to SCL of the temperature sensor\ni2c = I2C(0, scl=Pin(22), sda=Pin(21), freq=100000)\n\n# Address of the temperature sensor (example: TMP102)\nTEMP_SENSOR_ADDR = 0x48\n\ndef read_temp(sensor_addr):\n    # Reading temperature register from sensor\n    # TMP102 Temp register starts at 0x00\n    temp_bytes = i2c.readfrom_mem(sensor_addr, 0x00, 2)\n    \n    # Convert the data to 12-bits (sensor specific conversion)\n    temp_raw = (temp_bytes[0] << 4) | (temp_bytes[1] >> 4)\n    \n    # Convert to temperature in Celsius\n    if temp_raw > 0x7FF:\n        temp_raw -= 4096\n    temperature = temp_raw * 0.0625\n    \n    return temperature\n\nwhile True:\n    temperature = read_temp(TEMP_SENSOR_ADDR)\n    print('Temperature:', temperature, 'C')\n    time.sleep(1)\n```",
    "label": 1
  },
  {
    "codes": "```vue\n<template>\n  <div>\n    <h1>Stock Market Data Visualization</h1>\n    <select v-model=\"selectedStock\" @change=\"fetchStockData\">\n      <option v-for=\"stock in stocks\" :key=\"stock.symbol\" :value=\"stock.symbol\">\n        {{ stock.name }} ({{ stock.symbol }})\n      </option>\n    </select>\n    <line-chart :data=\"chartData\" v-if=\"chartData.length\"></line-chart>\n  </div>\n</template>\n\n<script>\nimport LineChart from './LineChart.vue'; // Import a LineChart component\n\nexport default {\n  components: {\n    LineChart\n  },\n  data() {\n    return {\n      stocks: [\n        { name: 'Apple', symbol: 'AAPL' },\n        { name: 'Google', symbol: 'GOOGL' },\n        { name: 'Microsoft', symbol: 'MSFT' },\n      ],\n      selectedStock: 'AAPL',\n      chartData: []\n    };\n  },\n  methods: {\n    async fetchStockData() {\n      try {\n        const response = await fetch(`https://api.example.com/stock/${this.selectedStock}/historical`);\n        const data = await response.json();\n        this.chartData = data.map(item => ({\n          date: item.date,\n          price: item.close\n        }));\n      } catch (error) {\n        console.error('Error fetching stock data:', error);\n      }\n    }\n  },\n  mounted() {\n    this.fetchStockData();\n  }\n};\n</script>\n\n<style scoped>\nh1 {\n  text-align: center;\n}\n</style>\n```\n\n```vue\n<!-- LineChart.vue -->\n<template>\n  <canvas ref=\"canvas\"></canvas>\n</template>\n\n<script>\nexport default {\n  props: ['data'],\n  mounted() {\n    this.renderChart();\n  },\n  watch: {\n    data: 'renderChart' // Watch data changes to re-render the chart\n  },\n  methods: {\n    renderChart() {\n      const ctx = this.$refs.canvas.getContext('2d');\n      const labels = this.data.map(item => item.date);\n      const prices = this.data.map(item => item.price);\n\n      new Chart(ctx, {\n        type: 'line',\n        data: {\n          labels: labels,\n          datasets: [{\n            label: 'Stock Price',\n            data: prices,\n            fill: false,\n            borderColor: 'blue',\n            tension: 0.1\n          }]\n        },\n        options: {\n          responsive: true,\n          scales: {\n            x: {\n              type: 'time',\n              time: {\n                unit: 'day'\n              }\n            },\n            y: {\n              beginAtZero: false\n            }\n          }\n        }\n      });\n    }\n  }\n};\n</script>\n\n<style scoped>\ncanvas {\n  max-width: 600px;\n  margin: 0 auto;\n}\n</style>\n```",
    "label": 1
  },
  {
    "codes": "def get_dimord(measure, calc=None, community=None):\n    \"\"\"\n    Get the dimension order of a network measure.\n\n    Parameters\n    ----------\n\n    measure : str\n        Name of funciton in teneto.networkmeasures.\n    calc : str, default=None\n        Calc parameter for the function\n    community : bool, default=None\n        If not null, then community property is assumed to be believed.\n\n    Returns\n    -------\n\n    dimord : str\n        Dimension order. So \"node,node,time\" would define the dimensions of the network measure.\n\n    \"\"\"\n\n    if not calc:\n        calc = ''\n    else:\n        calc = '_' + calc\n    if not community:\n        community = ''\n    else:\n        community = 'community'\n    if 'community' in calc and 'community' in community:\n        community = ''\n    if calc == 'community_avg' or calc == 'community_pairs':\n        community = ''\n\n    dimord_dict = {\n        'temporal_closeness_centrality': 'node',\n        'temporal_degree_centrality': 'node',\n        'temporal_degree_centralit_avg': 'node',\n        'temporal_degree_centrality_time': 'node,time',\n        'temporal_efficiency': 'global',\n        'temporal_efficiency_global': 'global',\n        'temporal_efficiency_node': 'node',\n        'temporal_efficiency_to': 'node',\n        'sid_global': 'global,time',\n        'community_pairs': 'community,community,time',\n        'community_avg': 'community,time',\n        'sid': 'community,community,time',\n        'reachability_latency_global': 'global',\n        'reachability_latency': 'global',\n        'reachability_latency_node': 'node',\n        'fluctuability': 'node',\n        'fluctuability_global': 'global',\n        'bursty_coeff': 'edge,edge',\n        'bursty_coeff_edge': 'edge,edge',\n        'bursty_coeff_node': 'node',\n        'bursty_coeff_meanEdgePerNode': 'node',\n        'volatility_global': 'time',\n    }\n    if measure + calc + community in dimord_dict:\n        return dimord_dict[measure + calc + community]\n    else:\n        print('WARNINGL: get_dimord() returned unknown dimension labels')\n        return 'unknown'",
    "label": 0
  },
  {
    "codes": "```rust\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse std::time::{Duration, Instant};\nuse tokio::sync::Mutex;\nuse hyper::{Body, Request, Response, Server};\nuse hyper::service::{make_service_fn, service_fn};\nuse hyper::StatusCode;\n\n// Struct to hold rate limit data for each IP\nstruct RateLimiter {\n    requests: HashMap<String, (Instant, u32)>,\n    limit: u32,\n    window: Duration,\n}\n\nimpl RateLimiter {\n    fn new(limit: u32, window: Duration) -> Self {\n        Self {\n            requests: HashMap::new(),\n            limit,\n            window,\n        }\n    }\n\n    // Check if the IP has exceeded the rate limit\n    fn check(&mut self, ip: &str) -> bool {\n        let now = Instant::now();\n        let (last_request, count) = self.requests.entry(ip.to_string())\n            .or_insert((now, 0));\n\n        if now.duration_since(*last_request) > self.window {\n            *last_request = now;\n            *count = 1;\n            true\n        } else if *count < self.limit {\n            *count += 1;\n            true\n        } else {\n            false\n        }\n    }\n}\n\n// Handler for incoming HTTP requests\nasync fn handle_request(\n    req: Request<Body>,\n    rate_limiter: Arc<Mutex<RateLimiter>>,\n) -> Result<Response<Body>, hyper::Error> {\n    let ip = req.remote_addr().unwrap().ip().to_string();\n\n    // Check rate limit\n    let mut limiter = rate_limiter.lock().await;\n    if !limiter.check(&ip) {\n        return Ok(Response::builder()\n            .status(StatusCode::TOO_MANY_REQUESTS)\n            .body(Body::from(\"Rate limit exceeded\"))\n            .unwrap());\n    }\n\n    // Process the request\n    Ok(Response::new(Body::from(\"Hello, World!\")))\n}\n\n#[tokio::main]\nasync fn main() {\n    // Initialize rate limiter with a limit of 10 requests per 60 seconds\n    let rate_limiter = Arc::new(Mutex::new(RateLimiter::new(10, Duration::from_secs(60)));\n\n    // Create a service to handle requests\n    let make_svc = make_service_fn(move |_| {\n        let rate_limiter = Arc::clone(&rate_limiter);\n        async move {\n            Ok::<_, hyper::Error>(service_fn(move |req| {\n                handle_request(req, Arc::clone(&rate_limiter))\n            }))\n        }\n    });\n\n    // Bind the server to localhost:3000\n    let addr = ([127, 0, 0, 1], 3000).into();\n    let server = Server::bind(&addr).serve(make_svc);\n\n    // Run the server\n    if let Err(e) = server.await {\n        eprintln!(\"Server error: {}\", e);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<IssuerBundle> deleteCertificateIssuerAsync(String vaultBaseUrl, String issuerName, final ServiceCallback<IssuerBundle> serviceCallback) {\n        return ServiceFuture.fromResponse(deleteCertificateIssuerWithServiceResponseAsync(vaultBaseUrl, issuerName), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "public void deleteAtManagementGroup(String policySetDefinitionName, String managementGroupId) {\n        deleteAtManagementGroupWithServiceResponseAsync(policySetDefinitionName, managementGroupId).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "func (s *Action) SetJobName(v string) *Action {\n\ts.JobName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```verilog\n// PWM Generator Module\nmodule pwm_generator #(\n    parameter WIDTH = 8  // Bit width for PWM resolution\n)(\n    input wire clk,          // Clock signal\n    input wire rst,          // Reset signal\n    input wire [WIDTH-1:0] duty_cycle,  // Duty cycle input\n    output reg pwm_out       // PWM output signal\n);\n\n    reg [WIDTH-1:0] counter;  // Counter for PWM generation\n\n    always @(posedge clk or posedge rst) begin\n        if (rst) begin\n            counter <= 0;      // Reset counter on reset\n            pwm_out <= 0;       // Set PWM output to 0\n        end else begin\n            counter <= counter + 1;  // Increment counter\n            if (counter < duty_cycle) begin\n                pwm_out <= 1;  // Set PWM high if counter < duty_cycle\n            end else begin\n                pwm_out <= 0;  // Set PWM low otherwise\n            end\n        end\n    end\n\nendmodule\n```",
    "label": 1
  },
  {
    "codes": "```yaml\n# Chaos Experiment: Terminate Random Kubernetes Pods\n# Description: This chaos experiment script randomly selects and terminates a number of Kubernetes pods within a specified namespace\n# for the purpose of testing the resilience and recovery capabilities of Kubernetes services.\n# Prerequisites: kubectl, chaos-mesh installed in the cluster\n\napiVersion: chaos-mesh.org/v1alpha1\nkind: PodChaos\nmetadata:\n  name: pod-kill-experiment\n  namespace: chaos-testing # Namespace where Chaos Mesh is installed\nspec:\n  action: pod-kill # Type of chaos experiment\n  mode: random # Randomly select pods to target\n  selector:\n    namespaces:\n    - target-namespace # Replace with the namespace containing the target pods\n    labelSelectors:\n      \"app\": \"target-app\" # Replace with label selector to specify pods to target\n  duration: \"30s\" # Duration for which the chaos action should last\n  scheduler:\n    cron: \"@every 1m\" # Run the experiment every minute\n\n# Note: Be sure to replace \"target-namespace\" and \"target-app\" with actual values specific to your deployment.\n# This script will run indefinitely every minute, killing random pods in the specified application namespace\n# and then letting Kubernetes self-heal and recover.\n```",
    "label": 1
  },
  {
    "codes": "def subset(self, subset_id):\n        \"\"\"Returns information regarding the set\"\"\"\n        if subset_id in self.subsetcache:\n            return self.subsetcache[subset_id]\n        set_uri = self.get_set_uri(subset_id)\n        for row in self.graph.query(\"SELECT ?seturi ?setid ?setlabel ?setopen WHERE { ?seturi rdf:type skos:Collection . OPTIONAL { ?seturi skos:notation ?setid } OPTIONAL { ?seturi skos:prefLabel ?setlabel } OPTIONAL { ?seturi fsd:open ?setopen } FILTER (?seturi = <\" + str(set_uri)+\">) }\"):\n            self.subsetcache[str(row.setid)] = {'uri': str(row.seturi), 'id': str(row.setid), 'label': str(row.setlabel) if row.setlabel else \"\", 'open': bool(row.setopen) }\n            return self.subsetcache[str(row.setid)]\n        raise DeepValidationError(\"Unable to find subset (set_uri=\" + str(set_uri)+\")\")",
    "label": 0
  },
  {
    "codes": "public SasDefinitionBundle setSasDefinition(String vaultBaseUrl, String storageAccountName, String sasDefinitionName, String templateUri, SasTokenType sasType, String validityPeriod) {\n        return setSasDefinitionWithServiceResponseAsync(vaultBaseUrl, storageAccountName, sasDefinitionName, templateUri, sasType, validityPeriod).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "function () {\n        var axis = this;\n        var labelModel = axis.getLabelModel();\n\n        var ordinalScale = axis.scale;\n        var ordinalExtent = ordinalScale.getExtent();\n        // Providing this method is for optimization:\n        // avoid generating a long array by `getTicks`\n        // in large category data case.\n        var tickCount = ordinalScale.count();\n\n        if (ordinalExtent[1] - ordinalExtent[0] < 1) {\n            return 0;\n        }\n\n        var tickValue = ordinalExtent[0];\n        var unitSpan = axis.dataToCoord(tickValue + 1) - axis.dataToCoord(tickValue);\n        var unitH = Math.abs(unitSpan);\n\n        // Not precise, just use height as text width\n        // and each distance from axis line yet.\n        var rect = textContain.getBoundingRect(\n            tickValue, labelModel.getFont(), 'center', 'top'\n        );\n        var maxH = Math.max(rect.height, 7);\n\n        var dh = maxH / unitH;\n        // 0/0 is NaN, 1/0 is Infinity.\n        isNaN(dh) && (dh = Infinity);\n        var interval = Math.max(0, Math.floor(dh));\n\n        var cache = inner(axis.model);\n        var lastAutoInterval = cache.lastAutoInterval;\n        var lastTickCount = cache.lastTickCount;\n\n        // Use cache to keep interval stable while moving zoom window,\n        // otherwise the calculated interval might jitter when the zoom\n        // window size is close to the interval-changing size.\n        if (lastAutoInterval != null\n            && lastTickCount != null\n            && Math.abs(lastAutoInterval - interval) <= 1\n            && Math.abs(lastTickCount - tickCount) <= 1\n            // Always choose the bigger one, otherwise the critical\n            // point is not the same when zooming in or zooming out.\n            && lastAutoInterval > interval\n        ) {\n            interval = lastAutoInterval;\n        }\n        // Only update cache if cache not used, otherwise the\n        // changing of interval is too insensitive.\n        else {\n            cache.lastTickCount = tickCount;\n            cache.lastAutoInterval = interval;\n        }\n\n        return interval;\n    }",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define the directory containing the artifacts and PGP signatures\nARTIFACT_DIR=\"./artifacts\"\nPGP_KEY=\"YOUR_PGP_KEY_ID\"\n\n# Change to the artifact directory\ncd \"$ARTIFACT_DIR\" || exit 1\n\n# Loop through all files with .sig extension\nfor sig_file in *.sig; do\n    # Get the corresponding artifact file name by stripping the .sig extension\n    artifact_file=\"${sig_file%.sig}\"\n\n    # Check if the artifact file exists\n    if [[ ! -f \"$artifact_file\" ]]; then\n        echo \"Artifact file '$artifact_file' does not exist. Skipping signature verification for '$sig_file'.\"\n        continue\n    fi\n\n    # Verify the PGP signature\n    gpg --verify \"$sig_file\" \"$artifact_file\"\n    \n    # Check the status of the verification\n    if [[ $? -eq 0 ]]; then\n        echo \"Signature for '$artifact_file' is valid.\"\n    else\n        echo \"Signature for '$artifact_file' is NOT valid!\"\n        exit 1\n    fi\ndone\n\necho \"All signatures verified.\"\nexit 0\n```",
    "label": 1
  },
  {
    "codes": "def send(token, title, **kwargs):\n    \"\"\"\n    Site: https://boxcar.io/\n    API: http://help.boxcar.io/knowledgebase/topics/48115-boxcar-api\n    Desc: Best app for system administrators\n    \"\"\"\n    headers = {\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n        \"User-Agent\": \"DBMail/%s\" % get_version(),\n    }\n\n    data = {\n        \"user_credentials\": token,\n        \"notification[title]\": from_unicode(title),\n        \"notification[sound]\": \"notifier-2\"\n    }\n\n    for k, v in kwargs.items():\n        data['notification[%s]' % k] = from_unicode(v)\n\n    http = HTTPSConnection(kwargs.pop(\"api_url\", \"new.boxcar.io\"))\n    http.request(\n        \"POST\", \"/api/notifications\",\n        headers=headers,\n        body=urlencode(data))\n    response = http.getresponse()\n\n    if response.status != 201:\n        raise BoxcarError(response.reason)\n    return True",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <cstdlib>\n\n// Function to allocate memory and intentionally leak it\nvoid createMemoryLeak() {\n    int* leakyPointer = new int(42); // Allocate memory\n    // Intentionally not deleting the pointer to create a memory leak\n}\n\nint main() {\n    // Call the function that creates a memory leak\n    createMemoryLeak();\n\n    // Print a message to indicate the program has run\n    std::cout << \"Program executed. Check for memory leaks using Valgrind.\" << std::endl;\n\n    return 0;\n}\n```\n\n```bash\n# Compile the program with debugging symbols\ng++ -g -o memory_leak_debug memory_leak_debug.cpp\n\n# Run the program with Valgrind to detect memory leaks\nvalgrind --leak-check=full ./memory_leak_debug\n```\n\n```bash\n# Expected Valgrind output indicating a memory leak\n==12345== HEAP SUMMARY:\n==12345==     in use at exit: 4 bytes in 1 blocks\n==12345==   total heap usage: 1 allocs, 0 frees, 4 bytes allocated\n==12345== \n==12345== 4 bytes in 1 blocks are definitely lost in loss record 1 of 1\n==12345==    at 0x4C2BBAF: operator new(unsigned long) (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==12345==    by 0x4006F6: createMemoryLeak() (memory_leak_debug.cpp:6)\n==12345==    by 0x40071A: main (memory_leak_debug.cpp:12)\n==12345== \n==12345== LEAK SUMMARY:\n==12345==    definitely lost: 4 bytes in 1 blocks\n==12345==    indirectly lost: 0 bytes in 0 blocks\n==12345==      possibly lost: 0 bytes in 0 blocks\n==12345==    still reachable: 0 bytes in 0 blocks\n==12345==         suppressed: 0 bytes in 0 blocks\n```",
    "label": 1
  },
  {
    "codes": "```java\n// MainActivity.java\npackage com.example.objectdetection;\n\nimport android.Manifest;\nimport android.content.pm.PackageManager;\nimport android.graphics.Bitmap;\nimport android.graphics.BitmapFactory;\nimport android.os.Bundle;\nimport android.util.Log;\nimport android.view.SurfaceView;\nimport android.widget.Toast;\n\nimport androidx.annotation.NonNull;\nimport androidx.appcompat.app.AppCompatActivity;\nimport androidx.camera.core.Camera;\nimport androidx.camera.core.CameraSelector;\nimport androidx.camera.core.ImageAnalysis;\nimport androidx.camera.core.ImageProxy;\nimport androidx.camera.lifecycle.ProcessCameraProvider;\nimport androidx.camera.view.PreviewView;\nimport androidx.core.app.ActivityCompat;\nimport androidx.core.content.ContextCompat;\n\nimport com.google.common.util.concurrent.ListenableFuture;\nimport com.google.mlkit.vision.common.InputImage;\nimport com.google.mlkit.vision.objects.DetectedObject;\nimport com.google.mlkit.vision.objects.ObjectDetection;\nimport com.google.mlkit.vision.objects.ObjectDetector;\nimport com.google.mlkit.vision.objects.defaults.ObjectDetectorOptions;\n\nimport java.util.List;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n\npublic class MainActivity extends AppCompatActivity {\n\n    private static final int REQUEST_CODE_PERMISSIONS = 10;\n    private static final String[] REQUIRED_PERMISSIONS = {Manifest.permission.CAMERA};\n\n    private PreviewView previewView;\n    private ExecutorService cameraExecutor;\n    private ObjectDetector objectDetector;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        previewView = findViewById(R.id.previewView);\n\n        // Initialize the object detector\n        ObjectDetectorOptions options = new ObjectDetectorOptions.Builder()\n                .setDetectorMode(ObjectDetectorOptions.STREAM_MODE)\n                .enableClassification()\n                .build();\n        objectDetector = ObjectDetection.getClient(options);\n\n        // Check for camera permissions\n        if (allPermissionsGranted()) {\n            startCamera();\n        } else {\n            ActivityCompat.requestPermissions(this, REQUIRED_PERMISSIONS, REQUEST_CODE_PERMISSIONS);\n        }\n\n        cameraExecutor = Executors.newSingleThreadExecutor();\n    }\n\n    private void startCamera() {\n        ListenableFuture<ProcessCameraProvider> cameraProviderFuture = ProcessCameraProvider.getInstance(this);\n\n        cameraProviderFuture.addListener(() -> {\n            try {\n                ProcessCameraProvider cameraProvider = cameraProviderFuture.get();\n\n                // Set up the preview use case\n                Preview preview = new Preview.Builder().build();\n                preview.setSurfaceProvider(previewView.getSurfaceProvider());\n\n                // Set up the image analysis use case\n                ImageAnalysis imageAnalysis = new ImageAnalysis.Builder()\n                        .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)\n                        .build();\n\n                imageAnalysis.setAnalyzer(cameraExecutor, imageProxy -> {\n                    processImage(imageProxy);\n                    imageProxy.close();\n                });\n\n                // Select the back camera as the default\n                CameraSelector cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA;\n\n                // Unbind all use cases before rebinding\n                cameraProvider.unbindAll();\n\n                // Bind the camera to the lifecycle\n                Camera camera = cameraProvider.bindToLifecycle(\n                        this, cameraSelector, preview, imageAnalysis);\n\n            } catch (ExecutionException | InterruptedException e) {\n                Log.e(\"CameraX\", \"Use case binding failed\", e);\n            }\n        }, ContextCompat.getMainExecutor(this));\n    }\n\n    private void processImage(ImageProxy imageProxy) {\n        Bitmap bitmap = BitmapFactory.decodeByteArray(\n                imageProxy.getPlanes()[0].getBuffer().array(),\n                0,\n                imageProxy.getPlanes()[0].getBuffer().remaining()\n        );\n\n        InputImage image = InputImage.fromBitmap(bitmap, imageProxy.getImageInfo().getRotationDegrees());\n\n        objectDetector.process(image)\n                .addOnSuccessListener(detectedObjects -> {\n                    for (DetectedObject detectedObject : detectedObjects) {\n                        // Handle detected objects\n                        Log.d(\"ObjectDetection\", \"Detected object: \" + detectedObject.getTrackingId());\n                    }\n                })\n                .addOnFailureListener(e -> {\n                    Log.e(\"ObjectDetection\", \"Object detection failed\", e);\n                });\n    }\n\n    private boolean allPermissionsGranted() {\n        for (String permission : REQUIRED_PERMISSIONS) {\n            if (ContextCompat.checkSelfPermission(this, permission) != PackageManager.PERMISSION_GRANTED) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    @Override\n    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults);\n        if (requestCode == REQUEST_CODE_PERMISSIONS) {\n            if (allPermissionsGranted()) {\n                startCamera();\n            } else {\n                Toast.makeText(this, \"Permissions not granted by the user.\", Toast.LENGTH_SHORT).show();\n                finish();\n            }\n        }\n    }\n\n    @Override\n    protected void onDestroy() {\n        super.onDestroy();\n        cameraExecutor.shutdown();\n    }\n}\n```\n\n```xml\n<!-- activity_main.xml -->\n<androidx.constraintlayout.widget.ConstraintLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    tools:context=\".MainActivity\">\n\n    <androidx.camera.view.PreviewView\n        android:id=\"@+id/previewView\"\n        android:layout_width=\"0dp\"\n        android:layout_height=\"0dp\"\n        app:layout_constraintBottom_toBottomOf=\"parent\"\n        app:layout_constraintEnd_toEndOf=\"parent\"\n        app:layout_constraintStart_toStartOf=\"parent\"\n        app:layout_constraintTop_toTopOf=\"parent\" />\n\n</androidx.constraintlayout.widget.ConstraintLayout>\n```\n\n```xml\n<!-- AndroidManifest.xml -->\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.example.objectdetection\">\n\n    <uses-permission android:name=\"android.permission.CAMERA\" />\n\n    <application\n        android:allowBackup=\"true\"\n        android:icon=\"@mipmap/ic_launcher\"\n        android:label=\"@string/app_name\"\n        android:roundIcon=\"@mipmap/ic_launcher_round\"\n        android:supportsRtl=\"true\"\n        android:theme=\"@style/Theme.ObjectDetection\">\n        <activity android:name=\".MainActivity\">\n            <intent-filter>\n                <action android:name=\"android.intent.action.MAIN\" />\n                <category android:name=\"android.intent.category.LAUNCHER\" />\n            </intent-filter>\n        </activity>\n    </application>\n\n</manifest>\n```\n\n```gradle\n// build.gradle (Module: app)\ndependencies {\n    implementation 'androidx.appcompat:appcompat:1.3.1'\n    implementation 'androidx.camera:camera-core:1.0.0'\n    implementation 'androidx.camera:camera-camera2:1.0.0'\n    implementation 'androidx.camera:camera-lifecycle:1.0.0'\n    implementation 'androidx.camera:camera-view:1.0.0'\n    implementation 'com.google.mlkit:object-detection:16.2.3'\n}\n```",
    "label": 1
  },
  {
    "codes": "function computeChecksum(filename) {\n\tvar contents = fs.readFileSync(filename);\n\n\tvar hash = crypto\n\t\t.createHash('md5')\n\t\t.update(contents)\n\t\t.digest('base64')\n\t\t.replace(/=+$/, '');\n\n\treturn hash;\n}",
    "label": 0
  },
  {
    "codes": "```typescript\nimport { Injectable } from '@angular/core';\nimport { HttpClient, HttpParams } from '@angular/common/http';\nimport { Observable } from 'rxjs';\n\n@Injectable({\n  providedIn: 'root'\n})\nexport class PaginatedDataService {\n  private apiUrl = 'https://api.example.com/data'; // Replace with your API endpoint\n\n  constructor(private http: HttpClient) {}\n\n  // Fetch paginated data from the API\n  getPaginatedData(page: number, pageSize: number): Observable<any> {\n    // Set up query parameters for pagination\n    const params = new HttpParams()\n      .set('page', page.toString())\n      .set('pageSize', pageSize.toString());\n\n    // Make the HTTP GET request with the pagination parameters\n    return this.http.get<any>(this.apiUrl, { params });\n  }\n}\n```\n\n```typescript\n// Example usage in a component\nimport { Component, OnInit } from '@angular/core';\nimport { PaginatedDataService } from './paginated-data.service';\n\n@Component({\n  selector: 'app-data-list',\n  templateUrl: './data-list.component.html',\n  styleUrls: ['./data-list.component.css']\n})\nexport class DataListComponent implements OnInit {\n  data: any[] = [];\n  currentPage = 1;\n  pageSize = 10;\n\n  constructor(private dataService: PaginatedDataService) {}\n\n  ngOnInit(): void {\n    this.loadData();\n  }\n\n  loadData(): void {\n    this.dataService.getPaginatedData(this.currentPage, this.pageSize).subscribe(\n      (response) => {\n        this.data = response.data; // Assuming the API returns data in a 'data' field\n      },\n      (error) => {\n        console.error('Error fetching data:', error);\n      }\n    );\n  }\n\n  onPageChange(newPage: number): void {\n    this.currentPage = newPage;\n    this.loadData();\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```gdscript\nshader_type canvas_item;\n\nuniform sampler2D water_texture;\nuniform vec2 UV_scale = vec2(1.0);\nuniform float refraction_strength = 0.1;\n\nvoid fragment() {\n    // Get the UV coordinates for the sprite texture\n    vec2 uv = UV * UV_scale;\n\n    // Create a distortion effect to simulate refraction\n    vec2 refraction = vec2(\n        sin(VERTEX.x * 10.0 + TIME) * refraction_strength,\n        cos(VERTEX.y * 10.0 + TIME) * refraction_strength\n    );\n    \n    // Apply refraction distortion\n    vec2 refraction_uv = uv + refraction;\n    \n    // Sample the water texture with refraction applied\n    vec4 water_color = texture(water_texture, refraction_uv);\n    \n    // Set the output color\n    COLOR = water_color;\n}\n```",
    "label": 1
  },
  {
    "codes": "function transpileJavaScriptToPython3 ({ js, className, removeEmptyLines }) {\n\n    // transpile JS \u2192 Python 3\n    let python3Body = regexAll (js, pythonRegexes)\n\n    if (removeEmptyLines)\n        python3Body = python3Body.replace (/$\\s*$/gm, '')\n\n    python3Body = python3Body.replace (/\\'([\u0430\u0431\u0432\u0433\u0434\u0435\u0451\u0436\u0437\u0438\u0439\u043a\u043b\u043c\u043d\u043e\u043f\u0440\u0441\u0442\u0443\u0444\u0445\u0446\u0447\u0448\u0449\u044a\u044b\u044c\u044d\u044e\u044f\u670d\u52a1\u7aef\u5fd9\u788c]+)\\'/gm, \"u'$1'\")\n\n    // special case for Python OrderedDicts\n    let orderedDictRegex = /\\.ordered\\s+\\(\\{([^\\}]+)\\}\\)/g\n    let orderedDictMatches = undefined\n    while (orderedDictMatches = orderedDictRegex.exec (python3Body)) {\n        let replaced = orderedDictMatches[1].replace (/^(\\s+)([^\\:]+)\\:\\s*([^\\,]+)\\,$/gm, '$1($2, $3),')\n        python3Body = python3Body.replace (orderedDictRegex, '\\.ordered ([' + replaced + '])')\n    }\n\n    // special case for Python super\n    python3Body = python3Body.replace (/super\\./g, 'super(' + className + ', self).')\n\n    return python3Body\n}",
    "label": 0
  },
  {
    "codes": "```cpp\n// Compile your C++ application with debugging information enabled\n// Use the -g flag to include debug information\n// Example: g++ -g -o my_app my_app.cpp\n\n// Run your application with Valgrind to detect memory leaks\n// Use the valgrind command with --leak-check=full for detailed leak information\n// Example: valgrind --leak-check=full ./my_app\n\n// Example C++ program (my_app.cpp) to test for memory leaks:\n\n#include <iostream>\n\nvoid memoryLeakFunction() {\n    // Allocate memory without freeing it\n    int* leak = new int[100];\n    // Simulate some operation with the allocated memory\n    for (int i = 0; i < 100; ++i) {\n        leak[i] = i * 2;\n    }\n    // Leak memory by not deleting the allocated memory\n}\n\nint main() {\n    // Call a function that incorrectly handles memory\n    memoryLeakFunction();\n    // Output a completion message\n    std::cout << \"Program Complete\\n\";\n    return 0;\n}\n\n/*\nTo detect the memory leak above, compile and run your program with Valgrind:\n\n1. Compile the program:\n   g++ -g -o my_app my_app.cpp\n\n2. Use Valgrind to run the executable:\n   valgrind --leak-check=full ./my_app\n\n3. Analyze the Valgrind output for memory leak details, it should indicate\n   a leak related to the memory allocated in memoryLeakFunction().\n*/\n```\n",
    "label": 1
  },
  {
    "codes": "```python\nimport os\nimport tempfile\nfrom contextlib import contextmanager\n\n@contextmanager\ndef atomic_write(file_path, mode='w', **kwargs):\n    # Create a temporary file with a .tmp suffix\n    temp_file_path = file_path + '.tmp'\n    try:\n        # Open the temporary file\n        with open(temp_file_path, mode, **kwargs) as temp_file:\n            yield temp_file  # Yield the file object to the caller\n        # If everything succeeds, rename the temp file to the target file\n        os.replace(temp_file_path, file_path)\n    except Exception:\n        # If an error occurs, remove the temporary file\n        if os.path.exists(temp_file_path):\n            os.remove(temp_file_path)\n        raise  # Re-raise the exception\n```",
    "label": 1
  },
  {
    "codes": "func (s *DeleteArchiveInput) SetVaultName(v string) *DeleteArchiveInput {\n\ts.VaultName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```dart\nString formatCompactNumber(double number) {\n  if (number < 1000) {\n    return number.toStringAsFixed(0);\n  } else if (number < 1000000) {\n    return (number / 1000).toStringAsFixed(1) + 'K';\n  } else if (number < 1000000000) {\n    return (number / 1000000).toStringAsFixed(1) + 'M';\n  } else {\n    return (number / 1000000000).toStringAsFixed(1) + 'B';\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport os\nimport tempfile\nfrom contextlib import contextmanager\n\n@contextmanager\ndef atomic_write(file_path, mode='w', **kwargs):\n    # Create a temporary file with .tmp suffix\n    temp_file_path = file_path + '.tmp'\n    try:\n        # Open the temporary file\n        with open(temp_file_path, mode, **kwargs) as temp_file:\n            yield temp_file  # Yield the file object to the caller\n        # If everything succeeds, rename the temp file to the target file\n        os.replace(temp_file_path, file_path)\n    except Exception:\n        # If an error occurs, remove the temporary file\n        if os.path.exists(temp_file_path):\n            os.remove(temp_file_path)\n        raise  # Re-raise the exception\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary packages\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('path/to/your/model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.converters.convert(keras_model, \n                                     inputs=[ct.TensorType(shape=keras_model.input_shape)])\n\n# Define the output path for the CoreML model\noutput_path = 'path/to/save/model.mlmodel'\n\n# Save the CoreML model\ncoreml_model.save(output_path)\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glacier) DeleteVaultNotificationsRequest(input *DeleteVaultNotificationsInput) (req *request.Request, output *DeleteVaultNotificationsOutput) {\n\top := &request.Operation{\n\t\tName:       opDeleteVaultNotifications,\n\t\tHTTPMethod: \"DELETE\",\n\t\tHTTPPath:   \"/{accountId}/vaults/{vaultName}/notification-configuration\",\n\t}\n\n\tif input == nil {\n\t\tinput = &DeleteVaultNotificationsInput{}\n\t}\n\n\toutput = &DeleteVaultNotificationsOutput{}\n\treq = c.newRequest(op, input, output)\n\treq.Handlers.Unmarshal.Swap(restjson.UnmarshalHandler.Name, protocol.UnmarshalDiscardBodyHandler)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "function isControlTransfer(stmt, path, control = \"break\") {\n    const { [control]: type } = {\n      break: \"BreakStatement\",\n      continue: \"ContinueStatement\"\n    };\n    if (!type) {\n      throw new Error(\"Can only handle break and continue statements\");\n    }\n    const checker = `is${type}`;\n\n    if (stmt[checker]()) {\n      return _isControlTransfer(stmt, path);\n    }\n\n    let isTransferred = false;\n    let result = {\n      [control]: false,\n      bail: false\n    };\n\n    stmt.traverse({\n      [type](cPath) {\n        // if we already detected a break/continue statement,\n        if (isTransferred) return;\n\n        result = _isControlTransfer(cPath, path);\n\n        if (result.bail || result[control]) {\n          isTransferred = true;\n        }\n      }\n    });\n\n    return result;\n\n    function _isControlTransfer(cPath, path) {\n      const label = cPath.get(\"label\");\n\n      if (label.node !== null) {\n        // labels are fn scoped and not accessible by inner functions\n        // path is the switch statement\n        if (!isSameFunctionScope(path, cPath)) {\n          // we don't have to worry about this break statement\n          return {\n            break: false,\n            bail: false\n          };\n        }\n\n        // here we handle the break labels\n        // if they are outside switch, we bail out\n        // if they are within the case, we keep them\n        let labelPath;\n        if (path.scope.getLabel) {\n          labelPath = getLabel(label.node.name, path);\n        } else {\n          labelPath = path.scope.getBinding(label.node.name).path;\n        }\n        const _isAncestor = isAncestor(labelPath, path);\n\n        return {\n          bail: _isAncestor,\n          [control]: _isAncestor\n        };\n      }\n\n      // set the flag that it is indeed breaking\n      let isCTransfer = true;\n\n      // this flag is to capture\n      // switch(0) { case 0: while(1) if (x) break; }\n      let possibleRunTimeControlTransfer = false;\n\n      // and compute if it's breaking the correct thing\n      let parent = cPath.parentPath;\n\n      while (parent !== stmt.parentPath) {\n        // loops and nested switch cases\n        if (parent.isLoop() || parent.isSwitchCase()) {\n          // invalidate all the possible runtime breaks captured\n          // while (1) { if (x) break; }\n          possibleRunTimeControlTransfer = false;\n\n          // and set that it's not breaking our switch statement\n          isCTransfer = false;\n          break;\n        }\n        //\n        // this is a special case and depends on\n        // the fact that SwitchStatement is handled in the\n        // exit hook of the traverse\n        //\n        // switch (0) {\n        //   case 0: if (x) break;\n        // }\n        //\n        // here `x` is runtime only.\n        // in this case, we need to bail out. So we depend on exit hook\n        // of switch so that, it would have visited the IfStatement first\n        // before the SwitchStatement and would have removed the\n        // IfStatement if it was a compile time determined\n        //\n        if (parent.isIfStatement()) {\n          possibleRunTimeControlTransfer = true;\n        }\n        parent = parent.parentPath;\n      }\n\n      return {\n        [control]: possibleRunTimeControlTransfer || isCTransfer,\n        bail: possibleRunTimeControlTransfer\n      };\n    }\n  }",
    "label": 0
  },
  {
    "codes": "```c\n#include \"FreeRTOS.h\"\n#include \"task.h\"\n\n// Define task priorities\n#define CONTROL_TASK_PRIORITY         (2)\n#define SENSOR_TASK_PRIORITY          (3)\n#define COMMUNICATION_TASK_PRIORITY   (1)\n\n// Define task stack sizes\n#define CONTROL_TASK_STACK_SIZE       (configMINIMAL_STACK_SIZE)\n#define SENSOR_TASK_STACK_SIZE        (configMINIMAL_STACK_SIZE)\n#define COMMUNICATION_TASK_STACK_SIZE (configMINIMAL_STACK_SIZE)\n\n// Function prototypes for the tasks\nvoid vControlTask(void *pvParameters);\nvoid vSensorTask(void *pvParameters);\nvoid vCommunicationTask(void *pvParameters);\n\n// Task for controlling the robotic arm\nvoid vControlTask(void *pvParameters) {\n    for (;;) {\n        // Code for sending commands to the robotic arm\n        // ...\n        \n        vTaskDelay(pdMS_TO_TICKS(100));  // Task delay for 100ms\n    }\n}\n\n// Task for reading sensors attached to the robotic arm\nvoid vSensorTask(void *pvParameters) {\n    for (;;) {\n        // Code to read sensor values\n        // ...\n        \n        vTaskDelay(pdMS_TO_TICKS(50));   // Task delay for 50ms\n    }\n}\n\n// Task for handling communications\nvoid vCommunicationTask(void *pvParameters) {\n    for (;;) {\n        // Code for sending/receiving communication messages\n        // ...\n        \n        vTaskDelay(pdMS_TO_TICKS(200));  // Task delay for 200ms\n    }\n}\n\nint main(void) {\n    // Initialize hardware and peripherals\n    // ...\n\n    // Create tasks\n    xTaskCreate(vControlTask, \"Control Task\", CONTROL_TASK_STACK_SIZE, NULL, CONTROL_TASK_PRIORITY, NULL);\n    xTaskCreate(vSensorTask, \"Sensor Task\", SENSOR_TASK_STACK_SIZE, NULL, SENSOR_TASK_PRIORITY, NULL);\n    xTaskCreate(vCommunicationTask, \"Communication Task\", COMMUNICATION_TASK_STACK_SIZE, NULL, COMMUNICATION_TASK_PRIORITY, NULL);\n\n    // Start the scheduler\n    vTaskStartScheduler();\n\n    // Will only reach here if there was insufficient memory to create the idle task\n    for (;;);\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <title>Real-time Crypto Prices</title>\n  <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n  <style>\n    body {\n      font-family: Arial, sans-serif;\n    }\n    .axis text {\n      font-size: 12px;\n    }\n    .line {\n      fill: none;\n      stroke-width: 2px;\n    }\n  </style>\n</head>\n<body>\n  <h2>Real-time Cryptocurrency Prices</h2>\n  <svg width=\"800\" height=\"400\"></svg>\n\n  <script>\n    // Constants for SVG width and height\n    const svgWidth = 800;\n    const svgHeight = 400;\n\n    // Initialize scales for x and y axis\n    const xScale = d3.scaleTime().range([0, svgWidth - 100]);\n    const yScale = d3.scaleLinear().range([svgHeight - 60, 0]);\n\n    // Create SVG element and append to body\n    const svg = d3.select('svg')\n                 .attr('width', svgWidth)\n                 .attr('height', svgHeight)\n                 .append('g')\n                 .attr('transform', 'translate(50, 20)');\n\n    // Initialize line generator\n    const line = d3.line()\n                  .x(d => xScale(d.date))\n                  .y(d => yScale(d.price));\n\n    // Add axes to the SVG\n    svg.append('g').attr('class', 'x axis').attr('transform', `translate(0, ${svgHeight - 60})`);\n    svg.append('g').attr('class', 'y axis');\n\n    // Function to fetch and update data\n    function fetchData() {\n      // Fetch crypto price data from a public API\n      fetch('https://api.coingecko.com/api/v3/simple/price?ids=bitcoin&vs_currencies=usd')\n        .then(response => response.json())\n        .then(data => {\n          // Extract and format data\n          const now = new Date();\n          const cryptoData = {\n            date: now,\n            price: data.bitcoin.usd\n          };\n\n          // Update data, add new point\n          dataSet.push(cryptoData);\n          if (dataSet.length > 50) dataSet.shift(); // Limit data points\n\n          // Update scales domain\n          xScale.domain(d3.extent(dataSet, d => d.date));\n          yScale.domain([0, d3.max(dataSet, d => d.price)]);\n\n          // Bind data and create path\n          const path = svg.selectAll('.line').data([dataSet]);\n\n          // Enter, update, & exit for path\n          path.enter()\n            .append('path')\n            .attr('class', 'line')\n            .attr('stroke', 'steelblue')\n            .merge(path)\n            .transition()\n            .duration(500)\n            .attr('d', line);\n\n          path.exit().remove();\n\n          // Update axes\n          svg.select('.x.axis').transition().duration(500).call(d3.axisBottom(xScale).ticks(5));\n          svg.select('.y.axis').transition().duration(500).call(d3.axisLeft(yScale));\n        })\n        .catch(error => console.error('Error fetching data:', error));\n    }\n\n    // Initialize empty data set\n    const dataSet = [];\n\n    // Fetch data every 3 seconds\n    setInterval(fetchData, 3000);\n  </script>\n</body>\n</html>\n```",
    "label": 1
  },
  {
    "codes": "```kotlin\n// Sealed class representing domain-specific errors\nsealed class DomainError {\n    // Represents validation errors\n    sealed class ValidationError : DomainError() {\n        object InvalidEmail : ValidationError()\n        object InvalidPassword : ValidationError()\n        object MissingRequiredField : ValidationError()\n    }\n\n    // Represents network-related errors\n    sealed class NetworkError : DomainError() {\n        object NoInternetConnection : NetworkError()\n        object RequestTimeout : NetworkError()\n        object ServerError : NetworkError()\n    }\n\n    // Represents database-related errors\n    sealed class DatabaseError : DomainError() {\n        object RecordNotFound : DatabaseError()\n        object DuplicateEntry : DatabaseError()\n        object ConnectionFailed : DatabaseError()\n    }\n\n    // Represents business logic errors\n    sealed class BusinessError : DomainError() {\n        object InsufficientFunds : BusinessError()\n        object UnauthorizedAccess : BusinessError()\n        object ResourceLimitExceeded : BusinessError()\n    }\n\n    // Represents unknown or unexpected errors\n    object UnknownError : DomainError()\n}\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glacier) GetVaultNotifications(input *GetVaultNotificationsInput) (*GetVaultNotificationsOutput, error) {\n\treq, out := c.GetVaultNotificationsRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```csharp\nvar filteredProducts = products\n    .Where(p => p.Price >= minPrice \n                && p.Price <= maxPrice \n                && p.Category == desiredCategory)\n    .Select(p => new { p.Name, p.Price, p.Category });\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse reqwest::Client;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Your OAuth2 credentials and token endpoint\n    let token_url = \"https://example.com/oauth2/token\";\n    let client_id = \"your_client_id\";\n    let client_secret = \"your_client_secret\";\n\n    // Create HTTP client\n    let client = Client::new();\n\n    // Prepare request form\n    let mut params = HashMap::new();\n    params.insert(\"grant_type\", \"client_credentials\");\n    params.insert(\"client_id\", client_id);\n    params.insert(\"client_secret\", client_secret);\n\n    // Send token request\n    let res = client.post(token_url).form(&params).send().await?;\n\n    // Handle response\n    let token_response: TokenResponse = res.json().await?;\n    println!(\"Access Token: {}\", token_response.access_token);\n\n    Ok(())\n}\n\n// Define structure for deserializing token response\n#[derive(Debug, Serialize, Deserialize)]\nstruct TokenResponse {\n    access_token: String,\n    token_type: String,\n    expires_in: u64,\n}\n```\n\nThis code demonstrates a basic OAuth2 client credentials flow using Rust. It sends a request to the token endpoint and retrieves an access token. Note that you'll need to replace the example URL and credentials with your actual OAuth2 server details.",
    "label": 1
  },
  {
    "codes": "function addonsManager_installAddon(aSpec) {\n    var spec = aSpec || { };\n    var addon = spec.addon;\n    var timeout = spec.timeout;\n    var button = \"install\";\n    var waitFor = (spec.waitFor == undefined) ? true : spec.waitFor;\n\n    var button = this.getAddonButton({addon: addon, button: button});\n    this._controller.click(button);\n\n    if (waitFor)\n      this.waitForDownloaded({addon: addon, timeout: timeout});\n  }",
    "label": 0
  },
  {
    "codes": "```javascript\n// MongoDB Aggregation Pipeline to Calculate Monthly Sales Trends\n\n// Step 1: Match documents with sales data (optional if all documents are sales-related)\n{ $match: { saleDate: { $exists: true } } },\n\n// Step 2: Extract year and month from the saleDate field\n{ \n  $project: {\n    year: { $year: \"$saleDate\" },\n    month: { $month: \"$saleDate\" },\n    amount: 1 // Include the sales amount field\n  }\n},\n\n// Step 3: Group by year and month to calculate total sales per month\n{ \n  $group: {\n    _id: { year: \"$year\", month: \"$month\" },\n    totalSales: { $sum: \"$amount\" }\n  }\n},\n\n// Step 4: Sort by year and month for chronological order\n{ \n  $sort: { \"_id.year\": 1, \"_id.month\": 1 }\n},\n\n// Step 5: Optionally, format the output for better readability\n{ \n  $project: {\n    _id: 0,\n    year: \"$_id.year\",\n    month: \"$_id.month\",\n    totalSales: 1\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.IO;\nusing System.ServiceProcess;\nusing System.Threading;\n\npublic class FileMonitorService : ServiceBase\n{\n    private FileSystemWatcher _watcher;\n    private string _directoryToWatch = @\"C:\\Path\\To\\Monitor\";\n\n    protected override void OnStart(string[] args)\n    {\n        // Initialize the FileSystemWatcher\n        _watcher = new FileSystemWatcher\n        {\n            Path = _directoryToWatch,\n            NotifyFilter = NotifyFilters.LastWrite | NotifyFilters.FileName | NotifyFilters.DirectoryName,\n            Filter = \"*.*\", // Monitor all files\n            IncludeSubdirectories = true // Monitor subdirectories\n        };\n\n        // Attach event handlers\n        _watcher.Created += OnChanged;\n        _watcher.Deleted += OnChanged;\n        _watcher.Renamed += OnRenamed;\n        _watcher.Changed += OnChanged;\n\n        // Start monitoring\n        _watcher.EnableRaisingEvents = true;\n    }\n\n    protected override void OnStop()\n    {\n        // Stop monitoring\n        _watcher.EnableRaisingEvents = false;\n        _watcher.Dispose();\n    }\n\n    private void OnChanged(object source, FileSystemEventArgs e)\n    {\n        // Log or handle file changes\n        string logMessage = $\"File {e.FullPath} was {e.ChangeType}\";\n        LogToFile(logMessage);\n    }\n\n    private void OnRenamed(object source, RenamedEventArgs e)\n    {\n        // Log or handle file renaming\n        string logMessage = $\"File {e.OldFullPath} was renamed to {e.FullPath}\";\n        LogToFile(logMessage);\n    }\n\n    private void LogToFile(string message)\n    {\n        // Log messages to a file\n        string logPath = @\"C:\\Path\\To\\LogFile.txt\";\n        File.AppendAllText(logPath, $\"{DateTime.Now}: {message}{Environment.NewLine}\");\n    }\n\n    public static void Main()\n    {\n        // Run the service\n        ServiceBase.Run(new FileMonitorService());\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <fstream>\n#include <sstream>\n#include <map>\n#include <vector>\n#include <algorithm>\n\n// Structure to hold INI data\nstruct INI {\n    std::map<std::string, std::map<std::string, std::string>> data;\n    std::map<std::string, std::string> global; // Global section\n};\n\n// Function to trim whitespace from a string\nstd::string trim(const std::string& str) {\n    auto start = std::find_if_not(str.begin(), str.end(), [](unsigned char c) { return std::isspace(c); });\n    auto end = std::find_if_not(str.rbegin(), str.rend(), [](unsigned char c) { return std::isspace(c); }).base();\n    return (start < end) ? std::string(start, end) : std::string();\n}\n\n// Function to parse INI file with section inheritance\nINI parseINI(const std::string& filename) {\n    INI ini;\n    std::ifstream file(filename);\n    std::string line;\n    std::string currentSection = \"\";\n\n    while (std::getline(file, line)) {\n        line = trim(line);\n        if (line.empty() || line[0] == ';' || line[0] == '#') continue; // Skip comments and empty lines\n\n        if (line[0] == '[' && line.back() == ']') {\n            // Section line\n            currentSection = line.substr(1, line.size() - 2);\n            ini.data[currentSection] = ini.global; // Inherit from global section\n        } else {\n            // Key-value line\n            size_t equalsPos = line.find('=');\n            if (equalsPos != std::string::npos) {\n                std::string key = trim(line.substr(0, equalsPos));\n                std::string value = trim(line.substr(equalsPos + 1));\n                if (currentSection.empty()) {\n                    ini.global[key] = value; // Global key-value\n                } else {\n                    ini.data[currentSection][key] = value; // Section key-value\n                }\n            }\n        }\n    }\n\n    return ini;\n}\n\n// Function to print INI data\nvoid printINI(const INI& ini) {\n    for (const auto& section : ini.data) {\n        std::cout << \"[\" << section.first << \"]\" << std::endl;\n        for (const auto& keyValue : section.second) {\n            std::cout << keyValue.first << \" = \" << keyValue.second << std::endl;\n        }\n        std::cout << std::endl;\n    }\n}\n\nint main() {\n    INI ini = parseINI(\"example.ini\");\n    printINI(ini);\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n// Header file: DynamicWeatherSystem.h\n#pragma once\n\n#include \"CoreMinimal.h\"\n#include \"GameFramework/Actor.h\"\n#include \"DynamicWeatherSystem.generated.h\"\n\nUCLASS()\nclass YOURGAME_API ADynamicWeatherSystem : public AActor\n{\n    GENERATED_BODY()\n    \npublic:    \n    ADynamicWeatherSystem();\n\nprotected:\n    virtual void BeginPlay() override;\n\npublic:    \n    virtual void Tick(float DeltaTime) override;\n\n    // Weather states\n    UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"Weather\")\n    FString CurrentWeather;\n\n    UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"Weather\")\n    FString TargetWeather;\n\n    // Transition speed\n    UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"Weather\")\n    float TransitionSpeed;\n\n    // Function to start weather transition\n    UFUNCTION(BlueprintCallable, Category = \"Weather\")\n    void StartWeatherTransition(FString NewWeather);\n\nprivate:\n    // Internal function to handle weather transition\n    void UpdateWeatherTransition(float DeltaTime);\n};\n\n// Source file: DynamicWeatherSystem.cpp\n#include \"DynamicWeatherSystem.h\"\n\nADynamicWeatherSystem::ADynamicWeatherSystem()\n{\n    PrimaryActorTick.bCanEverTick = true;\n\n    // Initialize default values\n    CurrentWeather = \"Sunny\";\n    TargetWeather = \"Sunny\";\n    TransitionSpeed = 0.5f;\n}\n\nvoid ADynamicWeatherSystem::BeginPlay()\n{\n    Super::BeginPlay();\n}\n\nvoid ADynamicWeatherSystem::Tick(float DeltaTime)\n{\n    Super::Tick(DeltaTime);\n\n    // Update weather transition every frame\n    UpdateWeatherTransition(DeltaTime);\n}\n\nvoid ADynamicWeatherSystem::StartWeatherTransition(FString NewWeather)\n{\n    TargetWeather = NewWeather;\n}\n\nvoid ADynamicWeatherSystem::UpdateWeatherTransition(float DeltaTime)\n{\n    if (CurrentWeather != TargetWeather)\n    {\n        // Simulate transition by interpolating between weather states\n        // This is a placeholder for actual weather effects\n        FString TransitionWeather = FMath::Lerp(CurrentWeather, TargetWeather, TransitionSpeed * DeltaTime);\n\n        // Update current weather\n        CurrentWeather = TransitionWeather;\n\n        // Trigger weather change events (e.g., update skybox, particle effects, etc.)\n        // Implement custom logic here based on your game's requirements\n    }\n}\n```\n\n### Comments:\n- **DynamicWeatherSystem.h**: Defines the `ADynamicWeatherSystem` class with properties for current and target weather, transition speed, and a function to start weather transitions.\n- **DynamicWeatherSystem.cpp**: Implements the logic for weather transitions, including interpolation between weather states and updating the current weather.\n- **Transition Logic**: The `UpdateWeatherTransition` function simulates a smooth transition between weather states using linear interpolation (Lerp).\n- **Customization**: You can extend this system by adding more weather states, effects, and transitions based on your game's needs.",
    "label": 1
  },
  {
    "codes": "```java\nimport javafx.application.Application;\nimport javafx.scene.Scene;\nimport javafx.scene.canvas.Canvas;\nimport javafx.scene.canvas.GraphicsContext;\nimport javafx.scene.input.MouseEvent;\nimport javafx.scene.layout.Pane;\nimport javafx.scene.paint.Color;\nimport javafx.stage.Stage;\n\npublic class FractalDisplay extends Application {\n\n    private double zoomFactor = 200; // Initial zoom factor\n    private double offsetX = 0; // X-axis offset for centering fractal\n    private double offsetY = 0; // Y-axis offset for centering fractal\n\n    public static void main(String[] args) {\n        launch(args);\n    }\n\n    @Override\n    public void start(Stage primaryStage) {\n        primaryStage.setTitle(\"Interactive Fractal Patterns\");\n\n        Canvas canvas = new Canvas(800, 600);\n        drawFractal(canvas.getGraphicsContext2D());\n\n        canvas.addEventHandler(MouseEvent.MOUSE_CLICKED, event -> {\n            // Update offset and zoom based on mouse clicks\n            offsetX += (event.getX() - canvas.getWidth() / 2) / zoomFactor;\n            offsetY += (event.getY() - canvas.getHeight() / 2) / zoomFactor;\n            zoomFactor *= 1.5; // Zoom in\n            drawFractal(canvas.getGraphicsContext2D());\n        });\n\n        Pane root = new Pane(canvas);\n        primaryStage.setScene(new Scene(root));\n        primaryStage.show();\n    }\n\n    private void drawFractal(GraphicsContext gc) {\n        int width = (int) gc.getCanvas().getWidth();\n        int height = (int) gc.getCanvas().getHeight();\n        gc.setFill(Color.BLACK);\n        gc.fillRect(0, 0, width, height);\n\n        for (int x = 0; x < width; x++) {\n            for (int y = 0; y < height; y++) {\n                double c_re = (x - width / 2.0) / zoomFactor + offsetX;\n                double c_im = (y - height / 2.0) / zoomFactor + offsetY;\n                double re = c_re;\n                double im = c_im;\n                int maxIterations = 100;\n                int iterations = 0;\n\n                while (re * re + im * im < 4 && iterations < maxIterations) {\n                    double reNew = re * re - im * im + c_re;\n                    double imNew = 2 * re * im + c_im;\n                    re = reNew;\n                    im = imNew;\n                    iterations++;\n                }\n\n                if (iterations < maxIterations) {\n                    gc.getPixelWriter().setColor(x, y, Color.hsb(360.0 * iterations / maxIterations, 1.0, 1.0));\n                } else {\n                    gc.getPixelWriter().setColor(x, y, Color.BLACK);\n                }\n            }\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom transformers import pipeline\n\n# Initialize the summarization pipeline\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n# Example text to summarize\ntext = \"\"\"\nThe Hugging Face Transformers library provides an easy-to-use interface for various NLP tasks, \nincluding text summarization. It supports multiple pre-trained models that can be used out-of-the-box \nfor generating concise summaries of long documents. This makes it a powerful tool for developers \nand researchers working on natural language processing applications.\n\"\"\"\n\n# Generate the summary\nsummary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n\n# Print the summary\nprint(summary[0]['summary_text'])\n```",
    "label": 1
  },
  {
    "codes": "func (s *CreateClassifierInput) Validate() error {\n\tinvalidParams := request.ErrInvalidParams{Context: \"CreateClassifierInput\"}\n\tif s.CsvClassifier != nil {\n\t\tif err := s.CsvClassifier.Validate(); err != nil {\n\t\t\tinvalidParams.AddNested(\"CsvClassifier\", err.(request.ErrInvalidParams))\n\t\t}\n\t}\n\tif s.GrokClassifier != nil {\n\t\tif err := s.GrokClassifier.Validate(); err != nil {\n\t\t\tinvalidParams.AddNested(\"GrokClassifier\", err.(request.ErrInvalidParams))\n\t\t}\n\t}\n\tif s.JsonClassifier != nil {\n\t\tif err := s.JsonClassifier.Validate(); err != nil {\n\t\t\tinvalidParams.AddNested(\"JsonClassifier\", err.(request.ErrInvalidParams))\n\t\t}\n\t}\n\tif s.XMLClassifier != nil {\n\t\tif err := s.XMLClassifier.Validate(); err != nil {\n\t\t\tinvalidParams.AddNested(\"XMLClassifier\", err.(request.ErrInvalidParams))\n\t\t}\n\t}\n\n\tif invalidParams.Len() > 0 {\n\t\treturn invalidParams\n\t}\n\treturn nil\n}",
    "label": 0
  },
  {
    "codes": "function moveSinksRight(nodes, maxDepth) {\n    zrUtil.each(nodes, function (node) {\n        if (!isNodeDepth(node) && !node.outEdges.length) {\n            node.setLayout({depth: maxDepth}, true);\n        }\n    });\n}",
    "label": 0
  },
  {
    "codes": "```powershell\n# PowerShell Module to Manage Windows Service Dependencies\n# Filename: ServiceDependencyManager.psm1\n\n# Exported Functions\nExport-ModuleMember -Function Get-ServiceDependencies, Add-ServiceDependency, Remove-ServiceDependency\n\n# Function to retrieve a service's dependencies\nfunction Get-ServiceDependencies {\n    [CmdletBinding()]\n    param (\n        [Parameter(Mandatory = $true)]\n        [string]$ServiceName\n    )\n\n    try {\n        # Get the dependencies for the specified service\n        $service = Get-Service -Name $ServiceName -ErrorAction Stop\n        return $service.DependentServices.Name\n    } catch {\n        Write-Error \"Failed to get dependencies for service '$ServiceName': $_\"\n    }\n}\n\n# Function to add a dependency to a service\nfunction Add-ServiceDependency {\n    [CmdletBinding()]\n    param (\n        [Parameter(Mandatory = $true)]\n        [string]$ServiceName,\n\n        [Parameter(Mandatory = $true)]\n        [string]$DependencyServiceName\n    )\n\n    try {\n        # Use 'sc config' to add a dependency to the service\n        $service = Get-Service -Name $ServiceName -ErrorAction Stop\n        $currentDependencies = $service.DependentServices.Name\n        $newDependencies = @($currentDependencies + $DependencyServiceName) -join '/'\n        sc.exe config $ServiceName depend= $newDependencies | Out-Null\n        Write-Output \"Dependency '$DependencyServiceName' added to service '$ServiceName'.\"\n    } catch {\n        Write-Error \"Failed to add dependency '$DependencyServiceName' to service '$ServiceName': $_\"\n    }\n}\n\n# Function to remove a dependency from a service\nfunction Remove-ServiceDependency {\n    [CmdletBinding()]\n    param (\n        [Parameter(Mandatory = $true)]\n        [string]$ServiceName,\n\n        [Parameter(Mandatory = $true)]\n        [string]$DependencyServiceName\n    )\n\n    try {\n        # Use 'sc config' to remove a dependency from the service\n        $service = Get-Service -Name $ServiceName -ErrorAction Stop\n        $newDependencies = $service.DependentServices.Name | Where-Object { $_ -ne $DependencyServiceName } -join '/'\n        sc.exe config $ServiceName depend= $newDependencies | Out-Null\n        Write-Output \"Dependency '$DependencyServiceName' removed from service '$ServiceName'.\"\n    } catch {\n        Write-Error \"Failed to remove dependency '$DependencyServiceName' from service '$ServiceName': $_\"\n    }\n}\n\n# End of Module\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<SharedAccessSignatureAuthorizationRuleInner> getKeysForKeyNameAsync(String resourceGroupName, String resourceName, String keyName, final ServiceCallback<SharedAccessSignatureAuthorizationRuleInner> serviceCallback) {\n        return ServiceFuture.fromResponse(getKeysForKeyNameWithServiceResponseAsync(resourceGroupName, resourceName, keyName), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "public CertificateBundle mergeCertificate(String vaultBaseUrl, String certificateName, List<byte[]> x509Certificates, CertificateAttributes certificateAttributes, Map<String, String> tags) {\n        return mergeCertificateWithServiceResponseAsync(vaultBaseUrl, certificateName, x509Certificates, certificateAttributes, tags).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "function getGridOffset(boundingBox, cellSize) {\n  const {yMin, yMax} = boundingBox;\n  const latMin = yMin;\n  const latMax = yMax;\n  const centerLat = (latMin + latMax) / 2;\n\n  return calculateGridLatLonOffset(cellSize, centerLat);\n}",
    "label": 0
  },
  {
    "codes": "public CertificateOperation deleteCertificateOperation(String vaultBaseUrl, String certificateName) {\n        return deleteCertificateOperationWithServiceResponseAsync(vaultBaseUrl, certificateName).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "def get_lus_from_synset(self, syn_id):\n        \"\"\"Returns a list of (word, lu_id) tuples given a synset ID\"\"\"\n\n        root = self.get_synset_xml(syn_id)\n        elem_synonyms = root.find( \".//synonyms\" )\n\n\n        lus = []\n        for elem_synonym in elem_synonyms:\n            synonym_str = elem_synonym.get( \"c_lu_id-previewtext\" )        # get \"c_lu_id-previewtext\" attribute\n            # synonym_str ends with \":<num>\"\n            synonym = synonym_str.split( ':' )[ 0 ].strip()\n            lus.append( (synonym, elem_synonym.get( \"c_lu_id\") ) )\n        return lus",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<SecurityRuleInner>> createOrUpdateWithServiceResponseAsync(String resourceGroupName, String networkSecurityGroupName, String securityRuleName, SecurityRuleInner securityRuleParameters) {\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (networkSecurityGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter networkSecurityGroupName is required and cannot be null.\");\n        }\n        if (securityRuleName == null) {\n            throw new IllegalArgumentException(\"Parameter securityRuleName is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (securityRuleParameters == null) {\n            throw new IllegalArgumentException(\"Parameter securityRuleParameters is required and cannot be null.\");\n        }\n        Validator.validate(securityRuleParameters);\n        final String apiVersion = \"2018-08-01\";\n        Observable<Response<ResponseBody>> observable = service.createOrUpdate(resourceGroupName, networkSecurityGroupName, securityRuleName, this.client.subscriptionId(), securityRuleParameters, apiVersion, this.client.acceptLanguage(), this.client.userAgent());\n        return client.getAzureClient().getPutOrPatchResultAsync(observable, new TypeToken<SecurityRuleInner>() { }.getType());\n    }",
    "label": 0
  },
  {
    "codes": "func (s *TableInput) SetLastAnalyzedTime(v time.Time) *TableInput {\n\ts.LastAnalyzedTime = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n# Define the transformations for the dataset\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\n# Load CIFAR-10 dataset\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n\n# Define the CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n        self.fc2 = nn.Linear(256, 10)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.pool(nn.ReLU()(self.conv1(x)))\n        x = self.pool(nn.ReLU()(self.conv2(x)))\n        x = self.pool(nn.ReLU()(self.conv3(x)))\n        x = x.view(-1, 128 * 4 * 4)  # Flatten\n        x = self.dropout(nn.ReLU()(self.fc1(x)))\n        x = self.fc2(x)\n        return x\n\n# Initialize model, criterion and optimizer\nmodel = SimpleCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\ndef train_model(model, trainloader, criterion, optimizer, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for inputs, labels in trainloader:\n            optimizer.zero_grad()  # Zero the gradients\n            outputs = model(inputs)  # Forward pass\n            loss = criterion(outputs, labels)  # Compute loss\n            loss.backward()  # Backward pass\n            optimizer.step()  # Update weights\n            running_loss += loss.item()\n        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader)}')\n\n# Evaluate the model\ndef evaluate_model(model, testloader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in testloader:\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    print(f'Accuracy: {100 * correct / total}%')\n\n# Run training and evaluation\ntrain_model(model, trainloader, criterion, optimizer, epochs=10)\nevaluate_model(model, testloader)\n```",
    "label": 1
  },
  {
    "codes": "def set_environment_variables(json_file_path):\n    \"\"\"\n    Read and set environment variables from a flat json file.\n\n    Bear in mind that env vars set this way and later on read using\n    `os.getenv` function will be strings since after all env vars are just\n    that - plain strings.\n\n    Json file example:\n    ```\n    {\n        \"FOO\": \"bar\",\n        \"BAZ\": true\n    }\n    ```\n\n    :param json_file_path: path to flat json file\n    :type json_file_path: str\n    \"\"\"\n    if json_file_path:\n        with open(json_file_path) as json_file:\n            env_vars = json.loads(json_file.read())\n\n            export_variables(env_vars)",
    "label": 0
  },
  {
    "codes": "public WorkflowInner createOrUpdate(String resourceGroupName, String workflowName, WorkflowInner workflow) {\n        return createOrUpdateWithServiceResponseAsync(resourceGroupName, workflowName, workflow).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "def eval(self, expr, source_name=\"<string>\", inner_namespace={}):\n        \"\"\"Evaluate some Python code in the encapsulated environment.\n        :arg expr: A string containing a Python expression.\n        :arg source_name: A name for this string, for use in tracebacks.\n        :arg inner_namespace: A dict-like object that will be checked first\n          when `expr` attempts to access any variables.\n        :returns: The value of `expr`.\n        \"\"\"\n        code = compile(expr, source_name, \"eval\", self.flags, False)\n        return eval(code, {}, VarLookupDict([inner_namespace]\n                                            + self._namespaces))",
    "label": 0
  },
  {
    "codes": "public KeyBundle updateKey(String vaultBaseUrl, String keyName, String keyVersion, List<JsonWebKeyOperation> keyOps, KeyAttributes keyAttributes, Map<String, String> tags) {\n        return updateKeyWithServiceResponseAsync(vaultBaseUrl, keyName, keyVersion, keyOps, keyAttributes, tags).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<PatternRuleInfo> updatePatternAsync(UUID appId, String versionId, UUID patternId, PatternRuleUpdateObject pattern) {\n        return updatePatternWithServiceResponseAsync(appId, versionId, patternId, pattern).map(new Func1<ServiceResponse<PatternRuleInfo>, PatternRuleInfo>() {\n            @Override\n            public PatternRuleInfo call(ServiceResponse<PatternRuleInfo> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```lua\n-- Define the terrain part to be destructible\nlocal terrainPart = Instance.new(\"Part\", workspace)\nterrainPart.Size = Vector3.new(10, 10, 10)\nterrainPart.Position = Vector3.new(0, 5, 0)\nterrainPart.Anchored = true\nterrainPart.Material = Enum.Material.Grass\n\n-- Function to handle destruction\nlocal function destroyTerrain(hit)\n    if hit == terrainPart then\n        -- Create debris particles\n        local debris = Instance.new(\"Part\", workspace)\n        debris.Size = Vector3.new(1, 1, 1)\n        debris.Position = terrainPart.Position\n        debris.Velocity = Vector3.new(math.random(-10, 10), math.random(5, 15), math.random(-10, 10))\n        debris.Anchored = false\n        debris.Material = Enum.Material.Grass\n\n        -- Remove the original terrain part\n        terrainPart:Destroy()\n    end\nend\n\n-- Connect the destruction function to a touch event\nterrainPart.Touched:Connect(destroyTerrain)\n```\n\n```lua\n-- Alternative approach using a grid of smaller parts for more granular destruction\nlocal gridSize = 10\nlocal partSize = 2\n\n-- Create a grid of destructible parts\nfor x = 1, gridSize do\n    for y = 1, gridSize do\n        for z = 1, gridSize do\n            local part = Instance.new(\"Part\", workspace)\n            part.Size = Vector3.new(partSize, partSize, partSize)\n            part.Position = Vector3.new(x * partSize, y * partSize, z * partSize)\n            part.Anchored = true\n            part.Material = Enum.Material.Grass\n\n            -- Function to handle destruction of individual grid parts\n            part.Touched:Connect(function(hit)\n                if hit and hit.Parent then\n                    part:Destroy()\n                end\n            end)\n        end\n    end\nend\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\n\n# Generate random 3D molecular structure data\nnp.random.seed(42)\nnum_atoms = 50\nx = np.random.uniform(-10, 10, num_atoms)\ny = np.random.uniform(-10, 10, num_atoms)\nz = np.random.uniform(-10, 10, num_atoms)\natom_types = np.random.choice(['H', 'C', 'O', 'N'], num_atoms)\n\n# Create a DataFrame to hold the molecular data\ndf = pd.DataFrame({'x': x, 'y': y, 'z': z, 'atom_type': atom_types})\n\n# Define a color map for different atom types\ncolor_map = {'H': 'gray', 'C': 'black', 'O': 'red', 'N': 'blue'}\ndf['color'] = df['atom_type'].map(color_map)\n\n# Create a 3D scatter plot using Plotly Express\nfig = px.scatter_3d(df, x='x', y='y', z='z', color='color', \n                    title='3D Molecular Structure Visualization',\n                    labels={'x': 'X-axis', 'y': 'Y-axis', 'z': 'Z-axis'},\n                    size_max=10, opacity=0.8)\n\n# Customize the layout\nfig.update_layout(scene=dict(\n    xaxis_title='X-axis (\u00c5)',\n    yaxis_title='Y-axis (\u00c5)',\n    zaxis_title='Z-axis (\u00c5)',\n    aspectmode='data'\n))\n\n# Show the plot\nfig.show()\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<CredentialResultsInner>> listClusterUserCredentialsWithServiceResponseAsync(String resourceGroupName, String resourceName) {\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (resourceName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceName is required and cannot be null.\");\n        }\n        final String apiVersion = \"2019-02-01\";\n        return service.listClusterUserCredentials(this.client.subscriptionId(), resourceGroupName, resourceName, apiVersion, this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<CredentialResultsInner>>>() {\n                @Override\n                public Observable<ServiceResponse<CredentialResultsInner>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<CredentialResultsInner> clientResponse = listClusterUserCredentialsDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```python\ndef factorial(n):\n    # Define a helper function for recursion\n    def recursive_factorial(x):\n        if x <= 1:  # Base case: 0! = 1 and 1! = 1\n            return 1\n        else:\n            return x * recursive_factorial(x - 1)  # Recursive call\n\n    if not isinstance(n, int) or n < 0:  # Validate input\n        raise ValueError(\"Input must be a non-negative integer.\")\n    return recursive_factorial(n)  # Invoke the helper function\n\n# Example usage\n# print(factorial(5))  # Output: 120\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"testing\"\n\n\t\"github.com/tidwall/gjson\"\n)\n\n// Sample JSON data\nvar jsonData = []byte(`{\"name\":\"John\",\"age\":30,\"city\":\"New York\"}`)\n\n// Benchmark for standard library encoding/json\nfunc BenchmarkStdJSON(b *testing.B) {\n\tfor i := 0; i < b.N; i++ {\n\t\tvar result map[string]interface{}\n\t\t_ = json.Unmarshal(jsonData, &result)\n\t}\n}\n\n// Benchmark for gjson library\nfunc BenchmarkGJSON(b *testing.B) {\n\tfor i := 0; i < b.N; i++ {\n\t\t_ = gjson.GetBytes(jsonData, \"name\")\n\t\t_ = gjson.GetBytes(jsonData, \"age\")\n\t\t_ = gjson.GetBytes(jsonData, \"city\")\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "def padding(value):\n        \"\"\"int or dict : Padding around visualization\n\n        The padding defines the distance between the edge of the\n        visualization canvas to the visualization box. It does not count as\n        part of the visualization width/height. Values cannot be negative.\n\n        If a dict, padding must have all keys ``''top'``, ``'left'``,\n        ``'right'``, and ``'bottom'`` with int values.\n        \"\"\"\n        if isinstance(value, dict):\n            required_keys = ['top', 'left', 'right', 'bottom']\n            for key in required_keys:\n                if key not in value:\n                    error = ('Padding must have keys \"{0}\".'\n                             .format('\", \"'.join(required_keys)))\n                    raise ValueError(error)\n                _assert_is_type('padding: {0}'.format(key), value[key], int)\n                if value[key] < 0:\n                    raise ValueError('Padding cannot be negative.')\n        elif isinstance(value, int):\n            if value < 0:\n                raise ValueError('Padding cannot be negative.')\n        else:\n            if value not in (\"auto\", \"strict\"):\n                raise ValueError('Padding can only be auto or strict.')",
    "label": 0
  },
  {
    "codes": "func (s *GetJobOutputOutput) SetChecksum(v string) *GetJobOutputOutput {\n\ts.Checksum = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *DeletePartitionInput) SetPartitionValues(v []*string) *DeletePartitionInput {\n\ts.PartitionValues = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```java\n// Import necessary classes for Android and TensorFlow Lite\nimport android.Manifest;\nimport android.content.pm.PackageManager;\nimport android.graphics.Bitmap;\nimport android.media.Image;\nimport android.os.Bundle;\nimport android.util.Size;\nimport android.view.Surface;\nimport android.view.SurfaceHolder;\nimport android.view.SurfaceView;\nimport android.widget.TextView;\nimport androidx.annotation.NonNull;\nimport androidx.appcompat.app.AppCompatActivity;\nimport androidx.core.app.ActivityCompat;\nimport com.google.android.material.snackbar.Snackbar;\nimport org.tensorflow.lite.Interpreter;\nimport org.tensorflow.lite.support.image.TensorImage;\nimport org.tensorflow.lite.support.model.Model;\nimport org.tensorflow.lite.support.tensorbuffer.TensorBuffer;\nimport java.nio.ByteBuffer;\nimport java.util.List;\n\n// MainActivity class for the object detection app\npublic class MainActivity extends AppCompatActivity {\n    // Declare variables for TensorFlow Lite model, labels, and camera handling\n    private Interpreter tflite;\n    private List<String> labels;\n    private SurfaceView surfaceView;\n    private TextView resultTextView;\n\n    // Request codes and permission\n    private static final int PERMISSION_REQUEST_CODE = 101;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        \n        // Initialize UI components\n        surfaceView = findViewById(R.id.surface_view);\n        resultTextView = findViewById(R.id.result_text_view);\n\n        // Load model and labels\n        loadModelAndLabels();\n\n        // Check camera permission and start camera\n        if (ActivityCompat.checkSelfPermission(this, Manifest.permission.CAMERA) \n            != PackageManager.PERMISSION_GRANTED) {\n            requestCameraPermission();\n        } else {\n            startCamera();\n        }\n    }\n\n    // Method to load the TensorFlow Lite model and labels from assets\n    private void loadModelAndLabels() {\n        try {\n            // Load the model from assets directory\n            Model model = Model.createModel(this, \"model.tflite\");\n            tflite = new Interpreter(model);\n\n            // Load labels\n            labels = FileUtil.loadLabels(this, \"labels.txt\");\n        } catch (Exception e) {\n            e.printStackTrace();\n            Snackbar.make(surfaceView, \"Error loading model or labels\", \n                          Snackbar.LENGTH_SHORT).show();\n        }\n    }\n\n    // Request camera permission from the user\n    private void requestCameraPermission() {\n        ActivityCompat.requestPermissions(this,\n                new String[]{Manifest.permission.CAMERA}, PERMISSION_REQUEST_CODE);\n    }\n\n    // Handle the result from permission request\n    @Override\n    public void onRequestPermissionsResult(int requestCode, \n                                           @NonNull String[] permissions, \n                                           @NonNull int[] grantResults) {\n        if (requestCode == PERMISSION_REQUEST_CODE) {\n            if (grantResults.length > 0 && grantResults[0] == PackageManager.PERMISSION_GRANTED) {\n                startCamera();\n            } else {\n                Snackbar.make(surfaceView, \"Camera permission is required\", \n                              Snackbar.LENGTH_SHORT).show();\n            }\n        }\n    }\n\n    // Initialize and start the camera\n    private void startCamera() {\n        surfaceView.getHolder().addCallback(new SurfaceHolder.Callback() {\n            @Override\n            public void surfaceCreated(SurfaceHolder holder) {\n                // Start camera when surface is created\n                CameraUtils.startCamera(MainActivity.this, surfaceView,\n                        new Size(640, 480), (image) -> processImage(image));\n            }\n\n            @Override\n            public void surfaceChanged(SurfaceHolder holder, int format, \n                                       int width, int height) {}\n\n            @Override\n            public void surfaceDestroyed(SurfaceHolder holder) {}\n        });\n    }\n\n    // Process the camera image using the TensorFlow Lite model\n    private void processImage(Image image) {\n        Bitmap bitmap = CameraUtils.imageToBitmap(image);\n        TensorImage tImage = new TensorImage();\n        tImage.load(bitmap);\n\n        // Prepare the input and output buffers\n        ByteBuffer inputBuffer = tImage.getBuffer();\n        TensorBuffer outputBuffer = TensorBuffer.createFixedSize(\n                new int[]{1, labels.size()}, DataType.FLOAT32);\n        \n        // Run inference\n        tflite.run(inputBuffer, outputBuffer.getBuffer().rewind());\n\n        // Decode results and display\n        String resultText = getTopLabel(outputBuffer.getFloatArray());\n        runOnUiThread(() -> resultTextView.setText(resultText));\n    }\n\n    // Get the most probable label from the output array\n    private String getTopLabel(float[] output) {\n        int maxIndex = -1;\n        float maxProbability = Float.NEGATIVE_INFINITY;\n        for (int i = 0; i < output.length; ++i) {\n            if (output[i] > maxProbability) {\n                maxProbability = output[i];\n                maxIndex = i;\n            }\n        }\n        return labels.get(maxIndex);\n    }\n}\n```\n\n```xml\n<!-- activity_main.xml -->\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n    <SurfaceView\n        android:id=\"@+id/surface_view\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\" />\n\n    <TextView\n        android:id=\"@+id/result_text_view\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:background=\"@android:color/white\"\n        android:padding=\"8dp\"\n        android:layout_alignParentBottom=\"true\"\n        android:text=\"Detecting...\" />\n\n</RelativeLayout>\n```\n\n```xml\n<!-- AndroidManifest.xml -->\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.example.objectdetection\">\n\n    <uses-permission android:name=\"android.permission.CAMERA\" />\n\n    <application\n        android:allowBackup=\"true\"\n        android:icon=\"@mipmap/ic_launcher\"\n        android:label=\"@string/app_name\"\n        android:roundIcon=\"@mipmap/ic_launcher_round\"\n        android:supportsRtl=\"true\"\n        android:theme=\"@style/Theme.AppCompat.Light.NoActionBar\">\n        <activity\n            android:name=\".MainActivity\"\n            android:configChanges=\"orientation|screenSize\"\n            android:label=\"@string/app_name\"\n            android:screenOrientation=\"portrait\">\n            <intent-filter>\n                <action android:name=\"android.intent.action.MAIN\" />\n                <category android:name=\"android.intent.category.LAUNCHER\" />\n            </intent-filter>\n        </activity>\n    </application>\n</manifest>\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define variables\nBACKUP_DIR=\"/path/to/backup\"  # Directory to backup\nS3_BUCKET=\"s3://your-bucket-name\"  # S3 bucket name\nDATE=$(date +%Y-%m-%d_%H-%M-%S)  # Timestamp for backup file\nLOG_FILE=\"/var/log/backup.log\"  # Log file path\n\n# Check if backup directory exists\nif [ ! -d \"$BACKUP_DIR\" ]; then\n    echo \"Backup directory does not exist: $BACKUP_DIR\" >> \"$LOG_FILE\"\n    exit 1\nfi\n\n# Create a compressed archive of the backup directory\nBACKUP_FILE=\"/tmp/backup_$DATE.tar.gz\"\ntar -czf \"$BACKUP_FILE\" \"$BACKUP_DIR\" 2>> \"$LOG_FILE\"\n\n# Check if tar command was successful\nif [ $? -ne 0 ]; then\n    echo \"Failed to create backup archive: $BACKUP_FILE\" >> \"$LOG_FILE\"\n    exit 1\nfi\n\n# Upload the backup file to S3\naws s3 cp \"$BACKUP_FILE\" \"$S3_BUCKET/\" 2>> \"$LOG_FILE\"\n\n# Check if S3 upload was successful\nif [ $? -ne 0 ]; then\n    echo \"Failed to upload backup to S3: $BACKUP_FILE\" >> \"$LOG_FILE\"\n    exit 1\nfi\n\n# Clean up the local backup file\nrm -f \"$BACKUP_FILE\"\n\n# Log success message\necho \"Backup completed successfully: $BACKUP_FILE\" >> \"$LOG_FILE\"\nexit 0\n```",
    "label": 1
  },
  {
    "codes": "function getModulePath (mod, useAbsolutePath) {\n  const modPath =\n    mod === 'regenerator-runtime'\n      ? 'regenerator-runtime/runtime'\n      : `core-js/modules/${mod}`\n  return useAbsolutePath ? require.resolve(modPath) : modPath\n}",
    "label": 0
  },
  {
    "codes": "```javascript\nconst express = require('express');\nconst bodyParser = require('body-parser');\n\nconst app = express();\napp.use(bodyParser.json());\n\n// In-memory user storage (use a database in production)\nconst users = [\n  { username: 'user1', password: 'password1' }, // Sample user\n];\n\napp.post('/login', (req, res) => {\n  const { username, password } = req.body;\n\n  // Check if the username is present\n  if (!username) {\n    return res.status(400).send('Username required'); // Bad Request\n  }\n\n  // Check if the password is present\n  if (!password) {\n    return res.status(400).send('Password required'); // Bad Request\n  }\n\n  // Authenticate user\n  const user = users.find(u => u.username === username && u.password === password);\n  if (user) {\n    return res.status(200).send('Login successful'); // OK\n  } else {\n    return res.status(401).send('Invalid credentials'); // Unauthorized\n  }\n});\n\n// 404 handling for unsupported routes\napp.use((req, res) => {\n  res.status(404).send('Not found'); // Not Found\n});\n\nconst PORT = process.env.PORT || 3000;\napp.listen(PORT, () => {\n  console.log(`Server is running on port ${PORT}`); // Server started\n});\n```\n",
    "label": 1
  },
  {
    "codes": "def stop(self):\n        \"\"\"\n        Send a signal to the main broker task to terminate. You have to check\n        :attr:`running` and possibly wait for it to become :data:`False` ---\n        the task takes at least one loop through the event loop to terminate.\n\n        It is guarenteed that the task will not attempt to send stanzas over\n        the existing `xmlstream` after a call to :meth:`stop` has been made.\n\n        It is legal to call :meth:`stop` even if the task is already\n        stopped. It is a no-op in that case.\n        \"\"\"\n        if not self.running:\n            return\n        self._logger.debug(\"sending stop signal to task\")\n        self._task.cancel()",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <stdexcept>\n\nclass DatabaseConnection {\npublic:\n    // Constructor: Establishes a database connection\n    DatabaseConnection(const std::string& connectionString) {\n        // Simulate connection establishment\n        std::cout << \"Connecting to database: \" << connectionString << std::endl;\n        if (connectionString.empty()) {\n            throw std::runtime_error(\"Connection string is empty\");\n        }\n        // Assume connection is successful\n        isConnected = true;\n    }\n\n    // Destructor: Ensures the connection is closed\n    ~DatabaseConnection() {\n        if (isConnected) {\n            std::cout << \"Disconnecting from database\" << std::endl;\n            // Simulate disconnection\n            isConnected = false;\n        }\n    }\n\n    // Execute a query (example method)\n    void executeQuery(const std::string& query) {\n        if (!isConnected) {\n            throw std::runtime_error(\"Not connected to the database\");\n        }\n        std::cout << \"Executing query: \" << query << std::endl;\n    }\n\n    // Copy constructor and assignment operator are deleted to prevent copying\n    DatabaseConnection(const DatabaseConnection&) = delete;\n    DatabaseConnection& operator=(const DatabaseConnection&) = delete;\n\nprivate:\n    bool isConnected = false; // Tracks connection status\n};\n\nint main() {\n    try {\n        DatabaseConnection db(\"example_db\"); // RAII: Connection is managed automatically\n        db.executeQuery(\"SELECT * FROM users\");\n        // Connection will be automatically closed when 'db' goes out of scope\n    } catch (const std::exception& e) {\n        std::cerr << \"Error: \" << e.what() << std::endl;\n    }\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <cstdlib>\n\n// Function to allocate memory and intentionally leak it\nvoid createMemoryLeak() {\n    int* leakyPointer = new int(42); // Allocate memory and assign a value\n    // Intentionally not deleting the pointer to create a memory leak\n}\n\nint main() {\n    // Call the function that creates a memory leak\n    createMemoryLeak();\n\n    // Print a message to indicate the program has run\n    std::cout << \"Program executed, check for memory leaks using Valgrind.\" << std::endl;\n\n    return 0;\n}\n```\n\n```bash\n# Compile the program with debugging symbols\ng++ -g -o memory_leak_debug memory_leak_debug.cpp\n\n# Run the program with Valgrind to detect memory leaks\nvalgrind --leak-check=full ./memory_leak_debug\n```\n\n```plaintext\n# Expected Valgrind output indicating a memory leak\n==12345== HEAP SUMMARY:\n==12345==     in use at exit: 4 bytes in 1 blocks\n==12345==   total heap usage: 1 allocs, 0 frees, 4 bytes allocated\n==12345== \n==12345== 4 bytes in 1 blocks are definitely lost in loss record 1 of 1\n==12345==    at 0x4C2BBAF: operator new(unsigned long) (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\n==12345==    by 0x4006F6: createMemoryLeak() (memory_leak_debug.cpp:6)\n==12345==    by 0x40071A: main (memory_leak_debug.cpp:12)\n==12345== \n==12345== LEAK SUMMARY:\n==12345==    definitely lost: 4 bytes in 1 blocks\n==12345==    indirectly lost: 0 bytes in 0 blocks\n==12345==      possibly lost: 0 bytes in 0 blocks\n==12345==    still reachable: 0 bytes in 0 blocks\n==12345==         suppressed: 0 bytes in 0 blocks\n==12345== \n==12345== For counts of detected and suppressed errors, rerun with: -v\n==12345== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define log file path\nLOG_FILE=\"/var/log/apache2/access.log\"\n\n# Parse log file for 5xx errors and count occurrences per minute\ngrep -oP '\\[.*\\] \"\\d{3}\" 5\\d{2}' \"$LOG_FILE\" | awk '{print $1}' | sort | uniq -c | while read -r count timestamp; do\n    # Check if the count exceeds a threshold (e.g., 10 errors per minute)\n    if [ \"$count\" -gt 10 ]; then\n        echo \"5xx Error Spike Detected: $count errors at $timestamp\"\n    fi\ndone\n```\n\n### Explanation:\n- `grep -oP '\\[.*\\] \"\\d{3}\" 5\\d{2}'`: Extracts lines with 5xx errors and captures the timestamp.\n- `awk '{print $1}'`: Extracts the timestamp from the log entry.\n- `sort | uniq -c`: Counts occurrences of 5xx errors per minute.\n- `if [ \"$count\" -gt 10 ]`: Checks if the count exceeds a threshold (e.g., 10 errors per minute).\n- `echo \"5xx Error Spike Detected: $count errors at $timestamp\"`: Outputs a warning if a spike is detected.",
    "label": 1
  },
  {
    "codes": "```kotlin\nsealed class DomainError {\n    // Authentication errors\n    sealed class AuthError : DomainError() {\n        object InvalidCredentials : AuthError() // 401\n        object TokenExpired : AuthError() // 403\n        object AccessDenied : AuthError() // 403\n    }\n\n    // Validation errors\n    sealed class ValidationError : DomainError() {\n        object InvalidEmail : ValidationError() // 400\n        object InvalidPassword : ValidationError() // 400\n        object MissingRequiredField : ValidationError() // 400\n    }\n\n    // Network errors\n    sealed class NetworkError : DomainError() {\n        object Timeout : NetworkError() // 408\n        object ConnectionFailed : NetworkError() // 503\n        object ServerError : NetworkError() // 500\n    }\n\n    // Resource errors\n    sealed class ResourceError : DomainError() {\n        object NotFound : ResourceError() // 404\n        object AlreadyExists : ResourceError() // 409\n        object InsufficientPermissions : ResourceError() // 403\n    }\n\n    // Custom business logic errors\n    sealed class BusinessError : DomainError() {\n        object PaymentFailed : BusinessError() // 402\n        object OutOfStock : BusinessError() // 409\n        object InvalidOperation : BusinessError() // 400\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport xml.etree.ElementTree as ET\n\ndef parse_large_xml(file_path):\n    \"\"\"\n    Generator to yield elements from a large XML file one by one.\n    \"\"\"\n    context = ET.iterparse(file_path, events=(\"start\", \"end\"))\n    context = iter(context)\n    event, root = next(context)  # Get the root element\n\n    for event, elem in context:\n        if event == \"end\":\n            yield elem\n            root.clear()  # Free up memory by clearing processed elements\n```",
    "label": 1
  },
  {
    "codes": "function(gotoEnd){\n\t\tvar t = now();\n\n\t\tif ( gotoEnd || t >= this.options.duration + this.startTime ) {\n\t\t\tthis.now = this.end;\n\t\t\tthis.pos = this.state = 1;\n\t\t\tthis.update();\n\n\t\t\tthis.options.curAnim[ this.prop ] = true;\n\n\t\t\tvar done = true;\n\t\t\tfor ( var i in this.options.curAnim )\n\t\t\t\tif ( this.options.curAnim[i] !== true )\n\t\t\t\t\tdone = false;\n\n\t\t\tif ( done ) {\n\t\t\t\tif ( this.options.display != null ) {\n\t\t\t\t\t// Reset the overflow\n\t\t\t\t\tthis.elem.style.overflow = this.options.overflow;\n\n\t\t\t\t\t// Reset the display\n\t\t\t\t\tthis.elem.style.display = this.options.display;\n\t\t\t\t\tif ( jQuery.css(this.elem, \"display\") == \"none\" )\n\t\t\t\t\t\tthis.elem.style.display = \"block\";\n\t\t\t\t}\n\n\t\t\t\t// Hide the element if the \"hide\" operation was done\n\t\t\t\tif ( this.options.hide )\n\t\t\t\t\tjQuery(this.elem).hide();\n\n\t\t\t\t// Reset the properties, if the item has been hidden or shown\n\t\t\t\tif ( this.options.hide || this.options.show )\n\t\t\t\t\tfor ( var p in this.options.curAnim )\n\t\t\t\t\t\tjQuery.attr(this.elem.style, p, this.options.orig[p]);\n\t\t\t\t\t\n\t\t\t\t// Execute the complete function\n\t\t\t\tthis.options.complete.call( this.elem );\n\t\t\t}\n\n\t\t\treturn false;\n\t\t} else {\n\t\t\tvar n = t - this.startTime;\n\t\t\tthis.state = n / this.options.duration;\n\n\t\t\t// Perform the easing function, defaults to swing\n\t\t\tthis.pos = jQuery.easing[this.options.easing || (jQuery.easing.swing ? \"swing\" : \"linear\")](this.state, n, 0, 1, this.options.duration);\n\t\t\tthis.now = this.start + ((this.end - this.start) * this.pos);\n\n\t\t\t// Perform the next step of the animation\n\t\t\tthis.update();\n\t\t}\n\n\t\treturn true;\n\t}",
    "label": 0
  },
  {
    "codes": "```javascript\n// Initialize game variables and settings\nvar config = {\n    type: Phaser.AUTO,\n    width: 800,\n    height: 600,\n    scene: {\n        preload: preload,\n        create: create,\n        update: update\n    },\n    physics: {\n        default: 'arcade',\n        arcade: {\n            gravity: { y: 0 }\n        }\n    }\n};\n\nvar game = new Phaser.Game(config);\nvar player;\nvar cursors;\nvar bullets;\nvar particleEmitter;\n\n// Preload assets\nfunction preload() {\n    this.load.image('space', 'assets/images/space.png');\n    this.load.image('ship', 'assets/images/ship.png');\n    this.load.image('bullet', 'assets/images/bullet.png');\n}\n\n// Create game objects\nfunction create() {\n    this.add.image(400, 300, 'space');\n    \n    player = this.physics.add.image(400, 500, 'ship').setCollideWorldBounds(true);\n    \n    cursors = this.input.keyboard.createCursorKeys();\n    \n    bullets = this.physics.add.group({\n        defaultKey: 'bullet',\n        maxSize: 10\n    });\n    \n    // Create particle emitter\n    var particles = this.add.particles('bullet');\n    particleEmitter = particles.createEmitter({\n        speed: 200,\n        scale: { start: 1, end: 0 },\n        blendMode: 'ADD'\n    });\n}\n\n// Update game objects\nfunction update() {\n    player.setVelocity(0);\n    \n    if (cursors.left.isDown) {\n        player.setVelocityX(-300);\n    } else if (cursors.right.isDown) {\n        player.setVelocityX(300);\n    }\n    \n    if (cursors.up.isDown) {\n        player.setVelocityY(-300);\n    } else if (cursors.down.isDown) {\n        player.setVelocityY(300);\n    }\n    \n    if (cursors.space.isDown && bullets.getLength() < bullets.maxSize) {\n        shootBullet();\n    }\n}\n\n// Shoot a bullet\nfunction shootBullet() {\n    var bullet = bullets.get(player.x, player.y - 20);\n    if (bullet) {\n        bullet.setActive(true);\n        bullet.setVisible(true);\n        bullet.setVelocityY(-400);\n        particleEmitter.startFollow(bullet); // Attach particle effect to bullet\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```dart\nString formatTimestamp(DateTime timestamp) {\n  final now = DateTime.now();\n  final difference = now.difference(timestamp);\n\n  if (difference.inSeconds < 60) {\n    return '${difference.inSeconds} seconds ago';\n  } else if (difference.inMinutes < 60) {\n    return '${difference.inMinutes} minutes ago';\n  } else if (difference.inHours < 24) {\n    return '${difference.inHours} hours ago';\n  } else if (difference.inDays < 7) {\n    return '${difference.inDays} days ago';\n  } else if (difference.inDays < 30) {\n    final weeks = (difference.inDays / 7).floor();\n    return '$weeks week${weeks > 1 ? 's' : ''} ago';\n  } else if (difference.inDays < 365) {\n    final months = (difference.inDays / 30).floor();\n    return '$months month${months > 1 ? 's' : ''} ago';\n  } else {\n    final years = (difference.inDays / 365).floor();\n    return '$years year${years > 1 ? 's' : ''} ago';\n  }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<ImageList>> getDetailsWithServiceResponseAsync(String listId) {\n        if (this.client.baseUrl() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.baseUrl() is required and cannot be null.\");\n        }\n        if (listId == null) {\n            throw new IllegalArgumentException(\"Parameter listId is required and cannot be null.\");\n        }\n        String parameterizedHost = Joiner.on(\", \").join(\"{baseUrl}\", this.client.baseUrl());\n        return service.getDetails(listId, this.client.acceptLanguage(), parameterizedHost, this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<ImageList>>>() {\n                @Override\n                public Observable<ServiceResponse<ImageList>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<ImageList> clientResponse = getDetailsDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "function onProxyError(proxy) {\n  return (err, req, res) => {\n    const host = req.headers && req.headers.host;\n    console.log(\n      chalk.red('Proxy error:') +\n        ' Could not proxy request ' +\n        chalk.cyan(req.url) +\n        ' from ' +\n        chalk.cyan(host) +\n        ' to ' +\n        chalk.cyan(proxy) +\n        '.'\n    );\n    console.log(\n      'See https://nodejs.org/api/errors.html#errors_common_system_errors for more information (' +\n        chalk.cyan(err.code) +\n        ').'\n    );\n    console.log();\n\n    // And immediately send the proper error response to the client.\n    // Otherwise, the request will eventually timeout with ERR_EMPTY_RESPONSE on the client side.\n    if (res.writeHead && !res.headersSent) {\n      res.writeHead(500);\n    }\n    res.end(\n      'Proxy error: Could not proxy request ' +\n        req.url +\n        ' from ' +\n        host +\n        ' to ' +\n        proxy +\n        ' (' +\n        err.code +\n        ').'\n    );\n  };\n}",
    "label": 0
  },
  {
    "codes": "```typescript\n// Define a decorator function to retry failed async methods\nfunction Retry(maxRetries: number = 3, delay: number = 1000) {\n    return function (target: any, propertyKey: string, descriptor: PropertyDescriptor) {\n        const originalMethod = descriptor.value;\n\n        descriptor.value = async function (...args: any[]) {\n            let retries = 0;\n\n            while (retries < maxRetries) {\n                try {\n                    // Attempt to execute the original method\n                    const result = await originalMethod.apply(this, args);\n                    return result; // Return the result if successful\n                } catch (error) {\n                    retries++;\n                    if (retries >= maxRetries) {\n                        throw error; // Throw the error if max retries reached\n                    }\n                    // Wait for the specified delay before retrying\n                    await new Promise(resolve => setTimeout(resolve, delay));\n                }\n            }\n        };\n\n        return descriptor;\n    };\n}\n\n// Example usage of the Retry decorator\nclass ExampleService {\n    @Retry(5, 2000) // Retry up to 5 times with a 2-second delay\n    async fetchData() {\n        // Simulate a network request that might fail\n        const response = await fetch('https://api.example.com/data');\n        if (!response.ok) {\n            throw new Error('Failed to fetch data');\n        }\n        return response.json();\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *CreateVaultInput) SetAccountId(v string) *CreateVaultInput {\n\ts.AccountId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (c *Glue) BatchGetJobs(input *BatchGetJobsInput) (*BatchGetJobsOutput, error) {\n\treq, out := c.BatchGetJobsRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```ini\n; PlatformIO configuration file for cross-compiling to multiple MCUs\n\n[env]\nplatform = ststm32\nframework = stm32cube\n\n; Define common settings for all environments\nbuild_flags = \n    -DDEBUG\n    -DUSE_FULL_ASSERT\n\n; Environment for STM32F103C8 (Blue Pill)\n[env:bluepill]\nboard = bluepill_f103c8\nupload_protocol = stlink\nbuild_flags = \n    ${env.build_flags}\n    -DSTM32F103xB\n\n; Environment for STM32F407VG (Discovery Board)\n[env:discovery_f407]\nboard = disco_f407vg\nupload_protocol = stlink\nbuild_flags = \n    ${env.build_flags}\n    -DSTM32F407xx\n\n; Environment for STM32L432KC (Nucleo Board)\n[env:nucleo_l432]\nboard = nucleo_l432kc\nupload_protocol = stlink\nbuild_flags = \n    ${env.build_flags}\n    -DSTM32L432xx\n\n; Environment for STM32G071RB (Nucleo Board)\n[env:nucleo_g071]\nboard = nucleo_g071rb\nupload_protocol = stlink\nbuild_flags = \n    ${env.build_flags}\n    -DSTM32G071xx\n```",
    "label": 1
  },
  {
    "codes": "```python\nfrom fastapi import FastAPI, Depends, HTTPException, status\nfrom fastapi.security import APIKeyHeader\nfrom fastapi.middleware.rate_limiter import RateLimiter, RateLimitExceeded\nfrom fastapi.middleware import Middleware\nfrom fastapi.middleware.rate_limiter import RateLimiterMiddleware\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# API Key Authentication\napi_key_header = APIKeyHeader(name=\"X-API-Key\")\n\n# Rate Limiter Middleware\napp.add_middleware(\n    RateLimiterMiddleware,\n    limit=10,  # 10 requests per minute\n    window=60,  # 1 minute window\n)\n\n# Dependency to validate API key\ndef get_api_key(api_key: str = Depends(api_key_header)):\n    valid_keys = {\"valid_key_1\", \"valid_key_2\"}  # Replace with your valid keys\n    if api_key not in valid_keys:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid API Key\",\n        )\n    return api_key\n\n# Endpoint with rate limiting and API key authentication\n@app.get(\"/protected-endpoint\")\nasync def protected_endpoint(api_key: str = Depends(get_api_key)):\n    try:\n        # Your business logic here\n        return {\"message\": \"Access granted\"}\n    except RateLimitExceeded:\n        raise HTTPException(\n            status_code=status.HTTP_429_TOO_MANY_REQUESTS,\n            detail=\"Rate limit exceeded\",\n        )\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport os\nimport tempfile\nfrom contextlib import contextmanager\n\n@contextmanager\ndef atomic_write(file_path, mode='w', overwrite=False):\n    # Create a temporary file with .tmp suffix\n    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.tmp', dir=os.path.dirname(file_path), mode=mode)\n    try:\n        yield temp_file  # Provide the temporary file for writing\n        temp_file.close()\n\n        # Determine the target file path, handling overwrite option\n        if overwrite:\n            target_file_path = file_path\n        else:\n            target_file_path = file_path + '.tmp'\n\n        # Atomically replace the target file with the temporary one\n        os.replace(temp_file.name, target_file_path)\n    except Exception:\n        # Cleanup temp file in case of any issues\n        try:\n            os.remove(temp_file.name)\n        except OSError:\n            pass\n        raise  # Re-raise the exception to propagate error\n\n# Usage example:\n# with atomic_write('test.txt') as f:\n#     f.write('Hello, world!')\n```\n",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class MazeGenerator : MonoBehaviour\n{\n    public int width = 10;\n    public int height = 10;\n    public GameObject wallPrefab;\n    public GameObject floorPrefab;\n\n    private bool[,] grid;\n\n    void Start()\n    {\n        grid = new bool[width, height];\n        GenerateMaze();\n        DrawMaze();\n    }\n\n    void GenerateMaze()\n    {\n        // Initialize the grid with walls\n        for (int x = 0; x < width; x++)\n        {\n            for (int y = 0; y < height; y++)\n            {\n                grid[x, y] = true;\n            }\n        }\n\n        // Start the maze generation from a random point\n        CarvePassagesFrom(1, 1);\n    }\n\n    void CarvePassagesFrom(int currentX, int currentY)\n    {\n        int[] directions = { 0, 1, 2, 3 };\n        ShuffleDirections(directions);\n\n        foreach (var direction in directions)\n        {\n            int newX = currentX;\n            int newY = currentY;\n\n            switch (direction)\n            {\n                case 0: newY -= 2; break; // North\n                case 1: newX += 2; break; // East\n                case 2: newY += 2; break; // South\n                case 3: newX -= 2; break; // West\n            }\n\n            if (IsInBounds(newX, newY) && grid[newX, newY])\n            {\n                grid[newX, newY] = false;\n                grid[currentX + (newX - currentX) / 2, currentY + (newY - currentY) / 2] = false;\n                CarvePassagesFrom(newX, newY);\n            }\n        }\n    }\n\n    bool IsInBounds(int x, int y)\n    {\n        return x > 0 && x < width && y > 0 && y < height;\n    }\n\n    void ShuffleDirections(int[] directions)\n    {\n        for (int i = 0; i < directions.Length; i++)\n        {\n            int rnd = Random.Range(i, directions.Length);\n            int temp = directions[i];\n            directions[i] = directions[rnd];\n            directions[rnd] = temp;\n        }\n    }\n\n    void DrawMaze()\n    {\n        for (int x = 0; x < width; x++)\n        {\n            for (int y = 0; y < height; y++)\n            {\n                Vector3 position = new Vector3(x, 0, y);\n                if (grid[x, y])\n                {\n                    Instantiate(wallPrefab, position, Quaternion.identity, transform);\n                }\n                else\n                {\n                    Instantiate(floorPrefab, position, Quaternion.identity, transform);\n                }\n            }\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.decomposition import PCA\n\n# Load MNIST dataset\nmnist = fetch_openml('mnist_784', version=1)\nX, y = mnist.data, mnist.target.astype(int)\n\n# Reduce dimensionality using PCA for faster computation\npca = PCA(n_components=50)\nX_pca = pca.fit_transform(X)\n\n# Apply t-SNE to the PCA-reduced data\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\nX_tsne = tsne.fit_transform(X_pca)\n\n# Plot the t-SNE embeddings\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.6)\nplt.colorbar(scatter, label='Digit')\nplt.title('t-SNE Visualization of MNIST Digits')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.show()\n```\n\n```python\n# Optional: Interactive visualization with Plotly\nimport plotly.express as px\n\n# Create a DataFrame for Plotly\nimport pandas as pd\ndf = pd.DataFrame({'x': X_tsne[:, 0], 'y': X_tsne[:, 1], 'digit': y})\n\n# Plot with Plotly\nfig = px.scatter(df, x='x', y='y', color='digit', title='Interactive t-SNE Visualization of MNIST Digits')\nfig.show()\n```",
    "label": 1
  },
  {
    "codes": "Certainly! Below are example pytest cases to validate an API response schema. These cases focus on checking the response status codes.\n\n```python\nimport requests\n\ndef test_api_response_status_code_200():\n    # Test for successful response\n    response = requests.get('https://api.example.com/resource')\n    assert response.status_code == 200  # Expected: 200 OK\n\ndef test_api_response_status_code_404():\n    # Test for resource not found\n    response = requests.get('https://api.example.com/nonexistent')\n    assert response.status_code == 404  # Expected: 404 Not Found\n\ndef test_api_response_status_code_400():\n    # Test for bad request due to malformed input\n    response = requests.post('https://api.example.com/resource', data={'wrong': 'input'})\n    assert response.status_code == 400  # Expected: 400 Bad Request\n\ndef test_api_response_status_code_401():\n    # Test for unauthorized access\n    response = requests.get('https://api.example.com/protected')\n    assert response.status_code == 401  # Expected: 401 Unauthorized\n\ndef test_api_response_status_code_403():\n    # Test for forbidden access\n    response = requests.get('https://api.example.com/restricted')\n    assert response.status_code == 403  # Expected: 403 Forbidden\n\ndef test_api_response_status_code_500():\n    # Test for internal server error\n    response = requests.get('https://api.example.com/server-error')\n    assert response.status_code == 500  # Expected: 500 Internal Server Error\n```\n\nThese tests verify that the API handles different conditions properly by returning the correct HTTP status codes.",
    "label": 1
  },
  {
    "codes": "function mapExpr(array, func) {\n  var ret = [];\n  for (var i = 0; i < array.length; ++i) {\n    ret.push(func(array[i]));\n  }\n  return ret;\n}",
    "label": 0
  },
  {
    "codes": "function getDescriptorForAsyncProp(name) {\n  return {\n    configurable: false,\n    enumerable: true,\n    // Save the provided value for async props in a special map\n    set(newValue) {\n      if (typeof newValue === 'string' || newValue instanceof Promise) {\n        this._asyncPropOriginalValues[name] = newValue;\n      } else {\n        this._asyncPropResolvedValues[name] = newValue;\n      }\n    },\n    // Only the component's state knows the true value of async prop\n    get() {\n      if (this._asyncPropResolvedValues) {\n        // Prop value isn't async, so just return it\n        if (name in this._asyncPropResolvedValues) {\n          const value = this._asyncPropResolvedValues[name];\n\n          // Special handling - components expect null `data` prop expects to be replaced with `[]`\n          if (name === 'data') {\n            return value || this._asyncPropDefaultValues[name];\n          }\n\n          return value;\n        }\n\n        if (name in this._asyncPropOriginalValues) {\n          // It's an async prop value: look into component state\n          const state = this._component && this._component.internalState;\n          if (state && state.hasAsyncProp(name)) {\n            return state.getAsyncProp(name);\n          }\n        }\n      }\n\n      // the prop is not supplied, or\n      // component not yet initialized/matched, return the component's default value for the prop\n      return this._asyncPropDefaultValues[name];\n    }\n  };\n}",
    "label": 0
  },
  {
    "codes": "```rust\n// Cargo.toml\n[package]\nname = \"zip_with_password\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\nzip = \"0.5.13\"\ncrypto = \"0.2.36\"\naes = \"0.7.5\"\nblock-modes = \"0.8.1\"\nrand = \"0.8.5\"\n\n// src/lib.rs\nuse std::fs::File;\nuse std::io::{Read, Write, Seek, BufReader, BufWriter};\nuse zip::{ZipWriter, write::FileOptions, CompressionMethod, ZipArchive};\nuse crypto::{aes, buffer, buffer::{ReadBuffer, WriteBuffer, BufferResult}};\nuse block_modes::{BlockMode, Cbc};\nuse block_modes::block_padding::Pkcs7;\nuse aes::Aes256;\nuse rand::{Rng, RngCore};\nuse std::path::Path;\n\ntype Aes256Cbc = Cbc<Aes256, Pkcs7>;\n\nfn encrypt(data: &[u8], key: &[u8], iv: &[u8]) -> Vec<u8> {\n    let cipher = Aes256Cbc::new_var(key, iv).unwrap();\n    cipher.encrypt_vec(data)\n}\n\nfn decrypt(encrypted_data: &[u8], key: &[u8], iv: &[u8]) -> Vec<u8> {\n    let cipher = Aes256Cbc::new_var(key, iv).unwrap();\n    cipher.decrypt_vec(encrypted_data).unwrap()\n}\n\n/// Write data to a password protected ZIP file\npub fn write_to_zip(src_path: &str, zip_path: &str, password: &str) -> zip::result::ZipResult<()> {\n    let file = File::create(zip_path)?;\n    let mut writer = ZipWriter::new(BufWriter::new(file));\n\n    let options = FileOptions::default()\n        .compression_method(CompressionMethod::Deflated)\n        .unix_permissions(0o755);\n\n    let data_key: [u8; 32] = rand::thread_rng().gen();\n    let data_iv: [u8; 16] = rand::thread_rng().gen();\n\n    // Simple example of using password to generate key and iv with hashing\n    let key = password.as_bytes();\n\n    let path = Path::new(src_path);\n    let name = path.file_name().ok_or_else(|| zip::result::ZipError::FileNotFound)?;\n\n    writer.start_file(name.to_str().unwrap(), options)?;\n\n    let mut f = File::open(src_path)?;\n    let mut buffer = vec![];\n    f.read_to_end(&mut buffer)?;\n\n    let encrypted_data = encrypt(&buffer, key, &data_iv);\n    writer.write_all(&encrypted_data)?;\n    \n    writer.finish()?;\n    Ok(())\n}\n\n/// Read a password protected ZIP file\npub fn read_from_zip(zip_path: &str, password: &str) -> zip::result::ZipResult<Vec<u8>> {\n    let file = File::open(zip_path)?;\n    let mut archive = ZipArchive::new(BufReader::new(file))?;\n\n    let mut file = archive.by_index(0)?;\n\n    let key = password.as_bytes();\n    let data_iv: [u8; 16] = [0; 16]; // Replace with actual method to retrieve IV\n\n    let mut buffer = vec![];\n    file.read_to_end(&mut buffer)?;\n\n    let decrypted_data = decrypt(&buffer, key, &data_iv);\n    Ok(decrypted_data)\n}\n```\n\n**Note**: This illustrative code does not securely manage encryption keys, IVs, or error handling. For a production scenario, consider a thorough cryptographic review and proper key management strategies.",
    "label": 1
  },
  {
    "codes": "function (ecModel, api) {\n\n        ecModel.eachComponent('dataZoom', function (dataZoomModel) {\n            // We calculate window and reset axis here but not in model\n            // init stage and not after action dispatch handler, because\n            // reset should be called after seriesData.restoreData.\n            dataZoomModel.eachTargetAxis(function (dimNames, axisIndex, dataZoomModel) {\n                dataZoomModel.getAxisProxy(dimNames.name, axisIndex).reset(dataZoomModel, api);\n            });\n\n            // Caution: data zoom filtering is order sensitive when using\n            // percent range and no min/max/scale set on axis.\n            // For example, we have dataZoom definition:\n            // [\n            //      {xAxisIndex: 0, start: 30, end: 70},\n            //      {yAxisIndex: 0, start: 20, end: 80}\n            // ]\n            // In this case, [20, 80] of y-dataZoom should be based on data\n            // that have filtered by x-dataZoom using range of [30, 70],\n            // but should not be based on full raw data. Thus sliding\n            // x-dataZoom will change both ranges of xAxis and yAxis,\n            // while sliding y-dataZoom will only change the range of yAxis.\n            // So we should filter x-axis after reset x-axis immediately,\n            // and then reset y-axis and filter y-axis.\n            dataZoomModel.eachTargetAxis(function (dimNames, axisIndex, dataZoomModel) {\n                dataZoomModel.getAxisProxy(dimNames.name, axisIndex).filterData(dataZoomModel, api);\n            });\n        });\n\n        ecModel.eachComponent('dataZoom', function (dataZoomModel) {\n            // Fullfill all of the range props so that user\n            // is able to get them from chart.getOption().\n            var axisProxy = dataZoomModel.findRepresentativeAxisProxy();\n            var percentRange = axisProxy.getDataPercentWindow();\n            var valueRange = axisProxy.getDataValueWindow();\n\n            dataZoomModel.setRawRange({\n                start: percentRange[0],\n                end: percentRange[1],\n                startValue: valueRange[0],\n                endValue: valueRange[1]\n            }, true);\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```c\n#include <stdio.h>\n#include <dirent.h>\n#include <sys/stat.h>\n#include <string.h>\n\n// Function to list files recursively\nvoid listFilesRecursively(const char *basePath) {\n    char path[1000];\n    struct dirent *dp;\n    struct stat statbuf;\n\n    DIR *dir = opendir(basePath);\n    if (!dir) return;\n\n    while ((dp = readdir(dir)) != NULL) {\n        if (strcmp(dp->d_name, \".\") == 0 || strcmp(dp->d_name, \"..\") == 0)\n            continue;\n\n        // Construct new path\n        snprintf(path, sizeof(path), \"%s/%s\", basePath, dp->d_name);\n\n        // Get file status\n        if (stat(path, &statbuf) == -1)\n            continue;\n\n        // Check if it's a directory\n        if (S_ISDIR(statbuf.st_mode)) {\n            printf(\"Directory: %s\\n\", path);\n            listFilesRecursively(path); // Recursively list files\n        } else {\n            printf(\"File: %s\\n\", path); // Print file path\n        }\n    }\n\n    closedir(dir);\n}\n\nint main() {\n    char basePath[1000] = \".\"; // Start from current directory\n    listFilesRecursively(basePath);\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "def classes(self, set_uri_or_id=None, nestedhierarchy=False):\n        \"\"\"Returns a dictionary of classes for the specified (sub)set (if None, default, the main set is selected)\"\"\"\n        if set_uri_or_id and set_uri_or_id.startswith(('http://','https://')):\n            set_uri = set_uri_or_id\n        else:\n            set_uri = self.get_set_uri(set_uri_or_id)\n\n        assert set_uri is not None\n\n        classes= {}\n        uri2idmap = {}\n        for row in self.graph.query(\"SELECT ?classuri ?classid ?classlabel ?parentclass ?seqnr  WHERE { ?classuri rdf:type skos:Concept ; skos:notation ?classid. <\" + str(set_uri) + \"> skos:member ?classuri . OPTIONAL { ?classuri skos:prefLabel ?classlabel } OPTIONAL { ?classuri skos:narrower ?parentclass } OPTIONAL { ?classuri fsd:sequenceNumber ?seqnr } }\"):\n            classinfo = {'uri': str(row.classuri), 'id': str(row.classid),'label': str(row.classlabel) if row.classlabel else \"\" }\n            if nestedhierarchy:\n                uri2idmap[str(row.classuri)] = str(row.classid)\n            if row.parentclass:\n                classinfo['parentclass'] =  str(row.parentclass) #uri\n            if row.seqnr:\n                classinfo['seqnr'] =  int(row.seqnr)\n            classes[str(row.classid)] = classinfo\n\n        if nestedhierarchy:\n            #build hierarchy\n            removekeys = []\n            for classid, classinfo in classes.items():\n                if 'parentclass' in classinfo:\n                    removekeys.append(classid)\n                    parentclassid = uri2idmap[classinfo['parentclass']]\n                    if 'subclasses' not in classes[parentclassid]:\n                        classes[parentclassid]['subclasses'] = {}\n                    classes[parentclassid]['subclasses'][classid] = classinfo\n            for key in removekeys:\n                del classes[key]\n        return classes",
    "label": 0
  },
  {
    "codes": "```java\nimport ddf.minim.*;\nimport ddf.minim.analysis.*;\n\nMinim minim;\nAudioInput in;\nFFT fft;\n\nvoid setup() {\n  size(800, 400);\n  minim = new Minim(this);\n  in = minim.getLineIn(Minim.STEREO, 512);\n  fft = new FFT(in.bufferSize(), in.sampleRate());\n  fft.logAverages(22, 3); // Group frequencies into 22 bands, 3 octaves each\n}\n\nvoid draw() {\n  background(0);\n  fft.forward(in.mix);\n  \n  stroke(255);\n  noFill();\n  \n  // Draw the frequency spectrum as a series of vertical lines\n  for (int i = 0; i < fft.avgSize(); i++) {\n    float x = map(i, 0, fft.avgSize(), 0, width);\n    float h = map(fft.getAvg(i), 0, 50, 0, height);\n    line(x, height, x, height - h);\n  }\n}\n\nvoid stop() {\n  in.close();\n  minim.stop();\n  super.stop();\n}\n```",
    "label": 1
  },
  {
    "codes": "def sismember(self, name, value):\n        \"\"\"Emulate sismember.\"\"\"\n        redis_set = self._get_set(name, 'SISMEMBER')\n        if not redis_set:\n            return 0\n\n        result = self._encode(value) in redis_set\n        return 1 if result else 0",
    "label": 0
  },
  {
    "codes": "function error(err, req, res, next) {\n  // log it\n  if (!test) console.error(err.stack);\n\n  // respond with 500 \"Internal Server Error\".\n  res.status(500);\n  res.send('Internal Server Error');\n}",
    "label": 0
  },
  {
    "codes": "```python\nfrom locust import HttpUser, task, between\n\nclass ApiUser(HttpUser):\n    wait_time = between(1, 5)  # Random wait time between tasks\n\n    @task\n    def get_users(self):\n        # GET request to fetch users\n        response = self.client.get(\"/api/users\")\n        if response.status_code == 200:\n            print(\"GET /api/users - Success\")\n        else:\n            print(f\"GET /api/users - Failed with status code: {response.status_code}\")\n\n    @task(3)  # Higher weight, more frequent execution\n    def create_user(self):\n        # POST request to create a new user\n        payload = {\"name\": \"John Doe\", \"email\": \"john.doe@example.com\"}\n        response = self.client.post(\"/api/users\", json=payload)\n        if response.status_code == 201:\n            print(\"POST /api/users - Success\")\n        else:\n            print(f\"POST /api/users - Failed with status code: {response.status_code}\")\n\n    @task(2)\n    def update_user(self):\n        # PUT request to update a user\n        payload = {\"name\": \"Jane Doe\", \"email\": \"jane.doe@example.com\"}\n        response = self.client.put(\"/api/users/1\", json=payload)\n        if response.status_code == 200:\n            print(\"PUT /api/users/1 - Success\")\n        else:\n            print(f\"PUT /api/users/1 - Failed with status code: {response.status_code}\")\n\n    @task(1)\n    def delete_user(self):\n        # DELETE request to delete a user\n        response = self.client.delete(\"/api/users/1\")\n        if response.status_code == 204:\n            print(\"DELETE /api/users/1 - Success\")\n        else:\n            print(f\"DELETE /api/users/1 - Failed with status code: {response.status_code}\")\n```",
    "label": 1
  },
  {
    "codes": "func (s *Crawler) SetSchedule(v *Schedule) *Crawler {\n\ts.Schedule = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```julia\n# Import necessary packages\nusing VoronoiDelaunay\nusing Plots\n\n# Generate random points\nn = 50  # Number of points\npoints = rand(n, 2)  # Random 2D points\n\n# Compute Voronoi diagram\ntess = VoronoiTessellation(points)\n\n# Plot the Voronoi diagram\nplot(tess, points, legend=false, aspect_ratio=:equal, title=\"Voronoi Diagram\")\n```",
    "label": 1
  },
  {
    "codes": "def legend(self, title=None, scale='color', text_color=None):\n        \"\"\"Convience method for adding a legend to the figure.\n\n        Important: This defaults to the color scale that is generated with\n        Line, Area, Stacked Line, etc charts. For bar charts, the scale ref is\n        usually 'y'.\n\n        Parameters\n        ----------\n        title: string, default None\n            Legend Title\n        scale: string, default 'color'\n            Scale reference for legend\n        text_color: str, default None\n            Title and label color\n        \"\"\"\n\n        self.legends.append(Legend(title=title, fill=scale, offset=0,\n                                   properties=LegendProperties()))\n        if text_color:\n            color_props = PropertySet(fill=ValueRef(value=text_color))\n            self.legends[0].properties.labels = color_props\n            self.legends[0].properties.title = color_props\n        return self",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"net\"\n\t\"time\"\n)\n\n// Simulate packet loss with a given loss rate\nfunc shouldDropPacket(lossRate float64) bool {\n\treturn rand.Float64() < lossRate\n}\n\nfunc main() {\n\trand.Seed(time.Now().UnixNano()) // Initialize random seed based on current time\n\n\tlossRate := 0.2 // Define the packet loss rate (e.g., 20%)\n\n\tlistener, err := net.Listen(\"tcp\", \":8080\") // Listen for incoming TCP connections\n\tif err != nil {\n\t\tfmt.Println(\"Error setting up listener:\", err)\n\t\treturn\n\t}\n\tdefer listener.Close() // Ensure listener is closed when function exits\n\n\tfmt.Println(\"Listening on :8080 and simulating packet loss...\")\n\n\tfor {\n\t\tconn, err := listener.Accept() // Accept a new connection\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error accepting connection:\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\tgo handleConnection(conn, lossRate) // Handle connection in a separate goroutine\n\t}\n}\n\nfunc handleConnection(conn net.Conn, lossRate float64) {\n\tdefer conn.Close() // Ensure connection is closed when function exits\n\n\tbuffer := make([]byte, 1024) // Define buffer to store incoming data\n\n\tfor {\n\t\tn, err := conn.Read(buffer) // Read data from connection into buffer\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error reading from connection:\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Simulate packet loss\n\t\tif shouldDropPacket(lossRate) {\n\t\t\tfmt.Println(\"Simulating packet loss, packet dropped\")\n\t\t\tcontinue // Drop the packet\n\t\t}\n\n\t\tfmt.Printf(\"Received data: %s\", string(buffer[:n])) // Print received data\n\n\t\t_, err = conn.Write(buffer[:n]) // Echo data back to the client\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error writing to connection:\", err)\n\t\t\treturn\n\t\t}\n\t}\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```java\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.security.KeyStore;\nimport java.security.cert.Certificate;\nimport java.security.cert.CertificateException;\nimport java.security.cert.CertificateFactory;\nimport java.security.cert.X509Certificate;\nimport java.util.Enumeration;\n\npublic class KeyStoreWrapper {\n\n    private KeyStore keyStore;\n    private final String keyStoreType;\n    private final char[] password;\n\n    // Constructor to initialize KeyStore\n    public KeyStoreWrapper(String keyStoreType, char[] password) throws Exception {\n        this.keyStoreType = keyStoreType;\n        this.password = password;\n        this.keyStore = KeyStore.getInstance(keyStoreType);\n        this.keyStore.load(null, password);\n    }\n\n    // Load keystore from file\n    public void loadFromFile(String filePath) throws Exception {\n        try (FileInputStream fis = new FileInputStream(filePath)) {\n            keyStore.load(fis, password);\n        }\n    }\n\n    // Save keystore to file\n    public void saveToFile(String filePath) throws Exception {\n        try (FileOutputStream fos = new FileOutputStream(filePath)) {\n            keyStore.store(fos, password);\n        }\n    }\n\n    // Add certificate to keystore\n    public void addCertificate(String alias, String certFilePath) throws IOException, CertificateException {\n        CertificateFactory certFactory = CertificateFactory.getInstance(\"X.509\");\n        try (FileInputStream fis = new FileInputStream(certFilePath)) {\n            X509Certificate certificate = (X509Certificate) certFactory.generateCertificate(fis);\n            keyStore.setCertificateEntry(alias, certificate);\n        }\n    }\n\n    // Get certificate from keystore\n    public X509Certificate getCertificate(String alias) throws Exception {\n        return (X509Certificate) keyStore.getCertificate(alias);\n    }\n\n    // List all certificates in keystore\n    public Enumeration<String> listCertificates() throws Exception {\n        return keyStore.aliases();\n    }\n\n    // Check if a certificate exists\n    public boolean containsCertificate(String alias) throws Exception {\n        return keyStore.isCertificateEntry(alias);\n    }\n\n    // Remove a certificate from keystore\n    public void removeCertificate(String alias) throws Exception {\n        if (keyStore.isCertificateEntry(alias)) {\n            keyStore.deleteEntry(alias);\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "func (s *GetConnectionOutput) SetConnection(v *Connection) *GetConnectionOutput {\n\ts.Connection = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ResourceGroupInner> patchAsync(String resourceGroupName, ResourceGroupInner parameters) {\n        return patchWithServiceResponseAsync(resourceGroupName, parameters).map(new Func1<ServiceResponse<ResourceGroupInner>, ResourceGroupInner>() {\n            @Override\n            public ResourceGroupInner call(ServiceResponse<ResourceGroupInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```hcl\nprovider \"aws\" {\n  region = \"us-west-2\"\n}\n\n# Create an IAM role for SageMaker\nresource \"aws_iam_role\" \"sagemaker_role\" {\n  name = \"sagemaker-execution-role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = \"sts:AssumeRole\"\n        Effect = \"Allow\"\n        Principal = {\n          Service = \"sagemaker.amazonaws.com\"\n        }\n      },\n    ]\n  })\n}\n\n# Attach policies to the IAM role\nresource \"aws_iam_role_policy_attachment\" \"sagemaker_policy\" {\n  role       = aws_iam_role.sagemaker_role.name\n  policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonSageMakerFullAccess\"\n}\n\n# Create S3 bucket for model artifacts\nresource \"aws_s3_bucket\" \"model_bucket\" {\n  bucket = \"my-model-artifacts-bucket\"\n  acl    = \"private\"\n\n  versioning {\n    enabled = true\n  }\n}\n\n# Create a SageMaker model\nresource \"aws_sagemaker_model\" \"ml_model\" {\n  name                 = \"my-ml-model\"\n  execution_role_arn  = aws_iam_role.sagemaker_role.arn\n  primary_container {\n    image         = \"your-docker-image-uri\" # Example: \"123456789012.dkr.ecr.us-west-2.amazonaws.com/my-image:latest\"\n    model_data_url = \"s3://${aws_s3_bucket.model_bucket.bucket}/model.tar.gz\" # URL pointing to the model artifacts\n  }\n}\n\n# Create a SageMaker endpoint configuration\nresource \"aws_sagemaker_endpoint_configuration\" \"endpoint_config\" {\n  name      = \"my-endpoint-config\"\n  production_variants {\n    variant_name = \"AllTraffic\"\n    model_name  = aws_sagemaker_model.ml_model.name\n    initial_instance_count = 1\n    instance_type = \"ml.m5.large\"\n  }\n}\n\n# Create a SageMaker endpoint\nresource \"aws_sagemaker_endpoint\" \"ml_endpoint\" {\n  name                   = \"my-ml-endpoint\"\n  endpoint_config_name   = aws_sagemaker_endpoint_configuration.endpoint_config.name\n}\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing Microsoft.AspNetCore.Http;\nusing Microsoft.AspNetCore.Mvc;\nusing Microsoft.AspNetCore.Mvc.Filters;\nusing System;\nusing System.Linq;\nusing System.Security.Claims;\n\n[AttributeUsage(AttributeTargets.Class | AttributeTargets.Method)]\npublic class ValidateJwtClaimsAttribute : Attribute, IAuthorizationFilter\n{\n    private readonly string _claimType;\n    private readonly string _claimValue;\n\n    public ValidateJwtClaimsAttribute(string claimType, string claimValue)\n    {\n        _claimType = claimType;\n        _claimValue = claimValue;\n    }\n\n    public void OnAuthorization(AuthorizationFilterContext context)\n    {\n        // Retrieve the claims from the HTTP context\n        var claimsIdentity = context.HttpContext.User.Identity as ClaimsIdentity;\n        \n        // Check if the identity is authenticated and claims exist\n        if (claimsIdentity == null || !claimsIdentity.IsAuthenticated)\n        {\n            context.Result = new UnauthorizedResult();\n            return;\n        }\n\n        // Look for the specified claim type and value\n        var claim = claimsIdentity.Claims\n            .FirstOrDefault(c => c.Type == _claimType && c.Value == _claimValue);\n\n        // If the claim is missing or does not have the required value, deny access\n        if (claim == null)\n        {\n            context.Result = new ForbidResult();\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.IO;\nusing System.ServiceProcess;\nusing System.Management;\n\npublic class UsbMonitorService : ServiceBase\n{\n    private ManagementEventWatcher insertWatcher;\n    private ManagementEventWatcher removeWatcher;\n\n    public UsbMonitorService()\n    {\n        this.ServiceName = \"UsbMonitorService\";\n    }\n\n    protected override void OnStart(string[] args)\n    {\n        WqlEventQuery insertQuery = new WqlEventQuery(\"SELECT * FROM __InstanceCreationEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_USBControllerDevice'\");\n        WqlEventQuery removeQuery = new WqlEventQuery(\"SELECT * FROM __InstanceDeletionEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_USBControllerDevice'\");\n\n        insertWatcher = new ManagementEventWatcher(insertQuery);\n        removeWatcher = new ManagementEventWatcher(removeQuery);\n\n        insertWatcher.EventArrived += new EventArrivedEventHandler(OnUsbInserted);\n        removeWatcher.EventArrived += new EventArrivedEventHandler(OnUsbRemoved);\n\n        insertWatcher.Start();\n        removeWatcher.Start();\n    }\n\n    protected override void OnStop()\n    {\n        insertWatcher.Stop();\n        removeWatcher.Stop();\n    }\n\n    private void OnUsbInserted(object sender, EventArrivedEventArgs e)\n    {\n        // Log USB device insertion\n        File.AppendAllText(@\"C:\\UsbLog.txt\", DateTime.Now + \" - USB device inserted.\\n\");\n    }\n\n    private void OnUsbRemoved(object sender, EventArrivedEventArgs e)\n    {\n        // Log USB device removal\n        File.AppendAllText(@\"C:\\UsbLog.txt\", DateTime.Now + \" - USB device removed.\\n\");\n    }\n\n    public static void Main()\n    {\n        ServiceBase.Run(new UsbMonitorService());\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom transformers import pipeline\n\n# Initialize the summarization pipeline using a pre-trained model\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n# Example text to summarize\ntext = \"\"\"\nThe Hugging Face Transformers library provides an easy-to-use interface for various NLP tasks, \nincluding text summarization. It leverages pre-trained models like BART and T5 to generate concise \nsummaries of long documents. This is particularly useful for quickly understanding the main points \nof lengthy articles or reports.\n\"\"\"\n\n# Generate a summary of the text\nsummary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n\n# Print the generated summary\nprint(summary[0]['summary_text'])\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define variables\nBACKUP_DIR=\"/path/to/backup\"  # Directory to backup\nS3_BUCKET=\"s3://your-bucket-name\"  # S3 bucket name\nDATE=$(date +%Y%m%d_%H%M%S)  # Timestamp for backup file\nLOG_FILE=\"/var/log/backup_$DATE.log\"  # Log file\n\n# Check if backup directory exists\nif [ ! -d \"$BACKUP_DIR\" ]; then\n    echo \"Backup directory does not exist. Exiting.\" >> \"$LOG_FILE\"\n    exit 1\nfi\n\n# Create a compressed archive of the backup directory\ntar -czf \"/tmp/backup_$DATE.tar.gz\" \"$BACKUP_DIR\" >> \"$LOG_FILE\" 2>&1\n\n# Check if tar command was successful\nif [ $? -ne 0 ]; then\n    echo \"Failed to create backup archive. Exiting.\" >> \"$LOG_FILE\"\n    exit 1\nfi\n\n# Upload the backup archive to S3\naws s3 cp \"/tmp/backup_$DATE.tar.gz\" \"$S3_BUCKET/\" >> \"$LOG_FILE\" 2>&1\n\n# Check if upload was successful\nif [ $? -ne 0 ]; then\n    echo \"Failed to upload backup to S3. Exiting.\" >> \"$LOG_FILE\"\n    exit 1\nfi\n\n# Clean up temporary backup file\nrm -f \"/tmp/backup_$DATE.tar.gz\" >> \"$LOG_FILE\" 2>&1\n\n# Log success message\necho \"Backup completed successfully on $DATE.\" >> \"$LOG_FILE\"\n```",
    "label": 1
  },
  {
    "codes": "function getCurrencySettings(locale, localeData) {\n  const currencyInfo = localeData.main(`numbers/currencies`);\n  let currentCurrency = '';\n\n  // find the currency currently used in this country\n  const currencies =\n    localeData.get(`supplemental/currencyData/region/${localeData.attributes.territory}`) ||\n    localeData.get(`supplemental/currencyData/region/${localeData.attributes.language.toUpperCase()}`);\n\n  if (currencies) {\n    currencies.some(currency => {\n      const keys = Object.keys(currency);\n      return keys.some(key => {\n        if (currency[key]._from && !currency[key]._to) {\n          return currentCurrency = key;\n        }\n      });\n    });\n\n    if (!currentCurrency) {\n      throw new Error(`Unable to find currency for locale \"${locale}\"`);\n    }\n  }\n\n  let currencySettings = [undefined, undefined];\n\n  if (currentCurrency) {\n    currencySettings = [currencyInfo[currentCurrency].symbol, currencyInfo[currentCurrency].displayName];\n  }\n\n  return currencySettings;\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport random\n\n# Define elemental types and their weaknesses\nelemental_weaknesses = {\n    \"Fire\": \"Water\",\n    \"Water\": \"Electric\",\n    \"Electric\": \"Earth\",\n    \"Earth\": \"Fire\"\n}\n\nclass Character:\n    def __init__(self, name, element, health):\n        self.name = name\n        self.element = element\n        self.health = health\n        self.attack_power = 10\n\n    def attack(self, other):\n        # Calculate base damage\n        damage = self.attack_power\n        # Check for elemental weakness\n        if elemental_weaknesses[self.element] == other.element:\n            damage *= 2  # Double damage for weakness\n\n        print(f\"{self.name} attacks {other.name} ({other.element}) with {self.element}!\")\n        print(f\"It deals {damage} damage.\")\n        other.take_damage(damage)\n\n    def take_damage(self, damage):\n        self.health -= damage\n        print(f\"{self.name} has {self.health} health left!\")\n\ndef combat_round(player1, player2):\n    # Randomly determine who attacks first\n    if random.choice([True, False]):\n        player1.attack(player2)\n        if player2.health > 0:\n            player2.attack(player1)\n    else:\n        player2.attack(player1)\n        if player1.health > 0:\n            player1.attack(player2)\n\ndef main():\n    # Initialize characters\n    player1 = Character(name=\"Arthur\", element=\"Fire\", health=50)\n    player2 = Character(name=\"Luna\", element=\"Water\", health=50)\n\n    # Combat loop\n    round_number = 1\n    while player1.health > 0 and player2.health > 0:\n        print(f\"\\n--- Round {round_number} ---\")\n        combat_round(player1, player2)\n        round_number += 1\n\n    # Determine winner\n    if player1.health > 0:\n        print(\"\\nArthur is victorious!\")\n    else:\n        print(\"\\nLuna is victorious!\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nThis combat system includes two characters with elemental types that possess specific weaknesses. The attack method considers these weaknesses to apply double damage when appropriate.",
    "label": 1
  },
  {
    "codes": "public DatabaseInner beginResume(String resourceGroupName, String serverName, String databaseName) {\n        return beginResumeWithServiceResponseAsync(resourceGroupName, serverName, databaseName).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<List<AzureResourceSkuInner>>> listSkusByResourceWithServiceResponseAsync(String resourceGroupName, String clusterName) {\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (clusterName == null) {\n            throw new IllegalArgumentException(\"Parameter clusterName is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        return service.listSkusByResource(resourceGroupName, clusterName, this.client.subscriptionId(), this.client.apiVersion(), this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<List<AzureResourceSkuInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<List<AzureResourceSkuInner>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<PageImpl<AzureResourceSkuInner>> result = listSkusByResourceDelegate(response);\n                        List<AzureResourceSkuInner> items = null;\n                        if (result.body() != null) {\n                            items = result.body().items();\n                        }\n                        ServiceResponse<List<AzureResourceSkuInner>> clientResponse = new ServiceResponse<List<AzureResourceSkuInner>>(items, result.response());\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *BatchCreatePartitionOutput) SetErrors(v []*PartitionError) *BatchCreatePartitionOutput {\n\ts.Errors = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Source and destination directories\nSOURCE_DIR=\"/path/to/source\"\nDEST_DIR=\"/path/to/destination\"\n\n# Rsync options:\n# -a: archive mode (preserves permissions, times, symbolic links)\n# -v: verbose output\n# -u: skip files that are newer on destination\n# -n: dry run (remove this option for actual sync)\n# -h: human-readable output\n# -z: compress file data during transfer\nRSYNC_OPTIONS=\"-avuhnzh\"\n\n# Perform dry run of rsync operation\nrsync $RSYNC_OPTIONS \"$SOURCE_DIR/\" \"$DEST_DIR/\"\n\n# Check exit status of rsync\nif [ $? -eq 0 ]; then\n  echo \"Dry run successful. To perform actual sync, remove the '-n' option.\"\nelse\n  echo \"Dry run failed. Please check paths and permissions.\"\nfi\n```\n\n",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<Page<JobResponseInner>>> listJobsSinglePageAsync(final String resourceGroupName, final String resourceName) {\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (resourceName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceName is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        return service.listJobs(this.client.subscriptionId(), resourceGroupName, resourceName, this.client.apiVersion(), this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<Page<JobResponseInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<JobResponseInner>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<PageImpl<JobResponseInner>> result = listJobsDelegate(response);\n                        return Observable.just(new ServiceResponse<Page<JobResponseInner>>(result.body(), result.response()));\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *GetDatabasesOutput) SetDatabaseList(v []*Database) *GetDatabasesOutput {\n\ts.DatabaseList = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function parse_Window1(blob) {\n\tvar xWn = blob.read_shift(2), yWn = blob.read_shift(2), dxWn = blob.read_shift(2), dyWn = blob.read_shift(2);\n\tvar flags = blob.read_shift(2), iTabCur = blob.read_shift(2), iTabFirst = blob.read_shift(2);\n\tvar ctabSel = blob.read_shift(2), wTabRatio = blob.read_shift(2);\n\treturn { Pos: [xWn, yWn], Dim: [dxWn, dyWn], Flags: flags, CurTab: iTabCur,\n\t\tFirstTab: iTabFirst, Selected: ctabSel, TabRatio: wTabRatio };\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<IntegrationAccountSchemaInner>>> listByIntegrationAccountsNextWithServiceResponseAsync(final String nextPageLink) {\n        return listByIntegrationAccountsNextSinglePageAsync(nextPageLink)\n            .concatMap(new Func1<ServiceResponse<Page<IntegrationAccountSchemaInner>>, Observable<ServiceResponse<Page<IntegrationAccountSchemaInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<IntegrationAccountSchemaInner>>> call(ServiceResponse<Page<IntegrationAccountSchemaInner>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(listByIntegrationAccountsNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```javascript\n// Utility function to deep clone objects with circular references\nfunction deepClone(obj, hash = new WeakMap()) {\n    // If the object is null or not an object, return it directly\n    if (obj === null || typeof obj !== 'object') {\n        return obj;\n    }\n\n    // If the object is already cloned, return the cloned version\n    if (hash.has(obj)) {\n        return hash.get(obj);\n    }\n\n    // Handle Date objects\n    if (obj instanceof Date) {\n        return new Date(obj);\n    }\n\n    // Handle RegExp objects\n    if (obj instanceof RegExp) {\n        return new RegExp(obj);\n    }\n\n    // Handle Arrays\n    if (Array.isArray(obj)) {\n        const cloneArr = [];\n        hash.set(obj, cloneArr); // Store the cloned array in the hash map\n        obj.forEach((item, index) => {\n            cloneArr[index] = deepClone(item, hash); // Recursively clone each item\n        });\n        return cloneArr;\n    }\n\n    // Handle Objects\n    const cloneObj = {};\n    hash.set(obj, cloneObj); // Store the cloned object in the hash map\n    for (const key in obj) {\n        if (obj.hasOwnProperty(key)) {\n            cloneObj[key] = deepClone(obj[key], hash); // Recursively clone each property\n        }\n    }\n\n    return cloneObj;\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport psutil\nimport time\nimport logging\n\n# Configure logging\nlogging.basicConfig(filename='cpu_usage.log', level=logging.INFO, \n                    format='%(asctime)s - %(message)s')\n\n# Threshold for CPU usage anomaly (in percentage)\nTHRESHOLD = 80\n\ndef monitor_cpu():\n    while True:\n        cpu_usage = psutil.cpu_percent(interval=1)  # Get CPU usage over 1 second\n        if cpu_usage > THRESHOLD:\n            logging.warning(f\"High CPU usage detected: {cpu_usage}%\")\n        time.sleep(5)  # Check every 5 seconds\n\nif __name__ == \"__main__\":\n    monitor_cpu()\n```",
    "label": 1
  },
  {
    "codes": "function plainHTMLBlocks() {\n\n    const plainBlocks = ['code-example', 'code-tabs'];\n\n    // Create matchers for each block\n    const anyBlockMatcher = new RegExp('^' + createOpenMatcher(`(${plainBlocks.join('|')})`));\n\n    const Parser = this.Parser;\n    const blockTokenizers = Parser.prototype.blockTokenizers;\n    const blockMethods = Parser.prototype.blockMethods;\n\n    blockTokenizers.plainHTMLBlocks = tokenizePlainHTMLBlocks;\n    blockMethods.splice(blockMethods.indexOf('html'), 0, 'plainHTMLBlocks');\n\n    function tokenizePlainHTMLBlocks(eat, value, silent) {\n      const openMatch = anyBlockMatcher.exec(value);\n      if (openMatch) {\n        const blockName = openMatch[1];\n        try {\n          const fullMatch = matchRecursiveRegExp(value, createOpenMatcher(blockName), createCloseMatcher(blockName))[0];\n          if (silent || !fullMatch) {\n            // either we are not eating (silent) or the match failed\n            return !!fullMatch;\n          }\n          return eat(fullMatch[0])({\n            type: 'html',\n            value: fullMatch[0]\n          });\n        } catch(e) {\n          this.file.fail('Unmatched plain HTML block tag ' + e.message);\n        }\n      }\n    }\n  }",
    "label": 0
  },
  {
    "codes": "def gettextdelimiter(self, retaintokenisation=False):\n        \"\"\"See :meth:`AbstractElement.gettextdelimiter`\"\"\"\n        for e in self:\n            if isinstance(e, New) or isinstance(e, Current):\n                return e.gettextdelimiter(retaintokenisation)\n        return \"\"",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/rwcarlsen/goexif/exif\"\n)\n\nfunc main() {\n\t// Open the JPEG file\n\tfile, err := os.Open(\"example.jpg\")\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to open file: %v\", err)\n\t}\n\tdefer file.Close()\n\n\t// Decode EXIF metadata\n\texifData, err := exif.Decode(file)\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to decode EXIF data: %v\", err)\n\t}\n\n\t// Iterate over all tags and print them\n\terr = exifData.Walk(&exifPrinter{})\n\tif err != nil {\n\t\tlog.Fatalf(\"error walking through EXIF data: %v\", err)\n\t}\n}\n\n// exifPrinter implements the exif.Walker interface to print EXIF tags\ntype exifPrinter struct{}\n\n// Walk is called for each tag found in the EXIF data\nfunc (p *exifPrinter) Walk(name exif.FieldName, tag *exif.Tag) error {\n\tfmt.Printf(\"%s: %s\\n\", name, tag)\n\treturn nil\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *DeleteArchiveInput) SetArchiveId(v string) *DeleteArchiveInput {\n\ts.ArchiveId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport java.util.concurrent.CompletableFuture;\n\npublic class CompletableFutureChainWithFallbacks {\n\n    public static void main(String[] args) {\n        CompletableFuture<String> futureChain = CompletableFuture.supplyAsync(() -> {\n            // Simulate async task 1\n            return task1();\n        }).thenApply(result -> {\n            // Handle result of task 1 and proceed to task 2\n            return task2(result);\n        }).exceptionally(ex -> {\n            // Provide fallback for task 2 in case of exception\n            return fallbackForTask2(ex);\n        }).thenApply(result -> {\n            // Handle result of task 2 (or its fallback) and proceed to task 3\n            return task3(result);\n        }).exceptionally(ex -> {\n            // Provide fallback for task 3 in case of exception\n            return fallbackForTask3(ex);\n        }).thenAccept(result -> {\n            // Final result processing\n            finalResultProcessing(result);\n        });\n\n        // Wait for the chain to finish\n        futureChain.join();\n    }\n\n    private static String task1() {\n        // Simulate some computation\n        return \"Result from Task 1\";\n    }\n\n    private static String task2(String input) {\n        // Simulate some dependent computation that might fail\n        if (input.equals(\"Fail\")) {\n            throw new RuntimeException(\"Simulated failure in Task 2\");\n        }\n        return \"Result from Task 2\";\n    }\n\n    private static String task3(String input) {\n        // Simulate another dependent computation that might fail\n        if (input.equals(\"Fail\")) {\n            throw new RuntimeException(\"Simulated failure in Task 3\");\n        }\n        return \"Result from Task 3\";\n    }\n\n    private static String fallbackForTask2(Throwable ex) {\n        // Provide a default value or recovery for a failed task 2\n        return \"Fallback Result for Task 2\";\n    }\n\n    private static String fallbackForTask3(Throwable ex) {\n        // Provide a default value or recovery for a failed task 3\n        return \"Fallback Result for Task 3\";\n    }\n\n    private static void finalResultProcessing(String result) {\n        // Handle the final result (or its fallback)\n        System.out.println(\"Final Result: \" + result);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function addonsManager_getCategoryId(aSpec) {\n    var spec = aSpec || { };\n    var category = spec.category;\n\n    if (!category)\n      throw new Error(arguments.callee.name + \": Category not specified.\");\n\n    return category.getNode().id;\n  }",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"crypto/rand\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\t\"math/big\"\n)\n\n// Function to generate a random password\nfunc generatePassword(length int, useUpper bool, useLower bool, useNumbers bool, useSymbols bool) (string, error) {\n\tcharSet := \"\"\n\tif useUpper {\n\t\tcharSet += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\t}\n\tif useLower {\n\t\tcharSet += \"abcdefghijklmnopqrstuvwxyz\"\n\t}\n\tif useNumbers {\n\t\tcharSet += \"0123456789\"\n\t}\n\tif useSymbols {\n\t\tcharSet += \"!@#$%^&*()-_=+[]{}|;:,.<>?/\"\n\t}\n\n\tif len(charSet) == 0 {\n\t\treturn \"\", fmt.Errorf(\"no character set selected\")\n\t}\n\n\tpassword := make([]byte, length)\n\n\tfor i := range password {\n\t\t// Generate a random index for the character set\n\t\tindex, err := rand.Int(rand.Reader, big.NewInt(int64(len(charSet))))\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\t// Add the selected character to the password\n\t\tpassword[i] = charSet[index.Int64()]\n\t}\n\n\treturn string(password), nil\n}\n\nfunc main() {\n\tlength := 12\n\tpassword, err := generatePassword(length, true, true, true, true)\n\tif err != nil {\n\t\tfmt.Println(\"Error generating password:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Generated Password:\", password)\n}\n```",
    "label": 1
  },
  {
    "codes": "```yaml\n# Horizontal Pod Autoscaler (HPA) to scale based on queue depth\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: pytorch-inference-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: pytorch-inference\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Pods\n    pods:\n      metric:\n        name: queue_depth\n      target:\n        type: AverageValue\n        averageValue: 1000 # Adjust based on your queue depth threshold\n\n---\n# Custom Metrics API to expose queue depth\napiVersion: v1\nkind: Service\nmetadata:\n  name: queue-depth-exporter\nspec:\n  selector:\n    app: queue-depth-exporter\n  ports:\n  - port: 8080\n    targetPort: 8080\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: queue-depth-exporter\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: queue-depth-exporter\n  template:\n    metadata:\n      labels:\n        app: queue-depth-exporter\n    spec:\n      containers:\n      - name: queue-depth-exporter\n        image: your-queue-depth-exporter-image\n        ports:\n        - containerPort: 8080\n\n---\n# Deployment for PyTorch inference\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pytorch-inference\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pytorch-inference\n  template:\n    metadata:\n      labels:\n        app: pytorch-inference\n    spec:\n      containers:\n      - name: pytorch-inference\n        image: your-pytorch-inference-image\n        ports:\n        - containerPort: 5000\n```",
    "label": 1
  },
  {
    "codes": "```javascript\n// Import necessary libraries\nconst xss = require('xss');\n\n// Middleware to sanitize user input against XSS attacks\nconst sanitizeInput = (req, res, next) => {\n    // Sanitize request body\n    if (req.body) {\n        for (let key in req.body) {\n            if (typeof req.body[key] === 'string') {\n                req.body[key] = xss(req.body[key]);\n            }\n        }\n    }\n\n    // Sanitize request query parameters\n    if (req.query) {\n        for (let key in req.query) {\n            if (typeof req.query[key] === 'string') {\n                req.query[key] = xss(req.query[key]);\n            }\n        }\n    }\n\n    // Sanitize request params\n    if (req.params) {\n        for (let key in req.params) {\n            if (typeof req.params[key] === 'string') {\n                req.params[key] = xss(req.params[key]);\n            }\n        }\n    }\n\n    // Proceed to the next middleware or route handler\n    next();\n};\n\n// Export the middleware\nmodule.exports = sanitizeInput;\n```",
    "label": 1
  },
  {
    "codes": "```typescript\nimport { Injectable } from '@angular/core';\nimport { HttpClient, HttpParams } from '@angular/common/http';\nimport { Observable } from 'rxjs';\n\n@Injectable({\n  providedIn: 'root'\n})\nexport class PaginatedDataService {\n  private apiUrl = 'https://api.example.com/data'; // Replace with your API endpoint\n\n  constructor(private http: HttpClient) {}\n\n  // Fetch paginated data from the API\n  getPaginatedData(page: number, pageSize: number): Observable<any> {\n    // Set up query parameters for pagination\n    const params = new HttpParams()\n      .set('page', page.toString())\n      .set('pageSize', pageSize.toString());\n\n    // Make the HTTP GET request with the pagination parameters\n    return this.http.get<any>(this.apiUrl, { params });\n  }\n}\n```\n\n```typescript\n// Example usage in a component\nimport { Component, OnInit } from '@angular/core';\nimport { PaginatedDataService } from './paginated-data.service';\n\n@Component({\n  selector: 'app-data-list',\n  templateUrl: './data-list.component.html',\n  styleUrls: ['./data-list.component.css']\n})\nexport class DataListComponent implements OnInit {\n  data: any[] = [];\n  currentPage = 1;\n  pageSize = 10;\n\n  constructor(private paginatedDataService: PaginatedDataService) {}\n\n  ngOnInit(): void {\n    this.loadData();\n  }\n\n  loadData(): void {\n    this.paginatedDataService.getPaginatedData(this.currentPage, this.pageSize)\n      .subscribe(response => {\n        this.data = response.data; // Assuming the API returns data in a 'data' field\n      });\n  }\n\n  onPageChange(newPage: number): void {\n    this.currentPage = newPage;\n    this.loadData();\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *SecurityConfiguration) SetName(v string) *SecurityConfiguration {\n\ts.Name = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *RemoveTagsFromVaultInput) SetAccountId(v string) *RemoveTagsFromVaultInput {\n\ts.AccountId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public Observable<Page<ResourceMetricInner>> listWorkerPoolInstanceMetricsAsync(final String resourceGroupName, final String name, final String workerPoolName, final String instance) {\n        return listWorkerPoolInstanceMetricsWithServiceResponseAsync(resourceGroupName, name, workerPoolName, instance)\n            .map(new Func1<ServiceResponse<Page<ResourceMetricInner>>, Page<ResourceMetricInner>>() {\n                @Override\n                public Page<ResourceMetricInner> call(ServiceResponse<Page<ResourceMetricInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```python\nfrom fastapi import FastAPI, Depends, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom pydantic import BaseModel\nfrom typing import Optional\nimport jwt\nfrom datetime import datetime, timedelta\n\n# Define the FastAPI app\napp = FastAPI()\n\n# Secret key for JWT encoding/decoding\nSECRET_KEY = \"your_secret_key\"\nALGORITHM = \"HS256\"\nACCESS_TOKEN_EXPIRE_MINUTES = 30\n\n# Mock user database\nfake_users_db = {\n    \"user1\": {\n        \"username\": \"user1\",\n        \"password\": \"password1\",\n    }\n}\n\n# Pydantic model for user authentication\nclass User(BaseModel):\n    username: str\n    password: str\n\n# Pydantic model for token response\nclass Token(BaseModel):\n    access_token: str\n    token_type: str\n\n# Pydantic model for prediction input\nclass PredictionInput(BaseModel):\n    feature1: float\n    feature2: float\n\n# Pydantic model for prediction output\nclass PredictionOutput(BaseModel):\n    prediction: float\n\n# OAuth2 scheme for token authentication\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n# Function to create JWT token\ndef create_access_token(data: dict, expires_delta: Optional[timedelta] = None):\n    to_encode = data.copy()\n    if expires_delta:\n        expire = datetime.utcnow() + expires_delta\n    else:\n        expire = datetime.utcnow() + timedelta(minutes=15)\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n    return encoded_jwt\n\n# Function to authenticate user\ndef authenticate_user(username: str, password: str):\n    user = fake_users_db.get(username)\n    if not user or user[\"password\"] != password:\n        return False\n    return user\n\n# Endpoint to get JWT token\n@app.post(\"/token\", response_model=Token)\nasync def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):\n    user = authenticate_user(form_data.username, form_data.password)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect username or password\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n    access_token = create_access_token(\n        data={\"sub\": user[\"username\"]}, expires_delta=access_token_expires\n    )\n    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n\n# Endpoint to make predictions (requires authentication)\n@app.post(\"/predict\", response_model=PredictionOutput)\nasync def make_prediction(\n    input_data: PredictionInput,\n    token: str = Depends(oauth2_scheme)\n):\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username: str = payload.get(\"sub\")\n        if username is None:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Invalid authentication credentials\",\n            )\n    except jwt.PyJWTError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid authentication credentials\",\n        )\n\n    # Mock prediction logic\n    prediction = input_data.feature1 * 0.5 + input_data.feature2 * 0.5\n    return {\"prediction\": prediction}\n```",
    "label": 1
  },
  {
    "codes": "function parse_PtgExtraMem(blob, cce, opts) {\n\tvar count = blob.read_shift((opts.biff == 12) ? 4 : 2);\n\tvar out = [];\n\tfor(var i = 0; i != count; ++i) out.push(((opts.biff == 12) ? parse_UncheckedRfX : parse_Ref8U)(blob, 8));\n\treturn out;\n}",
    "label": 0
  },
  {
    "codes": "```powershell\n# Module Manifest\n@{\n    ModuleVersion = '1.0'\n    GUID = '823b8724-91ab-4c78-87be-c348104b3c69'\n    Author = 'Your Name'\n    Description = 'Manage Windows service dependencies.'\n    FunctionsToExport = @('Get-ServiceDependencies', 'Add-ServiceDependency', 'Remove-ServiceDependency')\n}\n\n# Function to get service dependencies\nfunction Get-ServiceDependencies {\n    param (\n        [string]$ServiceName\n    )\n    # Retrieve and display dependencies\n    Get-WmiObject -Class Win32_Service -Filter \"Name='$ServiceName'\" | ForEach-Object {\n        [PSCustomObject]@{\n            ServiceName = $_.Name\n            Dependencies = $_.Dependencies\n        }\n    }\n}\n\n# Function to add a service dependency\nfunction Add-ServiceDependency {\n    param (\n        [string]$ServiceName,\n        [string]$Dependency\n    )\n    # Retrieve the service\n    $service = Get-WmiObject -Class Win32_Service -Filter \"Name='$ServiceName'\"\n    # Check existing dependencies and add new ones\n    if ($service) {\n        $dependencies = $service.Dependencies\n        if (-not $dependencies) {\n            $dependencies = @()\n        }\n        if ($dependencies -notcontains $Dependency) {\n            $dependencies += $Dependency\n            # Update dependencies\n            $service.Change($null, $null, $null, $null, $null, [string[]]$dependencies, $null, $null, $null)\n        }\n    }\n}\n\n# Function to remove a service dependency\nfunction Remove-ServiceDependency {\n    param (\n        [string]$ServiceName,\n        [string]$Dependency\n    )\n    # Retrieve the service\n    $service = Get-WmiObject -Class Win32_Service -Filter \"Name='$ServiceName'\"\n    if ($service) {\n        $dependencies = $service.Dependencies\n        # Remove the specified dependency\n        if ($dependencies -contains $Dependency) {\n            $dependencies = $dependencies | Where-Object { $_ -ne $Dependency }\n            # Update dependencies\n            $service.Change($null, $null, $null, $null, $null, [string[]]$dependencies, $null, $null, $null)\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (c *CloudTrail) CreateTrailWithContext(ctx aws.Context, input *CreateTrailInput, opts ...request.Option) (*CreateTrailOutput, error) {\n\treq, out := c.CreateTrailRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "def get_pipeline_steps(pipeline, steps_group):\n    \"\"\"Get the steps attribute of module pipeline.\n\n    If there is no steps sequence on the pipeline, return None. Guess you\n    could theoretically want to run a pipeline with nothing in it.\n    \"\"\"\n    logger.debug(\"starting\")\n    assert pipeline\n    assert steps_group\n\n    logger.debug(f\"retrieving {steps_group} steps from pipeline\")\n    if steps_group in pipeline:\n        steps = pipeline[steps_group]\n\n        if steps is None:\n            logger.warn(\n                f\"{steps_group}: sequence has no elements. So it won't do \"\n                \"anything.\")\n            logger.debug(\"done\")\n            return None\n\n        steps_count = len(steps)\n\n        logger.debug(f\"{steps_count} steps found under {steps_group} in \"\n                     \"pipeline definition.\")\n\n        logger.debug(\"done\")\n        return steps\n    else:\n        logger.debug(\n            f\"pipeline doesn't have a {steps_group} collection. Add a \"\n            f\"{steps_group}: sequence to the yaml if you want {steps_group} \"\n            \"actually to do something.\")\n        logger.debug(\"done\")\n        return None",
    "label": 0
  },
  {
    "codes": "```ruby\n# lib/ssh_config_generator.rb\nrequire 'yaml'\n\nmodule SshConfigGenerator\n  class Generator\n    def initialize(template_path)\n      @template = YAML.load_file(template_path)\n    end\n\n    def generate(output_path)\n      File.open(output_path, 'w') do |file|\n        @template.each do |host, config|\n          file.puts \"Host #{host}\"\n          config.each do |key, value|\n            file.puts \"  #{key} #{value}\"\n          end\n          file.puts\n        end\n      end\n    end\n  end\nend\n\n# Example usage:\n# generator = SshConfigGenerator::Generator.new('template.yml')\n# generator.generate('~/.ssh/config')\n```\n\n```yaml\n# template.yml\nhost1:\n  HostName: 192.168.1.1\n  User: user1\n  IdentityFile: ~/.ssh/id_rsa\n\nhost2:\n  HostName: 192.168.1.2\n  User: user2\n  IdentityFile: ~/.ssh/id_rsa2\n```\n\n```ruby\n# ssh_config_generator.gemspec\nGem::Specification.new do |s|\n  s.name        = 'ssh_config_generator'\n  s.version     = '0.1.0'\n  s.summary     = 'Generate SSH config files from YAML templates'\n  s.description = 'A simple gem to generate SSH config files from YAML templates'\n  s.authors     = ['Your Name']\n  s.email       = 'your.email@example.com'\n  s.files       = ['lib/ssh_config_generator.rb']\n  s.homepage    = 'https://github.com/yourusername/ssh_config_generator'\n  s.license     = 'MIT'\nend\n```\n\n```ruby\n# Rakefile\nrequire 'bundler/gem_tasks'\n```\n\n```ruby\n# Gemfile\nsource 'https://rubygems.org'\n\n# Specify your gem's dependencies in ssh_config_generator.gemspec\ngemspec\n```\n\n```ruby\n# test/test_ssh_config_generator.rb\nrequire 'minitest/autorun'\nrequire 'ssh_config_generator'\n\nclass TestSshConfigGenerator < Minitest::Test\n  def test_generate\n    generator = SshConfigGenerator::Generator.new('test/template.yml')\n    generator.generate('test/output_config')\n\n    expected_output = <<~CONFIG\n      Host host1\n        HostName 192.168.1.1\n        User user1\n        IdentityFile ~/.ssh/id_rsa\n\n      Host host2\n        HostName 192.168.1.2\n        User user2\n        IdentityFile ~/.ssh/id_rsa2\n    CONFIG\n\n    assert_equal expected_output, File.read('test/output_config')\n  end\nend\n```\n\n```yaml\n# test/template.yml\nhost1:\n  HostName: 192.168.1.1\n  User: user1\n  IdentityFile: ~/.ssh/id_rsa\n\nhost2:\n  HostName: 192.168.1.2\n  User: user2\n  IdentityFile: ~/.ssh/id_rsa2\n```\n\n```ruby\n# .gitignore\n*.gem\n*.rbc\n/.config\n/coverage/\n/InstalledFiles\n/pkg/\n/spec/reports/\n/test/tmp/\n/test/output_config\n```\n\n```ruby\n# README.md\n# SSH Config Generator\n\nA simple Ruby gem to generate SSH config files from YAML templates.\n\n## Installation\n\nAdd this line to your application's Gemfile:\n\n```ruby\ngem 'ssh_config_generator'\n```\n\nAnd then execute:\n\n```bash\n$ bundle install\n```\n\nOr install it yourself as:\n\n```bash\n$ gem install ssh_config_generator\n```\n\n## Usage\n\n```ruby\nrequire 'ssh_config_generator'\n\ngenerator = SshConfigGenerator::Generator.new('template.yml')\ngenerator.generate('~/.ssh/config')\n```\n\n## Contributing\n\nBug reports and pull requests are welcome on GitHub at https://github.com/yourusername/ssh_config_generator.\n\n## License\n\nThe gem is available as open source under the terms of the [MIT License](https://opensource.org/licenses/MIT).\n```",
    "label": 1
  },
  {
    "codes": "function (target, e) {\n            e.preventDefault();\n            var x = e.pageX || e.originalEvent.touches && e.originalEvent.touches[0].pageX;\n            if (e.button === 2) {\n                return;\n            }\n\n            if (target === \"both\") {\n                this.setTempMinInterval();\n            }\n\n            if (!target) {\n                target = this.target || \"from\";\n            }\n\n            this.current_plugin = this.plugin_count;\n            this.target = target;\n\n            this.is_active = true;\n            this.dragging = true;\n\n            this.coords.x_gap = this.$cache.rs.offset().left;\n            this.coords.x_pointer = x - this.coords.x_gap;\n\n            this.calcPointerPercent();\n            this.changeLevel(target);\n\n            if (is_old_ie) {\n                $(\"*\").prop(\"unselectable\", true);\n            }\n\n            this.$cache.line.trigger(\"focus\");\n\n            this.updateScene();\n        }",
    "label": 0
  },
  {
    "codes": "```python\nimport time\nfrom functools import wraps\n\ndef measure_execution_time(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.perf_counter_ns()  # Start time in nanoseconds\n        result = func(*args, **kwargs)  # Execute the function\n        end_time = time.perf_counter_ns()  # End time in nanoseconds\n        execution_time = end_time - start_time  # Calculate execution time\n        print(f\"{func.__name__} executed in {execution_time} nanoseconds\")\n        return result\n    return wrapper\n```",
    "label": 1
  },
  {
    "codes": "```kotlin\n// Extension function to reverse words in a string while preserving whitespace\nfun String.reverseWords(): String {\n    // Split the string into words and whitespace using regex\n    val parts = this.split(\"(?<=\\\\s)|(?=\\\\s)\".toRegex())\n    \n    // Reverse each word while keeping whitespace intact\n    return parts.joinToString(\"\") { part ->\n        if (part.isNotBlank()) part.reversed() else part\n    }\n}\n```\n\n```kotlin\n// Example usage\nfun main() {\n    val input = \"Hello  world!  Kotlin is fun.\"\n    val result = input.reverseWords()\n    println(result)  // Output: \"olleH  dlrow!  niltoK si nuf.\"\n}\n```",
    "label": 1
  },
  {
    "codes": "def hastext(self, cls='current',strict=True, correctionhandling=CorrectionHandling.CURRENT):\n        \"\"\"See :meth:`AbstractElement.hastext`\"\"\"\n        if cls == 'original': correctionhandling = CorrectionHandling.ORIGINAL #backward compatibility\n        if correctionhandling in (CorrectionHandling.CURRENT, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, New) or isinstance(e, Current):\n                    return e.hastext(cls,strict, correctionhandling)\n        if correctionhandling in (CorrectionHandling.ORIGINAL, CorrectionHandling.EITHER):\n            for e in self:\n                if isinstance(e, Original):\n                    return e.hastext(cls,strict, correctionhandling)\n        return False",
    "label": 0
  },
  {
    "codes": "```python\nimport functools\nimport traceback\nimport logging\n\n# Configure a basic logger\nlogging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef log_exceptions(func):\n    \"\"\"Decorator to log exceptions with traceback.\"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            # Capture and log the exception traceback\n            logging.error(\"Exception occurred in function '%s': %s\", func.__name__, traceback.format_exc())\n            raise\n\n    return wrapper\n```\n",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n)\n\nfunc handleConnection(conn net.Conn, wg *sync.WaitGroup) {\n\tdefer conn.Close()\n\tdefer wg.Done()\n\n\treader := bufio.NewScanner(conn)\n\n\tfor reader.Scan() {\n\t\t// Read data from the client\n\t\tline := reader.Text()\n\t\tfmt.Printf(\"Received: %s\\n\", line)\n\n\t\t// Write back a message to the client\n\t\tif _, err := conn.Write([]byte(\"Message received: \" + line + \"\\n\")); err != nil {\n\t\t\tfmt.Println(\"Error writing to client:\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Simulate exit condition\n\t\tif strings.TrimSpace(line) == \"exit\" {\n\t\t\tfmt.Println(\"Closing connection\")\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif err := reader.Err(); err != nil {\n\t\tfmt.Println(\"Error reading from client:\", err)\n\t}\n}\n\nfunc main() {\n\tlistener, err := net.Listen(\"tcp\", \":8080\")\n\tif err != nil {\n\t\tfmt.Println(\"Error starting server:\", err)\n\t\treturn\n\t}\n\tdefer listener.Close()\n\n\tfmt.Println(\"Server started. Listening on :8080...\")\n\n\tvar wg sync.WaitGroup\n\n\tfor {\n\t\t// Accept a new connection\n\t\tconn, err := listener.Accept()\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error accepting connection:\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Increment the WaitGroup counter\n\t\twg.Add(1)\n\n\t\t// Handle the connection in a new goroutine\n\t\tgo handleConnection(conn, &wg)\n\t}\n\n\t// Wait for all connections to be handled\n\twg.Wait()\n}\n```",
    "label": 1
  },
  {
    "codes": "```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: custom-metrics-config\n  namespace: custom-metrics\ndata:\n  config.yaml: |\n    rules:\n    - seriesQuery: 'queue_depth_metric{name=~\"^myqueue.*\"}'\n      seriesFilters: []\n      resources:\n        names: {}\n        template: '{{resourceName}}'\n      name:\n        matches: \"^queue_depth_metric_(.*)\"\n        as: \"queue_depth_per_pod\"\n      metricsQuery: 'max(rate(queue_depth_metric{<<.LabelMatchers>>}[2m]))'\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-adapter-config\n  namespace: custom-metrics\ndata:\n  config.yaml: |\n    rules:\n    - seriesQuery: 'queue_depth_metric{name=~\"^myqueue.*\"}'\n      seriesFilters: []\n      resources:\n        template: 'service'\n      name:\n        matches: '^queue_depth_metric_(.*)$'\n        as: 'queue_depth_per_service'\n      metricsQuery: 'max(rate(queue_depth_metric{<<.LabelMatchers>>}[2m]))'\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pytorch-inference\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pytorch-inference\n  template:\n    metadata:\n      labels:\n        app: pytorch-inference\n    spec:\n      containers:\n      - name: pytorch-container\n        image: your-pytorch-inference-image:latest\n        resources:\n          requests:\n            cpu: \"1\"\n            memory: \"1Gi\"\n          limits:\n            cpu: \"2\"\n            memory: \"2Gi\"\n\n---\napiVersion: v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: pytorch-inference-hpa\n  namespace: default\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: pytorch-inference\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Pods\n    pods:\n      metric:\n        name: queue_depth_per_pod\n      target:\n        type: AverageValue\n        averageValue: 10\n```",
    "label": 1
  },
  {
    "codes": "def defaultannotator(self, annotationtype, set=None):\n        \"\"\"Obtain the default annotator for the specified annotation type and set.\n\n        Arguments:\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\n            set (str): the set, should formally be a URL pointing to the set definition\n\n        Returns:\n            the set (str)\n\n        Raises:\n            :class:`NoDefaultError` if the annotation type does not exist or if there is ambiguity (multiple sets for the same type)\n        \"\"\"\n\n        if inspect.isclass(annotationtype) or isinstance(annotationtype,AbstractElement): annotationtype = annotationtype.ANNOTATIONTYPE\n        if not set: set = self.defaultset(annotationtype)\n        try:\n            return self.annotationdefaults[annotationtype][set]['annotator']\n        except KeyError:\n            raise NoDefaultError",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<DeletedStorageAccountItem>>> getDeletedStorageAccountsNextSinglePageAsync(final String nextPageLink) {\n        if (nextPageLink == null) {\n            throw new IllegalArgumentException(\"Parameter nextPageLink is required and cannot be null.\");\n        }\n        String nextUrl = String.format(\"%s\", nextPageLink);\n        return service.getDeletedStorageAccountsNext(nextUrl, this.acceptLanguage(), this.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<Page<DeletedStorageAccountItem>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<DeletedStorageAccountItem>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<PageImpl<DeletedStorageAccountItem>> result = getDeletedStorageAccountsNextDelegate(response);\n                        return Observable.just(new ServiceResponse<Page<DeletedStorageAccountItem>>(result.body(), result.response()));\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "function getLevel(nameOrValue) {\n  if (typeof nameOrValue === 'string') {\n    return LEVELS_BY_NAME.get(nameOrValue) || Level.ALL;\n  }\n  if (typeof nameOrValue !== 'number') {\n    throw new TypeError('not a string or number');\n  }\n  for (let level of ALL_LEVELS) {\n    if (nameOrValue >= level.value) {\n      return level;\n    }\n  }\n  return Level.ALL;\n}",
    "label": 0
  },
  {
    "codes": "def Lab_to_LCHab(cobj, *args, **kwargs):\n    \"\"\"\n    Convert from CIE Lab to LCH(ab).\n    \"\"\"\n    lch_l = cobj.lab_l\n    lch_c = math.sqrt(\n        math.pow(float(cobj.lab_a), 2) + math.pow(float(cobj.lab_b), 2))\n    lch_h = math.atan2(float(cobj.lab_b), float(cobj.lab_a))\n\n    if lch_h > 0:\n        lch_h = (lch_h / math.pi) * 180\n    else:\n        lch_h = 360 - (math.fabs(lch_h) / math.pi) * 180\n\n    return LCHabColor(\n        lch_l, lch_c, lch_h, observer=cobj.observer, illuminant=cobj.illuminant)",
    "label": 0
  },
  {
    "codes": "def addidsuffix(self, idsuffix, recursive = True):\n        \"\"\"Appends a suffix to this element's ID, and optionally to all child IDs as well. There is sually no need to call this directly, invoked implicitly by :meth:`copy`\"\"\"\n        if self.id: self.id += idsuffix\n        if recursive:\n            for e in self:\n                try:\n                    e.addidsuffix(idsuffix, recursive)\n                except Exception:\n                    pass",
    "label": 0
  },
  {
    "codes": "function TitlebarGridList() {\n  const classes = useStyles();\n\n  return (\n    <div className={classes.root}>\n      <GridList cellHeight={180} className={classes.gridList}>\n        <GridListTile key=\"Subheader\" cols={2} style={{ height: 'auto' }}>\n          <ListSubheader component=\"div\">December</ListSubheader>\n        </GridListTile>\n        {tileData.map(tile => (\n          <GridListTile key={tile.img}>\n            <img src={tile.img} alt={tile.title} />\n            <GridListTileBar\n              title={tile.title}\n              subtitle={<span>by: {tile.author}</span>}\n              actionIcon={\n                <IconButton className={classes.icon}>\n                  <InfoIcon />\n                </IconButton>\n              }\n            />\n          </GridListTile>\n        ))}\n      </GridList>\n    </div>\n  );\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.springframework.stereotype.Component;\nimport io.github.resilience4j.circuitbreaker.annotation.CircuitBreaker;\n\n@Aspect\n@Component\npublic class CircuitBreakerAspect {\n\n    @Around(\"@annotation(circuitBreaker)\")\n    public Object circuitBreakerAdvice(ProceedingJoinPoint pjp, CircuitBreaker circuitBreaker) throws Throwable {\n        // Get the circuit breaker name from annotation\n        String circuitBreakerName = circuitBreaker.name();\n\n        // Wrap the method execution within a try-catch block\n        try {\n            // Proceed with the method execution\n            return pjp.proceed();\n        } catch (Exception ex) {\n            // Handle exception and implement circuit breaking logic as needed\n            System.out.println(\"Exception in method execution: \" + ex.getMessage());\n            // Typically, you would interact with a circuit breaker library here\n            throw ex;\n        }\n    }\n}\n```\n\n```java\n// Example usage:\nimport io.github.resilience4j.circuitbreaker.annotation.CircuitBreaker;\n\npublic class ExampleService {\n\n    @CircuitBreaker(name = \"exampleServiceCircuitBreaker\", fallbackMethod = \"fallback\")\n    public String performService() {\n        // Simulated service logic that may throw an exception\n        // ...\n\n        return \"Service Result\";\n    }\n\n    public String fallback(Throwable t) {\n        // Fallback logic in case of failure\n        return \"Fallback Result\";\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing Microsoft.AspNetCore.Http;\nusing Microsoft.AspNetCore.Mvc;\nusing Microsoft.AspNetCore.Mvc.Filters;\nusing System;\nusing System.IdentityModel.Tokens.Jwt;\nusing System.Linq;\n\n[AttributeUsage(AttributeTargets.Class | AttributeTargets.Method)]\npublic class ValidateJwtClaimsAttribute : Attribute, IAuthorizationFilter\n{\n    private readonly string _claimType;\n    private readonly string _claimValue;\n\n    public ValidateJwtClaimsAttribute(string claimType, string claimValue)\n    {\n        _claimType = claimType;\n        _claimValue = claimValue;\n    }\n\n    public void OnAuthorization(AuthorizationFilterContext context)\n    {\n        var bearerToken = context.HttpContext.Request.Headers[\"Authorization\"].FirstOrDefault()?.Split(\" \").Last();\n        if (bearerToken == null)\n        {\n            context.Result = new UnauthorizedResult();\n            return;\n        }\n\n        var handler = new JwtSecurityTokenHandler();\n        var jwtToken = handler.ReadToken(bearerToken) as JwtSecurityToken;\n        if (jwtToken == null)\n        {\n            context.Result = new UnauthorizedResult();\n            return;\n        }\n\n        var hasClaim = jwtToken.Claims.Any(c => c.Type == _claimType && c.Value == _claimValue);\n        if (!hasClaim)\n        {\n            context.Result = new ForbidResult();\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom transformers import pipeline\n\n# Initialize the summarization pipeline with a different model\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n# Example text to summarize\ntext = \"\"\"\nThe Hugging Face Transformers library provides an easy-to-use interface for various NLP tasks, \nincluding text summarization. It supports multiple pre-trained models that can be used out-of-the-box \nfor generating summaries of long documents or articles.\n\"\"\"\n\n# Generate the summary\nsummary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n\n# Print the summary\nprint(summary)\n```",
    "label": 1
  },
  {
    "codes": "```python\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\nimport time\n\n# Configure the WebDriver (you can switch to Firefox or another browser if needed)\nservice = Service(ChromeDriverManager().install())\ndriver = webdriver.Chrome(service=service)\n\ntry:\n    # Navigate to the login page\n    driver.get(\"https://example.com/login\")\n    time.sleep(2)  # Wait for the page to load\n\n    # Locate the username field and enter the username\n    username_field = driver.find_element(By.ID, \"username\")\n    username_field.send_keys(\"your_username\")\n\n    # Locate the password field and enter the password\n    password_field = driver.find_element(By.ID, \"password\")\n    password_field.send_keys(\"your_password\")\n\n    # Submit the login form\n    password_field.send_keys(Keys.RETURN)\n\n    # Wait for the login process\n    time.sleep(3)\n\n    # Verify login success by checking the presence of an element on the post-login page\n    # (e.g., a logout button) - change the selector according to your application\n    success_indicator = driver.find_element(By.ID, \"logout-button\")\n    assert success_indicator.is_displayed(), \"Login failed\"\n\nfinally:\n    # Close the browser window\n    driver.quit()\n```\n",
    "label": 1
  },
  {
    "codes": "```swift\nimport UIKit\nimport PlaygroundSupport\n\n// Create a view to be animated\nlet view = UIView(frame: CGRect(x: 100, y: 100, width: 100, height: 100))\nview.backgroundColor = .blue\n\n// Setup the playground live view\nPlaygroundPage.current.liveView = view\n\n// Create a key path animation for rotation\nlet rotationAnimation = CABasicAnimation(keyPath: \"transform.rotation.z\")\n// Set the initial and final values for the rotation in radians\nrotationAnimation.fromValue = 0\nrotationAnimation.toValue = Double.pi * 2 // 360 degrees in radians\n// Set the duration of the animation\nrotationAnimation.duration = 2\n// Set the animation to repeat forever\nrotationAnimation.repeatCount = .infinity\n\n// Add the animation to the view's layer\nview.layer.add(rotationAnimation, forKey: \"rotateAnimation\")\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponseWithHeaders<Void, FileGetPropertiesFromComputeNodeHeaders>> getPropertiesFromComputeNodeWithServiceResponseAsync(String poolId, String nodeId, String filePath, FileGetPropertiesFromComputeNodeOptions fileGetPropertiesFromComputeNodeOptions) {\n        if (this.client.batchUrl() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.batchUrl() is required and cannot be null.\");\n        }\n        if (poolId == null) {\n            throw new IllegalArgumentException(\"Parameter poolId is required and cannot be null.\");\n        }\n        if (nodeId == null) {\n            throw new IllegalArgumentException(\"Parameter nodeId is required and cannot be null.\");\n        }\n        if (filePath == null) {\n            throw new IllegalArgumentException(\"Parameter filePath is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        Validator.validate(fileGetPropertiesFromComputeNodeOptions);\n        Integer timeout = null;\n        if (fileGetPropertiesFromComputeNodeOptions != null) {\n            timeout = fileGetPropertiesFromComputeNodeOptions.timeout();\n        }\n        UUID clientRequestId = null;\n        if (fileGetPropertiesFromComputeNodeOptions != null) {\n            clientRequestId = fileGetPropertiesFromComputeNodeOptions.clientRequestId();\n        }\n        Boolean returnClientRequestId = null;\n        if (fileGetPropertiesFromComputeNodeOptions != null) {\n            returnClientRequestId = fileGetPropertiesFromComputeNodeOptions.returnClientRequestId();\n        }\n        DateTime ocpDate = null;\n        if (fileGetPropertiesFromComputeNodeOptions != null) {\n            ocpDate = fileGetPropertiesFromComputeNodeOptions.ocpDate();\n        }\n        DateTime ifModifiedSince = null;\n        if (fileGetPropertiesFromComputeNodeOptions != null) {\n            ifModifiedSince = fileGetPropertiesFromComputeNodeOptions.ifModifiedSince();\n        }\n        DateTime ifUnmodifiedSince = null;\n        if (fileGetPropertiesFromComputeNodeOptions != null) {\n            ifUnmodifiedSince = fileGetPropertiesFromComputeNodeOptions.ifUnmodifiedSince();\n        }\n        String parameterizedHost = Joiner.on(\", \").join(\"{batchUrl}\", this.client.batchUrl());\n        DateTimeRfc1123 ocpDateConverted = null;\n        if (ocpDate != null) {\n            ocpDateConverted = new DateTimeRfc1123(ocpDate);\n        }\n        DateTimeRfc1123 ifModifiedSinceConverted = null;\n        if (ifModifiedSince != null) {\n            ifModifiedSinceConverted = new DateTimeRfc1123(ifModifiedSince);\n        }\n        DateTimeRfc1123 ifUnmodifiedSinceConverted = null;\n        if (ifUnmodifiedSince != null) {\n            ifUnmodifiedSinceConverted = new DateTimeRfc1123(ifUnmodifiedSince);\n        }\n        return service.getPropertiesFromComputeNode(poolId, nodeId, filePath, this.client.apiVersion(), this.client.acceptLanguage(), timeout, clientRequestId, returnClientRequestId, ocpDateConverted, ifModifiedSinceConverted, ifUnmodifiedSinceConverted, parameterizedHost, this.client.userAgent())\n                .flatMap(new Func1<Response<Void>, Observable<ServiceResponseWithHeaders<Void, FileGetPropertiesFromComputeNodeHeaders>>>() {\n                    @Override\n                    public Observable<ServiceResponseWithHeaders<Void, FileGetPropertiesFromComputeNodeHeaders>> call(Response<Void> response) {\n                        try {\n                            ServiceResponseWithHeaders<Void, FileGetPropertiesFromComputeNodeHeaders> clientResponse = getPropertiesFromComputeNodeDelegate(response);\n                            return Observable.just(clientResponse);\n                        } catch (Throwable t) {\n                            return Observable.error(t);\n                        }\n                    }\n                });\n    }",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"github.com/BurntSushi/toml\"\n\t\"gopkg.in/yaml.v3\"\n)\n\n// Define supported formats\nconst (\n\tFormatJSON = \"json\"\n\tFormatYAML = \"yaml\"\n\tFormatTOML = \"toml\"\n)\n\nfunc main() {\n\t// Define CLI flags\n\tinputFile := flag.String(\"input\", \"\", \"Input file path\")\n\toutputFormat := flag.String(\"format\", \"\", \"Output format (json, yaml, toml)\")\n\toutputFile := flag.String(\"output\", \"\", \"Output file path (optional)\")\n\tflag.Parse()\n\n\t// Validate input\n\tif *inputFile == \"\" || *outputFormat == \"\" {\n\t\tfmt.Println(\"Error: input file and output format are required\")\n\t\tflag.Usage()\n\t\tos.Exit(1)\n\t}\n\n\t// Read input file\n\tdata, err := ioutil.ReadFile(*inputFile)\n\tif err != nil {\n\t\tfmt.Printf(\"Error reading input file: %v\\n\", err)\n\t\tos.Exit(1)\n\t}\n\n\t// Determine input format based on file extension\n\tinputFormat := filepath.Ext(*inputFile)[1:] // Remove the dot\n\n\t// Parse input data\n\tvar parsedData interface{}\n\tswitch inputFormat {\n\tcase FormatJSON:\n\t\terr = json.Unmarshal(data, &parsedData)\n\tcase FormatYAML:\n\t\terr = yaml.Unmarshal(data, &parsedData)\n\tcase FormatTOML:\n\t\terr = toml.Unmarshal(data, &parsedData)\n\tdefault:\n\t\tfmt.Printf(\"Unsupported input format: %s\\n\", inputFormat)\n\t\tos.Exit(1)\n\t}\n\tif err != nil {\n\t\tfmt.Printf(\"Error parsing input data: %v\\n\", err)\n\t\tos.Exit(1)\n\t}\n\n\t// Convert to desired output format\n\tvar outputData []byte\n\tswitch *outputFormat {\n\tcase FormatJSON:\n\t\toutputData, err = json.MarshalIndent(parsedData, \"\", \"  \")\n\tcase FormatYAML:\n\t\toutputData, err = yaml.Marshal(parsedData)\n\tcase FormatTOML:\n\t\toutputData, err = toml.Marshal(parsedData)\n\tdefault:\n\t\tfmt.Printf(\"Unsupported output format: %s\\n\", *outputFormat)\n\t\tos.Exit(1)\n\t}\n\tif err != nil {\n\t\tfmt.Printf(\"Error converting data: %v\\n\", err)\n\t\tos.Exit(1)\n\t}\n\n\t// Write output to file or stdout\n\tif *outputFile != \"\" {\n\t\terr = ioutil.WriteFile(*outputFile, outputData, 0644)\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"Error writing output file: %v\\n\", err)\n\t\t\tos.Exit(1)\n\t\t}\n\t\tfmt.Printf(\"Successfully converted to %s: %s\\n\", *outputFormat, *outputFile)\n\t} else {\n\t\tfmt.Println(string(outputData))\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"log\"\n    \"net/http\"\n    \"github.com/gorilla/websocket\"\n)\n\n// Upgrader to upgrade HTTP connection to WebSocket\nvar upgrader = websocket.Upgrader{\n    CheckOrigin: func(r *http.Request) bool {\n        return true // Allow all connections\n    },\n}\n\n// Client represents a connected WebSocket client\ntype Client struct {\n    conn *websocket.Conn\n    send chan []byte\n}\n\n// Hub manages all connected clients\ntype Hub struct {\n    clients    map[*Client]bool\n    broadcast  chan []byte\n    register   chan *Client\n    unregister chan *Client\n}\n\n// NewHub initializes a new Hub\nfunc NewHub() *Hub {\n    return &Hub{\n        clients:    make(map[*Client]bool),\n        broadcast:  make(chan []byte),\n        register:   make(chan *Client),\n        unregister: make(chan *Client),\n    }\n}\n\n// Run starts the Hub to manage clients\nfunc (h *Hub) Run() {\n    for {\n        select {\n        case client := <-h.register:\n            h.clients[client] = true\n        case client := <-h.unregister:\n            if _, ok := h.clients[client]; ok {\n                delete(h.clients, client)\n                close(client.send)\n            }\n        case message := <-h.broadcast:\n            for client := range h.clients {\n                select {\n                case client.send <- message:\n                default:\n                    close(client.send)\n                    delete(h.clients, client)\n                }\n            }\n        }\n    }\n}\n\n// ReadPump reads messages from the WebSocket connection\nfunc (c *Client) ReadPump(hub *Hub) {\n    defer func() {\n        hub.unregister <- c\n        c.conn.Close()\n    }()\n    for {\n        _, message, err := c.conn.ReadMessage()\n        if err != nil {\n            break\n        }\n        hub.broadcast <- message\n    }\n}\n\n// WritePump sends messages to the WebSocket connection\nfunc (c *Client) WritePump() {\n    defer func() {\n        c.conn.Close()\n    }()\n    for {\n        select {\n        case message, ok := <-c.send:\n            if !ok {\n                c.conn.WriteMessage(websocket.CloseMessage, []byte{})\n                return\n            }\n            if err := c.conn.WriteMessage(websocket.TextMessage, message); err != nil {\n                return\n            }\n        }\n    }\n}\n\n// ServeWebSocket handles WebSocket requests\nfunc ServeWebSocket(hub *Hub, w http.ResponseWriter, r *http.Request) {\n    conn, err := upgrader.Upgrade(w, r, nil)\n    if err != nil {\n        log.Println(err)\n        return\n    }\n    client := &Client{conn: conn, send: make(chan []byte, 256)}\n    hub.register <- client\n\n    go client.WritePump()\n    go client.ReadPump(hub)\n}\n\nfunc main() {\n    hub := NewHub()\n    go hub.Run()\n\n    http.HandleFunc(\"/ws\", func(w http.ResponseWriter, r *http.Request) {\n        ServeWebSocket(hub, w, r)\n    })\n\n    log.Println(\"WebSocket server started on :8080\")\n    if err := http.ListenAndServe(\":8080\", nil); err != nil {\n        log.Fatal(\"ListenAndServe:\", err)\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```typescript\nimport { Injectable } from '@angular/core';\nimport { HttpClient, HttpParams } from '@angular/common/http';\nimport { Observable } from 'rxjs';\n\n@Injectable({\n  providedIn: 'root'\n})\nexport class PaginatedDataService {\n  private apiUrl = 'https://api.example.com/data'; // Replace with your API endpoint\n\n  constructor(private http: HttpClient) {}\n\n  // Fetch paginated data from the API\n  getPaginatedData(page: number, pageSize: number): Observable<any> {\n    // Set up query parameters for pagination\n    const params = new HttpParams()\n      .set('page', page.toString())\n      .set('pageSize', pageSize.toString());\n\n    // Make the HTTP GET request with the pagination parameters\n    return this.http.get<any>(this.apiUrl, { params });\n  }\n}\n```\n\n```typescript\n// Example usage in a component\nimport { Component, OnInit } from '@angular/core';\nimport { PaginatedDataService } from './paginated-data.service';\n\n@Component({\n  selector: 'app-data-list',\n  templateUrl: './data-list.component.html',\n  styleUrls: ['./data-list.component.css']\n})\nexport class DataListComponent implements OnInit {\n  data: any[] = [];\n  currentPage = 1;\n  pageSize = 10;\n\n  constructor(private dataService: PaginatedDataService) {}\n\n  ngOnInit(): void {\n    this.loadData();\n  }\n\n  loadData(): void {\n    this.dataService.getPaginatedData(this.currentPage, this.pageSize).subscribe(\n      (response) => {\n        this.data = response.data; // Assuming the API returns data in a 'data' field\n      },\n      (error) => {\n        console.error('Error fetching data:', error);\n      }\n    );\n  }\n\n  onPageChange(newPage: number): void {\n    this.currentPage = newPage;\n    this.loadData();\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *ListCrawlersInput) SetNextToken(v string) *ListCrawlersInput {\n\ts.NextToken = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "def export_history(self, dirname):\n        \"\"\"\n        Exports TenetoBIDShistory.py, tenetoinfo.json, requirements.txt (modules currently imported) to dirname\n\n        Parameters\n        ---------\n        dirname : str\n            directory to export entire TenetoBIDS history.\n\n        \"\"\"\n        mods = [(m.__name__, m.__version__)\n                for m in sys.modules.values() if m if hasattr(m, '__version__')]\n        with open(dirname + '/requirements.txt', 'w') as f:\n            for m in mods:\n                m = list(m)\n                if not isinstance(m[1], str):\n                    m[1] = m[1].decode(\"utf-8\")\n                f.writelines(m[0] + ' == ' + m[1] + '\\n')\n\n        with open(dirname + '/TenetoBIDShistory.py', 'w') as f:\n            f.writelines('import teneto\\n')\n            for func, args in self.history:\n                f.writelines(func + '(**' + str(args) + ')\\n')\n\n        with open(dirname + '/tenetoinfo.json', 'w') as f:\n            json.dump(self.tenetoinfo, f)",
    "label": 0
  },
  {
    "codes": "function (percent) {\n            var min = this.options.min,\n                max = this.options.max,\n                min_decimals = min.toString().split(\".\")[1],\n                max_decimals = max.toString().split(\".\")[1],\n                min_length, max_length,\n                avg_decimals = 0,\n                abs = 0;\n\n            if (percent === 0) {\n                return this.options.min;\n            }\n            if (percent === 100) {\n                return this.options.max;\n            }\n\n\n            if (min_decimals) {\n                min_length = min_decimals.length;\n                avg_decimals = min_length;\n            }\n            if (max_decimals) {\n                max_length = max_decimals.length;\n                avg_decimals = max_length;\n            }\n            if (min_length && max_length) {\n                avg_decimals = (min_length >= max_length) ? min_length : max_length;\n            }\n\n            if (min < 0) {\n                abs = Math.abs(min);\n                min = +(min + abs).toFixed(avg_decimals);\n                max = +(max + abs).toFixed(avg_decimals);\n            }\n\n            var number = ((max - min) / 100 * percent) + min,\n                string = this.options.step.toString().split(\".\")[1],\n                result;\n\n            if (string) {\n                number = +number.toFixed(string.length);\n            } else {\n                number = number / this.options.step;\n                number = number * this.options.step;\n\n                number = +number.toFixed(0);\n            }\n\n            if (abs) {\n                number -= abs;\n            }\n\n            if (string) {\n                result = +number.toFixed(string.length);\n            } else {\n                result = this.toFixed(number);\n            }\n\n            if (result < this.options.min) {\n                result = this.options.min;\n            } else if (result > this.options.max) {\n                result = this.options.max;\n            }\n\n            return result;\n        }",
    "label": 0
  },
  {
    "codes": "def modify_main_app(app, config: Config):\n    \"\"\"\n    Modify the app we're serving to make development easier, eg.\n    * modify responses to add the livereload snippet\n    * set ``static_root_url`` on the app\n    * setup the debug toolbar\n    \"\"\"\n    app._debug = True\n    dft_logger.debug('livereload enabled: %s', '\u2713' if config.livereload else '\u2716')\n\n    def get_host(request):\n        if config.infer_host:\n            return request.headers.get('host', 'localhost').split(':', 1)[0]\n        else:\n            return config.host\n\n    if config.livereload:\n        async def on_prepare(request, response):\n            if (not request.path.startswith('/_debugtoolbar') and\n                    'text/html' in response.content_type and\n                    getattr(response, 'body', False)):\n                lr_snippet = LIVE_RELOAD_HOST_SNIPPET.format(get_host(request), config.aux_port)\n                dft_logger.debug('appending live reload snippet \"%s\" to body', lr_snippet)\n                response.body += lr_snippet.encode()\n        app.on_response_prepare.append(on_prepare)\n\n    static_path = config.static_url.strip('/')\n    if config.infer_host and config.static_path is not None:\n        # we set the app key even in middleware to make the switch to production easier and for backwards compat.\n        @web.middleware\n        async def static_middleware(request, handler):\n            static_url = 'http://{}:{}/{}'.format(get_host(request), config.aux_port, static_path)\n            dft_logger.debug('settings app static_root_url to \"%s\"', static_url)\n            request.app['static_root_url'].change(static_url)\n            return await handler(request)\n\n        app.middlewares.insert(0, static_middleware)\n\n    if config.static_path is not None:\n        static_url = 'http://{}:{}/{}'.format(config.host, config.aux_port, static_path)\n        dft_logger.debug('settings app static_root_url to \"%s\"', static_url)\n        app['static_root_url'] = MutableValue(static_url)\n\n    if config.debug_toolbar and aiohttp_debugtoolbar:\n        aiohttp_debugtoolbar.setup(app, intercept_redirects=False)",
    "label": 0
  },
  {
    "codes": "func (s *GetPartitionsOutput) SetNextToken(v string) *GetPartitionsOutput {\n\ts.NextToken = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function findBaseTexture(item, queue)\n{\n    // Objects with textures, like Sprites/Text\n    if (item instanceof BaseTexture)\n    {\n        if (queue.indexOf(item) === -1)\n        {\n            queue.push(item);\n        }\n\n        return true;\n    }\n\n    return false;\n}",
    "label": 0
  },
  {
    "codes": "def activate_users(self, request, queryset):\n        \"\"\"\n        Activates the selected users, if they are not already\n        activated.\n\n        \"\"\"\n        n = 0\n        for user in queryset:\n            if not user.is_active:\n                user.activate()\n                n += 1\n        self.message_user(\n            request,\n            _('Successfully activated %(count)d %(items)s.') %\n            {'count': n, 'items': model_ngettext(self.opts, n)},  messages.SUCCESS)",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <Servo.h>\n#include <SoftwareSerial.h>\n\nServo myServo;  // Create a Servo object\nSoftwareSerial BTSerial(10, 11); // RX, TX\n\nint servoPin = 9;  // Servo connected to pin 9\nint pos = 0;       // Variable to store the servo position\n\nvoid setup() {\n  myServo.attach(servoPin);  // Attach the servo to the pin\n  BTSerial.begin(9600);      // Begin serial communication at 9600 baud rate\n}\n\nvoid loop() {\n  if (BTSerial.available()) {  // Check if data is available to read\n    char command = BTSerial.read();  // Read the incoming byte\n\n    if (command >= '0' && command <= '9') { // Check if command is a number between 0 and 9\n      pos = map(command, '0', '9', 0, 180); // Map '0'-'9' to angle 0-180\n      myServo.write(pos);        // Move servo to the specified angle\n    }\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n// Blueprint for Dynamic Weather System Transitions in Unreal Engine\n\n// Define an Enum for Weather States\nUENUM(BlueprintType)\nenum class EWeatherState : uint8\n{\n    Sunny     UMETA(DisplayName = \"Sunny\"),\n    Cloudy    UMETA(DisplayName = \"Cloudy\"),\n    Rainy     UMETA(DisplayName = \"Rainy\"),\n    Stormy    UMETA(DisplayName = \"Stormy\"),\n    Snowy     UMETA(DisplayName = \"Snowy\")\n};\n\n// Create a Weather Manager Class\nUCLASS(ClassGroup=(Custom), meta=(BlueprintSpawnableComponent))\nclass UWeatherManagerComponent : public UActorComponent\n{\n    GENERATED_BODY()\n\npublic:\n    // Sets default values for this component's properties\n    UWeatherManagerComponent();\n\nprotected:\n    // Called when the game starts\n    virtual void BeginPlay() override;\n\npublic:\n    // Function to change the weather state\n    UFUNCTION(BlueprintCallable, Category=\"Weather\")\n    void ChangeWeatherState(EWeatherState NewWeatherState, float TransitionDuration);\n\nprivate:\n    // Current Weather State\n    EWeatherState CurrentWeatherState;\n\n    // Handle the transition between weather states\n    void TransitionWeather(EWeatherState NewWeatherState, float TransitionDuration);\n\n    // BlueprintImplementableEvent to handle changes specific to each weather\n    UFUNCTION(BlueprintImplementableEvent, Category=\"Weather\")\n    void OnWeatherStateChanged(EWeatherState InWeatherState);\n};\n\n// Implementation in the CPP file\n\nUWeatherManagerComponent::UWeatherManagerComponent()\n{\n    // Set this component to be initialized when the game starts\n    PrimaryComponentTick.bCanEverTick = false;\n\n    // Initial Weather State\n    CurrentWeatherState = EWeatherState::Sunny;\n}\n\nvoid UWeatherManagerComponent::BeginPlay()\n{\n    Super::BeginPlay();\n}\n\nvoid UWeatherManagerComponent::ChangeWeatherState(EWeatherState NewWeatherState, float TransitionDuration)\n{\n    if (CurrentWeatherState != NewWeatherState)\n    {\n        TransitionWeather(NewWeatherState, TransitionDuration);\n    }\n}\n\nvoid UWeatherManagerComponent::TransitionWeather(EWeatherState NewWeatherState, float TransitionDuration)\n{\n    // Example transition logic (like fading clouds, changing light intensity, etc.)\n    // This would be where you implement the specific effects\n    // Call a Blueprint event to handle visual specifics for Unreal's editor\n    OnWeatherStateChanged(NewWeatherState);\n\n    // Set the current weather state to the new state\n    CurrentWeatherState = NewWeatherState;\n}\n```",
    "label": 1
  },
  {
    "codes": "async function main () {\n\n    const requirements = exchangeId && methodName\n    if (!requirements) {\n\n        printUsage ()\n\n    } else {\n\n        let args = params\n            .map (s => s.match (/^[0-9]{4}[-]?[0-9]{2}[-]?[0-9]{2}[T\\s]?[0-9]{2}[:]?[0-9]{2}[:]?[0-9]{2}/g) ? exchange.parse8601 (s) : s)\n            .map (s => (() => { try { return eval ('(() => (' + s + ')) ()') } catch (e) { return s } }) ())\n\n        const www = Array.isArray (exchange.urls.www) ? exchange.urls.www[0] : exchange.urls.www\n\n        if (cloudscrape)\n            exchange.headers = await scrapeCloudflareHttpHeaderCookie (www)\n\n        if (cfscrape)\n            exchange.headers = cfscrapeCookies (www)\n\n        if (cors) {\n            exchange.proxy =  'https://cors-anywhere.herokuapp.com/';\n            exchange.origin = exchange.uuid ()\n        }\n\n        no_load_markets = no_send ? true : no_load_markets\n\n        if (debug) {\n            exchange.verbose = verbose\n        }\n\n        if (!no_load_markets) {\n            await exchange.loadMarkets ()\n        }\n\n        exchange.verbose = verbose\n\n        if (no_send) {\n\n            exchange.verbose = no_send\n            exchange.fetch = function fetch (url, method = 'GET', headers = undefined, body = undefined) {\n                log.dim.noLocate ('-------------------------------------------')\n                log.dim.noLocate (exchange.iso8601 (exchange.milliseconds ()))\n                log.green.unlimited ({\n                    url,\n                    method,\n                    headers,\n                    body,\n                })\n                process.exit ()\n            }\n        }\n\n        if (typeof exchange[methodName] === 'function') {\n\n            log (exchange.id + '.' + methodName, '(' + args.join (', ') + ')')\n\n            while (true) {\n\n                try {\n\n                    const result = await exchange[methodName] (... args)\n                    printHumanReadable (exchange, result)\n\n                } catch (e) {\n\n                    if (e instanceof ExchangeError) {\n\n                        log.red (e.constructor.name, e.message)\n\n                    } else if (e instanceof NetworkError) {\n\n                        log.yellow (e.constructor.name, e.message)\n\n                    }\n\n                    log.dim ('---------------------------------------------------')\n\n                    // rethrow for call-stack // other errors\n                    throw e\n\n                }\n\n                if (!poll)\n                    break;\n            }\n\n        } else if (exchange[methodName] === undefined) {\n\n            log.red (exchange.id + '.' + methodName + ': no such property')\n\n        } else {\n\n            printHumanReadable (exchange, exchange[methodName])\n        }\n    }\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<List<StampCapacityInner>> listCapacitiesNextAsync(final String nextPageLink, final ServiceFuture<List<StampCapacityInner>> serviceFuture, final ListOperationCallback<StampCapacityInner> serviceCallback) {\n        return AzureServiceFuture.fromPageResponse(\n            listCapacitiesNextSinglePageAsync(nextPageLink),\n            new Func1<String, Observable<ServiceResponse<Page<StampCapacityInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<StampCapacityInner>>> call(String nextPageLink) {\n                    return listCapacitiesNextSinglePageAsync(nextPageLink);\n                }\n            },\n            serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "Certainly! Here is a Locust script that performs load testing on a REST API, focusing on logging response codes and including commentary:\n\n```python\nfrom locust import HttpUser, TaskSet, task, between\n\nclass UserBehavior(TaskSet):\n    @task(1)\n    def get_request(self):\n        # Sending a GET request to the API endpoint\n        with self.client.get(\"/api/resource\", catch_response=True) as response:\n            # Log the response status code\n            print(f\"GET /api/resource: {response.status_code}\")\n    \n    @task(2)\n    def post_request(self):\n        # Sending a POST request to the API endpoint\n        with self.client.post(\"/api/resource\", json={\"key\": \"value\"}, catch_response=True) as response:\n            # Log the response status code\n            print(f\"POST /api/resource: {response.status_code}\")\n\nclass WebsiteUser(HttpUser):\n    # Wait time between requests for each user\n    wait_time = between(1, 3)\n    # Define the behavior that the user will execute\n    tasks = [UserBehavior]\n```\n\nThis script outlines:\n\n- A `UserBehavior` class for simulating user tasks, which contains:\n  - `get_request` method for sending GET requests and logging response codes.\n  - `post_request` method for sending POST requests and logging response codes.\n- The `WebsiteUser` class to simulate a realistic waiting period between consecutive requests for each user.\n- Used `catch_response=True` to catch the request response in order to log the status code directly.",
    "label": 1
  },
  {
    "codes": "public PagedList<SecretItem> getSecretVersionsNext(final String nextPageLink) {\n        ServiceResponse<Page<SecretItem>> response = getSecretVersionsNextSinglePageAsync(nextPageLink).toBlocking().single();\n        return new PagedList<SecretItem>(response.body()) {\n            @Override\n            public Page<SecretItem> nextPage(String nextPageLink) {\n                return getSecretVersionsNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "```swift\nimport Foundation\n\n// A struct to model polynomial equations of the form: a_n*x^n + a_(n-1)*x^(n-1) + ... + a_1*x + a_0\nstruct Polynomial {\n    // Coefficients of the polynomial, where each element at index 'i' is the coefficient for x^i\n    var coefficients: [Double]\n\n    // Method to evaluate the polynomial at a given value of x\n    func evaluate(at x: Double) -> Double {\n        // Initialize the result to 0\n        var result: Double = 0\n        \n        // Iterate over the coefficients with their indices\n        for (exponent, coefficient) in coefficients.enumerated() {\n            // Using pow(x, exponent) to compute x^exponent and adding the term to the result\n            result += coefficient * pow(x, Double(exponent))\n        }\n        \n        // Returning the evaluation result\n        return result\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```swift\n// Define a struct to model a polynomial equation\nstruct Polynomial {\n    // Coefficients for the polynomial, ordered from highest degree to constant term\n    let coefficients: [Double]\n    \n    // Initializer to set the coefficients\n    init(coefficients: [Double]) {\n        self.coefficients = coefficients\n    }\n    \n    // Method to evaluate the polynomial at a given value of x\n    func evaluate(at x: Double) -> Double {\n        return coefficients.enumerated().reduce(0.0) { (result, pair) in\n            let (index, coefficient) = pair\n            let degree = coefficients.count - 1 - index\n            return result + coefficient * pow(x, Double(degree))\n        }\n    }\n}\n\n// Example usage:\n// Represents the polynomial 3x^2 + 2x + 1\nlet polynomial = Polynomial(coefficients: [3, 2, 1])\nlet result = polynomial.evaluate(at: 2)  // Evaluates 3*(2)^2 + 2*(2) + 1 = 17\n```",
    "label": 1
  },
  {
    "codes": "public StorageBundle setStorageAccount(String vaultBaseUrl, String storageAccountName, String resourceId, String activeKeyName, boolean autoRegenerateKey, String regenerationPeriod, StorageAccountAttributes storageAccountAttributes, Map<String, String> tags) {\n        return setStorageAccountWithServiceResponseAsync(vaultBaseUrl, storageAccountName, resourceId, activeKeyName, autoRegenerateKey, regenerationPeriod, storageAccountAttributes, tags).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport functools\nimport time\n\ndef timeit_ns(func):\n    \"\"\"Decorator to measure the execution time of a function in nanoseconds.\"\"\"\n    @functools.wraps(func)\n    def wrapper_timeit_ns(*args, **kwargs):\n        # Record the start time in nanoseconds\n        start_time = time.perf_counter_ns()\n        \n        # Call the original function\n        result = func(*args, **kwargs)\n        \n        # Calculate execution time by subtracting the start time from the current time\n        end_time = time.perf_counter_ns()\n        execution_time = end_time - start_time\n        \n        # Output the execution time to the console\n        print(f\"Execution time of {func.__name__}: {execution_time} ns\")\n        \n        return result\n    \n    return wrapper_timeit_ns\n\n# Example usage of the decorator\n@timeit_ns\ndef example_function(seconds):\n    \"\"\"Simple function that sleeps for a given number of seconds.\"\"\"\n    time.sleep(seconds)\n\n# Run the function to see the decorator in action\nexample_function(1)\n```\n",
    "label": 1
  },
  {
    "codes": "async function updateInfoPlist (version) {\n  const filePath = path.resolve(__dirname, '..', 'atom', 'browser', 'resources', 'mac', 'Info.plist')\n  const file = plist.parse(await readFile(filePath, { encoding: 'utf8' }))\n\n  file.CFBundleVersion = version\n  file.CFBundleShortVersionString = version\n\n  await writeFile(filePath, plist.build(file))\n}",
    "label": 0
  },
  {
    "codes": "```R\n# Load necessary libraries\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(plotly)\n\n# Load COVID-19 vaccination data (example dataset)\n# Replace with actual dataset\nvaccination_data <- read_csv(\"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv\")\n\n# Define UI for the Shiny app\nui <- fluidPage(\n  titlePanel(\"COVID-19 Vaccination Rates by Country\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"country\", \"Select Country:\", \n                  choices = unique(vaccination_data$location), \n                  selected = \"United States\"),\n      dateRangeInput(\"date_range\", \"Select Date Range:\",\n                     start = min(vaccination_data$date),\n                     end = max(vaccination_data$date))\n    ),\n    \n    mainPanel(\n      plotlyOutput(\"vaccination_plot\"),\n      dataTableOutput(\"vaccination_table\")\n    )\n  )\n)\n\n# Define server logic\nserver <- function(input, output) {\n  \n  # Filter data based on user input\n  filtered_data <- reactive({\n    vaccination_data %>%\n      filter(location == input$country,\n             date >= input$date_range[1],\n             date <= input$date_range[2])\n  })\n  \n  # Plot vaccination rates over time\n  output$vaccination_plot <- renderPlotly({\n    data <- filtered_data()\n    plot_ly(data, x = ~date, y = ~people_vaccinated_per_hundred, \n            type = 'scatter', mode = 'lines', \n            name = 'People Vaccinated per Hundred') %>%\n      layout(title = paste(\"Vaccination Rates in\", input$country),\n             xaxis = list(title = \"Date\"),\n             yaxis = list(title = \"Vaccination Rate (%)\"))\n  })\n  \n  # Display data table\n  output$vaccination_table <- renderDataTable({\n    filtered_data()\n  })\n}\n\n# Run the Shiny app\nshinyApp(ui = ui, server = server)\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<List<PatternRuleInfo>> batchAddPatternsAsync(UUID appId, String versionId, List<PatternRuleCreateObject> patterns, final ServiceCallback<List<PatternRuleInfo>> serviceCallback) {\n        return ServiceFuture.fromResponse(batchAddPatternsWithServiceResponseAsync(appId, versionId, patterns), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "func (s *CreateScriptInput) SetDagNodes(v []*CodeGenNode) *CreateScriptInput {\n\ts.DagNodes = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```csharp\nusing System;\nusing System.IO;\nusing System.ServiceProcess;\nusing System.Threading;\n\npublic class FileMonitorService : ServiceBase\n{\n    private FileSystemWatcher _watcher;\n    private string _directoryToWatch = @\"C:\\Path\\To\\Monitor\"; // Specify the directory to monitor\n\n    protected override void OnStart(string[] args)\n    {\n        // Initialize the FileSystemWatcher\n        _watcher = new FileSystemWatcher\n        {\n            Path = _directoryToWatch,\n            NotifyFilter = NotifyFilters.LastWrite | NotifyFilters.FileName | NotifyFilters.DirectoryName,\n            Filter = \"*.*\" // Monitor all files\n        };\n\n        // Attach event handlers\n        _watcher.Changed += OnChanged;\n        _watcher.Created += OnCreated;\n        _watcher.Deleted += OnDeleted;\n        _watcher.Renamed += OnRenamed;\n\n        // Begin watching\n        _watcher.EnableRaisingEvents = true;\n    }\n\n    protected override void OnStop()\n    {\n        // Stop the FileSystemWatcher\n        _watcher.EnableRaisingEvents = false;\n        _watcher.Dispose();\n    }\n\n    private void OnChanged(object source, FileSystemEventArgs e)\n    {\n        // Handle file change event\n        LogChange($\"File changed: {e.FullPath}\");\n    }\n\n    private void OnCreated(object source, FileSystemEventArgs e)\n    {\n        // Handle file creation event\n        LogChange($\"File created: {e.FullPath}\");\n    }\n\n    private void OnDeleted(object source, FileSystemEventArgs e)\n    {\n        // Handle file deletion event\n        LogChange($\"File deleted: {e.FullPath}\");\n    }\n\n    private void OnRenamed(object source, RenamedEventArgs e)\n    {\n        // Handle file rename event\n        LogChange($\"File renamed: {e.OldFullPath} to {e.FullPath}\");\n    }\n\n    private void LogChange(string message)\n    {\n        // Log changes to a file or other logging mechanism\n        string logPath = @\"C:\\Path\\To\\LogFile.txt\";\n        File.AppendAllText(logPath, $\"{DateTime.Now}: {message}{Environment.NewLine}\");\n    }\n\n    public static void Main()\n    {\n        // Run the service\n        ServiceBase.Run(new FileMonitorService());\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\n\t\"gopkg.in/yaml.v2\"\n\t\"github.com/BurntSushi/toml\"\n)\n\nfunc main() {\n\tif len(os.Args) != 4 {\n\t\tfmt.Println(\"Usage: converter <input_format> <output_format> <file_path>\")\n\t\treturn\n\t}\n\n\tinputFormat := os.Args[1]\n\toutputFormat := os.Args[2]\n\tfilePath := os.Args[3]\n\n\tdata, err := ioutil.ReadFile(filePath)\n\tif err != nil {\n\t\tfmt.Printf(\"Error reading file: %v\\n\", err)\n\t\treturn\n\t}\n\n\tvar jsonData []byte\n\n\tswitch inputFormat {\n\tcase \"json\":\n\t\tjsonData = data\n\tcase \"yaml\":\n\t\tvar yamlData interface{}\n\t\tif err := yaml.Unmarshal(data, &yamlData); err != nil {\n\t\t\tfmt.Printf(\"Error unmarshalling YAML: %v\\n\", err)\n\t\t\treturn\n\t\t}\n\t\tjsonData, err = json.Marshal(yamlData)\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"Error marshalling to JSON: %v\\n\", err)\n\t\t\treturn\n\t\t}\n\tcase \"toml\":\n\t\tvar tomlData interface{}\n\t\tif _, err := toml.Decode(string(data), &tomlData); err != nil {\n\t\t\tfmt.Printf(\"Error decoding TOML: %v\\n\", err)\n\t\t\treturn\n\t\t}\n\t\tjsonData, err = json.Marshal(tomlData)\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"Error marshalling to JSON: %v\\n\", err)\n\t\t\treturn\n\t\t}\n\tdefault:\n\t\tfmt.Println(\"Unsupported input format. Use json, yaml or toml.\")\n\t\treturn\n\t}\n\n\tvar outputData []byte\n\tswitch outputFormat {\n\tcase \"json\":\n\t\toutputData = jsonData\n\tcase \"yaml\":\n\t\tvar yamlOutput interface{}\n\t\tif err := json.Unmarshal(jsonData, &yamlOutput); err != nil {\n\t\t\tfmt.Printf(\"Error unmarshalling JSON: %v\\n\", err)\n\t\t\treturn\n\t\t}\n\t\toutputData, err = yaml.Marshal(yamlOutput)\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"Error marshalling to YAML: %v\\n\", err)\n\t\t\treturn\n\t\t}\n\tcase \"toml\":\n\t\tvar tomlOutput interface{}\n\t\tif err := json.Unmarshal(jsonData, &tomlOutput); err != nil {\n\t\t\tfmt.Printf(\"Error unmarshalling JSON: %v\\n\", err)\n\t\t\treturn\n\t\t}\n\t\toutputData, err = toml.Marshal(tomlOutput)\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"Error marshalling to TOML: %v\\n\", err)\n\t\t\treturn\n\t\t}\n\tdefault:\n\t\tfmt.Println(\"Unsupported output format. Use json, yaml or toml.\")\n\t\treturn\n\t}\n\n\tif err := ioutil.WriteFile(\"output.\"+outputFormat, outputData, 0644); err != nil {\n\t\tfmt.Printf(\"Error writing output file: %v\\n\", err)\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport random\n\nclass Character:\n    def __init__(self, name, health, attack, elemental_type):\n        self.name = name\n        self.health = health\n        self.attack = attack\n        self.elemental_type = elemental_type\n\n    def is_alive(self):\n        return self.health > 0\n\n    def take_damage(self, damage):\n        self.health -= damage\n        if self.health < 0:\n            self.health = 0\n\n    def attack_opponent(self, opponent):\n        damage = self.attack\n        if self.elemental_advantage(opponent):\n            damage *= 2  # Double damage if player has elemental advantage\n        opponent.take_damage(damage)\n        print(f\"{self.name} attacks {opponent.name} for {damage} damage!\")\n\n    def elemental_advantage(self, opponent):\n        # Define elemental interactions\n        interactions = {\n            'fire': 'grass',\n            'grass': 'water',\n            'water': 'fire'\n        }\n        return interactions.get(self.elemental_type) == opponent.elemental_type\n\n    def __str__(self):\n        return f\"{self.name}: {self.health} HP\"\n\ndef combat(player, enemy):\n    while player.is_alive() and enemy.is_alive():\n        player.attack_opponent(enemy)\n        if enemy.is_alive():\n            enemy.attack_opponent(player)\n\n        print(player)\n        print(enemy)\n        print(\"--------------------\")\n\n    if player.is_alive():\n        print(f\"{player.name} wins!\")\n    else:\n        print(f\"{enemy.name} wins!\")\n\ndef main():\n    player_name = input(\"Enter your character's name: \")\n    player_elemental_type = input(\"Choose your elemental type (fire, grass, water): \").lower()\n    player = Character(player_name, 100, 10, player_elemental_type)\n\n    enemy_name = \"Goblin\"\n    enemy_elemental_type = random.choice(['fire', 'grass', 'water'])\n    enemy = Character(enemy_name, 50, 5, enemy_elemental_type)\n\n    print(f\"A wild {enemy_name} appears! It is of type '{enemy_elemental_type}'.\")\n    \n    combat(player, enemy)\n\nif __name__ == \"__main__\":\n    main()\n```",
    "label": 1
  },
  {
    "codes": "public Observable<TaskInner> beginUpdateAsync(String resourceGroupName, String registryName, String taskName, TaskUpdateParameters taskUpdateParameters) {\n        return beginUpdateWithServiceResponseAsync(resourceGroupName, registryName, taskName, taskUpdateParameters).map(new Func1<ServiceResponse<TaskInner>, TaskInner>() {\n            @Override\n            public TaskInner call(ServiceResponse<TaskInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *CreateTrailOutput) SetName(v string) *CreateTrailOutput {\n\ts.Name = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```javascript\nimport React, { useState, useEffect } from 'react';\n\n// RealTimeChat component\nconst RealTimeChat = () => {\n  const [messages, setMessages] = useState([]); // State to store chat messages\n  const [inputValue, setInputValue] = useState(''); // State to store input value\n\n  // Simulate real-time message updates (e.g., from a WebSocket or API)\n  useEffect(() => {\n    const interval = setInterval(() => {\n      // Simulate receiving a new message\n      const newMessage = { id: messages.length + 1, text: `New message ${messages.length + 1}` };\n      setMessages((prevMessages) => [...prevMessages, newMessage]);\n    }, 5000); // Add a new message every 5 seconds\n\n    return () => clearInterval(interval); // Cleanup interval on unmount\n  }, [messages]);\n\n  // Handle sending a new message\n  const handleSendMessage = () => {\n    if (inputValue.trim()) {\n      const newMessage = { id: messages.length + 1, text: inputValue };\n      setMessages((prevMessages) => [...prevMessages, newMessage]);\n      setInputValue(''); // Clear input after sending\n    }\n  };\n\n  return (\n    <div style={styles.chatContainer}>\n      <div style={styles.messagesContainer}>\n        {messages.map((message) => (\n          <div key={message.id} style={styles.message}>\n            {message.text}\n          </div>\n        ))}\n      </div>\n      <div style={styles.inputContainer}>\n        <input\n          type=\"text\"\n          value={inputValue}\n          onChange={(e) => setInputValue(e.target.value)}\n          style={styles.input}\n          placeholder=\"Type a message...\"\n        />\n        <button onClick={handleSendMessage} style={styles.sendButton}>\n          Send\n        </button>\n      </div>\n    </div>\n  );\n};\n\n// Styles for the chat interface\nconst styles = {\n  chatContainer: {\n    width: '300px',\n    height: '400px',\n    border: '1px solid #ccc',\n    borderRadius: '8px',\n    display: 'flex',\n    flexDirection: 'column',\n    justifyContent: 'space-between',\n    padding: '10px',\n  },\n  messagesContainer: {\n    flex: 1,\n    overflowY: 'auto',\n    marginBottom: '10px',\n  },\n  message: {\n    padding: '5px',\n    borderBottom: '1px solid #eee',\n  },\n  inputContainer: {\n    display: 'flex',\n  },\n  input: {\n    flex: 1,\n    padding: '5px',\n    borderRadius: '4px',\n    border: '1px solid #ccc',\n  },\n  sendButton: {\n    marginLeft: '10px',\n    padding: '5px 10px',\n    borderRadius: '4px',\n    border: 'none',\n    backgroundColor: '#007bff',\n    color: '#fff',\n    cursor: 'pointer',\n  },\n};\n\nexport default RealTimeChat;\n```",
    "label": 1
  },
  {
    "codes": "def get_local_file_list(self):\n        \"\"\"\n        Walk the local build directory and create a list of relative and\n        absolute paths to files.\n        \"\"\"\n        file_list = []\n        for (dirpath, dirnames, filenames) in os.walk(self.build_dir):\n            for fname in filenames:\n                # relative path, to sync with the S3 key\n                local_key = os.path.join(\n                    os.path.relpath(dirpath, self.build_dir),\n                    fname\n                )\n                if local_key.startswith('./'):\n                    local_key = local_key[2:]\n                file_list.append(local_key)\n        return file_list",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <Servo.h>\n#include <SoftwareSerial.h>\n\n// Define Bluetooth RX and TX pins\n#define BT_RX 10\n#define BT_TX 11\n\n// Create a SoftwareSerial object for Bluetooth communication\nSoftwareSerial bluetooth(BT_RX, BT_TX);\n\n// Create Servo objects\nServo servo1;\nServo servo2;\n\n// Variables to store servo positions\nint pos1 = 90; // Initial position for servo1\nint pos2 = 90; // Initial position for servo2\n\nvoid setup() {\n  // Attach servos to pins\n  servo1.attach(9);  // Servo1 on pin 9\n  servo2.attach(8);  // Servo2 on pin 8\n\n  // Set initial positions\n  servo1.write(pos1);\n  servo2.write(pos2);\n\n  // Initialize Bluetooth communication\n  bluetooth.begin(9600);\n}\n\nvoid loop() {\n  // Check if Bluetooth data is available\n  if (bluetooth.available() >= 2) {\n    // Read the first byte (servo number)\n    int servoNum = bluetooth.read();\n\n    // Read the second byte (position value)\n    int newPos = bluetooth.read();\n\n    // Update the corresponding servo position\n    if (servoNum == 1) {\n      pos1 = newPos;\n      servo1.write(pos1);\n    } else if (servoNum == 2) {\n      pos2 = newPos;\n      servo2.write(pos2);\n    }\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```csharp\n// Assuming 'products' is a collection of Product objects with properties 'Price' and 'Category'\nvar filteredProducts = products\n    .Where(p => p.Price >= minPrice && p.Price <= maxPrice && p.Category == targetCategory)\n    .ToList();\n```\n\n```csharp\n// Alternative approach using query syntax\nvar filteredProducts = \n    from p in products\n    where p.Price >= minPrice && p.Price <= maxPrice && p.Category == targetCategory\n    select p;\n```\n\n```csharp\n// Another approach using a combination of LINQ methods\nvar filteredProducts = products\n    .Where(p => p.Category == targetCategory)\n    .Where(p => p.Price >= minPrice && p.Price <= maxPrice)\n    .ToList();\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"log\"\n    \"net/http\"\n    \"runtime/debug\"\n)\n\n// RecoverMiddleware is a middleware that recovers from panics in HTTP handlers\nfunc RecoverMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        defer func() {\n            if err := recover(); err != nil {\n                // Log the error and stack trace\n                log.Printf(\"panic: %v\\n%s\", err, debug.Stack())\n                // Respond with a 500 Internal Server Error\n                http.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError)\n            }\n        }()\n        // Call the next handler in the chain\n        next.ServeHTTP(w, r)\n    })\n}\n\nfunc main() {\n    // Example usage of RecoverMiddleware\n    mux := http.NewServeMux()\n    mux.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n        // Example panic for demonstration purposes\n        panic(\"example panic\")\n    })\n\n    // Wrap the mux with the RecoverMiddleware\n    loggedMux := RecoverMiddleware(mux)\n\n    // Start the HTTP server\n    log.Fatal(http.ListenAndServe(\":8080\", loggedMux))\n}\n```",
    "label": 1
  },
  {
    "codes": "def env_set(context):\n    \"\"\"Set $ENVs to specified string. from the pypyr context.\n\n    Args:\n        context: is dictionary-like. context is mandatory.\n                 context['env']['set'] must exist. It's a dictionary.\n                 Values are strings to write to $ENV.\n                 Keys are the names of the $ENV values to which to write.\n\n    For example, say input context is:\n        key1: value1\n        key2: value2\n        key3: value3\n        env:\n            set:\n                MYVAR1: {key1}\n                MYVAR2: before_{key3}_after\n                MYVAR3: arbtexthere\n\n    This will result in the following $ENVs:\n    $MYVAR1 = value1\n    $MYVAR2 = before_value3_after\n    $MYVAR3 = arbtexthere\n\n    Note that the $ENVs are not persisted system-wide, they only exist for\n    pypyr sub-processes, and as such for the following steps during this pypyr\n    pipeline execution. If you set an $ENV here, don't expect to see it in your\n    system environment variables after the pipeline finishes running.\n    \"\"\"\n    env_set = context['env'].get('set', None)\n\n    exists = False\n    if env_set:\n        logger.debug(\"started\")\n\n        for k, v in env_set.items():\n            logger.debug(f\"setting ${k} to context[{v}]\")\n            os.environ[k] = context.get_formatted_string(v)\n\n        logger.info(f\"set {len(env_set)} $ENVs from context.\")\n        exists = True\n\n        logger.debug(\"done\")\n\n    return exists",
    "label": 0
  },
  {
    "codes": "public SasDefinitionBundle updateSasDefinition(String vaultBaseUrl, String storageAccountName, String sasDefinitionName) {\n        return updateSasDefinitionWithServiceResponseAsync(vaultBaseUrl, storageAccountName, sasDefinitionName).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<ManagedClusterAccessProfileInner> getAccessProfileAsync(String resourceGroupName, String resourceName, String roleName) {\n        return getAccessProfileWithServiceResponseAsync(resourceGroupName, resourceName, roleName).map(new Func1<ServiceResponse<ManagedClusterAccessProfileInner>, ManagedClusterAccessProfileInner>() {\n            @Override\n            public ManagedClusterAccessProfileInner call(ServiceResponse<ManagedClusterAccessProfileInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```elixir\n# Define a GenServer module for distributed state management\ndefmodule DistributedStateManager do\n  use GenServer\n\n  # Client API\n\n  # Start the GenServer with an initial state\n  def start_link(initial_state) do\n    GenServer.start_link(__MODULE__, initial_state, name: __MODULE__)\n  end\n\n  # Get the current state\n  def get_state do\n    GenServer.call(__MODULE__, :get_state)\n  end\n\n  # Update the state with a new value\n  def update_state(new_value) do\n    GenServer.cast(__MODULE__, {:update_state, new_value})\n  end\n\n  # Server Callbacks\n\n  @impl true\n  def init(initial_state) do\n    {:ok, initial_state}\n  end\n\n  @impl true\n  def handle_call(:get_state, _from, state) do\n    {:reply, state, state}\n  end\n\n  @impl true\n  def handle_cast({:update_state, new_value}, _state) do\n    {:noreply, new_value}\n  end\nend\n```\n\n```erlang\n% Define a GenServer module for distributed state management\n-module(distributed_state_manager).\n-behaviour(gen_server).\n\n% Client API\n\n% Start the GenServer with an initial state\nstart_link(InitialState) ->\n    gen_server:start_link({local, ?MODULE}, ?MODULE, InitialState, []).\n\n% Get the current state\nget_state() ->\n    gen_server:call(?MODULE, get_state).\n\n% Update the state with a new value\nupdate_state(NewValue) ->\n    gen_server:cast(?MODULE, {update_state, NewValue}).\n\n% Server Callbacks\n\ninit(InitialState) ->\n    {ok, InitialState}.\n\nhandle_call(get_state, _From, State) ->\n    {reply, State, State}.\n\nhandle_cast({update_state, NewValue}, _State) ->\n    {noreply, NewValue}.\n```\n\n```elixir\n# Example usage in Elixir\n{:ok, pid} = DistributedStateManager.start_link(%{key: \"initial_value\"})\nDistributedStateManager.update_state(%{key: \"new_value\"})\ncurrent_state = DistributedStateManager.get_state()\nIO.inspect(current_state)  # Output: %{key: \"new_value\"}\n```\n\n```erlang\n% Example usage in Erlang\n{ok, Pid} = distributed_state_manager:start_link(#{key => \"initial_value\"}).\ndistributed_state_manager:update_state(#{key => \"new_value\"}).\nCurrentState = distributed_state_manager:get_state().\nio:format(\"~p~n\", [CurrentState]).  % Output: #{key => \"new_value\"}\n```",
    "label": 1
  },
  {
    "codes": "function downloadManager_deleteDownloadedFiles(downloads) {\n    downloads.forEach(function(download) {\n      try {\n        var file = getLocalFileFromNativePathOrUrl(download.target);\n        file.remove(false);\n      } catch (ex) {\n      }\n    });\n  }",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"net/http\"\n\n\t\"github.com/gorilla/websocket\"\n)\n\n// Client represents a single connection to the WebSocket server\ntype Client struct {\n\tconn *websocket.Conn\n\tsend chan []byte\n}\n\n// Server maintains the set of active clients and broadcasts messages to them\ntype Server struct {\n\tclients    map[*Client]bool\n\tbroadcast  chan []byte\n\tregister   chan *Client\n\tunregister chan *Client\n}\n\nvar upgrader = websocket.Upgrader{\n\tCheckOrigin: func(r *http.Request) bool {\n\t\treturn true\n\t},\n}\n\n// NewServer initializes a new Server\nfunc NewServer() *Server {\n\treturn &Server{\n\t\tclients:    make(map[*Client]bool),\n\t\tbroadcast:  make(chan []byte),\n\t\tregister:   make(chan *Client),\n\t\tunregister: make(chan *Client),\n\t}\n}\n\n// Start launches the WebSocket server and manages client connections\nfunc (s *Server) Start() {\n\tfor {\n\t\tselect {\n\t\tcase client := <-s.register:\n\t\t\ts.clients[client] = true\n\t\tcase client := <-s.unregister:\n\t\t\tif _, ok := s.clients[client]; ok {\n\t\t\t\tdelete(s.clients, client)\n\t\t\t\tclose(client.send)\n\t\t\t}\n\t\tcase message := <-s.broadcast:\n\t\t\tfor client := range s.clients {\n\t\t\t\tselect {\n\t\t\t\tcase client.send <- message:\n\t\t\t\tdefault:\n\t\t\t\t\tclose(client.send)\n\t\t\t\t\tdelete(s.clients, client)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// HandleConnection handles new WebSocket connections\nfunc (s *Server) HandleConnection(w http.ResponseWriter, r *http.Request) {\n\tconn, err := upgrader.Upgrade(w, r, nil)\n\tif err != nil {\n\t\thttp.Error(w, \"Could not upgrade connection\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tclient := &Client{\n\t\tconn: conn,\n\t\tsend: make(chan []byte, 256),\n\t}\n\n\ts.register <- client\n\n\tgo client.Read(s)\n\tgo client.Write()\n}\n\n// Read listens for incoming messages from the client\nfunc (c *Client) Read(s *Server) {\n\tdefer func() {\n\t\ts.unregister <- c\n\t\tc.conn.Close()\n\t}()\n\n\tfor {\n\t\t_, msg, err := c.conn.ReadMessage()\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\t\ts.broadcast <- msg\n\t}\n}\n\n// Write sends messages to the client\nfunc (c *Client) Write() {\n\tdefer c.conn.Close()\n\tfor msg := range c.send {\n\t\terr := c.conn.WriteMessage(websocket.TextMessage, msg)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// main function sets up the HTTP server and WebSocket endpoint\nfunc main() {\n\tserver := NewServer()\n\tgo server.Start()\n\n\thttp.HandleFunc(\"/ws\", server.HandleConnection)\n\taddr := \"localhost:8080\"\n\tfmt.Printf(\"Starting server on %s\\n\", addr)\n\tif err := http.ListenAndServe(addr, nil); err != nil {\n\t\tfmt.Println(\"ListenAndServe:\", err)\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "def unbuild_object(self, obj):\n        \"\"\"\n        Deletes the directory at self.get_build_path.\n        \"\"\"\n        logger.debug(\"Unbuilding %s\" % obj)\n        target_path = os.path.split(self.get_build_path(obj))[0]\n        if self.fs.exists(target_path):\n            logger.debug(\"Removing {}\".format(target_path))\n            self.fs.removetree(target_path)",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <opencv2/opencv.hpp>\n#include <iostream>\n\nint main() {\n    // Open video file\n    cv::VideoCapture cap(\"input_video.mp4\");\n    if (!cap.isOpened()) {\n        std::cerr << \"Error opening video file\" << std::endl;\n        return -1;\n    }\n\n    cv::Mat frame, resizedFrame, grayFrame, normalizedFrame;\n\n    while (cap.read(frame)) {\n        // Resize frame to model input size (e.g., 224x224)\n        cv::resize(frame, resizedFrame, cv::Size(224, 224));\n\n        // Convert to grayscale (if required by the model)\n        cv::cvtColor(resizedFrame, grayFrame, cv::COLOR_BGR2GRAY);\n\n        // Normalize pixel values to [0, 1] (if required by the model)\n        grayFrame.convertTo(normalizedFrame, CV_32F, 1.0 / 255.0);\n\n        // Further preprocessing (e.g., mean subtraction, scaling, etc.)\n        // normalizedFrame = (normalizedFrame - mean) / std;\n\n        // Display the preprocessed frame (optional)\n        cv::imshow(\"Preprocessed Frame\", normalizedFrame);\n        if (cv::waitKey(30) >= 0) break;\n    }\n\n    cap.release();\n    cv::destroyAllWindows();\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\n// MainActivity.java\npackage com.example.objectdetection;\n\nimport android.Manifest;\nimport android.content.pm.PackageManager;\nimport android.graphics.Bitmap;\nimport android.graphics.BitmapFactory;\nimport android.os.Bundle;\nimport android.util.Log;\nimport android.widget.ImageView;\nimport android.widget.TextView;\n\nimport androidx.annotation.NonNull;\nimport androidx.appcompat.app.AppCompatActivity;\nimport androidx.core.app.ActivityCompat;\nimport androidx.core.content.ContextCompat;\n\nimport org.tensorflow.lite.Interpreter;\nimport org.tensorflow.lite.support.image.TensorImage;\nimport org.tensorflow.lite.support.label.Category;\nimport org.tensorflow.lite.task.vision.detector.ObjectDetector;\nimport org.tensorflow.lite.task.vision.detector.ObjectDetector.ObjectDetectorOptions;\n\nimport java.io.IOException;\nimport java.nio.MappedByteBuffer;\nimport java.nio.channels.FileChannel;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\nimport java.util.List;\n\npublic class MainActivity extends AppCompatActivity {\n\n    private static final int CAMERA_PERMISSION_REQUEST_CODE = 1001;\n    private static final String MODEL_PATH = \"model.tflite\";\n    private Interpreter tflite;\n    private ImageView imageView;\n    private TextView resultTextView;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        imageView = findViewById(R.id.imageView);\n        resultTextView = findViewById(R.id.resultTextView);\n\n        // Check and request camera permission\n        if (ContextCompat.checkSelfPermission(this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {\n            ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.CAMERA}, CAMERA_PERMISSION_REQUEST_CODE);\n        } else {\n            initializeModel();\n        }\n    }\n\n    private void initializeModel() {\n        try {\n            tflite = new Interpreter(loadModelFile());\n        } catch (IOException e) {\n            Log.e(\"TFLite\", \"Error loading model\", e);\n        }\n    }\n\n    private MappedByteBuffer loadModelFile() throws IOException {\n        FileChannel fileChannel = (FileChannel) Files.newByteChannel(Paths.get(getAssets().openFd(MODEL_PATH).getFileDescriptor()));\n        return fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, fileChannel.size());\n    }\n\n    private void detectObjects(Bitmap bitmap) {\n        TensorImage tensorImage = TensorImage.fromBitmap(bitmap);\n        ObjectDetectorOptions options = ObjectDetectorOptions.builder()\n                .setMaxResults(5)\n                .setScoreThreshold(0.5f)\n                .build();\n\n        ObjectDetector objectDetector = null;\n        try {\n            objectDetector = ObjectDetector.createFromFileAndOptions(this, MODEL_PATH, options);\n        } catch (IOException e) {\n            Log.e(\"TFLite\", \"Error creating object detector\", e);\n        }\n\n        if (objectDetector != null) {\n            List<Category> results = objectDetector.detect(tensorImage);\n            displayResults(results);\n        }\n    }\n\n    private void displayResults(List<Category> results) {\n        StringBuilder resultText = new StringBuilder();\n        for (Category category : results) {\n            resultText.append(category.getLabel()).append(\": \").append(category.getScore()).append(\"\\n\");\n        }\n        resultTextView.setText(resultText.toString());\n    }\n\n    @Override\n    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults);\n        if (requestCode == CAMERA_PERMISSION_REQUEST_CODE && grantResults.length > 0 && grantResults[0] == PackageManager.PERMISSION_GRANTED) {\n            initializeModel();\n        }\n    }\n\n    @Override\n    protected void onDestroy() {\n        super.onDestroy();\n        if (tflite != null) {\n            tflite.close();\n        }\n    }\n}\n```\n\n```xml\n<!-- activity_main.xml -->\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n    <ImageView\n        android:id=\"@+id/imageView\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"wrap_content\"\n        android:scaleType=\"centerCrop\" />\n\n    <TextView\n        android:id=\"@+id/resultTextView\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:layout_alignParentBottom=\"true\"\n        android:layout_margin=\"16dp\"\n        android:background=\"#80000000\"\n        android:padding=\"8dp\"\n        android:textColor=\"#FFFFFF\"\n        android:textSize=\"16sp\" />\n</RelativeLayout>\n```\n\n```xml\n<!-- AndroidManifest.xml -->\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.example.objectdetection\">\n\n    <uses-permission android:name=\"android.permission.CAMERA\" />\n\n    <application\n        android:allowBackup=\"true\"\n        android:icon=\"@mipmap/ic_launcher\"\n        android:label=\"@string/app_name\"\n        android:roundIcon=\"@mipmap/ic_launcher_round\"\n        android:supportsRtl=\"true\"\n        android:theme=\"@style/Theme.ObjectDetection\">\n        <activity android:name=\".MainActivity\">\n            <intent-filter>\n                <action android:name=\"android.intent.action.MAIN\" />\n                <category android:name=\"android.intent.category.LAUNCHER\" />\n            </intent-filter>\n        </activity>\n    </application>\n</manifest>\n```\n\n```gradle\n// build.gradle (Module: app)\ndependencies {\n    implementation 'org.tensorflow:tensorflow-lite-task-vision:0.3.1'\n    implementation 'org.tensorflow:tensorflow-lite-support:0.3.1'\n    implementation 'org.tensorflow:tensorflow-lite:2.8.0'\n}\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.IO;\nusing System.ServiceProcess;\nusing System.Threading;\n\npublic class FileMonitorService : ServiceBase\n{\n    private FileSystemWatcher _watcher;\n\n    public FileMonitorService()\n    {\n        ServiceName = \"FileMonitorService\";\n    }\n\n    protected override void OnStart(string[] args)\n    {\n        // Initialize the FileSystemWatcher\n        _watcher = new FileSystemWatcher();\n        _watcher.Path = @\"C:\\Path\\To\\Monitor\"; // Set the directory to monitor\n        _watcher.IncludeSubdirectories = true; // Monitor subdirectories\n\n        // Set up event handlers\n        _watcher.Created += OnFileCreated;\n        _watcher.Deleted += OnFileDeleted;\n        _watcher.Changed += OnFileChanged;\n        _watcher.Renamed += OnFileRenamed;\n\n        // Enable the watcher\n        _watcher.EnableRaisingEvents = true;\n    }\n\n    protected override void OnStop()\n    {\n        // Disable the watcher when the service stops\n        _watcher.EnableRaisingEvents = false;\n        _watcher.Dispose();\n    }\n\n    private void OnFileCreated(object sender, FileSystemEventArgs e)\n    {\n        // Handle file creation\n        LogChange($\"File created: {e.FullPath}\");\n    }\n\n    private void OnFileDeleted(object sender, FileSystemEventArgs e)\n    {\n        // Handle file deletion\n        LogChange($\"File deleted: {e.FullPath}\");\n    }\n\n    private void OnFileChanged(object sender, FileSystemEventArgs e)\n    {\n        // Handle file changes\n        LogChange($\"File changed: {e.FullPath}\");\n    }\n\n    private void OnFileRenamed(object sender, RenamedEventArgs e)\n    {\n        // Handle file renaming\n        LogChange($\"File renamed: {e.OldFullPath} to {e.FullPath}\");\n    }\n\n    private void LogChange(string message)\n    {\n        // Log changes to a file or other logging mechanism\n        File.AppendAllText(@\"C:\\Path\\To\\Log\\FileMonitorLog.txt\", $\"{DateTime.Now}: {message}{Environment.NewLine}\");\n    }\n\n    public static void Main()\n    {\n        // Run the service\n        ServiceBase.Run(new FileMonitorService());\n    }\n}\n```\n\n### Comments:\n- **FileSystemWatcher**: Used to monitor file system changes in real-time.\n- **OnStart**: Initializes the watcher and sets up event handlers.\n- **OnStop**: Disables the watcher when the service stops.\n- **Event Handlers**: Handle file creation, deletion, changes, and renaming.\n- **LogChange**: Logs changes to a file for tracking purposes.\n- **Main**: Entry point to run the service.",
    "label": 1
  },
  {
    "codes": "```swift\nimport SpriteKit\n\nfunc animatePuzzlePiece(_ piece: SKSpriteNode) {\n    // Scale the piece up slightly to emphasize selection.\n    let scaleUp = SKAction.scale(to: 1.1, duration: 0.2)\n    \n    // Rotate the piece slightly for a dynamic effect.\n    let rotateLeft = SKAction.rotate(byAngle: .pi / 16, duration: 0.15)\n    let rotateRight = SKAction.rotate(byAngle: -.pi / 16, duration: 0.15)\n    \n    // Combine scale and rotation to make the piece pop.\n    let groupAction = SKAction.group([scaleUp, rotateLeft, rotateRight])\n    \n    // Return to original scale and orientation.\n    let scaleDown = SKAction.scale(to: 1.0, duration: 0.2)\n    let rotateBack = SKAction.rotate(toAngle: 0, duration: 0.2)\n    \n    // Add sound effect for placing the piece.\n    let playSound = SKAction.playSoundFileNamed(\"PiecePlace.wav\", waitForCompletion: false)\n    \n    // Create a sequence to run actions in order.\n    let sequence = SKAction.sequence([groupAction, playSound, scaleDown, rotateBack])\n    \n    // Execute the sequence on the puzzle piece.\n    piece.run(sequence)\n}\n```\n",
    "label": 1
  },
  {
    "codes": "func (c *Glue) ImportCatalogToGlueWithContext(ctx aws.Context, input *ImportCatalogToGlueInput, opts ...request.Option) (*ImportCatalogToGlueOutput, error) {\n\treq, out := c.ImportCatalogToGlueRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "def prepare_image(self, image):\n        \"\"\"Returns image of shape `(256, 256, 3)`, as expected by\n        `transform` when `classify_direct = True`.\n        \"\"\"\n        from decaf.util import transform  # soft dep\n        _JEFFNET_FLIP = True\n\n        # first, extract the 256x256 center.\n        image = transform.scale_and_extract(transform.as_rgb(image), 256)\n        # convert to [0,255] float32\n        image = image.astype(np.float32) * 255.\n        if _JEFFNET_FLIP:\n            # Flip the image if necessary, maintaining the c_contiguous order\n            image = image[::-1, :].copy()\n        # subtract the mean\n        image -= self.net_._data_mean\n        return image",
    "label": 0
  },
  {
    "codes": "def build_year(self, dt):\n        \"\"\"\n        Build the page for the provided year.\n        \"\"\"\n        self.year = str(dt.year)\n        logger.debug(\"Building %s\" % self.year)\n        self.request = self.create_request(self.get_url())\n        target_path = self.get_build_path()\n        self.build_file(target_path, self.get_content())",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<List<SasDefinitionItem>> getSasDefinitionsNextAsync(final String nextPageLink, final ServiceFuture<List<SasDefinitionItem>> serviceFuture, final ListOperationCallback<SasDefinitionItem> serviceCallback) {\n        return AzureServiceFuture.fromPageResponse(\n            getSasDefinitionsNextSinglePageAsync(nextPageLink),\n            new Func1<String, Observable<ServiceResponse<Page<SasDefinitionItem>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<SasDefinitionItem>>> call(String nextPageLink) {\n                    return getSasDefinitionsNextSinglePageAsync(nextPageLink);\n                }\n            },\n            serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "def resourceprep(string, allow_unassigned=False):\n    \"\"\"\n    Process the given `string` using the Resourceprep (`RFC 6122`_) profile. In\n    the error cases defined in `RFC 3454`_ (stringprep), a :class:`ValueError`\n    is raised.\n    \"\"\"\n\n    chars = list(string)\n    _resourceprep_do_mapping(chars)\n    do_normalization(chars)\n    check_prohibited_output(\n        chars,\n        (\n            stringprep.in_table_c12,\n            stringprep.in_table_c21,\n            stringprep.in_table_c22,\n            stringprep.in_table_c3,\n            stringprep.in_table_c4,\n            stringprep.in_table_c5,\n            stringprep.in_table_c6,\n            stringprep.in_table_c7,\n            stringprep.in_table_c8,\n            stringprep.in_table_c9,\n        ))\n    check_bidi(chars)\n\n    if not allow_unassigned:\n        check_unassigned(\n            chars,\n            (\n                stringprep.in_table_a1,\n            )\n        )\n\n    return \"\".join(chars)",
    "label": 0
  },
  {
    "codes": "func (c *Glue) GetTableVersion(input *GetTableVersionInput) (*GetTableVersionOutput, error) {\n\treq, out := c.GetTableVersionRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```typescript\nimport { Injectable } from '@angular/core';\nimport { HttpClient, HttpParams } from '@angular/common/http';\nimport { Observable } from 'rxjs';\n\n@Injectable({\n  providedIn: 'root'\n})\nexport class PaginatedDataService {\n  private apiUrl = 'https://api.example.com/data'; // Replace with your API endpoint\n\n  constructor(private http: HttpClient) {}\n\n  // Fetch paginated data from the API\n  getPaginatedData(page: number, pageSize: number): Observable<any> {\n    // Set up query parameters for pagination\n    const params = new HttpParams()\n      .set('page', page.toString())\n      .set('pageSize', pageSize.toString());\n\n    // Make the HTTP GET request with the pagination parameters\n    return this.http.get<any>(this.apiUrl, { params });\n  }\n}\n```\n\n```typescript\n// Example usage in a component\nimport { Component, OnInit } from '@angular/core';\nimport { PaginatedDataService } from './paginated-data.service';\n\n@Component({\n  selector: 'app-data-list',\n  templateUrl: './data-list.component.html',\n  styleUrls: ['./data-list.component.css']\n})\nexport class DataListComponent implements OnInit {\n  data: any[] = [];\n  currentPage = 1;\n  pageSize = 10;\n\n  constructor(private paginatedDataService: PaginatedDataService) {}\n\n  ngOnInit(): void {\n    this.loadData();\n  }\n\n  loadData(): void {\n    this.paginatedDataService.getPaginatedData(this.currentPage, this.pageSize)\n      .subscribe(response => {\n        this.data = response.data; // Assuming the API returns data in a 'data' field\n      });\n  }\n\n  onPageChange(newPage: number): void {\n    this.currentPage = newPage;\n    this.loadData();\n  }\n}\n```\n\n```html\n<!-- Example HTML template for pagination -->\n<div *ngFor=\"let item of data\">\n  {{ item.name }} <!-- Replace with your data structure -->\n</div>\n\n<button (click)=\"onPageChange(currentPage - 1)\" [disabled]=\"currentPage === 1\">Previous</button>\n<button (click)=\"onPageChange(currentPage + 1)\">Next</button>\n```",
    "label": 1
  },
  {
    "codes": "```c\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <openssl/err.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n// Error handling\n#define handle_error(msg) \\\n    do { \\\n        fprintf(stderr, \"%s\\n\", msg); \\\n        ERR_print_errors_fp(stderr); \\\n        exit(EXIT_FAILURE); \\\n    } while (0)\n\n// Encrypt plaintext using AES-GCM\nint aes_gcm_encrypt(const unsigned char *plaintext, size_t plaintext_len,\n                    const unsigned char *key, const unsigned char *iv,\n                    unsigned char *ciphertext, unsigned char *tag) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int ciphertext_len;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new()))\n        handle_error(\"Failed to create context\");\n\n    // Initialize the encryption operation\n    if (1 != EVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL))\n        handle_error(\"Failed to initialize encryption\");\n\n    // Set IV length\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_IVLEN, 12, NULL))\n        handle_error(\"Failed to set IV length\");\n\n    // Initialize key and IV\n    if (1 != EVP_EncryptInit_ex(ctx, NULL, NULL, key, iv))\n        handle_error(\"Failed to set key and IV\");\n\n    // Encrypt the plaintext\n    if (1 != EVP_EncryptUpdate(ctx, ciphertext, &len, plaintext, plaintext_len))\n        handle_error(\"Encryption failed\");\n    ciphertext_len = len;\n\n    // Finalize encryption\n    if (1 != EVP_EncryptFinal_ex(ctx, ciphertext + len, &len))\n        handle_error(\"Finalization failed\");\n    ciphertext_len += len;\n\n    // Get the tag\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_GET_TAG, 16, tag))\n        handle_error(\"Failed to get tag\");\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    return ciphertext_len;\n}\n\n// Decrypt ciphertext using AES-GCM\nint aes_gcm_decrypt(const unsigned char *ciphertext, size_t ciphertext_len,\n                    const unsigned char *key, const unsigned char *iv,\n                    const unsigned char *tag, unsigned char *plaintext) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int plaintext_len;\n    int ret;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new()))\n        handle_error(\"Failed to create context\");\n\n    // Initialize the decryption operation\n    if (1 != EVP_DecryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL))\n        handle_error(\"Failed to initialize decryption\");\n\n    // Set IV length\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_IVLEN, 12, NULL))\n        handle_error(\"Failed to set IV length\");\n\n    // Initialize key and IV\n    if (1 != EVP_DecryptInit_ex(ctx, NULL, NULL, key, iv))\n        handle_error(\"Failed to set key and IV\");\n\n    // Decrypt the ciphertext\n    if (1 != EVP_DecryptUpdate(ctx, plaintext, &len, ciphertext, ciphertext_len))\n        handle_error(\"Decryption failed\");\n    plaintext_len = len;\n\n    // Set expected tag value\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_TAG, 16, (void *)tag))\n        handle_error(\"Failed to set tag\");\n\n    // Finalize decryption\n    ret = EVP_DecryptFinal_ex(ctx, plaintext + len, &len);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    if (ret > 0) {\n        plaintext_len += len;\n        return plaintext_len;\n    } else {\n        handle_error(\"Tag verification failed\");\n        return -1;\n    }\n}\n\n// Example usage\nint main() {\n    unsigned char key[32]; // 256-bit key\n    unsigned char iv[12];  // 96-bit IV\n    unsigned char tag[16]; // 128-bit tag\n    unsigned char plaintext[] = \"Hello, AES-GCM!\";\n    unsigned char ciphertext[128];\n    unsigned char decryptedtext[128];\n\n    // Generate random key and IV\n    if (!RAND_bytes(key, sizeof(key)) || !RAND_bytes(iv, sizeof(iv)))\n        handle_error(\"Failed to generate random key or IV\");\n\n    // Encrypt\n    int ciphertext_len = aes_gcm_encrypt(plaintext, strlen((char *)plaintext), key, iv, ciphertext, tag);\n    printf(\"Ciphertext: \");\n    for (int i = 0; i < ciphertext_len; i++) printf(\"%02x\", ciphertext[i]);\n    printf(\"\\n\");\n\n    // Decrypt\n    int decryptedtext_len = aes_gcm_decrypt(ciphertext, ciphertext_len, key, iv, tag, decryptedtext);\n    decryptedtext[decryptedtext_len] = '\\0';\n    printf(\"Decrypted text: %s\\n\", decryptedtext);\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport os\nimport tempfile\nfrom contextlib import contextmanager\n\n@contextmanager\ndef atomic_write(file_path, mode='w', **kwargs):\n    # Create a temporary file with .tmp suffix\n    tmp_file_path = f\"{file_path}.tmp\"\n    try:\n        # Open the temporary file\n        with open(tmp_file_path, mode, **kwargs) as tmp_file:\n            yield tmp_file  # Yield the file object to the caller\n        # If no exceptions, rename the temporary file to the target file\n        os.replace(tmp_file_path, file_path)\n    except Exception:\n        # If an exception occurs, remove the temporary file\n        if os.path.exists(tmp_file_path):\n            os.remove(tmp_file_path)\n        raise  # Re-raise the exception\n```",
    "label": 1
  },
  {
    "codes": "def size(self):\n        \"\"\"Size is number of nodes under the trie, including the current node\"\"\"\n        if self.children:\n            return sum( ( c.size() for c in self.children.values() ) ) + 1\n        else:\n            return 1",
    "label": 0
  },
  {
    "codes": "```javascript\nconst express = require('express');\nconst bodyParser = require('body-parser');\nconst app = express();\nconst PORT = process.env.PORT || 3000;\n\n// Middleware\napp.use(bodyParser.json());\n\n// Fake user database\nconst users = [\n  { id: 1, username: 'user1', password: 'pass1' },\n  { id: 2, username: 'user2', password: 'pass2' }\n];\n\n// Login route\napp.post('/login', (req, res) => {\n  const { username, password } = req.body;\n  \n  // Check if username and password are provided\n  if (!username || !password) {\n    // 400 Bad Request: Username or password not provided\n    return res.status(400).json({ message: 'Username and password are required' });\n  }\n  \n  // Find the user with the matching username and password\n  const user = users.find(u => u.username === username && u.password === password);\n  \n  // If user not found, respond with an error\n  if (!user) {\n    // 401 Unauthorized: Invalid username or password\n    return res.status(401).json({ message: 'Invalid username or password' });\n  }\n\n  // If user is found, respond with success\n  // 200 OK: Login successful\n  res.status(200).json({ message: 'Login successful', userId: user.id });\n});\n\n// Start server\napp.listen(PORT, () => {\n  console.log(`Server running on port ${PORT}`);\n});\n```\n",
    "label": 1
  },
  {
    "codes": "```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <dirent.h>\n#include <string.h>\n#include <sys/stat.h>\n\nvoid listFilesRecursively(char *basePath) {\n    char path[1000];\n    struct dirent *dp;\n    DIR *dir = opendir(basePath);\n\n    // Check if directory can be opened\n    if (!dir)\n        return;\n\n    // Iterate over directory entries\n    while ((dp = readdir(dir)) != NULL) {\n        if (strcmp(dp->d_name, \".\") != 0 && strcmp(dp->d_name, \"..\") != 0) {\n            // Construct new path from our base path\n            strcpy(path, basePath);\n            strcat(path, \"/\");\n            strcat(path, dp->d_name);\n\n            // Print the file/directory path\n            printf(\"%s\\n\", path);\n\n            // Check if current entry is a directory\n            struct stat statbuf;\n            stat(path, &statbuf);\n            if (S_ISDIR(statbuf.st_mode)) {\n                // Recursively call with new path\n                listFilesRecursively(path);\n            }\n        }\n    }\n\n    // Close the directory stream\n    closedir(dir);\n}\n\nint main() {\n    // Specify the initial directory path\n    char basePath[100] = \".\";\n\n    // Recursively list all files starting from base directory\n    listFilesRecursively(basePath);\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "def delta_e_cie1976(color1, color2):\n    \"\"\"\n    Calculates the Delta E (CIE1976) of two colors.\n    \"\"\"\n    color1_vector = _get_lab_color1_vector(color1)\n    color2_matrix = _get_lab_color2_matrix(color2)\n    delta_e = color_diff_matrix.delta_e_cie1976(color1_vector, color2_matrix)[0]\n    return numpy.asscalar(delta_e)",
    "label": 0
  },
  {
    "codes": "func (s *JobUpdate) SetConnections(v *ConnectionsList) *JobUpdate {\n\ts.Connections = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *GetConnectionsInput) SetNextToken(v string) *GetConnectionsInput {\n\ts.NextToken = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function sortTopologically (originalOrder, edgesById) {\n  const sorted = []\n  const marked = new Set()\n\n  const visit = (mark) => {\n    if (marked.has(mark)) return\n    marked.add(mark)\n    const edges = edgesById.get(mark)\n    if (edges != null) {\n      edges.forEach(visit)\n    }\n    sorted.push(mark)\n  }\n\n  originalOrder.forEach(visit)\n  return sorted\n}",
    "label": 0
  },
  {
    "codes": "def security_layer(tls_provider, sasl_providers):\n    \"\"\"\n    .. deprecated:: 0.6\n\n       Replaced by :class:`SecurityLayer`.\n\n    Return a configured :class:`SecurityLayer`. `tls_provider` must be a\n    :class:`STARTTLSProvider`.\n\n    The return value can be passed to the constructor of\n    :class:`~.node.Client`.\n\n    Some very basic checking on the input is also performed.\n    \"\"\"\n\n    sasl_providers = tuple(sasl_providers)\n\n    if not sasl_providers:\n        raise ValueError(\"At least one SASL provider must be given.\")\n    for sasl_provider in sasl_providers:\n        sasl_provider.execute  # check that sasl_provider has execute method\n\n    result = SecurityLayer(\n        tls_provider.ssl_context_factory,\n        tls_provider.certificate_verifier_factory,\n        tls_provider.tls_required,\n        sasl_providers\n    )\n\n    return result",
    "label": 0
  },
  {
    "codes": "def save(filepath, obj, compressed=True):\n    \"\"\"\n    Save the object to a .npz file.\n\n    Parameters\n    ----------\n    filepath : str\n        The path to save the file.\n    obj: `pypianoroll.Multitrack` objects\n        The object to be saved.\n\n    \"\"\"\n    if not isinstance(obj, Multitrack):\n        raise TypeError(\"Support only `pypianoroll.Multitrack` class objects\")\n    obj.save(filepath, compressed)",
    "label": 0
  },
  {
    "codes": "func (s *BatchGetCrawlersInput) SetCrawlerNames(v []*string) *BatchGetCrawlersInput {\n\ts.CrawlerNames = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport time\nimport functools\n\ndef measure_time_ns(func):\n    @functools.wraps(func)\n    def wrapper_measure_time_ns(*args, **kwargs):\n        start_time = time.perf_counter_ns()  # Start time in nanoseconds\n        result = func(*args, **kwargs)\n        end_time = time.perf_counter_ns()    # End time in nanoseconds\n        execution_time_ns = end_time - start_time  # Calculate execution time in nanoseconds\n        print(f\"Function '{func.__name__}' executed in {execution_time_ns} ns\")\n        return result\n    return wrapper_measure_time_ns\n\n# Example usage:\n@measure_time_ns\ndef example_function():\n    # Some code that takes time to execute\n    for _ in range(1000):\n        pass\n\n# Call the decorated function\nexample_function()\n```",
    "label": 1
  },
  {
    "codes": "def from_value(self, instance, value):\n        \"\"\"\n        Convert the given value using the set `type_` and store it into\n        `instance`\u2019 attribute.\n        \"\"\"\n        try:\n            parsed = self.type_.parse(value)\n        except (TypeError, ValueError):\n            if self.erroneous_as_absent:\n                return False\n            raise\n        self._set_from_recv(instance, parsed)\n        return True",
    "label": 0
  },
  {
    "codes": "```java\nimport javafx.application.Application;\nimport javafx.scene.Scene;\nimport javafx.scene.canvas.Canvas;\nimport javafx.scene.canvas.GraphicsContext;\nimport javafx.scene.input.MouseEvent;\nimport javafx.scene.layout.StackPane;\nimport javafx.scene.paint.Color;\nimport javafx.stage.Stage;\n\npublic class FractalApp extends Application {\n\n    private final int canvasWidth = 800;\n    private final int canvasHeight = 800;\n\n    public static void main(String[] args) {\n        launch(args);\n    }\n\n    @Override\n    public void start(Stage primaryStage) {\n        Canvas canvas = new Canvas(canvasWidth, canvasHeight);\n        GraphicsContext gc = canvas.getGraphicsContext2D();\n\n        drawFractal(gc, 0, 0, canvasWidth, canvasHeight, 5);\n\n        canvas.addEventHandler(MouseEvent.MOUSE_CLICKED, event -> {\n            gc.clearRect(0, 0, canvasWidth, canvasHeight);\n            drawFractal(gc, 0, 0, canvasWidth, canvasHeight, (int)(Math.random() * 6 + 3));\n        });\n\n        StackPane root = new StackPane(canvas);\n        Scene scene = new Scene(root, canvasWidth, canvasHeight);\n        primaryStage.setScene(scene);\n        primaryStage.setTitle(\"Interactive Fractal Patterns\");\n        primaryStage.show();\n    }\n\n    private void drawFractal(GraphicsContext gc, double x, double y, double width, double height, int depth) {\n        if (depth == 0) {\n            gc.setFill(Color.color(Math.random(), Math.random(), Math.random()));\n            gc.fillRect(x, y, width, height);\n        } else {\n            double newWidth = width / 3;\n            double newHeight = height / 3;\n\n            for (int dx = 0; dx < 3; dx++) {\n                for (int dy = 0; dy < 3; dy++) {\n                    if (dx != 1 || dy != 1) {\n                        drawFractal(gc, x + dx * newWidth, y + dy * newHeight, newWidth, newHeight, depth - 1);\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\nComments:\n1. A `JavaFX` application is created by extending the `Application` class.\n2. A `Canvas` is used to draw graphics, with dimensions defined in `canvasWidth` and `canvasHeight`.\n3. The `drawFractal` method recursively draws a fractal pattern by dividing the current area into 9 smaller squares but only draws in 8 of them, leaving the center clear.\n4. Mouse click events are handled by clearing the canvas and redrawing the fractal with a random depth between 3 and 8.\n5. The base case of recursion fills a rectangle with colors created from random RGB values.\n6. A `StackPane` layout is used to contain the `Canvas`, and it's added to a `Scene`. \n7. The `Stage` is set with the title \"Interactive Fractal Patterns\" and shown, starting the application.",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\n\t\"github.com/pelletier/go-toml\"\n\t\"gopkg.in/yaml.v3\"\n\t\"encoding/json\"\n)\n\nfunc main() {\n\tif len(os.Args) != 4 {\n\t\tfmt.Println(\"Usage: convert <input-file> <output-file> <output-format>\")\n\t\tos.Exit(1)\n\t}\n\n\tinputFile := os.Args[1]\n\toutputFile := os.Args[2]\n\toutputFormat := os.Args[3]\n\n\tdata, err := ioutil.ReadFile(inputFile)\n\tif err != nil {\n\t\tfmt.Println(\"Error reading input file:\", err)\n\t\tos.Exit(1)\n\t}\n\n\tformat := detectFormat(inputFile)\n\tif format == \"\" {\n\t\tfmt.Println(\"Unsupported input format.\")\n\t\tos.Exit(1)\n\t}\n\n\tvar intermediate interface{}\n\n\tswitch format {\n\tcase \"yaml\":\n\t\terr = yaml.Unmarshal(data, &intermediate)\n\tcase \"toml\":\n\t\terr = toml.Unmarshal(data, &intermediate)\n\tcase \"json\":\n\t\terr = json.Unmarshal(data, &intermediate)\n\t}\n\n\tif err != nil {\n\t\tfmt.Println(\"Error parsing input file:\", err)\n\t\tos.Exit(1)\n\t}\n\n\tvar outputData []byte\n\tswitch outputFormat {\n\tcase \"yaml\":\n\t\toutputData, err = yaml.Marshal(intermediate)\n\tcase \"toml\":\n\t\toutputData, err = toml.Marshal(intermediate)\n\tcase \"json\":\n\t\toutputData, err = json.MarshalIndent(intermediate, \"\", \"  \")\n\tdefault:\n\t\tfmt.Println(\"Unsupported output format.\")\n\t\tos.Exit(1)\n\t}\n\n\tif err != nil {\n\t\tfmt.Println(\"Error serializing output data:\", err)\n\t\tos.Exit(1)\n\t}\n\n\terr = ioutil.WriteFile(outputFile, outputData, 0644)\n\tif err != nil {\n\t\tfmt.Println(\"Error writing output file:\", err)\n\t\tos.Exit(1)\n\t}\n\n\tfmt.Println(\"Conversion successful.\")\n}\n\nfunc detectFormat(filename string) string {\n\tif len(filename) < 4 {\n\t\treturn \"\"\n\t}\n\n\tswitch filename[len(filename)-4:] {\n\tcase \"yaml\", \"yml\":\n\t\treturn \"yaml\"\n\tcase \"json\":\n\t\treturn \"json\"\n\tcase \"toml\":\n\t\treturn \"toml\"\n\tdefault:\n\t\treturn \"\"\n\t}\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Chaos Engineering Experiment: Random Pod Deletion in Kubernetes\n\n# Set the namespace (change as needed)\nNAMESPACE=\"default\"\n\n# Get a random pod from the namespace\nRANDOM_POD=$(kubectl get pods -n $NAMESPACE -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\\n' | shuf -n 1)\n\n# Check if a pod was found\nif [ -z \"$RANDOM_POD\" ]; then\n  echo \"No pods found in namespace $NAMESPACE.\"\n  exit 1\nfi\n\n# Log the pod to be deleted\necho \"Deleting pod: $RANDOM_POD in namespace $NAMESPACE\"\n\n# Delete the pod\nkubectl delete pod $RANDOM_POD -n $NAMESPACE\n\n# Verify deletion\nif kubectl get pod $RANDOM_POD -n $NAMESPACE &> /dev/null; then\n  echo \"Pod $RANDOM_POD still exists. Deletion failed.\"\n  exit 1\nelse\n  echo \"Pod $RANDOM_POD successfully deleted.\"\nfi\n\n# Monitor pod recovery (optional)\necho \"Monitoring pod recovery...\"\nkubectl get pods -n $NAMESPACE -w\n```",
    "label": 1
  },
  {
    "codes": "def get_file_mode_for_reading(context):\n    \"\"\"Get file mode for reading from tar['format'].\n\n    This should return r:*, r:gz, r:bz2 or r:xz. If user specified something\n    wacky in tar.Format, that's their business.\n\n    In theory r:* will auto-deduce the correct format.\n    \"\"\"\n    format = context['tar'].get('format', None)\n\n    if format or format == '':\n        mode = f\"r:{context.get_formatted_string(format)}\"\n    else:\n        mode = 'r:*'\n\n    return mode",
    "label": 0
  },
  {
    "codes": "function Snapsie() {\n    // private methods\n    \n    function isQuirksMode(inDocument) {\n        return (inDocument.compatMode == 'BackCompat');\n    }\n    \n    function getDrawableElement(inDocument) {\n        if (isQuirksMode(inDocument)) {\n            var body = inDocument.getElementsByTagName('body')[0];\n            return body;\n        }\n        else {\n            // standards mode\n            return inDocument.documentElement;\n        }\n    }\n    \n    /**\n     * Returns the canonical Windows path for a given path. This means\n     * basically replacing any forwards slashes with backslashes.\n     *\n     * @param path  the path whose canonical form to return\n     */\n    function getCanonicalPath(path) {\n        path = path.replace(/\\//g, '\\\\');\n        path = path.replace(/\\\\\\\\/g, '\\\\');\n        return path;\n    }\n\n    // public methods\n    \n    /**\n     * Saves a screenshot of the current document to a file. If frameId is\n     * specified, a screenshot of just the frame is captured instead.\n     *\n     * @param outputFile  the file to which to save the screenshot\n     * @param frameId     the frame to capture; omit to capture entire document\n     */\n    this.saveSnapshot = function(outputFile, frameId) {\n        var drawableElement = getDrawableElement(document);\n        var drawableInfo = {\n              overflow  : drawableElement.style.overflow\n            , scrollLeft: drawableElement.scrollLeft\n            , scrollTop : drawableElement.scrollTop\n        };\n        drawableElement.style.overflow = 'hidden';\n        \n        var capturableDocument;\n        var frameBCR = { left: 0, top: 0 };\n        if (!frameId) {\n            capturableDocument = document;\n        }\n        else {\n            var frame = document.getElementById(frameId);\n            capturableDocument = frame.document;\n            \n            // scroll as much of the frame into view as possible\n            frameBCR = frame.getBoundingClientRect();\n            window.scroll(frameBCR.left, frameBCR.top);\n            frameBCR = frame.getBoundingClientRect();\n        }\n        \n        var nativeObj = new ActiveXObject('Snapsie.CoSnapsie');\n        nativeObj.saveSnapshot(\n            getCanonicalPath(outputFile),\n            frameId,\n            drawableElement.scrollWidth,\n            drawableElement.scrollHeight,\n            drawableElement.clientWidth,\n            drawableElement.clientHeight,\n            drawableElement.clientLeft,\n            drawableElement.clientTop,\n            frameBCR.left,\n            frameBCR.top\n        );\n        \n        // revert\n        \n        drawableElement.style.overflow = drawableInfo.overflow;\n        drawableElement.scrollLeft = drawableInfo.scrollLeft;\n        drawableElement.scrollTop = drawableInfo.scrollTop;\n    }\n}",
    "label": 0
  },
  {
    "codes": "func (s *GetCrawlerMetricsInput) SetMaxResults(v int64) *GetCrawlerMetricsInput {\n\ts.MaxResults = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```rust\nuse serde_json::Value;\nuse std::collections::HashMap;\nuse std::env;\nuse std::fs;\nuse std::process;\n\nfn main() {\n    // Collect command line arguments\n    let args: Vec<String> = env::args().collect();\n    if args.len() < 3 {\n        eprintln!(\"Usage: my_json_parser <input-file> <query>\");\n        process::exit(1);\n    }\n    \n    // Read input JSON file\n    let file_content = fs::read_to_string(&args[1]).expect(\"Could not read file\");\n    let json_data: Value = serde_json::from_str(&file_content).expect(\"Invalid JSON format\");\n\n    // Parse the query string\n    let query = &args[2];\n    let keys: Vec<&str> = query.split('.').collect();\n    \n    // Traverse the JSON using the query keys\n    match query_json(&json_data, &keys) {\n        Some(result) => println!(\"{}\", result),\n        None => eprintln!(\"Key not found in JSON\"),\n    }\n}\n\n// Recursive function to query JSON structure\nfn query_json<'a>(data: &'a Value, keys: &[&str]) -> Option<&'a Value> {\n    if keys.is_empty() {\n        return Some(data);\n    }\n\n    match data {\n        Value::Object(map) => map.get(keys[0]).and_then(|v| query_json(v, &keys[1..])),\n        Value::Array(arr) => {\n            if let Ok(index) = keys[0].parse::<usize>() {\n                arr.get(index).and_then(|v| query_json(v, &keys[1..]))\n            } else {\n                None\n            }\n        }\n        _ => None,\n    }\n}\n\n```\n",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define log file path\nLOG_FILE=\"/var/log/apache2/access.log\"\n\n# Parse logs and count 5xx errors per minute\ngrep $(date +\"%d/%b/%Y:%H:%M\" --date='1 minute ago') $LOG_FILE | awk '{ if ($9 >= 500 && $9 < 600) print $9 }' | sort | uniq -c | awk '{ if ($1 > 10) print \"Spike detected: \" $1 \" 5xx errors in the last minute.\" }'\n\n# Check for spikes in the last 5 minutes\nfor i in {1..5}; do\n    grep $(date +\"%d/%b/%Y:%H:%M\" --date=\"$i minute ago\") $LOG_FILE | awk '{ if ($9 >= 500 && $9 < 600) print $9 }' | sort | uniq -c | awk -v interval=\"$i\" '{ if ($1 > 10) print \"Spike detected: \" $1 \" 5xx errors \" interval \" minute(s) ago.\" }'\ndone\n```\n\n### Comments:\n- `LOG_FILE`: Path to the Apache access log file.\n- `grep`: Filters logs for the specified timestamp.\n- `awk`: Extracts and checks for 5xx status codes.\n- `sort | uniq -c`: Counts occurrences of each 5xx error.\n- `awk`: Detects spikes if errors exceed a threshold (e.g., 10 in a minute).\n- Loop: Checks for spikes in the last 5 minutes.",
    "label": 1
  },
  {
    "codes": "def song_detail(ids):\n    \"\"\"\u901a\u8fc7\u6b4c\u66f2 ID \u83b7\u53d6\u6b4c\u66f2\u7684\u8be6\u7ec6\u4fe1\u606f\n\n    :param ids: \u6b4c\u66f2 ID \u7684 list\n    \"\"\"\n    if not isinstance(ids, list):\n        raise ParamsError()\n    c = []\n    for id in ids:\n        c.append({'id': id})\n    r = NCloudBot()\n    r.method = 'SONG_DETAIL'\n    r.data = {'c': json.dumps(c), 'ids': c, \"csrf_token\": \"\"}\n    r.send()\n\n    return r.response",
    "label": 0
  },
  {
    "codes": "def common_axis_properties(self, color=None, title_size=None):\n        \"\"\"Set common axis properties such as color\n\n        Parameters\n        ----------\n        color: str, default None\n            Hex color str, etc\n        \"\"\"\n        if self.axes:\n            for axis in self.axes:\n                self._set_axis_properties(axis)\n                self._set_all_axis_color(axis, color)\n                if title_size:\n                    ref = ValueRef(value=title_size)\n                    axis.properties.title.font_size = ref\n        else:\n            raise ValueError('This Visualization has no axes!')\n        return self",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<Void> deleteFromTaskAsync(String jobId, String taskId, String filePath, Boolean recursive, FileDeleteFromTaskOptions fileDeleteFromTaskOptions, final ServiceCallback<Void> serviceCallback) {\n        return ServiceFuture.fromHeaderResponse(deleteFromTaskWithServiceResponseAsync(jobId, taskId, filePath, recursive, fileDeleteFromTaskOptions), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```swift\n// Define the puzzle piece node\nlet puzzlePiece = SKSpriteNode(imageNamed: \"puzzlePiece\")\n\n// Define the initial position and scale\npuzzlePiece.position = CGPoint(x: 100, y: 100)\npuzzlePiece.setScale(0.5)\n\n// Define the actions for the puzzle piece animation\nlet moveAction = SKAction.move(to: CGPoint(x: 300, y: 300), duration: 1.0)\nlet rotateAction = SKAction.rotate(byAngle: .pi / 2, duration: 1.0)\nlet scaleAction = SKAction.scale(to: 1.0, duration: 1.0)\nlet fadeInAction = SKAction.fadeIn(withDuration: 1.0)\n\n// Create a sequence of actions\nlet sequence = SKAction.sequence([moveAction, rotateAction, scaleAction, fadeInAction])\n\n// Run the sequence on the puzzle piece\npuzzlePiece.run(sequence)\n```",
    "label": 1
  },
  {
    "codes": "function() {\n    var key,\n        i;\n\n    for (key in this.series) {\n      for (i = 0; i < this.series[key].length; i++) {\n        this.series[key][i].clear();\n      }\n    }\n    this.scale = this.baseScale;\n    this.transX = this.baseTransX;\n    this.transY = this.baseTransY;\n    this.applyTransform();\n  }",
    "label": 0
  },
  {
    "codes": "def view(url: str, **kwargs) -> bool:\n    \"\"\"\n    View the page whether rendered properly. (ensure the <base> tag to make external links work)\n\n    Args:\n        url (str): The url of the site.\n    \"\"\"\n    kwargs.setdefault('headers', DEFAULT_HEADERS)\n    html = requests.get(url, **kwargs).content\n    if b'<base' not in html:\n        repl = f'<head><base href=\"{url}\">'\n        html = html.replace(b'<head>', repl.encode('utf-8'))\n    fd, fname = tempfile.mkstemp('.html')\n    os.write(fd, html)\n    os.close(fd)\n    return webbrowser.open(f'file://{fname}')",
    "label": 0
  },
  {
    "codes": "function nthChild($element, $arguments, $traverse) {\n\tswitch ($arguments) {\n\t\tcase \"n\": return true;\n\t\tcase \"even\": $arguments = \"2n\"; break;\n\t\tcase \"odd\": $arguments = \"2n+1\";\n\t}\n\n\tvar $$children = childElements($element.parentNode);\n\tfunction _checkIndex($index) {\n\t\tvar $index = ($traverse == nextElementSibling) ? $$children.length - $index : $index - 1;\n\t\treturn $$children[$index] == $element;\n\t};\n\n\t//\tit was just a number (no \"n\")\n\tif (!isNaN($arguments)) return _checkIndex($arguments);\n\n\t$arguments = $arguments.split(\"n\");\n\tvar $multiplier = parseInt($arguments[0]);\n\tvar $step = parseInt($arguments[1]);\n\n\tif ((isNaN($multiplier) || $multiplier == 1) && $step == 0) return true;\n\tif ($multiplier == 0 && !isNaN($step)) return _checkIndex($step);\n\tif (isNaN($step)) $step = 0;\n\n\tvar $count = 1;\n\twhile ($element = $traverse($element)) $count++;\n\n\tif (isNaN($multiplier) || $multiplier == 1)\n\t\treturn ($traverse == nextElementSibling) ? ($count <= $step) : ($step >= $count);\n\n\treturn ($count % $multiplier) == $step;\n}",
    "label": 0
  },
  {
    "codes": "function prepareSymbolSize(\n    data, dataIndex, layout, symbolRepeat, symbolClip, boundingLength,\n    pxSign, symbolPatternSize, opt, output\n) {\n    var valueDim = opt.valueDim;\n    var categoryDim = opt.categoryDim;\n    var categorySize = Math.abs(layout[categoryDim.wh]);\n\n    var symbolSize = data.getItemVisual(dataIndex, 'symbolSize');\n    if (zrUtil.isArray(symbolSize)) {\n        symbolSize = symbolSize.slice();\n    }\n    else {\n        if (symbolSize == null) {\n            symbolSize = '100%';\n        }\n        symbolSize = [symbolSize, symbolSize];\n    }\n\n    // Note: percentage symbolSize (like '100%') do not consider lineWidth, because it is\n    // to complicated to calculate real percent value if considering scaled lineWidth.\n    // So the actual size will bigger than layout size if lineWidth is bigger than zero,\n    // which can be tolerated in pictorial chart.\n\n    symbolSize[categoryDim.index] = parsePercent(\n        symbolSize[categoryDim.index],\n        categorySize\n    );\n    symbolSize[valueDim.index] = parsePercent(\n        symbolSize[valueDim.index],\n        symbolRepeat ? categorySize : Math.abs(boundingLength)\n    );\n\n    output.symbolSize = symbolSize;\n\n    // If x or y is less than zero, show reversed shape.\n    var symbolScale = output.symbolScale = [\n        symbolSize[0] / symbolPatternSize,\n        symbolSize[1] / symbolPatternSize\n    ];\n    // Follow convention, 'right' and 'top' is the normal scale.\n    symbolScale[valueDim.index] *= (opt.isHorizontal ? -1 : 1) * pxSign;\n}",
    "label": 0
  },
  {
    "codes": "```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# Define the pipeline\npipeline = Pipeline(steps=[\n    ('imputation', SimpleImputer(strategy='mean')),  # Step to handle missing data\n    ('scaling', StandardScaler())                    # Step to scale features\n])\n\n# 'imputation' step addresses missing data by replacing it with the mean of each column\n# 'scaling' step scales features to have zero mean and unit variance\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport org.junit.jupiter.api.Test;\nimport org.junit.jupiter.api.BeforeEach;\nimport static org.junit.jupiter.api.Assertions.*;\n\nimport java.util.Arrays;\n\npublic class SortingAlgorithmTest {\n\n    private SortingAlgorithm sorter;\n\n    @BeforeEach\n    public void setUp() {\n        sorter = new SortingAlgorithm(); // Assume SortingAlgorithm is the class with the sorting method\n    }\n\n    @Test\n    public void testEmptyArray() {\n        int[] input = {};\n        int[] expected = {};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Empty array should remain unchanged\");\n    }\n\n    @Test\n    public void testSingleElementArray() {\n        int[] input = {42};\n        int[] expected = {42};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Single element array should remain unchanged\");\n    }\n\n    @Test\n    public void testAlreadySortedArray() {\n        int[] input = {1, 2, 3, 4, 5};\n        int[] expected = {1, 2, 3, 4, 5};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Already sorted array should remain unchanged\");\n    }\n\n    @Test\n    public void testReverseSortedArray() {\n        int[] input = {5, 4, 3, 2, 1};\n        int[] expected = {1, 2, 3, 4, 5};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Reverse sorted array should be sorted in ascending order\");\n    }\n\n    @Test\n    public void testArrayWithDuplicates() {\n        int[] input = {4, 2, 4, 1, 2};\n        int[] expected = {1, 2, 2, 4, 4};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Array with duplicates should be sorted correctly\");\n    }\n\n    @Test\n    public void testArrayWithNegativeNumbers() {\n        int[] input = {-3, -1, -2, -4};\n        int[] expected = {-4, -3, -2, -1};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Array with negative numbers should be sorted correctly\");\n    }\n\n    @Test\n    public void testArrayWithMixedNumbers() {\n        int[] input = {0, -1, 2, -3, 4};\n        int[] expected = {-3, -1, 0, 2, 4};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Array with mixed positive and negative numbers should be sorted correctly\");\n    }\n\n    @Test\n    public void testLargeArray() {\n        int[] input = new int[1000];\n        for (int i = 0; i < input.length; i++) {\n            input[i] = input.length - i - 1;\n        }\n        int[] expected = new int[1000];\n        for (int i = 0; i < expected.length; i++) {\n            expected[i] = i;\n        }\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Large array should be sorted correctly\");\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *CodeGenNodeArg) SetValue(v string) *CodeGenNodeArg {\n\ts.Value = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<TaskInner> beginUpdateAsync(String resourceGroupName, String registryName, String taskName, TaskUpdateParameters taskUpdateParameters, final ServiceCallback<TaskInner> serviceCallback) {\n        return ServiceFuture.fromResponse(beginUpdateWithServiceResponseAsync(resourceGroupName, registryName, taskName, taskUpdateParameters), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "function fixBadRegex(grammar) {\n\tconst scopeResolution = grammar.repository['scope-resolution'];\n\tif (scopeResolution) {\n\t\tconst match = scopeResolution.patterns[0].match;\n\t\tif (match === '(?i)([a-z_\\\\x{7f}-\\\\x{7fffffff}\\\\\\\\][a-z0-9_\\\\x{7f}-\\\\x{7fffffff}\\\\\\\\]*)(?=\\\\s*::)') {\n\t\t\tscopeResolution.patterns[0].match = '([A-Za-z_\\\\x{7f}-\\\\x{7fffffff}\\\\\\\\][A-Za-z0-9_\\\\x{7f}-\\\\x{7fffffff}\\\\\\\\]*)(?=\\\\s*::)';\n\t\t\treturn;\n\t\t}\n\t}\n\n\tthrow new Error(`fixBadRegex callback couldn't patch the regex. It may be obsolete`);\n}",
    "label": 0
  },
  {
    "codes": "public Observable<Void> resetAADProfileAsync(String resourceGroupName, String resourceName, ManagedClusterAADProfile parameters) {\n        return resetAADProfileWithServiceResponseAsync(resourceGroupName, resourceName, parameters).map(new Func1<ServiceResponse<Void>, Void>() {\n            @Override\n            public Void call(ServiceResponse<Void> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\n\t\"github.com/segmentio/kafka-go\"\n)\n\n// worker processes messages from the input channel and sends results to the output channel\nfunc worker(id int, in <-chan kafka.Message, out chan<- string, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\tfor msg := range in {\n\t\t// Simulate processing\n\t\tresult := fmt.Sprintf(\"Worker %d processed message: %s\", id, string(msg.Value))\n\t\tout <- result\n\t}\n}\n\n// fanIn collects results from multiple workers into a single channel\nfunc fanIn(out chan<- string, ins ...<-chan string) {\n\tvar wg sync.WaitGroup\n\twg.Add(len(ins))\n\n\tfor _, in := range ins {\n\t\tgo func(ch <-chan string) {\n\t\t\tfor result := range ch {\n\t\t\t\tout <- result\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(in)\n\t}\n\n\twg.Wait()\n\tclose(out)\n}\n\nfunc main() {\n\t// Kafka reader configuration\n\treader := kafka.NewReader(kafka.ReaderConfig{\n\t\tBrokers:   []string{\"localhost:9092\"},\n\t\tTopic:     \"test-topic\",\n\t\tPartition: 0,\n\t\tMinBytes:  10e3, // 10KB\n\t\tMaxBytes:  10e6, // 10MB\n\t})\n\tdefer reader.Close()\n\n\t// Channels for fan-out/fan-in\n\tmsgChan := make(chan kafka.Message, 10)\n\tresultChans := make([]chan string, 5)\n\tfor i := range resultChans {\n\t\tresultChans[i] = make(chan string, 10)\n\t}\n\tfinalOut := make(chan string, 10)\n\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < 5; i++ {\n\t\twg.Add(1)\n\t\tgo worker(i, msgChan, resultChans[i], &wg)\n\t}\n\n\t// Fan-in results from workers\n\tgo fanIn(finalOut, resultChans...)\n\n\t// Read messages from Kafka and distribute to workers\n\tgo func() {\n\t\tfor {\n\t\t\tmsg, err := reader.ReadMessage(context.Background())\n\t\t\tif err != nil {\n\t\t\t\tfmt.Println(\"Error reading message:\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tmsgChan <- msg\n\t\t}\n\t}()\n\n\t// Collect and print final results\n\tfor result := range finalOut {\n\t\tfmt.Println(result)\n\t}\n\n\t// Wait for all workers to finish\n\twg.Wait()\n\tclose(msgChan)\n\tfor _, ch := range resultChans {\n\t\tclose(ch)\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\n// Import D3.js library (ensure it's included in your HTML)\n// <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n\n// Set up SVG dimensions\nconst width = 800;\nconst height = 400;\nconst margin = { top: 20, right: 30, bottom: 30, left: 40 };\n\n// Create SVG container\nconst svg = d3.select(\"body\")\n  .append(\"svg\")\n  .attr(\"width\", width)\n  .attr(\"height\", height);\n\n// Define scales\nconst xScale = d3.scaleTime().range([margin.left, width - margin.right]);\nconst yScale = d3.scaleLinear().range([height - margin.bottom, margin.top]);\n\n// Create axes\nconst xAxis = d3.axisBottom(xScale);\nconst yAxis = d3.axisLeft(yScale);\n\n// Append axes to SVG\nsvg.append(\"g\")\n  .attr(\"transform\", `translate(0,${height - margin.bottom})`)\n  .call(xAxis);\n\nsvg.append(\"g\")\n  .attr(\"transform\", `translate(${margin.left},0)`)\n  .call(yAxis);\n\n// Initialize data array\nlet data = [];\n\n// Function to fetch real-time cryptocurrency price\nasync function fetchPrice() {\n  const response = await fetch(\"https://api.coingecko.com/api/v3/simple/price?ids=bitcoin&vs_currencies=usd\");\n  const result = await response.json();\n  const price = result.bitcoin.usd;\n  const timestamp = new Date();\n  return { timestamp, price };\n}\n\n// Function to update the chart\nfunction updateChart(newData) {\n  // Update scales\n  xScale.domain(d3.extent(newData, d => d.timestamp));\n  yScale.domain([0, d3.max(newData, d => d.price)]);\n\n  // Update axes\n  svg.select(\".x-axis\").call(xAxis);\n  svg.select(\".y-axis\").call(yAxis);\n\n  // Create line generator\n  const line = d3.line()\n    .x(d => xScale(d.timestamp))\n    .y(d => yScale(d.price));\n\n  // Bind data to the line\n  svg.selectAll(\".line\")\n    .data([newData])\n    .join(\"path\")\n    .attr(\"class\", \"line\")\n    .attr(\"d\", line)\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"steelblue\")\n    .attr(\"stroke-width\", 2);\n}\n\n// Function to simulate real-time updates\nasync function simulateRealTime() {\n  const newPrice = await fetchPrice();\n  data.push(newPrice);\n\n  // Keep only the last 50 data points\n  if (data.length > 50) data.shift();\n\n  // Update the chart\n  updateChart(data);\n\n  // Schedule the next update\n  setTimeout(simulateRealTime, 1000);\n}\n\n// Start the real-time simulation\nsimulateRealTime();\n```",
    "label": 1
  },
  {
    "codes": "```typescript\nimport React from 'react';\n\n// Define the type for a Tree Node\ninterface TreeNode {\n  id: string;\n  label: string;\n  children?: TreeNode[];\n}\n\n// Props for the Tree component\ninterface TreeProps {\n  data: TreeNode[];\n}\n\n// Tree component\nconst Tree: React.FC<TreeProps> = ({ data }) => {\n  // Recursive function to render each node\n  const renderNode = (node: TreeNode): JSX.Element => {\n    return (\n      <li key={node.id}>\n        <div>{node.label}</div>\n        {node.children && (\n          <ul>\n            {node.children.map(child => renderNode(child))}\n          </ul>\n        )}\n      </li>\n    );\n  };\n\n  // Render the entire tree\n  return (\n    <ul>\n      {data.map(node => renderNode(node))}\n    </ul>\n  );\n};\n\nexport default Tree;\n\n/*\nUsage:\n\nconst treeData: TreeNode[] = [\n  {\n    id: '1',\n    label: 'Root',\n    children: [\n      { id: '2', label: 'Child 1' },\n      {\n        id: '3',\n        label: 'Child 2',\n        children: [\n          { id: '4', label: 'Grandchild 1' },\n        ]\n      }\n    ]\n  }\n];\n\n<Tree data={treeData} />;\n*/\n```",
    "label": 1
  },
  {
    "codes": "function transpileJavaScriptToPythonAndPHP (args) {\n\n    //-------------------------------------------------------------------------\n\n    // transpile JS \u2192 Python 3\n    let python3Body = transpileJavaScriptToPython3 (args)\n\n    //-------------------------------------------------------------------------\n\n    // remove await from Python 2 body (transpile Python 3 \u2192 Python 2)\n    let python2Body = transpilePython3ToPython2 (python3Body)\n\n    //-------------------------------------------------------------------------\n\n    // transpile JS \u2192 PHP\n    let phpBody = transpileJavaScriptToPHP (args)\n\n    return { python3Body, python2Body, phpBody }\n}",
    "label": 0
  },
  {
    "codes": "public JobResponseInner exportDevices(String resourceGroupName, String resourceName, ExportDevicesRequest exportDevicesParameters) {\n        return exportDevicesWithServiceResponseAsync(resourceGroupName, resourceName, exportDevicesParameters).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define function to verify PGP signature\nverify_signature() {\n  local file=\"$1\"\n  local signature=\"${file}.asc\"\n  local keyring=\"$2\"\n\n  # Import the public keys if a keyring file is provided\n  if [ -n \"$keyring\" ]; then\n    gpg --import \"$keyring\" 2>/dev/null\n  fi\n\n  # Verify the signature\n  if gpg --verify \"$signature\" \"$file\" 2>/dev/null; then\n    echo \"Signature verification passed for $file\"\n  else\n    echo \"Signature verification failed for $file\"\n    exit 1\n  fi\n}\n\n# Usage: ./verify_pgp.sh <file-to-verify> <optional-public-keys-file>\nif [ $# -eq 0 ]; then\n  echo \"Usage: $0 <artifact-file> [public-keys-file]\"\n  exit 1\nfi\n\n# Call the function with arguments\nverify_signature \"$1\" \"$2\"\n\n# Note: This script assumes the use of GPG and that the corresponding .asc file is present.\n```",
    "label": 1
  },
  {
    "codes": "def _get_bakery_dynamic_attr(self, attname, obj, args=None, default=None):\n        \"\"\"\n        Allows subclasses to provide an attribute (say, 'foo') in three\n        different ways: As a fixed class-level property or as a method\n        foo(self) or foo(self, obj). The second argument argument 'obj' is\n        the \"subject\" of the current Feed invocation. See the Django Feed\n        documentation for details.\n\n        This method was shamelessly stolen from the Feed class and extended\n        with the ability to pass additional arguments to subclass methods.\n        \"\"\"\n        try:\n            attr = getattr(self, attname)\n        except AttributeError:\n            return default\n\n        if callable(attr) or args:\n            args = args[:] if args else []\n\n            # Check co_argcount rather than try/excepting the function and\n            # catching the TypeError, because something inside the function\n            # may raise the TypeError. This technique is more accurate.\n            try:\n                code = six.get_function_code(attr)\n            except AttributeError:\n                code = six.get_function_code(attr.__call__)\n            if code.co_argcount == 2 + len(args):  # one argument is 'self'\n                args.append(obj)\n            return attr(*args)\n\n        return attr",
    "label": 0
  },
  {
    "codes": "function parseGridData(data, getPosition, getWeight = null) {\n  const pointCount = count(data);\n  const positions = new Float32Array(pointCount * 2);\n  const positions64xyLow = new Float32Array(pointCount * 2);\n  const weightValues = new Float32Array(pointCount * 3);\n\n  let yMin = Infinity;\n  let yMax = -Infinity;\n  let xMin = Infinity;\n  let xMax = -Infinity;\n  let y;\n  let x;\n  const {iterable, objectInfo} = createIterable(data);\n  for (const object of iterable) {\n    objectInfo.index++;\n    const position = getPosition(object, objectInfo);\n    const {index} = objectInfo;\n    x = position[0];\n    y = position[1];\n    positions[index * 2] = x;\n    positions[index * 2 + 1] = y;\n\n    positions64xyLow[index * 2] = fp64LowPart(x);\n    positions64xyLow[index * 2 + 1] = fp64LowPart(y);\n\n    const weight = getWeight ? getWeight(object, objectInfo) : DEFAULT_WEIGHT;\n    // Aggregator expects each weight is an array of size 3\n    if (Array.isArray(weight)) {\n      weightValues[index * 3] = weight[0];\n      weightValues[index * 3 + 1] = weight[1];\n      weightValues[index * 3 + 2] = weight[2];\n    } else {\n      // backward compitability\n      weightValues[index * 3] = weight;\n    }\n\n    if (Number.isFinite(y) && Number.isFinite(x)) {\n      yMin = y < yMin ? y : yMin;\n      yMax = y > yMax ? y : yMax;\n\n      xMin = x < xMin ? x : xMin;\n      xMax = x > xMax ? x : xMax;\n    }\n  }\n  const weights = {\n    weight1: {\n      size: 1,\n      operation: AGGREGATION_OPERATION.SUM,\n      needMax: true,\n      values: weightValues\n    }\n  };\n  const boundingBox = {xMin, xMax, yMin, yMax};\n  return {\n    positions,\n    positions64xyLow,\n    weights,\n    boundingBox\n  };\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.ServerSocketChannel;\nimport java.nio.channels.SocketChannel;\nimport java.util.Iterator;\nimport java.util.Set;\n\npublic class NonBlockingNIOServer {\n\n    public static void main(String[] args) {\n        try {\n            // Create a selector to handle multiple channels\n            Selector selector = Selector.open();\n\n            // Create a server socket channel\n            ServerSocketChannel serverChannel = ServerSocketChannel.open();\n            serverChannel.configureBlocking(false); // Configure non-blocking mode\n\n            // Bind the channel to a port\n            serverChannel.socket().bind(new InetSocketAddress(8080));\n            \n            // Register the server channel with the selector\n            serverChannel.register(selector, SelectionKey.OP_ACCEPT);\n\n            // Event loop for handling incoming connections and I/O\n            while (true) {\n                // Blocking call that returns when at least one channel is ready\n                selector.select();\n                \n                // Get a set of keys representing ready channels\n                Set<SelectionKey> selectedKeys = selector.selectedKeys();\n                Iterator<SelectionKey> iter = selectedKeys.iterator();\n                \n                // Iterate over the set of selected keys\n                while (iter.hasNext()) {\n                    SelectionKey key = iter.next();\n                    \n                    // Check if the key is acceptable (server ready to accept a new connection)\n                    if (key.isAcceptable()) {\n                        // Accept the client connection\n                        SocketChannel clientChannel = serverChannel.accept();\n                        clientChannel.configureBlocking(false);\n                        // Register the client channel with the selector for reading\n                        clientChannel.register(selector, SelectionKey.OP_READ);\n                    }\n\n                    // Check if the key is readable (server ready to read data from client)\n                    if (key.isReadable()) {\n                        // Retrieve the channel associated with the key\n                        SocketChannel clientChannel = (SocketChannel) key.channel();\n                        ByteBuffer buffer = ByteBuffer.allocate(256);\n                        \n                        // Read data into the buffer\n                        int bytesRead = clientChannel.read(buffer);\n\n                        // Check for end-of-stream (-1) or errors (exceptions)\n                        if (bytesRead == -1) {\n                            // Close the connection\n                            clientChannel.close();\n                        } else if (bytesRead > 0) {\n                            // Flip the buffer to read mode and process the data\n                            buffer.flip();\n                            while (buffer.hasRemaining()) {\n                                clientChannel.write(buffer); // Echo the data back to the client\n                            }\n                        }\n                    }\n                    iter.remove(); // Remove the processed key to prevent repeated processing\n                }\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public EventHubConsumerGroupInfoInner getEventHubConsumerGroup(String resourceGroupName, String resourceName, String eventHubEndpointName, String name) {\n        return getEventHubConsumerGroupWithServiceResponseAsync(resourceGroupName, resourceName, eventHubEndpointName, name).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "def from_global_id(global_id):\n    '''\n    Takes the \"global ID\" created by toGlobalID, and retuns the type name and ID\n    used to create it.\n    '''\n    unbased_global_id = unbase64(global_id)\n    _type, _id = unbased_global_id.split(':', 1)\n    return _type, _id",
    "label": 0
  },
  {
    "codes": "def proceed(self, *,\n                action=adhoc_xso.ActionType.EXECUTE,\n                payload=None):\n        \"\"\"\n        Proceed command execution to the next stage.\n\n        :param action: Action type for proceeding\n        :type action: :class:`~.ActionTyp`\n        :param payload: Payload for the request, or :data:`None`\n        :return: The :attr:`~.xso.Command.first_payload` of the response\n\n        `action` must be one of the actions returned by\n        :attr:`allowed_actions`. It defaults to :attr:`~.ActionType.EXECUTE`,\n        which is (alongside with :attr:`~.ActionType.CANCEL`) always allowed.\n\n        `payload` may be a sequence of XSOs, a single XSO or :data:`None`. If\n        it is :data:`None`, the XSOs from the request are re-used. This is\n        useful if you modify the payload in-place (e.g. via\n        :attr:`first_payload`). Otherwise, the payload on the request is set to\n        the `payload` argument; if it is a single XSO, it is wrapped in a\n        sequence.\n\n        The :attr:`status`, :attr:`response` and related attributes get updated\n        with the newly received values.\n        \"\"\"\n\n        if self._response is None:\n            raise RuntimeError(\"command execution not started yet\")\n\n        if action not in self.allowed_actions:\n            raise ValueError(\"action {} not allowed in this stage\".format(\n                action\n            ))\n\n        cmd = adhoc_xso.Command(\n            self._command_name,\n            action=action,\n            payload=self._response.payload if payload is None else payload,\n            sessionid=self.sessionid,\n        )\n\n        request = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=self._peer_jid,\n            payload=cmd,\n        )\n\n        try:\n            self._response = \\\n                yield from self._stream.send_iq_and_wait_for_reply(\n                    request,\n                )\n        except (aioxmpp.errors.XMPPModifyError,\n                aioxmpp.errors.XMPPCancelError) as exc:\n            if isinstance(exc.application_defined_condition,\n                          (adhoc_xso.BadSessionID,\n                           adhoc_xso.SessionExpired)):\n                yield from self.close()\n                raise SessionError(exc.text)\n            if isinstance(exc, aioxmpp.errors.XMPPCancelError):\n                yield from self.close()\n            raise\n\n        return self._response.first_payload",
    "label": 0
  },
  {
    "codes": "```python\nfrom django.shortcuts import render, redirect\nfrom django.contrib.auth.models import User\nfrom django.contrib.auth import login\nfrom django.core.mail import send_mail\nfrom django.utils.crypto import get_random_string\nfrom django.template.loader import render_to_string\nfrom django.utils.html import strip_tags\nfrom django.conf import settings\nfrom django.urls import reverse\nfrom .models import EmailVerification\n\ndef register(request):\n    if request.method == 'POST':\n        username = request.POST['username']\n        email = request.POST['email']\n        password = request.POST['password']\n        \n        user = User(username=username, email=email)\n        user.set_password(password)\n        user.save()\n\n        # Create verification token\n        token = get_random_string(length=32)\n        EmailVerification.objects.create(user=user, token=token)\n\n        # Send verification email\n        subject = 'Verify your email address'\n        html_message = render_to_string('registration/verify_email.html', {'token': token})\n        plain_message = strip_tags(html_message)\n        from_email = settings.DEFAULT_FROM_EMAIL\n        to = email\n\n        send_mail(subject, plain_message, from_email, [to], html_message=html_message)\n\n        return redirect('registration_success')\n    \n    return render(request, 'registration/register.html')\n\n\ndef verify_email(request, token):\n    try:\n        verification = EmailVerification.objects.get(token=token)\n        user = verification.user\n        user.is_active = True\n        user.save()\n        verification.delete()\n        login(request, user)\n        return redirect('home')\n    except EmailVerification.DoesNotExist:\n        return render(request, 'registration/verification_failed.html')\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Definition for singly-linked list node\nclass ListNode:\n    def __init__(self, val=0, next=None):\n        self.val = val\n        self.next = next\n\n# Function to reverse the linked list\ndef reverse_linked_list(head):\n    # Initialize pointers\n    prev = None\n    current = head\n    \n    # Traverse the list and reverse the links\n    while current:\n        next_node = current.next  # Store the next node\n        current.next = prev      # Reverse the current node's pointer\n        prev = current           # Move prev to current node\n        current = next_node       # Move current to next node\n    \n    # Return the new head of the reversed list\n    return prev\n```",
    "label": 1
  },
  {
    "codes": "func (s *GetUserDefinedFunctionInput) SetFunctionName(v string) *GetUserDefinedFunctionInput {\n\ts.FunctionName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function (markAreaModel, ecModel, api) {\n        ecModel.eachSeries(function (seriesModel) {\n            var maModel = seriesModel.markAreaModel;\n            if (maModel) {\n                var areaData = maModel.getData();\n                areaData.each(function (idx) {\n                    var points = zrUtil.map(dimPermutations, function (dim) {\n                        return getSingleMarkerEndPoint(areaData, idx, dim, seriesModel, api);\n                    });\n                    // Layout\n                    areaData.setItemLayout(idx, points);\n                    var el = areaData.getItemGraphicEl(idx);\n                    el.setShape('points', points);\n                });\n            }\n        }, this);\n    }",
    "label": 0
  },
  {
    "codes": "```csharp\nvar filteredProducts = products\n    .Where(p => p.Price >= minPrice && p.Price <= maxPrice) // Filter by price range\n    .Where(p => p.Category == desiredCategory) // Filter by category\n    .ToList(); // Convert to list\n```",
    "label": 1
  },
  {
    "codes": "def create_traj_ranges(start, stop, N):\n    \"\"\"\n    Fills in the trajectory range.\n\n    # Adapted from https://stackoverflow.com/a/40624614\n    \"\"\"\n    steps = (1.0/(N-1)) * (stop - start)\n    if np.isscalar(steps):\n        return steps*np.arange(N) + start\n    else:\n        return steps[:, None]*np.arange(N) + start[:, None]",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<IotHubDescriptionInner> createOrUpdateAsync(String resourceGroupName, String resourceName, IotHubDescriptionInner iotHubDescription, String ifMatch, final ServiceCallback<IotHubDescriptionInner> serviceCallback) {\n        return ServiceFuture.fromResponse(createOrUpdateWithServiceResponseAsync(resourceGroupName, resourceName, iotHubDescription, ifMatch), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```rust\nuse hyper::{Body, Request, Response, Server, StatusCode};\nuse hyper::service::{make_service_fn, service_fn};\nuse std::collections::HashMap;\nuse std::convert::Infallible;\nuse std::net::SocketAddr;\nuse std::sync::{Arc, Mutex};\nuse tokio::time::{self, Duration};\n\ntype SharedState = Arc<Mutex<HashMap<String, u64>>>;\n\n// Set the maximum number of requests per IP\nconst MAX_REQUESTS_PER_IP: u64 = 100;\n// Define the window duration\nconst WINDOW_DURATION: Duration = Duration::from_secs(60);\n\nasync fn rate_limit_middleware(\n    req: Request<Body>, \n    state: SharedState\n) -> Result<Response<Body>, Infallible> {\n    let ip = match req.headers().get(\"x-forwarded-for\") {\n        Some(h) => h.to_str().unwrap_or(\"\").to_string(),\n        None => req\n            .remote_addr()\n            .map(|addr| addr.ip().to_string())\n            .unwrap_or_default(),\n    };\n\n    {\n        let mut state_lock = state.lock().unwrap();\n        let counter = state_lock.entry(ip.clone()).or_insert(0);\n        *counter += 1;\n        if *counter > MAX_REQUESTS_PER_IP {\n            // Rate limit exceeded\n            return Ok(Response::builder()\n                .status(StatusCode::TOO_MANY_REQUESTS)\n                .body(Body::from(\"Too Many Requests\"))\n                .unwrap());\n        }\n    }\n\n    // Serve the request\n    Ok(Response::builder()\n        .status(StatusCode::OK)\n        .body(Body::from(\"Request accepted\"))\n        .unwrap())\n}\n\nasync fn reset_counter(state: SharedState) {\n    let mut interval = time::interval(WINDOW_DURATION);\n    loop {\n        interval.tick().await;\n        let mut state_lock = state.lock().unwrap();\n        state_lock.clear();\n    }\n}\n\n#[tokio::main]\nasync fn main() {\n    let state: SharedState = Arc::new(Mutex::new(HashMap::new()));\n\n    let make_svc = make_service_fn(move |_| {\n        let state = state.clone();\n        async move {\n            Ok::<_, Infallible>(service_fn(move |req| {\n                rate_limit_middleware(req, state.clone())\n            }))\n        }\n    });\n\n    let addr = SocketAddr::from(([127, 0, 0, 1], 3000));\n    let server = Server::bind(&addr).serve(make_svc);\n\n    let reset_task = tokio::spawn(reset_counter(state.clone()));\n\n    // Run server and reset task concurrently\n    tokio::select! {\n        res = server => {\n            if let Err(e) = res {\n                eprintln!(\"server error: {}\", e);\n            }\n        },\n        _ = reset_task => {}\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *CreateTrailOutput) SetCloudWatchLogsRoleArn(v string) *CreateTrailOutput {\n\ts.CloudWatchLogsRoleArn = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```javascript\n// Require the DOMPurify library for sanitizing HTML\nconst DOMPurify = require('dompurify');\nconst { JSDOM } = require('jsdom');\n\n// Set up DOMPurify to work with server-side JSDOM\nconst window = new JSDOM('').window;\nconst purify = DOMPurify(window);\n\n// Middleware function to sanitize input\nfunction sanitizeInput(req, res, next) {\n  // Utility function to check input type and sanitize accordingly\n  const sanitize = (input) => {\n    if (typeof input === 'object' && input !== null) {\n      // Recursively sanitize each property if input is an object\n      for (const key in input) {\n        input[key] = sanitize(input[key]);\n      }\n    } else if (typeof input === 'string') {\n      // Sanitize string input using DOMPurify\n      return purify.sanitize(input);\n    }\n    return input; // Return input as is if it's not a string or object\n  };\n\n  // Sanitize incoming request properties\n  req.body = sanitize(req.body);\n  req.query = sanitize(req.query);\n  req.params = sanitize(req.params);\n\n  // Continue to the next middleware or route handler\n  next();\n}\n\n// Export the middleware function for use in other modules\nmodule.exports = sanitizeInput;\n```\n",
    "label": 1
  },
  {
    "codes": "func (s *CreateDevEndpointOutput) SetExtraJarsS3Path(v string) *CreateDevEndpointOutput {\n\ts.ExtraJarsS3Path = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *GetResourcePolicyOutput) SetCreateTime(v time.Time) *GetResourcePolicyOutput {\n\ts.CreateTime = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *ListJobsInput) SetLimit(v string) *ListJobsInput {\n\ts.Limit = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nfrom fastapi import FastAPI, Depends, HTTPException, Header\nfrom fastapi.security import APIKeyHeader\nfrom starlette.middleware.cors import CORSMiddleware\nfrom slowapi import Limiter\nfrom slowapi.errors import RateLimitExceeded\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker, declarative_base\n\n# Constants\nAPI_KEY = \"your_api_key\"  # Replace with your actual API key\nAPI_KEY_NAME = \"X-API-Key\"\nlimiter = Limiter(key_func=lambda: \"global\")\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# Add CORS middleware if needed\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Adjust the origins as necessary\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# API key security\napi_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=True)\n\n# Database setup (if needed)\nDATABASE_URL = \"sqlite:///./test.db\"\nengine = create_engine(DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = declarative_base()\n\n# Rate limit exceeded handler\n@app.exception_handler(RateLimitExceeded)\nasync def rate_limit_exceeded_handler(request, exc):\n    return JSONResponse(\n        status_code=429,\n        content={\"detail\": \"Rate limit exceeded.\"},\n    )\n\n# Dependency for API key validation\nasync def get_api_key(api_key: str = Depends(api_key_header)):\n    if api_key != API_KEY:\n        raise HTTPException(status_code=403, detail=\"Invalid API Key\")\n\n# Sample endpoint with rate limiting and authentication\n@app.get(\"/items/\", dependencies=[Depends(get_api_key), limiter.limit(\"5/minute\")])\nasync def read_items():\n    return {\"message\": \"Successfully accessed the items.\"}\n```",
    "label": 1
  },
  {
    "codes": "def _translate_limit(self, len_, start, num):\n        \"\"\"\n        Translate limit to valid bounds.\n        \"\"\"\n        if start > len_ or num <= 0:\n            return 0, 0\n        return min(start, len_), num",
    "label": 0
  },
  {
    "codes": "public List<RuleDescription> getRules(String topicName, String subscriptionName) throws ServiceBusException, InterruptedException {\n        return Utils.completeFuture(this.asyncClient.getRulesAsync(topicName, subscriptionName));\n    }",
    "label": 0
  },
  {
    "codes": "```typescript\n// Import Cypress commands\nimport { checkoutPage } from '../pages/checkoutPage';\n\ndescribe('E-commerce Checkout Functionality', () => {\n  beforeEach(() => {\n    // Visit the e-commerce website and navigate to the checkout page\n    cy.visit('/');\n    cy.addProductToCart(); // Custom command to add a product to the cart\n    cy.goToCheckout(); // Custom command to navigate to the checkout page\n  });\n\n  it('should successfully complete the checkout process', () => {\n    // Fill in the checkout form with valid details\n    checkoutPage.fillShippingDetails({\n      firstName: 'John',\n      lastName: 'Doe',\n      address: '123 Main St',\n      city: 'New York',\n      zipCode: '10001',\n      country: 'USA',\n    });\n\n    checkoutPage.fillPaymentDetails({\n      cardNumber: '4111111111111111',\n      expiryDate: '12/25',\n      cvv: '123',\n    });\n\n    // Submit the checkout form\n    checkoutPage.submitOrder();\n\n    // Verify that the order confirmation page is displayed\n    cy.url().should('include', '/order-confirmation');\n    cy.contains('Thank you for your order!').should('be.visible');\n  });\n\n  it('should display an error message for invalid payment details', () => {\n    // Fill in the checkout form with invalid payment details\n    checkoutPage.fillShippingDetails({\n      firstName: 'Jane',\n      lastName: 'Doe',\n      address: '456 Elm St',\n      city: 'Los Angeles',\n      zipCode: '90001',\n      country: 'USA',\n    });\n\n    checkoutPage.fillPaymentDetails({\n      cardNumber: '1234567812345678', // Invalid card number\n      expiryDate: '12/25',\n      cvv: '123',\n    });\n\n    // Submit the checkout form\n    checkoutPage.submitOrder();\n\n    // Verify that an error message is displayed\n    cy.contains('Invalid payment details').should('be.visible');\n  });\n});\n```\n\n```typescript\n// checkoutPage.ts\nexport const checkoutPage = {\n  fillShippingDetails(details: {\n    firstName: string;\n    lastName: string;\n    address: string;\n    city: string;\n    zipCode: string;\n    country: string;\n  }) {\n    cy.get('#firstName').type(details.firstName);\n    cy.get('#lastName').type(details.lastName);\n    cy.get('#address').type(details.address);\n    cy.get('#city').type(details.city);\n    cy.get('#zipCode').type(details.zipCode);\n    cy.get('#country').select(details.country);\n  },\n\n  fillPaymentDetails(details: { cardNumber: string; expiryDate: string; cvv: string }) {\n    cy.get('#cardNumber').type(details.cardNumber);\n    cy.get('#expiryDate').type(details.expiryDate);\n    cy.get('#cvv').type(details.cvv);\n  },\n\n  submitOrder() {\n    cy.get('#submitOrder').click();\n  },\n};\n```",
    "label": 1
  },
  {
    "codes": "public Observable<Page<TaskInner>> listAsync(final String resourceGroupName, final String registryName) {\n        return listWithServiceResponseAsync(resourceGroupName, registryName)\n            .map(new Func1<ServiceResponse<Page<TaskInner>>, Page<TaskInner>>() {\n                @Override\n                public Page<TaskInner> call(ServiceResponse<Page<TaskInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<List<PatternRuleInfo>> getPatternsAsync(UUID appId, String versionId, GetPatternsOptionalParameter getPatternsOptionalParameter, final ServiceCallback<List<PatternRuleInfo>> serviceCallback) {\n        return ServiceFuture.fromResponse(getPatternsWithServiceResponseAsync(appId, versionId, getPatternsOptionalParameter), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```javascript\nimport React, { useState, useEffect } from 'react';\n\n// RealTimeChat component\nconst RealTimeChat = () => {\n  const [messages, setMessages] = useState([]); // State to store chat messages\n  const [inputValue, setInputValue] = useState(''); // State to store input value\n\n  // Simulate real-time message updates (e.g., from a WebSocket or API)\n  useEffect(() => {\n    const interval = setInterval(() => {\n      setMessages((prevMessages) => [\n        ...prevMessages,\n        { id: prevMessages.length + 1, text: `New message ${prevMessages.length + 1}` },\n      ]);\n    }, 5000); // Add a new message every 5 seconds\n\n    return () => clearInterval(interval); // Cleanup interval on unmount\n  }, []);\n\n  // Handle sending a new message\n  const handleSendMessage = () => {\n    if (inputValue.trim()) {\n      setMessages((prevMessages) => [\n        ...prevMessages,\n        { id: prevMessages.length + 1, text: inputValue },\n      ]);\n      setInputValue(''); // Clear input after sending\n    }\n  };\n\n  return (\n    <div style={styles.chatContainer}>\n      <div style={styles.messagesContainer}>\n        {messages.map((message) => (\n          <div key={message.id} style={styles.message}>\n            {message.text}\n          </div>\n        ))}\n      </div>\n      <div style={styles.inputContainer}>\n        <input\n          type=\"text\"\n          value={inputValue}\n          onChange={(e) => setInputValue(e.target.value)}\n          style={styles.input}\n          placeholder=\"Type a message...\"\n        />\n        <button onClick={handleSendMessage} style={styles.sendButton}>\n          Send\n        </button>\n      </div>\n    </div>\n  );\n};\n\n// Styles for the chat interface\nconst styles = {\n  chatContainer: {\n    width: '300px',\n    height: '400px',\n    border: '1px solid #ccc',\n    borderRadius: '8px',\n    display: 'flex',\n    flexDirection: 'column',\n    justifyContent: 'space-between',\n    padding: '10px',\n  },\n  messagesContainer: {\n    flex: 1,\n    overflowY: 'auto',\n    marginBottom: '10px',\n  },\n  message: {\n    padding: '5px',\n    borderBottom: '1px solid #eee',\n  },\n  inputContainer: {\n    display: 'flex',\n    gap: '10px',\n  },\n  input: {\n    flex: 1,\n    padding: '5px',\n    borderRadius: '4px',\n    border: '1px solid #ccc',\n  },\n  sendButton: {\n    padding: '5px 10px',\n    borderRadius: '4px',\n    border: 'none',\n    backgroundColor: '#007bff',\n    color: '#fff',\n    cursor: 'pointer',\n  },\n};\n\nexport default RealTimeChat;\n```",
    "label": 1
  },
  {
    "codes": "```go\n// Go service for batch processing predictions using GPU workers\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"net/http\"\n    \"sync\"\n    \"time\"\n    // You would import GPU processing library here\n)\n\ntype PredictionTask struct {\n    InputData []float64\n    Result    float64\n    Err       error\n}\n\ntype GPUPool struct {\n    maxWorkers int\n    tasksChan  chan *PredictionTask\n    wg         sync.WaitGroup\n}\n\nfunc NewGPUPool(maxWorkers int) *GPUPool {\n    pool := &GPUPool{\n        maxWorkers: maxWorkers,\n        tasksChan:  make(chan *PredictionTask, maxWorkers),\n    }\n    for i := 0; i < maxWorkers; i++ {\n        pool.wg.Add(1)\n        go pool.worker()\n    }\n    return pool\n}\n\nfunc (p *GPUPool) SubmitTask(task *PredictionTask) {\n    p.tasksChan <- task\n}\n\nfunc (p *GPUPool) worker() {\n    defer p.wg.Done()\n    for task := range p.tasksChan {\n        // Simulate processing via GPU (Replace with actual GPU library call)\n        time.Sleep(100 * time.Millisecond)\n        task.Result = computePrediction(task.InputData) // Example function\n        task.Err = nil                                  // Handle error cases as needed\n    }\n}\n\n// Placeholder for actual GPU computation\nfunc computePrediction(data []float64) float64 {\n    // Normally, you'd use a GPU-specific library for computation here\n    return 42.0 // Mocked prediction result\n}\n\nfunc (p *GPUPool) Shutdown() {\n    close(p.tasksChan)\n    p.wg.Wait()\n}\n\nfunc batchPredictionHandler(pool *GPUPool) http.HandlerFunc {\n    return func(w http.ResponseWriter, r *http.Request) {\n        var tasks []*PredictionTask\n        \n        // Logic to decode request data into tasks\n        // Decode input data into tasks here\n        // ...\n\n        for _, task := range tasks {\n            pool.SubmitTask(task)\n        }\n\n        // Block until all tasks are processed\n        pool.wg.Wait()\n\n        // Construct response with task results\n        // Respond with JSON results\n        // ...\n        \n        fmt.Fprintf(w, \"Batch processing completed!\")\n    }\n}\n\nfunc main() {\n    gpupool := NewGPUPool(4) // Assume 4 GPU workers\n    defer gpupool.Shutdown()\n\n    http.HandleFunc(\"/batch_predict\", batchPredictionHandler(gpupool))\n    \n    fmt.Println(\"Server started at :8080\")\n    if err := http.ListenAndServe(\":8080\", nil); err != nil {\n        // Handle error\n        fmt.Println(err)\n    }\n}\n```\n\n// Note: Replace the mock implementation with actual GPU processing logic using libraries such as CUDA, TensorFlow, or any suitable for Go.",
    "label": 1
  },
  {
    "codes": "func (c *Glue) StartCrawlerRequest(input *StartCrawlerInput) (req *request.Request, output *StartCrawlerOutput) {\n\top := &request.Operation{\n\t\tName:       opStartCrawler,\n\t\tHTTPMethod: \"POST\",\n\t\tHTTPPath:   \"/\",\n\t}\n\n\tif input == nil {\n\t\tinput = &StartCrawlerInput{}\n\t}\n\n\toutput = &StartCrawlerOutput{}\n\treq = c.newRequest(op, input, output)\n\treq.Handlers.Unmarshal.Swap(jsonrpc.UnmarshalHandler.Name, protocol.UnmarshalDiscardBodyHandler)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "func (s *Table) SetCreateTime(v time.Time) *Table {\n\ts.CreateTime = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```cpp\n// DynamicWeatherSystem.h\n#pragma once\n\n#include \"CoreMinimal.h\"\n#include \"GameFramework/Actor.h\"\n#include \"DynamicWeatherSystem.generated.h\"\n\nUCLASS()\nclass YOURGAME_API ADynamicWeatherSystem : public AActor\n{\n    GENERATED_BODY()\n    \npublic:    \n    // Sets default values for this actor's properties\n    ADynamicWeatherSystem();\n\nprotected:\n    // Called when the game starts or when spawned\n    virtual void BeginPlay() override;\n\npublic:    \n    // Called every frame\n    virtual void Tick(float DeltaTime) override;\n\n    // Function to transition weather\n    UFUNCTION(BlueprintCallable, Category = \"Weather\")\n    void TransitionWeather(FName NewWeatherType, float TransitionDuration);\n\nprivate:\n    // Current weather type\n    UPROPERTY(VisibleAnywhere, Category = \"Weather\")\n    FName CurrentWeatherType;\n\n    // Target weather type\n    UPROPERTY(VisibleAnywhere, Category = \"Weather\")\n    FName TargetWeatherType;\n\n    // Transition progress (0 to 1)\n    UPROPERTY(VisibleAnywhere, Category = \"Weather\")\n    float TransitionProgress;\n\n    // Duration of the transition\n    UPROPERTY(EditAnywhere, Category = \"Weather\")\n    float TransitionTime;\n\n    // Function to update weather effects\n    void UpdateWeatherEffects(float DeltaTime);\n};\n\n// DynamicWeatherSystem.cpp\n#include \"DynamicWeatherSystem.h\"\n\n// Sets default values\nADynamicWeatherSystem::ADynamicWeatherSystem()\n{\n    // Set this actor to call Tick() every frame\n    PrimaryActorTick.bCanEverTick = true;\n\n    // Initialize default values\n    CurrentWeatherType = \"Clear\";\n    TargetWeatherType = \"Clear\";\n    TransitionProgress = 0.0f;\n    TransitionTime = 5.0f; // Default transition time\n}\n\n// Called when the game starts or when spawned\nvoid ADynamicWeatherSystem::BeginPlay()\n{\n    Super::BeginPlay();\n}\n\n// Called every frame\nvoid ADynamicWeatherSystem::Tick(float DeltaTime)\n{\n    Super::Tick(DeltaTime);\n\n    // Update weather effects during transition\n    if (TransitionProgress < 1.0f)\n    {\n        TransitionProgress += DeltaTime / TransitionTime;\n        UpdateWeatherEffects(DeltaTime);\n    }\n    else\n    {\n        // Transition complete\n        CurrentWeatherType = TargetWeatherType;\n        TransitionProgress = 0.0f;\n    }\n}\n\n// Function to transition weather\nvoid ADynamicWeatherSystem::TransitionWeather(FName NewWeatherType, float TransitionDuration)\n{\n    TargetWeatherType = NewWeatherType;\n    TransitionTime = TransitionDuration;\n    TransitionProgress = 0.0f;\n}\n\n// Function to update weather effects\nvoid ADynamicWeatherSystem::UpdateWeatherEffects(float DeltaTime)\n{\n    // Implement weather effect transitions here\n    // Example: Lerp between weather parameters (e.g., fog density, sky color, etc.)\n    // based on TransitionProgress and CurrentWeatherType/TargetWeatherType\n}\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse serde_json::Value;\nuse apache_avro::Schema;\nuse apache_avro::types::Record;\nuse apache_avro::Writer;\nuse std::fs::File;\nuse std::io::BufReader;\n\nfn main() {\n    // Load JSON file\n    let file = File::open(\"data.json\").expect(\"Failed to open JSON file\");\n    let reader = BufReader::new(file);\n    let json_value: Value = serde_json::from_reader(reader).expect(\"Failed to parse JSON\");\n\n    // Define Avro schema (example schema for a simple JSON structure)\n    let schema_str = r#\"\n    {\n        \"type\": \"record\",\n        \"name\": \"example\",\n        \"fields\": [\n            {\"name\": \"field1\", \"type\": \"string\"},\n            {\"name\": \"field2\", \"type\": \"int\"}\n        ]\n    }\"#;\n    let schema = Schema::parse_str(schema_str).expect(\"Failed to parse Avro schema\");\n\n    // Convert JSON to Avro Record\n    let mut record = Record::new(&schema).expect(\"Failed to create Avro record\");\n    if let Value::Object(map) = json_value {\n        for (key, value) in map {\n            match value {\n                Value::String(s) => record.put(key, s),\n                Value::Number(n) => record.put(key, n.as_i64().unwrap()),\n                _ => panic!(\"Unsupported JSON value type\"),\n            }\n        }\n    }\n\n    // Write Avro record to file\n    let avro_file = File::create(\"output.avro\").expect(\"Failed to create Avro file\");\n    let mut writer = Writer::new(&schema, avro_file);\n    writer.append(record).expect(\"Failed to write Avro record\");\n    writer.flush().expect(\"Failed to flush Avro writer\");\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport xml.etree.ElementTree as ET\n\ndef xml_element_generator(file_path):\n    \"\"\"Yield XML elements from a large XML file one by one.\"\"\"\n    context = ET.iterparse(file_path, events=('start', 'end'))\n    # Skip the root element\n    next(context)\n    for event, elem in context:\n        if event == 'end':\n            yield elem\n            elem.clear()  # Free memory by clearing the element after yielding\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"crypto/rand\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"strings\"\n)\n\n// generatePassword generates a secure random password of a given length\nfunc generatePassword(length int) (string, error) {\n\tconst charset = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!@#$%^&*()_+\"\n\tvar password strings.Builder\n\n\tfor i := 0; i < length; i++ {\n\t\t// Generate a random index within the charset range\n\t\trandIndex, err := rand.Int(rand.Reader, big.NewInt(int64(len(charset))))\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tpassword.WriteByte(charset[randIndex.Int64()])\n\t}\n\n\treturn password.String(), nil\n}\n\n// calculateEntropy calculates the entropy of a password in bits\nfunc calculateEntropy(password string) float64 {\n\tconst charsetSize = 94 // Total number of possible characters\n\tlength := float64(len(password))\n\treturn length * (log2(float64(charsetSize)))\n}\n\n// log2 calculates the base-2 logarithm of a number\nfunc log2(x float64) float64 {\n\treturn float64(1) / float64(0.69314718056) * float64(math.Log(x))\n}\n\nfunc main() {\n\tpassword, err := generatePassword(16) // Generate a 16-character password\n\tif err != nil {\n\t\tfmt.Println(\"Error generating password:\", err)\n\t\treturn\n\t}\n\n\tentropy := calculateEntropy(password)\n\tfmt.Printf(\"Generated Password: %s\\n\", password)\n\tfmt.Printf(\"Password Entropy: %.2f bits\\n\", entropy)\n\n\t// Check if entropy meets a minimum threshold (e.g., 80 bits)\n\tif entropy >= 80 {\n\t\tfmt.Println(\"Password entropy is sufficient.\")\n\t} else {\n\t\tfmt.Println(\"Password entropy is insufficient.\")\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"crypto/rand\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"unicode\"\n)\n\nconst (\n\tpasswordLength = 16 // Length of the password\n\tletters        = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\tdigits         = \"0123456789\"\n\tsymbols        = \"!@#%&*()-_=+[]{}<>?/.,\"\n)\n\nfunc main() {\n\tpassword, err := generateSecurePassword(passwordLength)\n\tif err != nil {\n\t\tfmt.Println(\"Error generating password:\", err)\n\t\treturn\n\t}\n\n\tif !checkEntropy(password, 64) {\n\t\tfmt.Println(\"Generated password does not meet the entropy requirement.\")\n\t\treturn\n\t}\n\n\tfmt.Println(\"Generated secure password:\", password)\n}\n\n// generateSecurePassword creates a random password with a given length.\nfunc generateSecurePassword(length int) (string, error) {\n\tif length < 4 {\n\t\treturn \"\", fmt.Errorf(\"password length should be at least 4\")\n\t}\n\t\n\tcharSet := letters + digits + symbols\n\tbytes := make([]byte, length)\n\n\tfor i := 0; i < length; i++ {\n\t\tnum, err := rand.Int(rand.Reader, big.NewInt(int64(len(charSet))))\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tbytes[i] = charSet[num.Int64()]\n\t}\n\t\n\t// Ensure password contains at least one letter, one number, and one symbol\n\thasLetter, hasDigit, hasSymbol := false, false, false\n\tfor _, c := range bytes {\n\t\tswitch {\n\t\tcase unicode.IsLetter(rune(c)):\n\t\t\thasLetter = true\n\t\tcase unicode.IsDigit(rune(c)):\n\t\t\thasDigit = true\n\t\tcase unicode.IsPunct(rune(c)) || unicode.IsSymbol(rune(c)):\n\t\t\thasSymbol = true\n\t\t}\n\t}\n\n\t// Regenerate if missing required character types\n\tif !hasLetter || !hasDigit || !hasSymbol {\n\t\treturn generateSecurePassword(length)\n\t}\n\n\treturn string(bytes), nil\n}\n\n// checkEntropy calculates the entropy of a given password and checks if it meets the required bit level.\nfunc checkEntropy(password string, requiredBits float64) bool {\n\tsize := float64(len(password))\n\tentropyBits := size * 8 * (log2(float64(len(letters+digits+symbols)))) / float64(len(password))\n\treturn entropyBits >= requiredBits\n}\n\n// log2 computes the base 2 logarithm of a number.\nfunc log2(x float64) float64 {\n\treturn math.Log(x) / math.Ln2\n}\n```",
    "label": 1
  },
  {
    "codes": "```rust\n// Function to calculate the Levenshtein distance between two strings\nfn levenshtein_distance(s1: &str, s2: &str) -> usize {\n    let len1 = s1.chars().count();\n    let len2 = s2.chars().count();\n    \n    // Create a 2D vector to store distances\n    let mut dp = vec![vec![0; len2 + 1]; len1 + 1];\n    \n    // Initialize the first row and column\n    for i in 0..=len1 {\n        dp[i][0] = i;\n    }\n    for j in 0..=len2 {\n        dp[0][j] = j;\n    }\n    \n    // Fill the DP table\n    for (i, char1) in s1.chars().enumerate() {\n        for (j, char2) in s2.chars().enumerate() {\n            let cost = if char1 == char2 { 0 } else { 1 };\n            dp[i + 1][j + 1] = (dp[i][j + 1] + 1) // Deletion\n                .min(dp[i + 1][j] + 1) // Insertion\n                .min(dp[i][j] + cost); // Substitution\n        }\n    }\n    \n    // Return the final distance\n    dp[len1][len2]\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nfunction mergeSortedArrays(arr1, arr2) {\n    let mergedArray = [];\n    let i = 0, j = 0;\n    \n    // Traverse both arrays and insert elements into the new array in sorted order\n    while (i < arr1.length && j < arr2.length) {\n        if (arr1[i] < arr2[j]) {\n            mergedArray.push(arr1[i]);\n            i++;\n        } else {\n            mergedArray.push(arr2[j]);\n            j++;\n        }\n    }\n    \n    // Add any remaining elements from arr1\n    while (i < arr1.length) {\n        mergedArray.push(arr1[i]);\n        i++;\n    }\n    \n    // Add any remaining elements from arr2\n    while (j < arr2.length) {\n        mergedArray.push(arr2[j]);\n        j++;\n    }\n    \n    return mergedArray;\n}\n```\n",
    "label": 1
  },
  {
    "codes": "def strip_rightmost(self):\n        \"\"\"\n        Strip the rightmost part of the language range. If the new rightmost\n        part is a singleton or ``x`` (i.e. starts an extension or private use\n        part), it is also stripped.\n\n        Return the newly created :class:`LanguageRange`.\n        \"\"\"\n\n        parts = self.print_str.split(\"-\")\n        parts.pop()\n        if parts and len(parts[-1]) == 1:\n            parts.pop()\n        return type(self).fromstr(\"-\".join(parts))",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<SecretBundle>> setSecretWithServiceResponseAsync(String vaultBaseUrl, String secretName, String value) {\n        if (vaultBaseUrl == null) {\n            throw new IllegalArgumentException(\"Parameter vaultBaseUrl is required and cannot be null.\");\n        }\n        if (secretName == null) {\n            throw new IllegalArgumentException(\"Parameter secretName is required and cannot be null.\");\n        }\n        if (this.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.apiVersion() is required and cannot be null.\");\n        }\n        if (value == null) {\n            throw new IllegalArgumentException(\"Parameter value is required and cannot be null.\");\n        }\n        final Map<String, String> tags = null;\n        final String contentType = null;\n        final SecretAttributes secretAttributes = null;\n        SecretSetParameters parameters = new SecretSetParameters();\n        parameters.withValue(value);\n        parameters.withTags(null);\n        parameters.withContentType(null);\n        parameters.withSecretAttributes(null);\n        String parameterizedHost = Joiner.on(\", \").join(\"{vaultBaseUrl}\", vaultBaseUrl);\n        return service.setSecret(secretName, this.apiVersion(), this.acceptLanguage(), parameters, parameterizedHost, this.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<SecretBundle>>>() {\n                @Override\n                public Observable<ServiceResponse<SecretBundle>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<SecretBundle> clientResponse = setSecretDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport psutil\nimport tabulate\nimport time\nimport argparse\n\ndef get_network_usage(interval=1):\n    # Record the initial sent and received bytes per connection\n    initial_conns = {conn.pid: psutil.net_io_counters(pernic=True, nowrap=True) for conn in psutil.net_connections(kind='inet')}\n\n    time.sleep(interval)\n\n    # Record the final sent and received bytes per connection\n    final_conns = {conn.pid: psutil.net_io_counters(pernic=True, nowrap=True) for conn in psutil.net_connections(kind='inet')}\n\n    data = []\n\n    for pid in final_conns:\n        if pid not in initial_conns: continue\n        \n        initial_io = initial_conns[pid]\n        final_io = final_conns[pid]\n\n        # Calculate bytes sent and received for the process\n        bytes_sent = sum((final_io[iface].bytes_sent - initial_io[iface].bytes_sent) for iface in final_io if iface in initial_io)\n        bytes_recv = sum((final_io[iface].bytes_recv - initial_io[iface].bytes_recv) for iface in final_io if iface in initial_io)\n\n        try:\n            process_name = psutil.Process(pid).name()\n        except psutil.NoSuchProcess:\n            process_name = \"Unknown\"\n\n        data.append((pid, process_name, bytes_sent, bytes_recv))\n\n    return data\n\ndef print_network_usage():\n    usage = get_network_usage()\n\n    # Print process-wise bandwidth usage\n    print(tabulate.tabulate(\n        [(\"PID\", \"Process Name\", \"Bytes Sent\", \"Bytes Received\")] + usage,\n        headers='firstrow', tablefmt='pretty'\n    ))\n\ndef main(interval):\n    try:\n        while True:\n            print(\"\\nNetwork usage per process:\")\n            print_network_usage()\n            time.sleep(interval)\n    except KeyboardInterrupt:\n        print(\"Exiting...\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Monitor network bandwidth usage per process.\")\n    parser.add_argument('-i', '--interval', type=int, default=1, help='Interval in seconds between checks')\n    args = parser.parse_args()\n\n    main(args.interval)\n```",
    "label": 1
  },
  {
    "codes": "```typescript\n// Define the User type\ntype User = {\n  id: ID!\n  username: String!\n  email: String!\n  posts: [Post!]!\n  followers: [User!]!\n  following: [User!]!\n  createdAt: String!\n  updatedAt: String!\n};\n\n// Define the Post type\ntype Post = {\n  id: ID!\n  content: String!\n  author: User!\n  likes: [User!]!\n  comments: [Comment!]!\n  createdAt: String!\n  updatedAt: String!\n};\n\n// Define the Comment type\ntype Comment = {\n  id: ID!\n  content: String!\n  author: User!\n  post: Post!\n  createdAt: String!\n  updatedAt: String!\n};\n\n// Define the Query type\ntype Query = {\n  // Fetch a user by ID\n  user(id: ID!): User\n\n  // Fetch all users\n  users: [User!]!\n\n  // Fetch a post by ID\n  post(id: ID!): Post\n\n  // Fetch all posts\n  posts: [Post!]!\n\n  // Fetch a comment by ID\n  comment(id: ID!): Comment\n\n  // Fetch all comments\n  comments: [Comment!]!\n};\n\n// Define the Mutation type\ntype Mutation = {\n  // Create a new user\n  createUser(username: String!, email: String!, password: String!): User!\n\n  // Create a new post\n  createPost(content: String!, authorId: ID!): Post!\n\n  // Create a new comment\n  createComment(content: String!, authorId: ID!, postId: ID!): Comment!\n\n  // Like a post\n  likePost(userId: ID!, postId: ID!): Post!\n\n  // Follow a user\n  followUser(followerId: ID!, followeeId: ID!): User!\n\n  // Unfollow a user\n  unfollowUser(followerId: ID!, followeeId: ID!): User!\n};\n\n// Define the Subscription type\ntype Subscription = {\n  // Subscribe to new posts by a specific user\n  newPost(userId: ID!): Post!\n\n  // Subscribe to new comments on a specific post\n  newComment(postId: ID!): Comment!\n};\n\n// Export the schema\nexport const schema = `\n  ${User}\n  ${Post}\n  ${Comment}\n  ${Query}\n  ${Mutation}\n  ${Subscription}\n`;\n```",
    "label": 1
  },
  {
    "codes": "```powershell\n# Define the event IDs to audit (common security event IDs)\n$eventIDs = @(4624, 4625, 4672, 4688, 4702, 4719, 4720, 4722, 4723, 4724, 4725, 4726, 4732, 4738, 4740, 4767, 4768, 4776, 4778, 4779, 4781, 4798, 4799, 4800, 4801, 4802, 4803, 4825, 4826, 4946, 4947, 4948, 4950, 4954, 4956, 4957, 5024, 5025, 5027, 5028, 5031, 5032, 5033, 5034, 5035, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5156, 5157, 5158, 5159, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 610",
    "label": 1
  },
  {
    "codes": "```javascript\nconst express = require('express');\nconst bodyParser = require('body-parser');\nconst jwt = require('jsonwebtoken');\n\nconst app = express();\napp.use(bodyParser.json());\n\nconst SECRET_KEY = 'your_secret_key';\n\n// Mock user data\nconst users = {\n    'user@example.com': 'password123'\n};\n\n// Login route\napp.post('/api/login', (req, res) => {\n    const { email, password } = req.body;\n\n    // Check if the user exists\n    if (users[email] && users[email] === password) {\n        // User authenticated, generate token\n        const token = jwt.sign({ email }, SECRET_KEY, { expiresIn: '1h' });\n\n        // Successful authentication, return 200 OK\n        return res.status(200).json({ token });\n    }\n\n    // Unauthorized - wrong email or password, return 401\n    return res.status(401).json({ message: 'Invalid email or password' });\n});\n\n// Error handling for undefined routes\napp.use((req, res, next) => {\n    // Not Found, return 404\n    res.status(404).json({ message: 'Route not found' });\n});\n\n// Error handling middleware\napp.use((err, req, res, next) => {\n    // Internal Server Error, return 500\n    res.status(500).json({ message: 'Internal server error' });\n});\n\nconst PORT = process.env.PORT || 3000;\napp.listen(PORT, () => {\n    console.log(`Server is running on port ${PORT}`);\n});\n```",
    "label": 1
  },
  {
    "codes": "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.next = None\n\ndef reverse_linked_list(head):\n    prev = None\n    current = head\n    while current is not None:\n        next_node = current.next  # Store the next node\n        current.next = prev       # Reverse the current node's pointer\n        prev = current            # Move the prev pointer up\n        current = next_node       # Proceed to the next node\n    return prev                  # New head of the reversed list\n```\n",
    "label": 1
  },
  {
    "codes": "def _apply_to_sets(self, func, operation, keys, *args):\n        \"\"\"Helper function for sdiff, sinter, and sunion\"\"\"\n        keys = self._list_or_args(keys, args)\n        if not keys:\n            raise TypeError(\"{} takes at least two arguments\".format(operation.lower()))\n        left = self._get_set(keys[0], operation) or set()\n        for key in keys[1:]:\n            right = self._get_set(key, operation) or set()\n            left = func(left, right)\n        return left",
    "label": 0
  },
  {
    "codes": "```julia\nusing VoronoiDelaunay\nusing Plots\n\n# Generate random points\nnum_points = 20\npoints = rand(Point2f0, num_points) # 2D points with Float32 coordinates\n\n# Compute Voronoi diagram\nvd = voronoi(points)\n\n# Plot\nplot()\nfor cell in 1:vd.n_cells\n    polygon = vd.cell_vertices(cell)\n    plot!(polygon, seriestype = :shape, fillalpha = 0.2, label=false)\nend\n\n# Plot points\nscatter!(points, marker=:circle, color=:red, label=\"Points\")\n\n# Display the plot\nplot!(legend=:topright, title=\"Voronoi Diagram\")\n```\n",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\n\npublic class Product\n{\n    public string Name { get; set; }\n    public decimal Price { get; set; }\n    public string Category { get; set; }\n}\n\npublic class Program\n{\n    public static void Main()\n    {\n        List<Product> products = new List<Product>\n        {\n            new Product { Name = \"Product1\", Price = 50m, Category = \"Electronics\" },\n            new Product { Name = \"Product2\", Price = 150m, Category = \"Clothing\" },\n            new Product { Name = \"Product3\", Price = 250m, Category = \"Electronics\" },\n            new Product { Name = \"Product4\", Price = 350m, Category = \"Clothing\" }\n        };\n\n        decimal minPrice = 100m;\n        decimal maxPrice = 300m;\n        string targetCategory = \"Electronics\";\n\n        // LINQ query to filter products by price range and category\n        var filteredProducts = products\n            .Where(p => p.Price >= minPrice && p.Price <= maxPrice && p.Category == targetCategory)\n            .ToList();\n\n        // Output filtered products\n        foreach (var product in filteredProducts)\n        {\n            Console.WriteLine($\"{product.Name} - {product.Price:C} - {product.Category}\");\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nconst fs = require('fs');\nconst { pipeline } = require('stream');\nconst csv = require('csv-parser');\nconst { Transform } = require('stream');\n\n// Create a transform stream to convert CSV to NDJSON\nconst csvToNdjson = new Transform({\n  objectMode: true,\n  transform(chunk, encoding, callback) {\n    const jsonLine = JSON.stringify(chunk) + '\\n';\n    callback(null, jsonLine);\n  }\n});\n\n// Source: CSV file read stream\nconst csvFileStream = fs.createReadStream('input.csv');\n\n// Destination: NDJSON file write stream\nconst ndjsonFileStream = fs.createWriteStream('output.ndjson');\n\n// Pipeline: CSV read -> CSV parse -> CSV to NDJSON -> NDJSON write\npipeline(\n  csvFileStream,\n  csv(),\n  csvToNdjson,\n  ndjsonFileStream,\n  (err) => {\n    if (err) {\n      console.error('Pipeline failed:', err);\n    } else {\n      console.log('Pipeline succeeded.');\n    }\n  }\n);\n```",
    "label": 1
  },
  {
    "codes": "```typescript\n// Import necessary modules\nimport { Product, User } from '../support/types';\nimport { checkoutPage, loginPage, productPage } from '../support/pages';\n\ndescribe('E-commerce Checkout Functionality', () => {\n  let user: User;\n  let product: Product;\n\n  before(() => {\n    // Load test data\n    cy.fixture('user').then((userData: User) => {\n      user = userData;\n    });\n    cy.fixture('product').then((productData: Product) => {\n      product = productData;\n    });\n  });\n\n  it('should complete the checkout process successfully', () => {\n    // Step 1: Login\n    loginPage.visit();\n    loginPage.login(user.email, user.password);\n\n    // Step 2: Add product to cart\n    productPage.visit(product.id);\n    productPage.addToCart();\n\n    // Step 3: Proceed to checkout\n    checkoutPage.visit();\n    checkoutPage.proceedToCheckout();\n\n    // Step 4: Fill in shipping details\n    checkoutPage.fillShippingDetails(user);\n\n    // Step 5: Select payment method\n    checkoutPage.selectPaymentMethod('credit_card');\n\n    // Step 6: Place order\n    checkoutPage.placeOrder();\n\n    // Step 7: Verify order confirmation\n    checkoutPage.verifyOrderConfirmation();\n  });\n});\n```\n\n```typescript\n// Example of supporting types and page objects\n// types.ts\nexport interface User {\n  email: string;\n  password: string;\n  firstName: string;\n  lastName: string;\n  address: string;\n  city: string;\n  zipCode: string;\n  country: string;\n}\n\nexport interface Product {\n  id: string;\n  name: string;\n  price: number;\n}\n\n// pages.ts\nexport const loginPage = {\n  visit: () => cy.visit('/login'),\n  login: (email: string, password: string) => {\n    cy.get('#email').type(email);\n    cy.get('#password').type(password);\n    cy.get('#login-button').click();\n  },\n};\n\nexport const productPage = {\n  visit: (productId: string) => cy.visit(`/product/${productId}`),\n  addToCart: () => cy.get('#add-to-cart-button').click(),\n};\n\nexport const checkoutPage = {\n  visit: () => cy.visit('/checkout'),\n  proceedToCheckout: () => cy.get('#proceed-to-checkout').click(),\n  fillShippingDetails: (user: User) => {\n    cy.get('#firstName').type(user.firstName);\n    cy.get('#lastName').type(user.lastName);\n    cy.get('#address').type(user.address);\n    cy.get('#city').type(user.city);\n    cy.get('#zipCode').type(user.zipCode);\n    cy.get('#country').select(user.country);\n  },\n  selectPaymentMethod: (method: string) => {\n    cy.get(`#${method}`).click();\n  },\n  placeOrder: () => cy.get('#place-order-button').click(),\n  verifyOrderConfirmation: () => {\n    cy.get('.order-confirmation').should('be.visible');\n    cy.get('.order-number').should('not.be.empty');\n  },\n};\n```\n\n```typescript\n// Example fixture data\n// fixtures/user.json\n{\n  \"email\": \"test@example.com\",\n  \"password\": \"password123\",\n  \"firstName\": \"John\",\n  \"lastName\": \"Doe\",\n  \"address\": \"123 Main St\",\n  \"city\": \"Anytown\",\n  \"zipCode\": \"12345\",\n  \"country\": \"USA\"\n}\n\n// fixtures/product.json\n{\n  \"id\": \"123\",\n  \"name\": \"Test Product\",\n  \"price\": 99.99\n}\n```",
    "label": 1
  },
  {
    "codes": "function(last) {\n\t\tvar me = this;\n\t\tvar chart = me.chart;\n\t\tvar scale = me._getIndexScale();\n\t\tvar stacked = scale.options.stacked;\n\t\tvar ilen = last === undefined ? chart.data.datasets.length : last + 1;\n\t\tvar stacks = [];\n\t\tvar i, meta;\n\n\t\tfor (i = 0; i < ilen; ++i) {\n\t\t\tmeta = chart.getDatasetMeta(i);\n\t\t\tif (meta.bar && chart.isDatasetVisible(i) &&\n\t\t\t\t(stacked === false ||\n\t\t\t\t(stacked === true && stacks.indexOf(meta.stack) === -1) ||\n\t\t\t\t(stacked === undefined && (meta.stack === undefined || stacks.indexOf(meta.stack) === -1)))) {\n\t\t\t\tstacks.push(meta.stack);\n\t\t\t}\n\t\t}\n\n\t\treturn stacks;\n\t}",
    "label": 0
  },
  {
    "codes": "public Observable<Page<UsageInner>> listMultiRoleUsagesNextAsync(final String nextPageLink) {\n        return listMultiRoleUsagesNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<UsageInner>>, Page<UsageInner>>() {\n                @Override\n                public Page<UsageInner> call(ServiceResponse<Page<UsageInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *EncryptionAtRest) Validate() error {\n\tinvalidParams := request.ErrInvalidParams{Context: \"EncryptionAtRest\"}\n\tif s.CatalogEncryptionMode == nil {\n\t\tinvalidParams.Add(request.NewErrParamRequired(\"CatalogEncryptionMode\"))\n\t}\n\tif s.SseAwsKmsKeyId != nil && len(*s.SseAwsKmsKeyId) < 1 {\n\t\tinvalidParams.Add(request.NewErrParamMinLen(\"SseAwsKmsKeyId\", 1))\n\t}\n\n\tif invalidParams.Len() > 0 {\n\t\treturn invalidParams\n\t}\n\treturn nil\n}",
    "label": 0
  },
  {
    "codes": "def scan(self, cursor='0', match=None, count=10):\n        \"\"\"Emulate scan.\"\"\"\n        def value_function():\n            return sorted(self.redis.keys())  # sorted list for consistent order\n        return self._common_scan(value_function, cursor=cursor, match=match, count=count)",
    "label": 0
  },
  {
    "codes": "def unique(lst):\n    \"\"\"\n    Return unique elements\n\n    :class:`pandas.unique` and :class:`numpy.unique` cast\n    mixed type lists to the same type. They are faster, but\n    some times we want to maintain the type.\n\n    Parameters\n    ----------\n    lst : list-like\n        List of items\n\n    Returns\n    -------\n    out : list\n        Unique items in the order that they appear in the\n        input.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> import numpy as np\n    >>> lst = ['one', 'two', 123, 'three']\n    >>> pd.unique(lst)\n    array(['one', 'two', '123', 'three'], dtype=object)\n    >>> np.unique(lst)\n    array(['123', 'one', 'three', 'two'],\n          dtype='<U5')\n    >>> unique(lst)\n    ['one', 'two', 123, 'three']\n\n    pandas and numpy cast 123 to a string!, and numpy does not\n    even maintain the order.\n    \"\"\"\n    seen = set()\n\n    def make_seen(x):\n        seen.add(x)\n        return x\n\n    return [make_seen(x) for x in lst if x not in seen]",
    "label": 0
  },
  {
    "codes": "```java\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.ServerSocketChannel;\nimport java.nio.channels.SocketChannel;\nimport java.util.Iterator;\nimport java.util.Set;\n\npublic class NonBlockingServer {\n\n    public static void main(String[] args) throws IOException {\n        // Create a selector to manage multiple channels\n        Selector selector = Selector.open();\n\n        // Open a server socket channel\n        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();\n        serverSocketChannel.bind(new InetSocketAddress(\"localhost\", 8080));\n        serverSocketChannel.configureBlocking(false); // Set to non-blocking mode\n\n        // Register the server socket channel with the selector for accepting connections\n        serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);\n\n        System.out.println(\"Server started on port 8080...\");\n\n        while (true) {\n            // Wait for events\n            selector.select();\n\n            // Get the set of keys with pending events\n            Set<SelectionKey> selectedKeys = selector.selectedKeys();\n            Iterator<SelectionKey> keyIterator = selectedKeys.iterator();\n\n            while (keyIterator.hasNext()) {\n                SelectionKey key = keyIterator.next();\n\n                if (key.isAcceptable()) {\n                    // Accept a new connection\n                    ServerSocketChannel serverChannel = (ServerSocketChannel) key.channel();\n                    SocketChannel clientChannel = serverChannel.accept();\n                    clientChannel.configureBlocking(false);\n\n                    // Register the client channel for reading\n                    clientChannel.register(selector, SelectionKey.OP_READ);\n                    System.out.println(\"Client connected: \" + clientChannel.getRemoteAddress());\n                } else if (key.isReadable()) {\n                    // Read data from the client\n                    SocketChannel clientChannel = (SocketChannel) key.channel();\n                    ByteBuffer buffer = ByteBuffer.allocate(1024);\n                    int bytesRead = clientChannel.read(buffer);\n\n                    if (bytesRead == -1) {\n                        // Client disconnected\n                        clientChannel.close();\n                        System.out.println(\"Client disconnected: \" + clientChannel.getRemoteAddress());\n                    } else if (bytesRead > 0) {\n                        // Process the received data\n                        buffer.flip();\n                        byte[] data = new byte[buffer.remaining()];\n                        buffer.get(data);\n                        String message = new String(data);\n                        System.out.println(\"Received: \" + message);\n\n                        // Echo the message back to the client\n                        ByteBuffer responseBuffer = ByteBuffer.wrap(data);\n                        clientChannel.write(responseBuffer);\n                    }\n                }\n\n                // Remove the key from the set to avoid processing it again\n                keyIterator.remove();\n            }\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport gym\nimport numpy as np\nfrom collections import defaultdict\n\n# Initialize the environment\nenv = gym.make('FrozenLake-v1')\n\n# Initialize Q-table with zeros\nQ = defaultdict(lambda: np.zeros(env.action_space.n))\n\n# Hyperparameters\nalpha = 0.1  # Learning rate\ngamma = 0.99  # Discount factor\nepsilon = 1.0  # Exploration rate\nepsilon_decay = 0.995  # Decay rate for exploration\nmin_epsilon = 0.01  # Minimum exploration rate\nnum_episodes = 10000\n\n# Training the agent\nfor episode in range(num_episodes):\n    state = env.reset()\n    done = False\n    \n    while not done:\n        # Epsilon-greedy action selection\n        if np.random.rand() < epsilon:\n            action = env.action_space.sample()  # Explore\n        else:\n            action = np.argmax(Q[state])  # Exploit\n        \n        # Take the action and observe the outcome\n        next_state, reward, done, info = env.step(action)\n        \n        # Update Q-value using the Q-learning formula\n        Q[state][action] = Q[state][action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n        \n        # Move to the next state\n        state = next_state\n    \n    # Decay epsilon\n    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n\n# Test the trained agent\nstate = env.reset()\ndone = False\ntotal_reward = 0\n\nwhile not done:\n    action = np.argmax(Q[state])\n    state, reward, done, info = env.step(action)\n    total_reward += reward\n\nprint(f\"Total reward: {total_reward}\")\n\n# Close the environment\nenv.close()\n```",
    "label": 1
  },
  {
    "codes": "```csharp\n// Define a Polly policy to handle transient faults\nvar retryPolicy = Policy\n    .Handle<HttpRequestException>() // Handle HTTP request exceptions\n    .Or<TimeoutException>()       // Handle timeout exceptions\n    .WaitAndRetryAsync(\n        retryCount: 3,             // Retry up to 3 times\n        sleepDurationProvider: retryAttempt => TimeSpan.FromSeconds(Math.Pow(2, retryAttempt)), // Exponential backoff\n        onRetry: (exception, retryCount, context) => \n        {\n            // Log retry attempts for monitoring\n            Console.WriteLine($\"Retry attempt {retryCount} due to {exception.Message}\");\n        });\n\n// Example usage in a microservice call\nawait retryPolicy.ExecuteAsync(async () =>\n{\n    var response = await httpClient.GetAsync(\"https://api.example.com/data\");\n    response.EnsureSuccessStatusCode(); // Ensure the response is successful\n});\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport org.apache.sanselan.ImageReadException;\nimport org.apache.sanselan.ImageWriteException;\nimport org.apache.sanselan.Sanselan;\nimport org.apache.sanselan.common.IImageMetadata;\nimport org.apache.sanselan.common.ImageMetadata;\nimport org.apache.sanselan.formats.jpeg.JpegImageMetadata;\nimport org.apache.sanselan.formats.jpeg.exifRewrite.ExifRewriter;\nimport org.apache.sanselan.formats.tiff.TiffImageMetadata;\nimport org.apache.sanselan.formats.tiff.write.TiffOutputSet;\n\nimport javax.imageio.ImageIO;\nimport java.awt.image.BufferedImage;\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.OutputStream;\nimport java.util.List;\n\npublic class ImageResizer {\n\n    // Resize images in a directory while preserving EXIF data\n    public static void batchResizeImages(String inputDir, String outputDir, int width, int height) throws IOException, ImageReadException, ImageWriteException {\n        File dir = new File(inputDir);\n        File[] files = dir.listFiles((d, name) -> name.endsWith(\".jpg\") || name.endsWith(\".jpeg\"));\n\n        if (files != null) {\n            for (File file : files) {\n                BufferedImage originalImage = ImageIO.read(file);\n                BufferedImage resizedImage = resizeImage(originalImage, width, height);\n\n                File outputFile = new File(outputDir, file.getName());\n                ImageIO.write(resizedImage, \"jpg\", outputFile);\n\n                // Preserve EXIF metadata\n                preserveExifMetadata(file, outputFile);\n            }\n        }\n    }\n\n    // Resize image to the specified width and height\n    private static BufferedImage resizeImage(BufferedImage originalImage, int width, int height) {\n        BufferedImage resizedImage = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB);\n        resizedImage.createGraphics().drawImage(originalImage.getScaledInstance(width, height, java.awt.Image.SCALE_SMOOTH), 0, 0, null);\n        return resizedImage;\n    }\n\n    // Preserve EXIF metadata from the original image to the resized image\n    private static void preserveExifMetadata(File originalFile, File resizedFile) throws IOException, ImageReadException, ImageWriteException {\n        IImageMetadata metadata = Sanselan.getMetadata(originalFile);\n        if (metadata instanceof JpegImageMetadata) {\n            JpegImageMetadata jpegMetadata = (JpegImageMetadata) metadata;\n            TiffImageMetadata exif = jpegMetadata.getExif();\n\n            if (exif != null) {\n                TiffOutputSet outputSet = exif.getOutputSet();\n                OutputStream os = new FileOutputStream(resizedFile);\n                new ExifRewriter().updateExifMetadataLossless(resizedFile, os, outputSet);\n                os.close();\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        try {\n            batchResizeImages(\"path/to/input/dir\", \"path/to/output/dir\", 800, 600);\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n)\n\n// Matrix represents a 2D matrix\ntype Matrix [][]int\n\n// Multiply two matrices\nfunc multiply(a, b Matrix) Matrix {\n\tn := len(a)\n\tresult := make(Matrix, n)\n\tfor i := range result {\n\t\tresult[i] = make([]int, n)\n\t\tfor j := range result[i] {\n\t\t\tfor k := 0; k < n; k++ {\n\t\t\t\tresult[i][j] += a[i][k] * b[k][j]\n\t\t\t}\n\t\t}\n\t}\n\treturn result\n}\n\n// Identity matrix of size n\nfunc identity(n int) Matrix {\n\tI := make(Matrix, n)\n\tfor i := range I {\n\t\tI[i] = make([]int, n)\n\t\tI[i][i] = 1\n\t}\n\treturn I\n}\n\n// Matrix exponentiation using exponentiation by squaring\nfunc matrixPower(mat Matrix, power int) Matrix {\n\tn := len(mat)\n\tresult := identity(n)\n\tfor power > 0 {\n\t\tif power%2 == 1 {\n\t\t\tresult = multiply(result, mat)\n\t\t}\n\t\tmat = multiply(mat, mat)\n\t\tpower /= 2\n\t}\n\treturn result\n}\n\nfunc main() {\n\tmat := Matrix{\n\t\t{1, 1},\n\t\t{1, 0},\n\t}\n\tpower := 5\n\tresult := matrixPower(mat, power)\n\tfmt.Println(result) // Output: [[8 5] [5 3]]\n}\n```",
    "label": 1
  },
  {
    "codes": "async function filter(array, fn, self = undefined) {\n  const v = await Promise.resolve(array);\n  if (!Array.isArray(v)) {\n    throw TypeError('not an array');\n  }\n\n  const arr = /** @type {!Array} */(v);\n  const n = arr.length;\n  const values = [];\n  let valuesLength = 0;\n\n  for (let i = 0; i < n; i++) {\n    if (i in arr) {\n      let value = arr[i];\n      let include = await fn.call(self, value, i, arr);\n      if (include) {\n        values[valuesLength++] = value;\n      }\n    }\n  }\n  return values;\n}",
    "label": 0
  },
  {
    "codes": "public Observable<Page<WorkerPoolResourceInner>> listWorkerPoolsNextAsync(final String nextPageLink) {\n        return listWorkerPoolsNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<WorkerPoolResourceInner>>, Page<WorkerPoolResourceInner>>() {\n                @Override\n                public Page<WorkerPoolResourceInner> call(ServiceResponse<Page<WorkerPoolResourceInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *UpdateTrailOutput) SetIncludeGlobalServiceEvents(v bool) *UpdateTrailOutput {\n\ts.IncludeGlobalServiceEvents = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function getNameAndBinary (args = process.argv) {\n  const options = minimist(args)\n\n  la(is.unemptyString(options.npm),\n    'missing --npm option', options)\n  la(is.unemptyString(options.binary),\n    'missing --binary option', options)\n\n  let npm = options.npm\n\n  if (fs.existsSync(options.npm)) {\n    console.log('loading NPM url from', options.npm)\n    npm = require(path.resolve(options.npm)).url\n    la(is.url(npm), 'not an url', npm)\n  }\n\n  let binary = options.binary\n\n  if (fs.existsSync(options.binary)) {\n    console.log('loading binary url from', options.binary)\n    binary = require(path.resolve(options.binary)).url\n    la(is.url(binary), 'not an url', binary)\n  }\n\n  return {\n    npm,\n    binary,\n  }\n}",
    "label": 0
  },
  {
    "codes": "func (c *CloudTrail) StopLoggingWithContext(ctx aws.Context, input *StopLoggingInput, opts ...request.Option) (*StopLoggingOutput, error) {\n\treq, out := c.StopLoggingRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "public Observable<Void> inquireAsync(String vaultName, String resourceGroupName, String fabricName, String containerName, String filter) {\n        return inquireWithServiceResponseAsync(vaultName, resourceGroupName, fabricName, containerName, filter).map(new Func1<ServiceResponse<Void>, Void>() {\n            @Override\n            public Void call(ServiceResponse<Void> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```rust\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse std::time::{Duration, Instant};\nuse tokio::sync::Mutex;\nuse hyper::{Body, Request, Response, Server};\nuse hyper::service::{make_service_fn, service_fn};\nuse hyper::{StatusCode, Method};\nuse tokio::time::sleep;\n\n// Struct to store rate limit data for each IP\nstruct RateLimiter {\n    requests: HashMap<String, (Instant, u32)>,\n    limit: u32,\n    window: Duration,\n}\n\nimpl RateLimiter {\n    fn new(limit: u32, window: Duration) -> Self {\n        Self {\n            requests: HashMap::new(),\n            limit,\n            window,\n        }\n    }\n\n    // Check if the IP has exceeded the rate limit\n    fn check_limit(&mut self, ip: &str) -> bool {\n        let now = Instant::now();\n        let (start_time, count) = self.requests.entry(ip.to_string()).or_insert((now, 0));\n\n        if now.duration_since(*start_time) > self.window {\n            *start_time = now;\n            *count = 0;\n        }\n\n        *count += 1;\n        *count <= self.limit\n    }\n}\n\n// Handler for incoming HTTP requests\nasync fn handle_request(\n    req: Request<Body>,\n    rate_limiter: Arc<Mutex<RateLimiter>>,\n) -> Result<Response<Body>, hyper::Error> {\n    let ip = req\n        .headers()\n        .get(\"x-forwarded-for\")\n        .and_then(|v| v.to_str().ok())\n        .unwrap_or(\"unknown\");\n\n    let mut limiter = rate_limiter.lock().await;\n\n    if !limiter.check_limit(ip) {\n        return Ok(Response::builder()\n            .status(StatusCode::TOO_MANY_REQUESTS)\n            .body(Body::from(\"Rate limit exceeded\"))\n            .unwrap());\n    }\n\n    // Simulate some processing time\n    sleep(Duration::from_millis(100)).await;\n\n    Ok(Response::new(Body::from(\"Hello, World!\")))\n}\n\n#[tokio::main]\nasync fn main() {\n    let rate_limiter = Arc::new(Mutex::new(RateLimiter::new(10, Duration::from_secs(60))));\n\n    let make_svc = make_service_fn(move |_conn| {\n        let rate_limiter = Arc::clone(&rate_limiter);\n        async move {\n            Ok::<_, hyper::Error>(service_fn(move |req| {\n                handle_request(req, Arc::clone(&rate_limiter))\n            }))\n        }\n    });\n\n    let addr = ([127, 0, 0, 1], 3000).into();\n    let server = Server::bind(&addr).serve(make_svc);\n\n    if let Err(e) = server.await {\n        eprintln!(\"Server error: {}\", e);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function getUploadParameters (req, res, next) {\n    // @ts-ignore The `uppy` property is added by middleware before reaching here.\n    const client = req.uppy.s3Client\n    const key = config.getKey(req, req.query.filename)\n    if (typeof key !== 'string') {\n      return res.status(500).json({ error: 's3: filename returned from `getKey` must be a string' })\n    }\n\n    const fields = {\n      acl: config.acl,\n      key: key,\n      success_action_status: '201',\n      'content-type': req.query.type\n    }\n\n    client.createPresignedPost({\n      Bucket: config.bucket,\n      Expires: ms('5 minutes') / 1000,\n      Fields: fields,\n      Conditions: config.conditions\n    }, (err, data) => {\n      if (err) {\n        next(err)\n        return\n      }\n      res.json({\n        method: 'post',\n        url: data.url,\n        fields: data.fields\n      })\n    })\n  }",
    "label": 0
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"CustomerClustering\") \\\n    .getOrCreate()\n\n# Load customer data\ncustomer_data = spark.read.csv(\"path/to/customer_data.csv\", header=True, inferSchema=True)\n\n# Select relevant features for clustering\nfeature_columns = [\"age\", \"annual_income\", \"spending_score\"]\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\ndata = assembler.transform(customer_data)\n\n# Scale the features\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\nscaler_model = scaler.fit(data)\nscaled_data = scaler_model.transform(data)\n\n# Train KMeans clustering model\nkmeans = KMeans(k=5, seed=42, featuresCol=\"scaled_features\", predictionCol=\"cluster\")\nmodel = kmeans.fit(scaled_data)\n\n# Get cluster predictions\npredictions = model.transform(scaled_data)\n\n# Show the cluster assignments\npredictions.select(\"customer_id\", \"cluster\").show()\n\n# Stop the Spark session\nspark.stop()\n```",
    "label": 1
  },
  {
    "codes": "```csharp\npublic static class StringExtensions\n{\n    // Extension method to truncate a string to a specified maximum length with ellipsis\n    public static string TruncateWithEllipsis(this string value, int maxLength)\n    {\n        const string ellipsis = \"...\"; // Define the ellipsis string\n        if (string.IsNullOrEmpty(value) || maxLength <= 0)\n        {\n            return string.Empty; // Return empty string for null, empty, or non-positive maxLength\n        }\n\n        if (value.Length <= maxLength)\n        {\n            return value; // Return the original string if it's within the maxLength\n        }\n        \n        int truncatedLength = maxLength - ellipsis.Length; // Calculate the length for the truncated string\n        return truncatedLength > 0 ? value.Substring(0, truncatedLength) + ellipsis : ellipsis;\n        // Truncate and append ellipsis, or just return ellipsis if maxLength is too small\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport org.junit.runner.RunWith;\nimport org.junit.runners.Suite;\nimport org.junit.Test;\nimport static org.junit.Assert.*;\n\n@RunWith(Suite.class)\n@Suite.SuiteClasses({\n    SortAlgorithmTest.EmptyArrayTest.class,\n    SortAlgorithmTest.SingleElementArrayTest.class,\n    SortAlgorithmTest.SortedArrayTest.class,\n    SortAlgorithmTest.ReverseSortedArrayTest.class,\n    SortAlgorithmTest.DuplicateElementsArrayTest.class\n})\npublic class SortAlgorithmTest {\n\n    public static class EmptyArrayTest {\n        @Test\n        public void testEmptyArray() {\n            int[] array = {};\n            sortArray(array);\n            assertArrayEquals(new int[]{}, array);\n        }\n    }\n\n    public static class SingleElementArrayTest {\n        @Test\n        public void testSingleElementArray() {\n            int[] array = {1};\n            sortArray(array);\n            assertArrayEquals(new int[]{1}, array);\n        }\n    }\n\n    public static class SortedArrayTest {\n        @Test\n        public void testSortedArray() {\n            int[] array = {1, 2, 3, 4, 5};\n            sortArray(array);\n            assertArrayEquals(new int[]{1, 2, 3, 4, 5}, array);\n        }\n    }\n\n    public static class ReverseSortedArrayTest {\n        @Test\n        public void testReverseSortedArray() {\n            int[] array = {5, 4, 3, 2, 1};\n            sortArray(array);\n            assertArrayEquals(new int[]{1, 2, 3, 4, 5}, array);\n        }\n    }\n\n    public static class DuplicateElementsArrayTest {\n        @Test\n        public void testDuplicateElementsArray() {\n            int[] array = {2, 3, 1, 3, 2};\n            sortArray(array);\n            assertArrayEquals(new int[]{1, 2, 2, 3, 3}, array);\n        }\n    }\n\n    private static void sortArray(int[] array) {\n        // implement the sorting algorithm here\n    }\n}\n```\nNote: Replace `sortArray` method's body with the actual sorting logic you want to test.",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<WorkerPoolResourceInner> beginCreateOrUpdateWorkerPoolAsync(String resourceGroupName, String name, String workerPoolName, WorkerPoolResourceInner workerPoolEnvelope, final ServiceCallback<WorkerPoolResourceInner> serviceCallback) {\n        return ServiceFuture.fromResponse(beginCreateOrUpdateWorkerPoolWithServiceResponseAsync(resourceGroupName, name, workerPoolName, workerPoolEnvelope), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```dart\n// Import necessary packages\nimport 'package:flutter/services.dart';\n\n// Define a method channel to communicate with native code\nconst MethodChannel _channel = MethodChannel('ml_accelerator_plugin');\n\n// Function to check if ML accelerator is available\nFuture<bool> isMLAcceleratorAvailable() async {\n  try {\n    final bool isAvailable = await _channel.invokeMethod('isMLAcceleratorAvailable');\n    return isAvailable;\n  } on PlatformException catch (e) {\n    print(\"Failed to check ML accelerator availability: ${e.message}\");\n    return false;\n  }\n}\n\n// Function to perform inference using the ML accelerator\nFuture<List<double>> performInference(List<double> inputData) async {\n  try {\n    final List<dynamic> result = await _channel.invokeMethod('performInference', inputData);\n    return result.cast<double>();\n  } on PlatformException catch (e) {\n    print(\"Failed to perform inference: ${e.message}\");\n    return [];\n  }\n}\n\n// Native code (Android example in Kotlin)\n/*\nclass MLAcceleratorPlugin(private val context: Context) : MethodCallHandler {\n\n    override fun onMethodCall(call: MethodCall, result: Result) {\n        when (call.method) {\n            \"isMLAcceleratorAvailable\" -> {\n                val isAvailable = checkMLAcceleratorAvailability()\n                result.success(isAvailable)\n            }\n            \"performInference\" -> {\n                val inputData = call.arguments as List<Double>\n                val outputData = performInference(inputData)\n                result.success(outputData)\n            }\n            else -> result.notImplemented()\n        }\n    }\n\n    private fun checkMLAcceleratorAvailability(): Boolean {\n        // Logic to check if ML accelerator is available\n        return true\n    }\n\n    private fun performInference(inputData: List<Double>): List<Double> {\n        // Logic to perform inference using ML accelerator\n        return inputData.map { it * 2.0 } // Example transformation\n    }\n}\n*/\n\n// Native code (iOS example in Swift)\n/*\nimport Flutter\nimport CoreML\n\npublic class SwiftMLAcceleratorPlugin: NSObject, FlutterPlugin {\n  public static func register(with registrar: FlutterPluginRegistrar) {\n    let channel = FlutterMethodChannel(name: \"ml_accelerator_plugin\", binaryMessenger: registrar.messenger())\n    let instance = SwiftMLAcceleratorPlugin()\n    registrar.addMethodCallDelegate(instance, channel: channel)\n  }\n\n  public func handle(_ call: FlutterMethodCall, result: @escaping FlutterResult) {\n    switch call.method {\n    case \"isMLAcceleratorAvailable\":\n      let isAvailable = checkMLAcceleratorAvailability()\n      result(isAvailable)\n    case \"performInference\":\n      if let inputData = call.arguments as? [Double] {\n        let outputData = performInference(inputData)\n        result(outputData)\n      } else {\n        result(FlutterError(code: \"INVALID_ARGUMENT\", message: \"Invalid input data\", details: nil))\n      }\n    default:\n      result(FlutterMethodNotImplemented)\n    }\n  }\n\n  private func checkMLAcceleratorAvailability() -> Bool {\n    // Logic to check if ML accelerator is available\n    return true\n  }\n\n  private func performInference(_ inputData: [Double]) -> [Double] {\n    // Logic to perform inference using ML accelerator\n    return inputData.map { $0 * 2.0 } // Example transformation\n  }\n}\n*/\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport pygame\nimport sys\n\n# Initialize Pygame\npygame.init()\n\n# Constants\nWIDTH, HEIGHT = 800, 600\nWHITE = (255, 255, 255)\nBLUE = (0, 0, 255)\nFPS = 60\nGRAVITY = 0.8\nJUMP_STRENGTH = -15\n\n# Game variables\nplayer_pos = pygame.Vector2(100, 500)\nplayer_velocity = pygame.Vector2(0, 0)\nis_grounded = False\njump_count = 0\n\n# Set up the display\nscreen = pygame.display.set_mode((WIDTH, HEIGHT))\nclock = pygame.time.Clock()\n\nwhile True:\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            pygame.quit()\n            sys.exit()\n\n    # Input handling\n    keys = pygame.key.get_pressed()\n    if keys[pygame.K_SPACE]:\n        if is_grounded:\n            player_velocity.y = JUMP_STRENGTH\n            is_grounded = False\n            jump_count = 1\n        elif jump_count < 2:\n            player_velocity.y = JUMP_STRENGTH\n            jump_count += 1\n\n    # Apply gravity\n    player_velocity.y += GRAVITY\n\n    # Move player\n    player_pos += player_velocity\n\n    # Simple ground collision\n    if player_pos.y > HEIGHT - 50:\n        player_pos.y = HEIGHT - 50\n        player_velocity.y = 0\n        is_grounded = True\n        jump_count = 0\n\n    # Clear screen\n    screen.fill(WHITE)\n\n    # Draw player\n    pygame.draw.rect(screen, BLUE, (player_pos.x, player_pos.y, 50, 50))\n\n    # Update the display\n    pygame.display.flip()\n    clock.tick(FPS)\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"os\"\n    \"os/signal\"\n    \"sync\"\n\n    \"github.com/Shopify/sarama\"\n)\n\n// processMessage simulates message processing\nfunc processMessage(msg *sarama.ConsumerMessage) {\n    fmt.Printf(\"Processing message: %s\\n\", string(msg.Value))\n}\n\nfunc fanIn(resultsCh chan struct{}, done chan bool, workerCount int) {\n    go func() {\n        completedWorkers := 0\n        for range resultsCh {\n            completedWorkers++\n            if completedWorkers == workerCount {\n                done <- true\n                close(resultsCh)\n                return\n            }\n        }\n    }()\n}\n\nfunc fanOut(messagesCh chan *sarama.ConsumerMessage, resultsCh chan struct{}, workerCount int, wg *sync.WaitGroup) {\n    for i := 0; i < workerCount; i++ {\n        wg.Add(1)\n        go func() {\n            defer wg.Done()\n            for msg := range messagesCh {\n                processMessage(msg)\n            }\n            resultsCh <- struct{}{}\n        }()\n    }\n}\n\nfunc main() {\n    broker := \"localhost:9092\"\n    topic := \"example-topic\"\n    workerCount := 5\n\n    consumer, err := sarama.NewConsumer([]string{broker}, nil)\n    if err != nil {\n        panic(err)\n    }\n    defer consumer.Close()\n\n    messagesCh := make(chan *sarama.ConsumerMessage)\n    resultsCh := make(chan struct{}, workerCount)\n    done := make(chan bool)\n    var wg sync.WaitGroup\n\n    partitionConsumer, err := consumer.ConsumePartition(topic, 0, sarama.OffsetOldest)\n    if err != nil {\n        panic(err)\n    }\n    defer partitionConsumer.Close()\n\n    go func() {\n        for msg := range partitionConsumer.Messages() {\n            messagesCh <- msg\n        }\n        close(messagesCh)\n    }()\n\n    fanIn(resultsCh, done, workerCount)\n    fanOut(messagesCh, resultsCh, workerCount, &wg)\n\n    // Wait for all workers to finish\n    <-done\n    wg.Wait()\n\n    // Handle OS signals for graceful termination\n    sigs := make(chan os.Signal, 1)\n    signal.Notify(sigs, os.Interrupt)\n    <-sigs\n\n    fmt.Println(\"Shutdown complete\")\n}\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing Polly;\nusing Polly.Retry;\nusing System;\n\n// Define the retry policy for handling transient faults\nRetryPolicy retryPolicy = Policy\n    .Handle<Exception>(ex => IsTransient(ex)) // Handle exceptions deemed transient\n    .WaitAndRetry(3, retryAttempt => \n        TimeSpan.FromSeconds(Math.Pow(2, retryAttempt)), // Exponential back-off\n        (exception, timeSpan, retryCount, context) => \n        {\n            // Log or take action on retry\n        });\n\n// Function to identify if an exception is transient\nbool IsTransient(Exception ex) => \n    ex is HttpRequestException || ex is TimeoutException || ex.Message.Contains(\"transient\");\n\n// Execute a code block under this policy\nretryPolicy.Execute(() =>\n{\n    // Your code that may experience transient faults goes here\n});\n```",
    "label": 1
  },
  {
    "codes": "public BackupShortTermRetentionPolicyInner beginUpdate(String resourceGroupName, String serverName, String databaseName, Integer retentionDays) {\n        return beginUpdateWithServiceResponseAsync(resourceGroupName, serverName, databaseName, retentionDays).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<CertificateBundle> recoverDeletedCertificateAsync(String vaultBaseUrl, String certificateName, final ServiceCallback<CertificateBundle> serviceCallback) {\n        return ServiceFuture.fromResponse(recoverDeletedCertificateWithServiceResponseAsync(vaultBaseUrl, certificateName), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "function getTokenAtPosition(sourceFile, position, allowPositionInLeadingTrivia, includeEndPosition) {\n    let current = sourceFile;\n    outer: while (true) {\n        // find the child that contains 'position'\n        for (const child of current.getChildren()) {\n            const start = allowPositionInLeadingTrivia ? child.getFullStart() : child.getStart(sourceFile, /*includeJsDoc*/ true);\n            if (start > position) {\n                // If this child begins after position, then all subsequent children will as well.\n                break;\n            }\n            const end = child.getEnd();\n            if (position < end || (position === end && (child.kind === ts.SyntaxKind.EndOfFileToken || includeEndPosition))) {\n                current = child;\n                continue outer;\n            }\n        }\n        return current;\n    }\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ExtendedServerBlobAuditingPolicyInner> getAsync(String resourceGroupName, String serverName) {\n        return getWithServiceResponseAsync(resourceGroupName, serverName).map(new Func1<ServiceResponse<ExtendedServerBlobAuditingPolicyInner>, ExtendedServerBlobAuditingPolicyInner>() {\n            @Override\n            public ExtendedServerBlobAuditingPolicyInner call(ServiceResponse<ExtendedServerBlobAuditingPolicyInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```java\n// Import necessary packages\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.http.HttpStatus;\nimport org.springframework.http.ResponseEntity;\nimport org.springframework.web.bind.annotation.*;\n\nimport java.util.List;\n\n@RestController\n@RequestMapping(\"/api/inventory\")\npublic class InventoryController {\n\n    @Autowired\n    private InventoryService inventoryService;\n\n    // Get all inventory items\n    @GetMapping\n    public ResponseEntity<List<InventoryItem>> getAllItems() {\n        List<InventoryItem> items = inventoryService.getAllItems();\n        return new ResponseEntity<>(items, HttpStatus.OK);\n    }\n\n    // Get a single inventory item by ID\n    @GetMapping(\"/{id}\")\n    public ResponseEntity<InventoryItem> getItemById(@PathVariable Long id) {\n        InventoryItem item = inventoryService.getItemById(id);\n        if (item != null) {\n            return new ResponseEntity<>(item, HttpStatus.OK);\n        } else {\n            return new ResponseEntity<>(HttpStatus.NOT_FOUND);\n        }\n    }\n\n    // Add a new inventory item\n    @PostMapping\n    public ResponseEntity<InventoryItem> addItem(@RequestBody InventoryItem item) {\n        InventoryItem newItem = inventoryService.addItem(item);\n        return new ResponseEntity<>(newItem, HttpStatus.CREATED);\n    }\n\n    // Update an existing inventory item\n    @PutMapping(\"/{id}\")\n    public ResponseEntity<InventoryItem> updateItem(@PathVariable Long id, @RequestBody InventoryItem item) {\n        InventoryItem updatedItem = inventoryService.updateItem(id, item);\n        if (updatedItem != null) {\n            return new ResponseEntity<>(updatedItem, HttpStatus.OK);\n        } else {\n            return new ResponseEntity<>(HttpStatus.NOT_FOUND);\n        }\n    }\n\n    // Delete an inventory item by ID\n    @DeleteMapping(\"/{id}\")\n    public ResponseEntity<Void> deleteItem(@PathVariable Long id) {\n        boolean isDeleted = inventoryService.deleteItem(id);\n        if (isDeleted) {\n            return new ResponseEntity<>(HttpStatus.NO_CONTENT);\n        } else {\n            return new ResponseEntity<>(HttpStatus.NOT_FOUND);\n        }\n    }\n}\n```\n\n```java\n// Service layer for inventory management\nimport org.springframework.stereotype.Service;\nimport java.util.List;\nimport java.util.Optional;\n\n@Service\npublic class InventoryService {\n\n    // Simulated in-memory storage for demonstration\n    private List<InventoryItem> inventoryItems = // initialize with some data;\n\n    public List<InventoryItem> getAllItems() {\n        return inventoryItems;\n    }\n\n    public InventoryItem getItemById(Long id) {\n        return inventoryItems.stream()\n                .filter(item -> item.getId().equals(id))\n                .findFirst()\n                .orElse(null);\n    }\n\n    public InventoryItem addItem(InventoryItem item) {\n        inventoryItems.add(item);\n        return item;\n    }\n\n    public InventoryItem updateItem(Long id, InventoryItem item) {\n        Optional<InventoryItem> existingItem = inventoryItems.stream()\n                .filter(i -> i.getId().equals(id))\n                .findFirst();\n        if (existingItem.isPresent()) {\n            InventoryItem updatedItem = existingItem.get();\n            updatedItem.setName(item.getName());\n            updatedItem.setQuantity(item.getQuantity());\n            return updatedItem;\n        } else {\n            return null;\n        }\n    }\n\n    public boolean deleteItem(Long id) {\n        return inventoryItems.removeIf(item -> item.getId().equals(id));\n    }\n}\n```\n\n```java\n// Entity class for inventory items\npublic class InventoryItem {\n    private Long id;\n    private String name;\n    private int quantity;\n\n    // Getters and Setters\n    public Long getId() {\n        return id;\n    }\n\n    public void setId(Long id) {\n        this.id = id;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public int getQuantity() {\n        return quantity;\n    }\n\n    public void setQuantity(int quantity) {\n        this.quantity = quantity;\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<BackupSecretResult>> backupSecretWithServiceResponseAsync(String vaultBaseUrl, String secretName) {\n        if (vaultBaseUrl == null) {\n            throw new IllegalArgumentException(\"Parameter vaultBaseUrl is required and cannot be null.\");\n        }\n        if (secretName == null) {\n            throw new IllegalArgumentException(\"Parameter secretName is required and cannot be null.\");\n        }\n        if (this.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.apiVersion() is required and cannot be null.\");\n        }\n        String parameterizedHost = Joiner.on(\", \").join(\"{vaultBaseUrl}\", vaultBaseUrl);\n        return service.backupSecret(secretName, this.apiVersion(), this.acceptLanguage(), parameterizedHost, this.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<BackupSecretResult>>>() {\n                @Override\n                public Observable<ServiceResponse<BackupSecretResult>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<BackupSecretResult> clientResponse = backupSecretDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def clean_community_indexes(communityID):\n    \"\"\"\n    Takes input of community assignments. Returns reindexed community assignment by using smallest numbers possible.\n\n    Parameters\n    ----------\n\n    communityID : array-like\n        list or array of integers. Output from community detection algorithems.\n\n    Returns\n    -------\n\n    new_communityID : array\n        cleaned list going from 0 to len(np.unique(communityID))-1\n\n    Note\n    -----\n\n    Behaviour of funciton entails that the lowest community integer in communityID will recieve the lowest integer in new_communityID.\n\n    \"\"\"\n    communityID = np.array(communityID)\n    cid_shape = communityID.shape\n    if len(cid_shape) > 1:\n        communityID = communityID.flatten()\n    new_communityID = np.zeros(len(communityID))\n    for i, n in enumerate(np.unique(communityID)):\n        new_communityID[communityID == n] = i\n    if len(cid_shape) > 1:\n        new_communityID = new_communityID.reshape(cid_shape)\n    return new_communityID",
    "label": 0
  },
  {
    "codes": "```powershell\n# Set the log path and the time range for the audit\n$logPath = \"Security\"\n$timeFrameStart = (Get-Date).AddDays(-7) # Last 7 days\n\n# Get security events from event log within the timeframe\n$events = Get-WinEvent -LogName $logPath -FilterXPath \"*[System[TimeCreated[@SystemTime>='$timeFrameStart']]]\"\n\n# Filter only critical security events (example: Event IDs for login failures)\n$securityEventIDs = @(4624, 4625, 4720, 4726, 4768, 4771) # Add relevant IDs as needed\n$filteredEvents = $events | Where-Object { $securityEventIDs -contains $_.Id }\n\n# Create an output object for reporting\n$auditReport = foreach ($event in $filteredEvents) {\n    [PSCustomObject]@{\n        TimeCreated = $event.TimeCreated\n        EventID     = $event.Id\n        Message     = $event.Message\n    }\n}\n\n# Output the report\n$auditReport | Format-Table -AutoSize\n\n# Optionally, export the report to a file\n$auditReport | Export-Csv -Path \"SecurityAudit.csv\" -NoTypeInformation\n```",
    "label": 1
  },
  {
    "codes": "```swift\nimport Foundation\n\n// Structure to model a polynomial equation\nstruct Polynomial {\n    // Coefficients of the polynomial, with the index representing the power of x\n    private var coefficients: [Double]\n\n    // Initializer to create a polynomial from given coefficients\n    init(coefficients: [Double]) {\n        self.coefficients = coefficients\n    }\n\n    // Method to evaluate the polynomial for a given value of x\n    func evaluate(at x: Double) -> Double {\n        var result: Double = 0.0\n        for (power, coefficient) in coefficients.enumerated() {\n            result += coefficient * pow(x, Double(power))\n        }\n        return result\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```cpp\nclass DatabaseConnection {\nprivate:\n    // Pointer to the database connection handle\n    void* connectionHandle;\n\npublic:\n    // Constructor: Establishes the database connection\n    DatabaseConnection(const std::string& connectionString) {\n        connectionHandle = openDatabaseConnection(connectionString);\n        if (!connectionHandle) {\n            throw std::runtime_error(\"Failed to connect to the database\");\n        }\n    }\n\n    // Destructor: Closes the database connection\n    ~DatabaseConnection() {\n        if (connectionHandle) {\n            closeDatabaseConnection(connectionHandle);\n        }\n    }\n\n    // Delete copy constructor and copy assignment operator\n    DatabaseConnection(const DatabaseConnection&) = delete;\n    DatabaseConnection& operator=(const DatabaseConnection&) = delete;\n\n    // Move constructor\n    DatabaseConnection(DatabaseConnection&& other) noexcept \n        : connectionHandle(other.connectionHandle) {\n        other.connectionHandle = nullptr;\n    }\n\n    // Move assignment operator\n    DatabaseConnection& operator=(DatabaseConnection&& other) noexcept {\n        if (this != &other) {\n            if (connectionHandle) {\n                closeDatabaseConnection(connectionHandle);\n            }\n            connectionHandle = other.connectionHandle;\n            other.connectionHandle = nullptr;\n        }\n        return *this;\n    }\n\n    // Function to execute a query\n    void executeQuery(const std::string& query) {\n        if (!connectionHandle) {\n            throw std::runtime_error(\"Database connection is not open\");\n        }\n        // Execute the query using the connection handle\n        executeDatabaseQuery(connectionHandle, query);\n    }\n\nprivate:\n    // Helper functions to simulate database operations\n    void* openDatabaseConnection(const std::string& connectionString) {\n        // Simulate opening a database connection\n        return reinterpret_cast<void*>(1); // Dummy handle\n    }\n\n    void closeDatabaseConnection(void* handle) {\n        // Simulate closing a database connection\n    }\n\n    void executeDatabaseQuery(void* handle, const std::string& query) {\n        // Simulate executing a query\n    }\n};\n```",
    "label": 1
  },
  {
    "codes": "def _get_base_dataframe(df):\n    \"\"\"\n    Remove all columns other than those grouped on\n    \"\"\"\n    if isinstance(df, GroupedDataFrame):\n        base_df = GroupedDataFrame(\n            df.loc[:, df.plydata_groups], df.plydata_groups,\n            copy=True)\n    else:\n        base_df = pd.DataFrame(index=df.index)\n    return base_df",
    "label": 0
  },
  {
    "codes": "function checkInContainer () {\n  const fs = require('fs')\n  if (fs.existsSync(`/proc/1/cgroup`)) {\n    const content = fs.readFileSync(`/proc/1/cgroup`, 'utf-8')\n    return /:\\/(lxc|docker|kubepods)\\//.test(content)\n  }\n}",
    "label": 0
  },
  {
    "codes": "```kotlin\n// MainActivity.kt\nimport android.hardware.Sensor\nimport android.hardware.SensorEvent\nimport android.hardware.SensorEventListener\nimport android.hardware.SensorManager\nimport android.os.Bundle\nimport androidx.appcompat.app.AppCompatActivity\nimport com.github.mikephil.charting.charts.LineChart\nimport com.github.mikephil.charting.data.Entry\nimport com.github.mikephil.charting.data.LineData\nimport com.github.mikephil.charting.data.LineDataSet\n\nclass MainActivity : AppCompatActivity(), SensorEventListener {\n\n    private lateinit var sensorManager: SensorManager\n    private lateinit var accelerometer: Sensor\n    private lateinit var lineChart: LineChart\n    private val entries = ArrayList<Entry>()\n    private var timeCounter = 0f\n\n    override fun onCreate(savedInstanceState: Bundle?) {\n        super.onCreate(savedInstanceState)\n        setContentView(R.layout.activity_main)\n\n        // Initialize chart\n        lineChart = findViewById(R.id.lineChart)\n        lineChart.description.isEnabled = false\n        lineChart.setTouchEnabled(false)\n        lineChart.setDrawGridBackground(false)\n\n        // Initialize sensor manager and accelerometer\n        sensorManager = getSystemService(SENSOR_SERVICE) as SensorManager\n        accelerometer = sensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER)\n    }\n\n    override fun onResume() {\n        super.onResume()\n        // Register sensor listener\n        sensorManager.registerListener(this, accelerometer, SensorManager.SENSOR_DELAY_NORMAL)\n    }\n\n    override fun onPause() {\n        super.onPause()\n        // Unregister sensor listener\n        sensorManager.unregisterListener(this)\n    }\n\n    override fun onSensorChanged(event: SensorEvent?) {\n        event?.let {\n            if (it.sensor.type == Sensor.TYPE_ACCELEROMETER) {\n                val xValue = it.values[0]\n                entries.add(Entry(timeCounter, xValue))\n                timeCounter += 1f\n\n                // Limit the number of entries to 100 for better performance\n                if (entries.size > 100) {\n                    entries.removeAt(0)\n                }\n\n                // Update chart data\n                val dataSet = LineDataSet(entries, \"Accelerometer Data\")\n                dataSet.color = android.R.color.holo_blue_light\n                dataSet.setDrawCircles(false)\n                val lineData = LineData(dataSet)\n                lineChart.data = lineData\n                lineChart.invalidate()\n            }\n        }\n    }\n\n    override fun onAccuracyChanged(sensor: Sensor?, accuracy: Int) {\n        // Not used in this example\n    }\n}\n```\n\n```xml\n<!-- activity_main.xml -->\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:orientation=\"vertical\">\n\n    <com.github.mikephil.charting.charts.LineChart\n        android:id=\"@+id/lineChart\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\" />\n</LinearLayout>\n```\n\n```gradle\n// build.gradle (Module: app)\ndependencies {\n    implementation 'com.github.PhilJay:MPAndroidChart:v3.1.0' // Add MPAndroidChart library\n}\n```\n\n```xml\n<!-- AndroidManifest.xml -->\n<uses-permission android:name=\"android.permission.ACCESS_FINE_LOCATION\" />\n<uses-permission android:name=\"android.permission.ACCESS_COARSE_LOCATION\" />\n```",
    "label": 1
  },
  {
    "codes": "```typescript\nfunction Retry(maxRetries: number = 3, delay: number = 1000) {\n    return function (target: any, propertyKey: string, descriptor: PropertyDescriptor) {\n        const originalMethod = descriptor.value;\n\n        descriptor.value = async function (...args: any[]) {\n            let retries = 0;\n\n            while (retries < maxRetries) {\n                try {\n                    // Attempt to call the original method\n                    const result = await originalMethod.apply(this, args);\n                    return result; // If successful, return the result\n                } catch (error) {\n                    retries++;\n                    if (retries >= maxRetries) {\n                        throw error; // If max retries reached, throw the error\n                    }\n                    await new Promise(resolve => setTimeout(resolve, delay)); // Wait before retrying\n                }\n            }\n        };\n\n        return descriptor;\n    };\n}\n\n// Example usage:\nclass MyService {\n    @Retry(5, 2000) // Retry 5 times with a 2-second delay\n    async fetchData() {\n        // Simulate a network request that might fail\n        const success = Math.random() > 0.5;\n        if (!success) {\n            throw new Error('Failed to fetch data');\n        }\n        return 'Data fetched successfully';\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function consoleHandler(record) {\n  if (typeof console === 'undefined' || !console) {\n    return;\n  }\n  record = /** @type {!LogRecord} */(record);\n  var timestamp = new Date(record.getMillis());\n  var msg =\n      '[' + timestamp.getUTCFullYear() + '-' +\n      padNumber(timestamp.getUTCMonth() + 1, 2) + '-' +\n      padNumber(timestamp.getUTCDate(), 2) + 'T' +\n      padNumber(timestamp.getUTCHours(), 2) + ':' +\n      padNumber(timestamp.getUTCMinutes(), 2) + ':' +\n      padNumber(timestamp.getUTCSeconds(), 2) + 'Z]' +\n      '[' + record.getLevel().name + ']' +\n      '[' + record.getLoggerName() + '] ' +\n      record.getMessage();\n\n  var level = record.getLevel().value;\n  if (level >= Logger.Level.SEVERE.value) {\n    console.error(msg);\n  } else if (level >= Logger.Level.WARNING.value) {\n    console.warn(msg);\n  } else {\n    console.log(msg);\n  }\n}",
    "label": 0
  },
  {
    "codes": "```rust\nuse std::collections::HashMap;\n\n// Function to calculate the nth Fibonacci number using memoization\nfn fibonacci(n: u32, memo: &mut HashMap<u32, u64>) -> u64 {\n    // Base cases\n    if n == 0 {\n        return 0;\n    } else if n == 1 {\n        return 1;\n    }\n\n    // Check if result is already in the memoization map\n    if let Some(&result) = memo.get(&n) {\n        return result;\n    }\n\n    // Calculate the Fibonacci number recursively \n    // and store the result in the memoization map\n    let result = fibonacci(n - 1, memo) + fibonacci(n - 2, memo);\n    memo.insert(n, result);\n\n    // Return the result\n    result\n}\n\nfn main() {\n    let mut memo = HashMap::new();\n    let n = 10;\n    let fib_number = fibonacci(n, &mut memo);\n    println!(\"The {}th Fibonacci number is {}\", n, fib_number);\n}\n```\n",
    "label": 1
  },
  {
    "codes": "func (s *ListTagsOutput) SetResourceTagList(v []*ResourceTag) *ListTagsOutput {\n\ts.ResourceTagList = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport javafx.application.Application;\nimport javafx.scene.Scene;\nimport javafx.scene.canvas.Canvas;\nimport javafx.scene.canvas.GraphicsContext;\nimport javafx.scene.layout.StackPane;\nimport javafx.scene.paint.Color;\nimport javafx.stage.Stage;\n\npublic class FractalApp extends Application {\n\n    private static final int WIDTH = 800;\n    private static final int HEIGHT = 800;\n\n    @Override\n    public void start(Stage primaryStage) {\n        Canvas canvas = new Canvas(WIDTH, HEIGHT);\n        GraphicsContext gc = canvas.getGraphicsContext2D();\n        drawFractal(gc, WIDTH / 2, HEIGHT / 2, 200, 0);\n\n        StackPane root = new StackPane(canvas);\n        Scene scene = new Scene(root, WIDTH, HEIGHT);\n        primaryStage.setTitle(\"Interactive Fractal Patterns\");\n        primaryStage.setScene(scene);\n        primaryStage.show();\n    }\n\n    private void drawFractal(GraphicsContext gc, double x, double y, double size, double angle) {\n        if (size < 2) return;\n\n        // Draw the current branch\n        double x2 = x + size * Math.cos(Math.toRadians(angle));\n        double y2 = y - size * Math.sin(Math.toRadians(angle));\n        gc.setStroke(Color.BLACK);\n        gc.strokeLine(x, y, x2, y2);\n\n        // Recursively draw smaller branches\n        drawFractal(gc, x2, y2, size * 0.7, angle + 30);\n        drawFractal(gc, x2, y2, size * 0.7, angle - 30);\n    }\n\n    public static void main(String[] args) {\n        launch(args);\n    }\n}\n```\n\n### Comments:\n- The application creates a simple JavaFX window with a canvas to draw fractal patterns.\n- The `drawFractal` method recursively draws branches, creating a fractal-like structure.\n- The fractal is drawn by recursively reducing the size of the branches and changing the angle.\n- The application can be extended to add interactivity, such as zooming or changing the fractal parameters dynamically.",
    "label": 1
  },
  {
    "codes": "```dart\nString formatRelativeTime(DateTime timestamp) {\n  final now = DateTime.now();\n  final difference = now.difference(timestamp);\n\n  if (difference.inDays > 365) {\n    final years = (difference.inDays / 365).floor();\n    return '$years year${years > 1 ? 's' : ''} ago';\n  } else if (difference.inDays > 30) {\n    final months = (difference.inDays / 30).floor();\n    return '$months month${months > 1 ? 's' : ''} ago';\n  } else if (difference.inDays > 0) {\n    return '${difference.inDays} day${difference.inDays > 1 ? 's' : ''} ago';\n  } else if (difference.inHours > 0) {\n    return '${difference.inHours} hour${difference.inHours > 1 ? 's' : ''} ago';\n  } else if (difference.inMinutes > 0) {\n    return '${difference.inMinutes} minute${difference.inMinutes > 1 ? 's' : ''} ago';\n  } else {\n    return 'just now';\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\n\n# Generate random data\ndata = np.random.randint(1, 100, 50)\nx = np.arange(0, 50, 1)\n\n# Bubble Sort Algorithm\ndef bubble_sort(arr):\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n-i-1):\n            if arr[j] > arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n                yield arr  # Yield the current state of the array\n\n# Function to update the plot\ndef update_fig(arr, bars):\n    for bar, val in zip(bars, arr):\n        bar.set_height(val)\n    return bars\n\n# Initialize the plot\nfig, ax = plt.subplots()\nbars = ax.bar(x, data, color='blue')\nax.set_ylim(0, 100)\n\n# Animate the sorting process\nani = animation.FuncAnimation(fig, func=update_fig, frames=bubble_sort(data), fargs=(bars,), interval=50, repeat=False)\n\nplt.show()\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"os/signal\"\n\t\"sync\"\n\n\t\"github.com/segmentio/kafka-go\"\n)\n\nconst (\n\ttopic        = \"your-topic\"\n\tbroker1      = \"localhost:9092\"\n\tbroker2      = \"localhost:9093\"\n\tbroker3      = \"localhost:9094\"\n\tworkerCount  = 5\n\tprocessCount = 10\n)\n\nfunc main() {\n\t// Setting up the consumer\n\tr := kafka.NewReader(kafka.ReaderConfig{\n\t\tBrokers: []string{broker1, broker2, broker3},\n\t\tTopic:   topic,\n\t\tGroupID: \"example-group\",\n\t})\n\n\t// Context for managing goroutines lifecycle\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\t// Channel to hold incoming messages\n\tmessageChan := make(chan kafka.Message, 100)\n\n\t// WaitGroup to manage goroutines\n\tvar wg sync.WaitGroup\n\n\t// Fan-out: start worker goroutines to process messages\n\tfor i := 0; i < workerCount; i++ {\n\t\twg.Add(1)\n\t\tgo worker(ctx, i, messageChan, &wg)\n\t}\n\n\t// Fan-in: reading the messages from Kafka\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\t// Read a message from the Kafka topic\n\t\t\tm, err := r.ReadMessage(ctx)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Println(\"Error reading message:\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Send message to channel for processing\n\t\t\tmessageChan <- m\n\t\t}\n\t}()\n\n\t// Handling CTRL+C to gracefully shutdown\n\tsigchan := make(chan os.Signal, 1)\n\tsignal.Notify(sigchan, os.Interrupt)\n\t<-sigchan\n\tfmt.Println(\"Shutting down...\")\n\tcancel()\n\tclose(messageChan)\n\n\twg.Wait()\n\tr.Close()\n}\n\nfunc worker(ctx context.Context, id int, messages <-chan kafka.Message, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\tfor {\n\t\tselect {\n\t\tcase m, ok := <-messages:\n\t\t\tif !ok {\n\t\t\t\tfmt.Printf(\"Worker %d: Channel closed, exiting\\n\", id)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Process each message\n\t\t\tfmt.Printf(\"Worker %d: Processing message: %s\\n\", id, string(m.Value))\n\t\tcase <-ctx.Done():\n\t\t\tfmt.Printf(\"Worker %d: Context cancelled, exiting\\n\", id)\n\t\t\treturn\n\t\t}\n\t}\n}\n```\n",
    "label": 1
  },
  {
    "codes": "def close(self):\n        \"\"\"\n        Close the stream and the underlying XML stream (if any is connected).\n\n        This is essentially a way of saying \"I do not want to use this stream\n        anymore\" (until the next call to :meth:`start`). If the stream is\n        currently running, the XML stream is closed gracefully (potentially\n        sending an SM ack), the worker is stopped and any Stream Management\n        state is cleaned up.\n\n        If an error occurs while the stream stops, the error is ignored.\n\n        After the call to :meth:`close` has started, :meth:`on_failure` will\n        not be emitted, even if the XML stream fails before closure has\n        completed.\n\n        After a call to :meth:`close`, the stream is stopped, all SM state is\n        discarded and calls to :meth:`enqueue_stanza` raise a\n        :class:`DestructionRequested` ``\"close() called\"``. Such a\n        :class:`StanzaStream` can be re-started by calling :meth:`start`.\n\n        .. versionchanged:: 0.8\n\n           Before 0.8, an error during a call to :meth:`close` would stop the\n           stream from closing completely, and the exception was re-raised. If\n           SM was enabled, the state would have been kept, allowing for\n           resumption and ensuring that stanzas still enqueued or\n           unacknowledged would get a chance to be sent.\n\n           If you want to have guarantees that all stanzas sent up to a certain\n           point are sent, you should be using :meth:`send_and_wait_for_sent`\n           with stream management.\n        \"\"\"\n        exc = DestructionRequested(\"close() called\")\n\n        if self.running:\n            if self.sm_enabled:\n                self._xmlstream.send_xso(nonza.SMAcknowledgement(\n                    counter=self._sm_inbound_ctr\n                ))\n\n            yield from self._xmlstream.close_and_wait()  # does not raise\n            yield from self.wait_stop()  # may raise\n\n        self._closed = True\n        self._xmlstream_exception = exc\n        self._destroy_stream_state(self._xmlstream_exception)\n        if self.sm_enabled:\n            self.stop_sm()",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <sys/inotify.h>\n#include <limits.h>\n#include <unistd.h>\n#include <iostream>\n#include <string.h>\n#include <cstdlib>\n#include <signal.h>\n#include <syslog.h>\n\n// Function to handle daemon termination\nvoid handle_signal(int signal) {\n    syslog(LOG_INFO, \"Daemon exiting...\");\n    exit(EXIT_SUCCESS);\n}\n\n// Function to spawn the daemon process\nvoid daemonize() {\n    pid_t pid, sid;\n    \n    // Fork off the parent process\n    pid = fork();\n    if (pid < 0) {\n        exit(EXIT_FAILURE);\n    }\n    \n    // If successful, terminate the parent\n    if (pid > 0) {\n        exit(EXIT_SUCCESS);\n    }\n    \n    // Change file mode mask\n    umask(0);\n    \n    // Create a new session ID for the child process\n    sid = setsid();\n    if (sid < 0) {\n        exit(EXIT_FAILURE);\n    }\n    \n    // Change the current working directory\n    if ((chdir(\"/\")) < 0) {\n        exit(EXIT_FAILURE);\n    }\n\n    // Redirect standard files to /dev/null\n    freopen(\"/dev/null\", \"r\", stdin);\n    freopen(\"/dev/null\", \"w\", stdout);\n    freopen(\"/dev/null\", \"w\", stderr);\n\n    // Handle termination signals\n    signal(SIGTERM, handle_signal);\n    signal(SIGHUP, SIG_IGN);\n\n    // Open system log\n    openlog(\"directory-watcher-daemon\", LOG_PID, LOG_DAEMON);\n}\n\n// Function to watch directory changes using inotify\nvoid watch_directory(const std::string &path) {\n    int fd, wd;\n    char buffer[4096]\n      __attribute__ ((aligned(__alignof__(struct inotify_event))));\n    \n    fd = inotify_init1(IN_NONBLOCK);\n    if (fd == -1) {\n        syslog(LOG_ERR, \"Failed to initialize inotify: %s\", strerror(errno));\n        exit(EXIT_FAILURE);\n    }\n\n    wd = inotify_add_watch(fd, path.c_str(), IN_CLOSE_WRITE | IN_CREATE | IN_DELETE);\n    if (wd == -1) {\n        syslog(LOG_ERR, \"Failed to add watch for %s: %s\", path.c_str(), strerror(errno));\n        exit(EXIT_FAILURE);\n    }\n\n    syslog(LOG_INFO, \"Watching directory changes on %s\", path.c_str());\n\n    while (true) {\n        ssize_t len, i = 0;\n\n        len = read(fd, buffer, sizeof(buffer));\n        while (i < len) {\n            struct inotify_event *event = (struct inotify_event *) &buffer[i];\n\n            if (event->mask & IN_CREATE)\n                syslog(LOG_INFO, \"File created: %s\", event->name);\n            if (event->mask & IN_DELETE)\n                syslog(LOG_INFO, \"File deleted: %s\", event->name);\n            if (event->mask & IN_CLOSE_WRITE)\n                syslog(LOG_INFO, \"File modified: %s\", event->name);\n\n            i += sizeof(struct inotify_event) + event->len;\n        }\n\n        sleep(1);\n    }\n\n    close(wd);\n    close(fd);\n}\n\nint main(int argc, char* argv[]) {\n    if (argc != 2) {\n        std::cerr << \"Usage: \" << argv[0] << \" <directory_to_watch>\" << std::endl;\n        exit(EXIT_FAILURE);\n    }\n    \n    daemonize();\n    watch_directory(argv[1]);\n\n    return EXIT_SUCCESS;\n}\n```\n\n**Comments:** \n- This code implements a simple daemon using the `inotify` API to watch for changes in a specified directory on a Linux system.\n- In the `main` function, the program takes a directory path as an argument.\n- The `daemonize` function forks the process to create a daemon and handles session creation, file redirection, and signal setup.\n- The `watch_directory` function sets up inotify to watch for `IN_CREATE`, `IN_DELETE`, and `IN_CLOSE_WRITE` events in the specified directory and logs these events with syslog.",
    "label": 1
  },
  {
    "codes": "```csharp\n// Extension method to truncate a string to a specified max length with ellipsis\npublic static class StringExtensions\n{\n    public static string TruncateWithEllipsis(this string input, int maxLength)\n    {\n        // Return the original string if it's null or shorter than the max length\n        if (string.IsNullOrEmpty(input) || input.Length <= maxLength)\n        {\n            return input;\n        }\n\n        // Truncate the string and append ellipsis\n        return input.Substring(0, maxLength - 3) + \"...\";\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glue) UpdateTableRequest(input *UpdateTableInput) (req *request.Request, output *UpdateTableOutput) {\n\top := &request.Operation{\n\t\tName:       opUpdateTable,\n\t\tHTTPMethod: \"POST\",\n\t\tHTTPPath:   \"/\",\n\t}\n\n\tif input == nil {\n\t\tinput = &UpdateTableInput{}\n\t}\n\n\toutput = &UpdateTableOutput{}\n\treq = c.newRequest(op, input, output)\n\treq.Handlers.Unmarshal.Swap(jsonrpc.UnmarshalHandler.Name, protocol.UnmarshalDiscardBodyHandler)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "```python\n# CircuitPython NMEA GPS Parser Library\n\nimport time\n\nclass NMEAParser:\n    def __init__(self):\n        self.latitude = None\n        self.longitude = None\n        self.altitude = None\n        self.speed = None\n        self.timestamp = None\n        self.satellites = None\n        self.fix_quality = None\n\n    def parse(self, sentence):\n        \"\"\"Parse an NMEA sentence and update attributes.\"\"\"\n        if not sentence.startswith('$'):\n            return False  # Invalid NMEA sentence\n\n        try:\n            parts = sentence.strip().split(',')\n            sentence_type = parts[0][3:]  # Extract sentence type (e.g., GGA, RMC)\n\n            if sentence_type == 'GGA':\n                self._parse_gga(parts)\n            elif sentence_type == 'RMC':\n                self._parse_rmc(parts)\n            else:\n                return False  # Unsupported sentence type\n\n            return True\n        except Exception as e:\n            print(f\"Error parsing NMEA sentence: {e}\")\n            return False\n\n    def _parse_gga(self, parts):\n        \"\"\"Parse GGA (Global Positioning System Fix Data) sentence.\"\"\"\n        if len(parts) < 10:\n            return\n\n        # Extract latitude and longitude\n        lat = parts[2]\n        lat_dir = parts[3]\n        lon = parts[4]\n        lon_dir = parts[5]\n\n        if lat and lon:\n            self.latitude = self._convert_to_decimal(lat, lat_dir)\n            self.longitude = self._convert_to_decimal(lon, lon_dir)\n\n        # Extract altitude\n        if parts[9]:\n            self.altitude = float(parts[9])\n\n        # Extract fix quality and number of satellites\n        self.fix_quality = int(parts[6])\n        self.satellites = int(parts[7])\n\n    def _parse_rmc(self, parts):\n        \"\"\"Parse RMC (Recommended Minimum Specific GPS/Transit Data) sentence.\"\"\"\n        if len(parts) < 10:\n            return\n\n        # Extract timestamp\n        if parts[1]:\n            self.timestamp = parts[1]\n\n        # Extract latitude and longitude\n        lat = parts[3]\n        lat_dir = parts[4]\n        lon = parts[5]\n        lon_dir = parts[6]\n\n        if lat and lon:\n            self.latitude = self._convert_to_decimal(lat, lat_dir)\n            self.longitude = self._convert_to_decimal(lon, lon_dir)\n\n        # Extract speed\n        if parts[7]:\n            self.speed = float(parts[7])\n\n    def _convert_to_decimal(self, value, direction):\n        \"\"\"Convert NMEA latitude/longitude to decimal degrees.\"\"\"\n        if not value or not direction:\n            return None\n\n        degrees = float(value[:2]) if direction in ['N', 'S'] else float(value[:3])\n        minutes = float(value[2:]) if direction in ['N', 'S'] else float(value[3:])\n        decimal = degrees + (minutes / 60.0)\n\n        if direction in ['S', 'W']:\n            decimal *= -1\n\n        return decimal\n\n    def clear(self):\n        \"\"\"Clear all parsed data.\"\"\"\n        self.latitude = None\n        self.longitude = None\n        self.altitude = None\n        self.speed = None\n        self.timestamp = None\n        self.satellites = None\n        self.fix_quality = None\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <iostream>\n#include <fstream>\n#include <vector>\n\n// Function to encrypt data using AES-256-CBC\nbool encryptFile(const std::string& inputFile, const std::string& outputFile, const std::vector<unsigned char>& key) {\n    // Initialize OpenSSL cipher context\n    EVP_CIPHER_CTX* ctx = EVP_CIPHER_CTX_new();\n    if (!ctx) return false;\n\n    // Generate a random IV (Initialization Vector)\n    unsigned char iv[EVP_MAX_IV_LENGTH];\n    if (RAND_bytes(iv, EVP_MAX_IV_LENGTH) != 1) {\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n\n    // Initialize encryption operation\n    if (EVP_EncryptInit_ex(ctx, EVP_aes_256_cbc(), nullptr, key.data(), iv) != 1) {\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n\n    // Open input and output files\n    std::ifstream inFile(inputFile, std::ios::binary);\n    std::ofstream outFile(outputFile, std::ios::binary);\n    if (!inFile || !outFile) {\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n\n    // Write IV to the output file\n    outFile.write(reinterpret_cast<char*>(iv), EVP_MAX_IV_LENGTH);\n\n    // Buffer for reading and encrypting data\n    const size_t bufferSize = 4096;\n    std::vector<unsigned char> inBuffer(bufferSize);\n    std::vector<unsigned char> outBuffer(bufferSize + EVP_MAX_BLOCK_LENGTH);\n\n    int bytesRead, outLen;\n    while ((bytesRead = inFile.read(reinterpret_cast<char*>(inBuffer.data()), bufferSize).gcount()) > 0) {\n        // Encrypt the data\n        if (EVP_EncryptUpdate(ctx, outBuffer.data(), &outLen, inBuffer.data(), bytesRead) != 1) {\n            EVP_CIPHER_CTX_free(ctx);\n            return false;\n        }\n        // Write encrypted data to output file\n        outFile.write(reinterpret_cast<char*>(outBuffer.data()), outLen);\n    }\n\n    // Finalize encryption\n    if (EVP_EncryptFinal_ex(ctx, outBuffer.data(), &outLen) != 1) {\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n    outFile.write(reinterpret_cast<char*>(outBuffer.data()), outLen);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n    return true;\n}\n\nint main() {\n    // Example key (32 bytes for AES-256)\n    std::vector<unsigned char> key = {\n        0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,\n        0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,\n        0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17,\n        0x18, 0x19, 0x1A, 0x1B, 0x1C, 0x1D, 0x1E, 0x1F\n    };\n\n    // Encrypt the file\n    if (encryptFile(\"input.txt\", \"output.enc\", key)) {\n        std::cout << \"File encrypted successfully.\" << std::endl;\n    } else {\n        std::cerr << \"Error encrypting file.\" << std::endl;\n    }\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse std::time::{Duration, Instant};\nuse tokio::sync::Mutex;\nuse hyper::{Body, Request, Response, Server};\nuse hyper::service::{make_service_fn, service_fn};\nuse hyper::StatusCode;\n\n// Struct to hold rate limit data for each IP\nstruct RateLimiter {\n    requests: HashMap<String, (Instant, u32)>,\n    limit: u32,\n    window: Duration,\n}\n\nimpl RateLimiter {\n    fn new(limit: u32, window: Duration) -> Self {\n        RateLimiter {\n            requests: HashMap::new(),\n            limit,\n            window,\n        }\n    }\n\n    // Check if the IP has exceeded the rate limit\n    fn check(&mut self, ip: &str) -> bool {\n        let now = Instant::now();\n        let (start_time, count) = self.requests.entry(ip.to_string()).or_insert((now, 0));\n\n        if now.duration_since(*start_time) > self.window {\n            *start_time = now;\n            *count = 1;\n            true\n        } else if *count < self.limit {\n            *count += 1;\n            true\n        } else {\n            false\n        }\n    }\n}\n\n// Handler for incoming HTTP requests\nasync fn handle_request(\n    req: Request<Body>,\n    rate_limiter: Arc<Mutex<RateLimiter>>,\n) -> Result<Response<Body>, hyper::Error> {\n    let ip = req\n        .headers()\n        .get(\"X-Forwarded-For\")\n        .and_then(|v| v.to_str().ok())\n        .unwrap_or(\"unknown\");\n\n    let mut limiter = rate_limiter.lock().await;\n\n    if limiter.check(ip) {\n        Ok(Response::new(Body::from(\"Request accepted\")))\n    } else {\n        Ok(Response::builder()\n            .status(StatusCode::TOO_MANY_REQUESTS)\n            .body(Body::from(\"Rate limit exceeded\"))\n            .unwrap())\n    }\n}\n\n#[tokio::main]\nasync fn main() {\n    let rate_limiter = Arc::new(Mutex::new(RateLimiter::new(10, Duration::from_secs(60))));\n\n    let make_svc = make_service_fn(move |_conn| {\n        let rate_limiter = Arc::clone(&rate_limiter);\n        async move {\n            Ok::<_, hyper::Error>(service_fn(move |req| {\n                handle_request(req, Arc::clone(&rate_limiter))\n            }))\n        }\n    });\n\n    let addr = ([127, 0, 0, 1], 3000).into();\n    let server = Server::bind(&addr).serve(make_svc);\n\n    if let Err(e) = server.await {\n        eprintln!(\"Server error: {}\", e);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *CreateUserDefinedFunctionInput) SetFunctionInput(v *UserDefinedFunctionInput) *CreateUserDefinedFunctionInput {\n\ts.FunctionInput = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```powershell\n# Define the directory where the files are located\n$directoryPath = \"C:\\Path\\To\\Your\\Files\"\n\n# Define the regex pattern to match files and the replacement pattern\n$regexPattern = \"oldpattern(\\d{4})\"\n$replacementPattern = \"newpattern$1\"\n\n# Get all files in the directory\nGet-ChildItem -Path $directoryPath | ForEach-Object {\n    $fileName = $_.Name\n    $newFileName = [Regex]::Replace($fileName, $regexPattern, $replacementPattern)\n    \n    # Only rename if the new file name is different\n    if ($newFileName -ne $fileName) {\n        $oldFilePath = Join-Path -Path $directoryPath -ChildPath $fileName\n        $newFilePath = Join-Path -Path $directoryPath -ChildPath $newFileName\n        \n        # Rename the file\n        Rename-Item -Path $oldFilePath -NewName $newFileName\n        Write-Host \"Renamed '$fileName' to '$newFileName'\"\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```python\nimport os\nfrom contextlib import contextmanager\n\n@contextmanager\ndef atomic_write(file_path, mode='w', encoding='utf-8'):\n    tmp_file_path = f'{file_path}.tmp'\n    try:\n        with open(tmp_file_path, mode, encoding=encoding) as tmp_file:\n            yield tmp_file\n        os.replace(tmp_file_path, file_path)\n    finally:\n        try:\n            os.remove(tmp_file_path)\n        except FileNotFoundError:\n            pass\n```\n",
    "label": 1
  },
  {
    "codes": "```swift\n// Define a struct to model a polynomial equation\nstruct Polynomial {\n    // Array to store coefficients, where index represents the power of x\n    let coefficients: [Double]\n    \n    // Initializer to set the coefficients\n    init(coefficients: [Double]) {\n        self.coefficients = coefficients\n    }\n    \n    // Method to evaluate the polynomial at a given value of x\n    func evaluate(at x: Double) -> Double {\n        return coefficients.enumerated().reduce(0.0) { result, term in\n            let (power, coefficient) = term\n            return result + coefficient * pow(x, Double(power))\n        }\n    }\n}\n\n// Example usage:\n// Represents the polynomial: 3x^2 + 2x + 1\nlet polynomial = Polynomial(coefficients: [1, 2, 3])\nlet result = polynomial.evaluate(at: 2)  // Evaluates 3*(2)^2 + 2*(2) + 1\nprint(result)  // Output: 17.0\n```",
    "label": 1
  },
  {
    "codes": "def _incr_exceptions(self, conn):\n        \"\"\"Increment the number of exceptions for the current connection.\n\n        :param psycopg2.extensions.connection conn: the psycopg2 connection\n\n        \"\"\"\n        self._pool_manager.get_connection(self.pid, conn).exceptions += 1",
    "label": 0
  },
  {
    "codes": "function setTextStyle(ctx, fontSize) {\n  ctx.font = `${fontSize}px Helvetica,Arial,sans-serif`;\n  ctx.fillStyle = '#000';\n  ctx.textBaseline = 'top';\n  ctx.textAlign = 'center';\n}",
    "label": 0
  },
  {
    "codes": "func (s *Table) SetOwner(v string) *Table {\n\ts.Owner = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *CloudWatchEncryption) SetKmsKeyArn(v string) *CloudWatchEncryption {\n\ts.KmsKeyArn = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Set the directory where log files are located\nlog_directory=\"/path/to/log/files\"\n\n# Set the directory where the archive will be stored\narchive_directory=\"/path/to/archive\"\n\n# Find log files older than 30 days and archive them in a tar.gz file\nfind \"$log_directory\" -type f -mtime +30 -name \"*.log\" -print0 | xargs -0 tar -czf \"$archive_directory/old_logs_$(date +%Y%m%d).tar.gz\"\n\n# Remove the original log files after archiving\nfind \"$log_directory\" -type f -mtime +30 -name \"*.log\" -exec rm {} \\;\n```\n",
    "label": 1
  },
  {
    "codes": "```julia\nusing VoronoiDelaunay\nusing Plots\n\n# Set the number of random points\nnum_points = 10\n\n# Generate random points\npoints = [rand(2) for _ in 1:num_points]\n\n# Create a Voronoi tesselation\ntess = voronoi(points)\n\n# Plot the Voronoi diagram\nplot()\nfor cell in tess.cells\n    polygon!([points[v] for v in cell.vertices], lw=2)\nend\nscatter!(map(p -> p[1], points), map(p -> p[2], points), label=\"Sites\")\n\ndisplay(plot)\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse std::collections::HashMap;\nuse std::net::SocketAddr;\nuse std::sync::{Arc, Mutex};\nuse std::time::{Duration, Instant};\nuse tokio::sync::oneshot;\nuse tokio::time::sleep;\nuse warp::Filter;\n\n// Struct to hold request information\nstruct RequestInfo {\n    count: u64,\n    last_request_time: Instant,\n}\n\n// Function to handle rate limiting logic\nasync fn handle_request(\n    limiter: Arc<Mutex<HashMap<SocketAddr, RequestInfo>>>,\n    addr: SocketAddr,\n    limit: u64,\n    window: Duration,\n) -> Result<impl warp::Reply, std::convert::Infallible> {\n    let mut guard = limiter.lock().unwrap();\n    let entry = guard.entry(addr).or_insert(RequestInfo {\n        count: 0,\n        last_request_time: Instant::now(),\n    });\n\n    let elapsed = entry.last_request_time.elapsed();\n    if elapsed > window {\n        entry.count = 0;\n        entry.last_request_time = Instant::now();\n    }\n\n    if entry.count < limit {\n        entry.count += 1;\n        Ok(warp::reply::with_status(\n            \"Request allowed\",\n            warp::http::StatusCode::OK,\n        ))\n    } else {\n        Ok(warp::reply::with_status(\n            \"Rate limit exceeded\",\n            warp::http::StatusCode::TOO_MANY_REQUESTS,\n        ))\n    }\n}\n\n// Main function to set up the warp server\n#[tokio::main]\nasync fn main() {\n    let rate_limiter = Arc::new(Mutex::new(HashMap::new()));\n    let limit = 5; // Allow 5 requests\n    let window = Duration::from_secs(60); // Per 60 seconds\n\n    let route = warp::any()\n        .and(warp::filters::addr::remote().and_then(\n            move |addr: Option<SocketAddr>| {\n                let limiter = rate_limiter.clone();\n                async move {\n                    match addr {\n                        Some(addr) => handle_request(limiter, addr, limit, window).await,\n                        None => Ok(warp::reply::with_status(\n                            \"Address not found\",\n                            warp::http::StatusCode::BAD_REQUEST,\n                        )),\n                    }\n                }\n            },\n        ));\n\n    warp::serve(route).run(([0, 0, 0, 0], 3030)).await;\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary Selenium modules\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# Initialize the WebDriver (e.g., Chrome)\ndriver = webdriver.Chrome()\n\n# Open the login page\ndriver.get(\"https://example.com/login\")\n\n# Wait for the username field to be present and enter the username\nusername_field = WebDriverWait(driver, 10).until(\n    EC.presence_of_element_located((By.ID, \"username\"))\n)\nusername_field.send_keys(\"test_user\")\n\n# Wait for the password field to be present and enter the password\npassword_field = WebDriverWait(driver, 10).until(\n    EC.presence_of_element_located((By.ID, \"password\"))\n)\npassword_field.send_keys(\"test_password\")\n\n# Click the login button\nlogin_button = WebDriverWait(driver, 10).until(\n    EC.element_to_be_clickable((By.ID, \"login-button\"))\n)\nlogin_button.click()\n\n# Wait for the dashboard to load and verify login success\ndashboard_element = WebDriverWait(driver, 10).until(\n    EC.presence_of_element_located((By.ID, \"dashboard\"))\n)\nassert \"Dashboard\" in driver.title\n\n# Close the browser\ndriver.quit()\n```",
    "label": 1
  },
  {
    "codes": "def append(self, child, *args, **kwargs):\n        \"\"\"See :meth:`AbstractElement.append`\"\"\"\n        #Accept Word instances instead of WordReference, references will be automagically used upon serialisation\n        if isinstance(child, (Word, Morpheme, Phoneme)) and WordReference in self.ACCEPTED_DATA:\n            #We don't really append but do an insertion so all references are in proper order\n            insertionpoint = len(self.data)\n            for i, sibling in enumerate(self.data):\n                if isinstance(sibling, (Word, Morpheme, Phoneme)):\n                    try:\n                        if not sibling.precedes(child):\n                            insertionpoint = i\n                    except: #happens if we can't determine common ancestors\n                        pass\n\n            self.data.insert(insertionpoint, child)\n            return child\n        elif isinstance(child, AbstractSpanAnnotation): #(covers span roles just as well)\n            insertionpoint = len(self.data)\n            try:\n                firstword = child.wrefs(0)\n            except IndexError:\n                #we have no basis to determine an insertionpoint for this child, just append it then\n                return super(AbstractSpanAnnotation,self).append(child, *args, **kwargs)\n\n            insertionpoint = len(self.data)\n            for i, sibling in enumerate(self.data):\n                if isinstance(sibling, (Word, Morpheme, Phoneme)):\n                    try:\n                        if not sibling.precedes(firstword):\n                            insertionpoint = i\n                    except: #happens if we can't determine common ancestors\n                        pass\n            return super(AbstractSpanAnnotation,self).insert(insertionpoint, child, *args, **kwargs)\n        else:\n            return super(AbstractSpanAnnotation,self).append(child, *args, **kwargs)",
    "label": 0
  },
  {
    "codes": "func (s *GetJobsOutput) SetNextToken(v string) *GetJobsOutput {\n\ts.NextToken = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function runListFeatures(callback) {\n  var rect = new messages.Rectangle();\n  var lo = new messages.Point();\n  lo.setLatitude(400000000);\n  lo.setLongitude(-750000000);\n  rect.setLo(lo);\n  var hi = new messages.Point();\n  hi.setLatitude(420000000);\n  hi.setLongitude(-730000000);\n  rect.setHi(hi);\n  console.log('Looking for features between 40, -75 and 42, -73');\n  var call = client.listFeatures(rect);\n  call.on('data', function(feature) {\n      console.log('Found feature called \"' + feature.getName() + '\" at ' +\n          feature.getLocation().getLatitude()/COORD_FACTOR + ', ' +\n          feature.getLocation().getLongitude()/COORD_FACTOR);\n  });\n  call.on('end', callback);\n}",
    "label": 0
  },
  {
    "codes": "func (s *JobParameters) SetInventoryRetrievalParameters(v *InventoryRetrievalJobInput) *JobParameters {\n\ts.InventoryRetrievalParameters = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function runRouteChat(callback) {\n  var call = client.routeChat();\n  call.on('data', function(note) {\n    console.log('Got message \"' + note.message + '\" at ' +\n        note.location.latitude + ', ' + note.location.longitude);\n  });\n\n  call.on('end', callback);\n\n  var notes = [{\n    location: {\n      latitude: 0,\n      longitude: 0\n    },\n    message: 'First message'\n  }, {\n    location: {\n      latitude: 0,\n      longitude: 1\n    },\n    message: 'Second message'\n  }, {\n    location: {\n      latitude: 1,\n      longitude: 0\n    },\n    message: 'Third message'\n  }, {\n    location: {\n      latitude: 0,\n      longitude: 0\n    },\n    message: 'Fourth message'\n  }];\n  for (var i = 0; i < notes.length; i++) {\n    var note = notes[i];\n    console.log('Sending message \"' + note.message + '\" at ' +\n        note.location.latitude + ', ' + note.location.longitude);\n    call.write(note);\n  }\n  call.end();\n}",
    "label": 0
  },
  {
    "codes": "```csharp\n// Assuming you have a list of products with properties: Price, Category.\nvar filteredProducts = products\n    .Where(product => product.Price >= minPrice && product.Price <= maxPrice)\n    .Where(product => product.Category == desiredCategory)\n    .ToList();\n```",
    "label": 1
  },
  {
    "codes": "public Observable<MetricDefinitionInner> listMetricDefinitionsAsync(String resourceGroupName, String name) {\n        return listMetricDefinitionsWithServiceResponseAsync(resourceGroupName, name).map(new Func1<ServiceResponse<MetricDefinitionInner>, MetricDefinitionInner>() {\n            @Override\n            public MetricDefinitionInner call(ServiceResponse<MetricDefinitionInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport time\nimport functools\n\ndef measure_time_ns(func):\n    \"\"\"\n    Decorator to measure the execution time of a function in nanoseconds.\n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.perf_counter_ns()  # Start the timer in nanoseconds\n        result = func(*args, **kwargs)\n        end_time = time.perf_counter_ns()  # End the timer in nanoseconds\n        execution_time = end_time - start_time  # Calculate the total time\n        print(f\"Execution time of {func.__name__}: {execution_time} nanoseconds\")\n        return result\n\n    return wrapper\n\n# Example usage\n@measure_time_ns\ndef example_function():\n    sum(range(100000))\n\nexample_function()\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport xml.etree.ElementTree as ET\n\ndef iterparse_large_xml(file_path):\n    # Context to iterate through events in the XML file\n    context = ET.iterparse(file_path, events=(\"start\", \"end\"))\n    _, root = next(context)  # Get the root element\n\n    for event, elem in context:\n        if event == \"end\":\n            yield elem  # Yield the element\n            root.clear()  # Clear processed elements to save memory\n```\n",
    "label": 1
  },
  {
    "codes": "func (s *GetPartitionsOutput) SetPartitions(v []*Partition) *GetPartitionsOutput {\n\ts.Partitions = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (c *Glacier) ListProvisionedCapacityRequest(input *ListProvisionedCapacityInput) (req *request.Request, output *ListProvisionedCapacityOutput) {\n\top := &request.Operation{\n\t\tName:       opListProvisionedCapacity,\n\t\tHTTPMethod: \"GET\",\n\t\tHTTPPath:   \"/{accountId}/provisioned-capacity\",\n\t}\n\n\tif input == nil {\n\t\tinput = &ListProvisionedCapacityInput{}\n\t}\n\n\toutput = &ListProvisionedCapacityOutput{}\n\treq = c.newRequest(op, input, output)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "```c\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <string.h>\n#include <stdio.h>\n#include <stdlib.h>\n\n// AES-GCM Encryption\nint aes_gcm_encrypt(const unsigned char *plaintext, int plaintext_len, \n                    const unsigned char *key, const unsigned char *iv, \n                    unsigned char *ciphertext, unsigned char *tag) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) return -1;\n\n    // Initialize the encryption operation\n    if (EVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL) != 1) return -1;\n\n    // Set IV length\n    if (EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_IVLEN, 12, NULL) != 1) return -1;\n\n    // Initialize key and IV\n    if (EVP_EncryptInit_ex(ctx, NULL, NULL, key, iv) != 1) return -1;\n\n    // Provide the message to be encrypted\n    if (EVP_EncryptUpdate(ctx, ciphertext, &len, plaintext, plaintext_len) != 1) return -1;\n    \n    // Finalize the encryption\n    if (EVP_EncryptFinal_ex(ctx, ciphertext + len, &len) != 1) return -1;\n\n    // Get the tag\n    if (EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_GET_TAG, 16, tag) != 1) return -1;\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n    return 0;\n}\n\n// AES-GCM Decryption\nint aes_gcm_decrypt(const unsigned char *ciphertext, int ciphertext_len, \n                    const unsigned char *key, const unsigned char *iv, \n                    const unsigned char *tag, unsigned char *plaintext) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int plaintext_len;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) return -1;\n\n    // Initialize the decryption operation\n    if (EVP_DecryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL) != 1) return -1;\n\n    // Set IV length\n    if (EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_IVLEN, 12, NULL) != 1) return -1;\n\n    // Initialize key and IV\n    if (EVP_DecryptInit_ex(ctx, NULL, NULL, key, iv) != 1) return -1;\n\n    // Provide the message to be decrypted\n    if (EVP_DecryptUpdate(ctx, plaintext, &len, ciphertext, ciphertext_len) != 1) return -1;\n    \n    plaintext_len = len;\n\n    // Set the expected tag value\n    if (EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_TAG, 16, (void *)tag) != 1) return -1;\n\n    // Finalize the decryption\n    if (EVP_DecryptFinal_ex(ctx, plaintext + len, &len) != 1) return -1;\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n    return plaintext_len + len; // Return total plaintext length\n}\n\n// Example usage of the wrapper\nvoid example_usage() {\n    unsigned char key[32]; // AES-256 key\n    unsigned char iv[12];  // Initialization vector\n    unsigned char plaintext[] = \"This is a test message.\";\n    unsigned char ciphertext[128];\n    unsigned char tag[16];\n    unsigned char decrypted[128];\n\n    // Generate random key and iv\n    RAND_bytes(key, sizeof(key));\n    RAND_bytes(iv, sizeof(iv));\n\n    // Encrypt the plaintext\n    if (aes_gcm_encrypt(plaintext, strlen((char *)plaintext), key, iv, ciphertext, tag) != 0) {\n        fprintf(stderr, \"Encryption failed\\n\");\n        return;\n    }\n\n    // Decrypt the ciphertext\n    int decrypted_len = aes_gcm_decrypt(ciphertext, sizeof(ciphertext), key, iv, tag, decrypted);\n    if (decrypted_len < 0) {\n        fprintf(stderr, \"Decryption failed\\n\");\n        return;\n    }\n\n    decrypted[decrypted_len] = '\\0'; // Null-terminate the decrypted string\n    printf(\"Decrypted text: %s\\n\", decrypted);\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *LookupEventsInput) SetMaxResults(v int64) *LookupEventsInput {\n\ts.MaxResults = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *Condition) SetState(v string) *Condition {\n\ts.State = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "def _do_write(fname, variable, version, date, table):\n        \"\"\"Write combining tables to filesystem as python code.\"\"\"\n        # pylint: disable=R0914\n        #         Too many local variables (19/15) (col 4)\n        print(\"writing {} ..\".format(fname))\n        import unicodedata\n        import datetime\n        import string\n        utc_now = datetime.datetime.utcnow()\n        indent = 4\n        with open(fname, 'w') as fout:\n            fout.write(\n                '\"\"\"{variable_proper} table. Created by setup.py.\"\"\"\\n'\n                \"# Generated: {iso_utc}\\n\"\n                \"# Source: {version}\\n\"\n                \"# Date: {date}\\n\"\n                \"{variable} = (\".format(iso_utc=utc_now.isoformat(),\n                                        version=version,\n                                        date=date,\n                                        variable=variable,\n                                        variable_proper=variable.title()))\n            for start, end in table:\n                ucs_start, ucs_end = unichr(start), unichr(end)\n                hex_start, hex_end = ('0x{0:04x}'.format(start),\n                                      '0x{0:04x}'.format(end))\n                try:\n                    name_start = string.capwords(unicodedata.name(ucs_start))\n                except ValueError:\n                    name_start = u''\n                try:\n                    name_end = string.capwords(unicodedata.name(ucs_end))\n                except ValueError:\n                    name_end = u''\n                fout.write('\\n' + (' ' * indent))\n                fout.write('({0}, {1},),'.format(hex_start, hex_end))\n                fout.write('  # {0:24s}..{1}'.format(\n                    name_start[:24].rstrip() or '(nil)',\n                    name_end[:24].rstrip()))\n            fout.write('\\n)\\n')\n        print(\"complete.\")",
    "label": 0
  },
  {
    "codes": "public Observable<List<ServerUsageInner>> listByServerAsync(String resourceGroupName, String serverName) {\n        return listByServerWithServiceResponseAsync(resourceGroupName, serverName).map(new Func1<ServiceResponse<List<ServerUsageInner>>, List<ServerUsageInner>>() {\n            @Override\n            public List<ServerUsageInner> call(ServiceResponse<List<ServerUsageInner>> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```dart\n// pubspec.yaml\ndependencies:\n  flutter:\n    sdk: flutter\n  platform: ^3.0.0\n  ffi: ^2.0.1  // For using native code\n\nflutter:\n  plugin:\n    platforms:\n      android:\n        package: com.example.ml_accelerator_plugin\n        pluginClass: MlAcceleratorPlugin\n      ios:\n        pluginClass: MlAcceleratorPlugin\n\n// lib/ml_accelerator.dart\nimport 'dart:async';\nimport 'dart:ffi';  // FFI bridge to native code\nimport 'package:flutter/services.dart';\n\nclass MlAccelerator {\n  static const MethodChannel _channel =\n      const MethodChannel('ml_accelerator');\n\n  static Future<bool> get isAvailable async {\n    final bool available = await _channel.invokeMethod('isAcceleratorAvailable');\n    return available;\n  }\n}\n\n// android/src/main/kotlin/com/example/ml_accelerator_plugin/MlAcceleratorPlugin.kt\npackage com.example.ml_accelerator_plugin\n\nimport androidx.annotation.NonNull\nimport io.flutter.embedding.engine.plugins.FlutterPlugin\nimport io.flutter.plugin.common.MethodCall\nimport io.flutter.plugin.common.MethodChannel\nimport io.flutter.plugin.common.MethodChannel.MethodCallHandler\nimport io.flutter.plugin.common.MethodChannel.Result\n\nclass MlAcceleratorPlugin: FlutterPlugin, MethodCallHandler {\n  private lateinit var channel : MethodChannel\n\n  override fun onAttachedToEngine(@NonNull flutterPluginBinding: FlutterPlugin.FlutterPluginBinding) {\n    channel = MethodChannel(flutterPluginBinding.binaryMessenger, \"ml_accelerator\")\n    channel.setMethodCallHandler(this)\n  }\n\n  override fun onMethodCall(@NonNull call: MethodCall, @NonNull result: Result) {\n    when (call.method) {\n      \"isAcceleratorAvailable\" -> result.success(checkAcceleratorAvailability())\n      else -> result.notImplemented()\n    }\n  }\n\n  private fun checkAcceleratorAvailability(): Boolean {\n    // Implement logic to check for ML accelerator availability\n    return true\n  }\n\n  override fun onDetachedFromEngine(@NonNull binding: FlutterPlugin.FlutterPluginBinding) {\n    channel.setMethodCallHandler(null)\n  }\n}\n\n// ios/Classes/MlAcceleratorPlugin.swift\nimport Flutter\nimport UIKit\n\npublic class SwiftMlAcceleratorPlugin: NSObject, FlutterPlugin {\n  public static func register(with registrar: FlutterPluginRegistrar) {\n    let channel = FlutterMethodChannel(name: \"ml_accelerator\", binaryMessenger: registrar.messenger())\n    let instance = SwiftMlAcceleratorPlugin()\n    registrar.addMethodCallDelegate(instance, channel: channel)\n  }\n\n  public func handle(_ call: FlutterMethodCall, result: @escaping FlutterResult) {\n    switch call.method {\n    case \"isAcceleratorAvailable\":\n      result(isAcceleratorAvailable())\n    default:\n      result(FlutterMethodNotImplemented)\n    }\n  }\n\n  private func isAcceleratorAvailable() -> Bool {\n    // Implement logic to check for ML accelerator availability on iOS\n    return true\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "function updateCollapsedAndCommonAncestor(range) {\n            range.collapsed = (range.startContainer === range.endContainer && range.startOffset === range.endOffset);\n            range.commonAncestorContainer = range.collapsed ?\n                range.startContainer : dom.getCommonAncestor(range.startContainer, range.endContainer);\n        }",
    "label": 0
  },
  {
    "codes": "```csharp\n// Custom attribute to validate JWT claims\n[AttributeUsage(AttributeTargets.Method | AttributeTargets.Class, AllowMultiple = true)]\npublic class ValidateJwtClaimAttribute : ActionFilterAttribute\n{\n    private readonly string _claimType;\n    private readonly string _claimValue;\n\n    public ValidateJwtClaimAttribute(string claimType, string claimValue)\n    {\n        _claimType = claimType;\n        _claimValue = claimValue;\n    }\n\n    public override void OnActionExecuting(ActionExecutingContext context)\n    {\n        var user = context.HttpContext.User;\n\n        // Check if the user has the specified claim\n        if (!user.HasClaim(_claimType, _claimValue))\n        {\n            context.Result = new ForbidResult(); // Return 403 Forbidden if claim is not valid\n            return;\n        }\n\n        base.OnActionExecuting(context);\n    }\n}\n\n// Usage example in a controller\n[ApiController]\n[Route(\"api/[controller]\")]\npublic class SampleController : ControllerBase\n{\n    [HttpGet(\"secure\")]\n    [ValidateJwtClaim(\"role\", \"admin\")] // Validate that the JWT has a \"role\" claim with value \"admin\"\n    public IActionResult SecureEndpoint()\n    {\n        return Ok(\"Access granted to secure endpoint.\");\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```lua\n-- Create a new terrain part\nlocal function createTerrain(position, size, material)\n    local part = Instance.new(\"Part\")\n    part.Size = size\n    part.Position = position\n    part.Anchored = true\n    part.Material = material\n    part.Parent = workspace\n    return part\nend\n\n-- Function to destroy terrain based on an explosion\nlocal function destroyTerrain(part, explosionPosition, explosionRadius)\n    local distance = (part.Position - explosionPosition).Magnitude\n    if distance <= explosionRadius then\n        part:Destroy()\n    end\nend\n\n-- Detects when a tool is used and triggers the explosion\nlocal function onToolActivated(tool)\n    local explosionRadius = 10\n    local toolPosition = tool.Handle.Position\n\n    for _, part in ipairs(workspace:GetChildren()) do\n        if part:IsA(\"Part\") and part.Anchored then\n            destroyTerrain(part, toolPosition, explosionRadius)\n        end\n    end\nend\n\n-- Setup a tool to trigger the terrain destruction\nlocal function setupDestructionTool(toolName)\n    local tool = Instance.new(\"Tool\")\n    tool.Name = toolName\n    tool.RequiresHandle = true\n    \n    local handle = Instance.new(\"Part\")\n    handle.Name = \"Handle\"\n    handle.Size = Vector3.new(1, 1, 1)\n    handle.Parent = tool\n    \n    tool.Activated:Connect(function() onToolActivated(tool) end)\n    tool.Parent = game.Players.LocalPlayer.Backpack\nend\n\n-- Setup an example terrain\ncreateTerrain(Vector3.new(0, 0, 0), Vector3.new(25, 1, 25), Enum.Material.Grass)\ncreateTerrain(Vector3.new(0, 1, 0), Vector3.new(25, 1, 25), Enum.Material.Sand)\n\n-- Create and configure the tool\nsetupDestructionTool(\"DestroyerTool\")\n```\n",
    "label": 1
  },
  {
    "codes": "```typescript\nimport { Injectable } from '@angular/core';\nimport { HttpClient, HttpParams } from '@angular/common/http';\nimport { Observable } from 'rxjs';\n\ninterface PaginatedResponse<T> {\n  data: T[];\n  total: number;\n  page: number;\n  pageSize: number;\n}\n\n@Injectable({\n  providedIn: 'root'\n})\nexport class ApiService {\n\n  // Base URL for the API endpoint\n  private readonly apiUrl = 'https://api.example.com/items';\n\n  constructor(private http: HttpClient) {}\n\n  // Fetch paginated data from the API\n  public fetchPaginatedData<T>(page: number, pageSize: number): Observable<PaginatedResponse<T>> {\n    const params = new HttpParams()\n      .set('page', page.toString())          // Set the page number\n      .set('pageSize', pageSize.toString()); // Set the number of items per page\n    // Make the HTTP GET request to fetch data\n    return this.http.get<PaginatedResponse<T>>(this.apiUrl, { params });\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "def expire(self, key, delta):\n        \"\"\"Emulate expire\"\"\"\n        delta = delta if isinstance(delta, timedelta) else timedelta(seconds=delta)\n        return self._expire(self._encode(key), delta)",
    "label": 0
  },
  {
    "codes": "```swift\nimport Security\n\nfunc saveKeyToKeychain(key: String, value: String, service: String) -> Bool {\n    // Convert the key and value to Data\n    guard let keyData = key.data(using: .utf8),\n          let valueData = value.data(using: .utf8) else {\n        return false\n    }\n    \n    // Create a query dictionary for the keychain\n    let query: [String: Any] = [\n        kSecClass as String: kSecClassGenericPassword,\n        kSecAttrService as String: service,\n        kSecAttrAccount as String: keyData,\n        kSecValueData as String: valueData\n    ]\n    \n    // Delete any existing item with the same key\n    SecItemDelete(query as CFDictionary)\n    \n    // Add the new item to the keychain\n    let status = SecItemAdd(query as CFDictionary, nil)\n    \n    return status == errSecSuccess\n}\n\nfunc retrieveKeyFromKeychain(key: String, service: String) -> String? {\n    // Convert the key to Data\n    guard let keyData = key.data(using: .utf8) else {\n        return nil\n    }\n    \n    // Create a query dictionary for the keychain\n    let query: [String: Any] = [\n        kSecClass as String: kSecClassGenericPassword,\n        kSecAttrService as String: service,\n        kSecAttrAccount as String: keyData,\n        kSecReturnData as String: true,\n        kSecMatchLimit as String: kSecMatchLimitOne\n    ]\n    \n    // Retrieve the item from the keychain\n    var item: CFTypeRef?\n    let status = SecItemCopyMatching(query as CFDictionary, &item)\n    \n    // Check if the item was found and convert it to a String\n    if status == errSecSuccess, let data = item as? Data {\n        return String(data: data, encoding: .utf8)\n    }\n    \n    return nil\n}\n\nfunc deleteKeyFromKeychain(key: String, service: String) -> Bool {\n    // Convert the key to Data\n    guard let keyData = key.data(using: .utf8) else {\n        return false\n    }\n    \n    // Create a query dictionary for the keychain\n    let query: [String: Any] = [\n        kSecClass as String: kSecClassGenericPassword,\n        kSecAttrService as String: service,\n        kSecAttrAccount as String: keyData\n    ]\n    \n    // Delete the item from the keychain\n    let status = SecItemDelete(query as CFDictionary)\n    \n    return status == errSecSuccess\n}\n```",
    "label": 1
  },
  {
    "codes": "def get_selected_files(self, pipeline='pipeline', forfile=None, quiet=0, allowedfileformats='default'):\n        \"\"\"\n        Parameters\n        ----------\n        pipeline : string\n            can be \\'pipeline\\' (main analysis pipeline, self in tnet.set_pipeline) or \\'confound\\' (where confound files are, set in tnet.set_confonud_pipeline()),\n            \\'functionalconnectivity\\'\n        quiet: int\n            If 1, prints results. If 0, no results printed.\n        forfile : str or dict\n            A filename or dictionary of file tags. If this is set, only files that match that subject\n        accepted_fileformat : list\n            list of files formats that are acceptable. Default list is: ['.tsv', '.nii.gz']\n\n        Returns\n        -------\n        found_files : list\n            The files which are currently selected with the current using the set pipeline, pipeline_subdir, space, parcellation, tasks, runs, subjects etc. There are the files that will generally be used if calling a make_ function.\n        \"\"\"\n        # This could be mnade better\n        file_dict = dict(self.bids_tags)\n        if allowedfileformats == 'default':\n            allowedfileformats = ['.tsv', '.nii.gz']\n        if forfile:\n            if isinstance(forfile, str):\n                forfile = get_bids_tag(forfile, 'all')\n            for n in forfile.keys():\n                file_dict[n] = [forfile[n]]\n        non_entries = []\n        for n in file_dict:\n            if not file_dict[n]:\n                non_entries.append(n)\n        for n in non_entries:\n            file_dict.pop(n)\n\n        # Only keep none empty elemenets\n        file_components = []\n        for k in ['sub', 'ses', 'task', 'run']:\n            if k in file_dict:\n                file_components.append([k + '-' + t for t in file_dict[k]])\n\n        file_list = list(itertools.product(*file_components))\n\n        # Specify main directory\n        if pipeline == 'pipeline':\n            mdir = self.BIDS_dir + '/derivatives/' + self.pipeline\n        elif pipeline == 'confound' and self.confound_pipeline:\n            mdir = self.BIDS_dir + '/derivatives/' + self.confound_pipeline\n        elif pipeline == 'confound':\n            mdir = self.BIDS_dir + '/derivatives/' + self.pipeline\n        elif pipeline == 'functionalconnectivity':\n            mdir = self.BIDS_dir + '/derivatives/teneto_' + teneto.__version__\n        else:\n            raise ValueError('unknown request')\n\n        found_files = []\n\n        for f in file_list:\n            wdir = str(mdir)\n            sub = [t for t in f if t.startswith('sub')]\n            ses = [t for t in f if t.startswith('ses')]\n            wdir += '/' + sub[0] + '/'\n            if ses:\n                wdir += '/' + ses[0] + '/'\n            wdir += '/func/'\n\n            if pipeline == 'pipeline':\n                wdir += '/' + self.pipeline_subdir + '/'\n                fileending = [self.bids_suffix +\n                              f for f in allowedfileformats]\n            elif pipeline == 'functionalconnectivity':\n                wdir += '/fc/'\n                fileending = ['conn' + f for f in allowedfileformats]\n            elif pipeline == 'confound':\n                fileending = ['confounds' + f for f in allowedfileformats]\n\n            if os.path.exists(wdir):\n                # make filenames\n                found = []\n                # Check that the tags are in the specified bids tags\n                for ff in os.listdir(wdir):\n                    ftags = get_bids_tag(ff, 'all')\n                    t = [t for t in ftags if t in file_dict and ftags[t]\n                         in file_dict[t]]\n                    if len(t) == len(file_dict):\n                        found.append(ff)\n                found = [f for f in found for e in fileending if f.endswith(e)]\n                # Include only if all analysis step tags are present\n                # Exclude if confounds tag is present\n                if pipeline == 'confound':\n                    found = [i for i in found if '_confounds' in i]\n                else:\n                    found = [i for i in found if '_confounds' not in i]\n                # Make full paths\n                found = list(\n                    map(str.__add__, [re.sub('/+', '/', wdir)]*len(found), found))\n                # Remove any files in bad files (could add json subcar reading here)\n                found = [i for i in found if not any(\n                    [bf in i for bf in self.bad_files])]\n                if found:\n                    found_files += found\n\n            if quiet == -1:\n                print(wdir)\n\n        found_files = list(set(found_files))\n        if quiet == 0:\n            print(found_files)\n        return found_files",
    "label": 0
  },
  {
    "codes": "```rust\n// Function to calculate the Levenshtein distance between two strings\nfn levenshtein_distance(s1: &str, s2: &str) -> usize {\n    let len1 = s1.chars().count();\n    let len2 = s2.chars().count();\n    \n    // Create a 2D vector to store distances\n    let mut dp = vec![vec![0; len2 + 1]; len1 + 1];\n    \n    // Initialize the first row and column\n    for i in 0..=len1 {\n        dp[i][0] = i;\n    }\n    for j in 0..=len2 {\n        dp[0][j] = j;\n    }\n    \n    // Fill the DP table\n    for (i, c1) in s1.chars().enumerate() {\n        for (j, c2) in s2.chars().enumerate() {\n            let cost = if c1 == c2 { 0 } else { 1 };\n            dp[i + 1][j + 1] = (dp[i][j + 1] + 1)\n                .min(dp[i + 1][j] + 1)\n                .min(dp[i][j] + cost);\n        }\n    }\n    \n    // Return the final distance\n    dp[len1][len2]\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *DatabaseInput) SetLocationUri(v string) *DatabaseInput {\n\ts.LocationUri = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *StorageDescriptor) SetStoredAsSubDirectories(v bool) *StorageDescriptor {\n\ts.StoredAsSubDirectories = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```kotlin\nimport kotlinx.coroutines.*\nimport java.net.HttpURLConnection\nimport java.net.URL\nimport kotlin.random.Random\nimport kotlin.time.Duration.Companion.seconds\n\nsuspend fun httpRequestWithRetry(url: String): String {\n    var currentDelay = 1.seconds\n    repeat(maxRetries) { attempt ->\n        try {\n            val response = makeHttpRequest(url)\n            if (response.first == HttpURLConnection.HTTP_OK) {\n                // Successful HTTP response, return the body\n                return response.second\n            }\n        } catch (e: Exception) {\n            // Log the exception if needed\n        }\n\n        // Calculate delay with exponential backoff, jitter added to prevent thundering herd problem\n        delay(currentDelay + Random.nextLong(0, 1000))\n        currentDelay *= 2\n    }\n    \n    throw Exception(\"Request failed after $maxRetries attempts.\")\n}\n\nsuspend fun makeHttpRequest(url: String): Pair<Int, String> {\n    withContext(Dispatchers.IO) {\n        val connection = URL(url).openConnection() as HttpURLConnection\n        try {\n            connection.connect()\n            val responseCode = connection.responseCode\n            val responseContent = connection.inputStream.bufferedReader().readText()\n            return@withContext Pair(responseCode, responseContent)\n        } finally {\n            connection.disconnect()\n        }\n    }\n}\n\nprivate const val maxRetries = 5\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport org.junit.runner.RunWith;\nimport org.junit.runners.Suite;\nimport org.junit.runners.Suite.SuiteClasses;\n\n@RunWith(Suite.class)\n@SuiteClasses({ SortingAlgorithmTest.class })\npublic class SortingAlgorithmTestSuite {\n    // This class remains empty; it is used only as a holder for the above annotations\n}\n\n// SortingAlgorithmTest.java\nimport static org.junit.Assert.assertArrayEquals;\nimport org.junit.Test;\n\npublic class SortingAlgorithmTest {\n    \n    @Test\n    public void testEmptyArray() {\n        int[] input = {};\n        int[] expected = {};\n        SortingAlgorithm.sort(input);\n        assertArrayEquals(expected, input);\n    }\n    \n    @Test\n    public void testSingleElementArray() {\n        int[] input = {1};\n        int[] expected = {1};\n        SortingAlgorithm.sort(input);\n        assertArrayEquals(expected, input);\n    }\n    \n    @Test\n    public void testIdenticalElementsArray() {\n        int[] input = {5, 5, 5, 5, 5};\n        int[] expected = {5, 5, 5, 5, 5};\n        SortingAlgorithm.sort(input);\n        assertArrayEquals(expected, input);\n    }\n    \n    @Test\n    public void testAlreadySortedArray() {\n        int[] input = {1, 2, 3, 4, 5};\n        int[] expected = {1, 2, 3, 4, 5};\n        SortingAlgorithm.sort(input);\n        assertArrayEquals(expected, input);\n    }\n\n    @Test\n    public void testReverseSortedArray() {\n        int[] input = {5, 4, 3, 2, 1};\n        int[] expected = {1, 2, 3, 4, 5};\n        SortingAlgorithm.sort(input);\n        assertArrayEquals(expected, input);\n    }\n\n    @Test\n    public void testArrayWithNegativeNumbers() {\n        int[] input = {-3, -1, -2, -7, -5};\n        int[] expected = {-7, -5, -3, -2, -1};\n        SortingAlgorithm.sort(input);\n        assertArrayEquals(expected, input);\n    }\n\n    @Test\n    public void testArrayWithMixedNumbers() {\n        int[] input = {4, -1, 0, -7, 5, 3};\n        int[] expected = {-7, -1, 0, 3, 4, 5};\n        SortingAlgorithm.sort(input);\n        assertArrayEquals(expected, input);\n    }\n\n    @Test\n    public void testLargeArray() {\n        int size = 100000;\n        int[] input = new int[size];\n        int[] expected = new int[size];\n        for (int i = 0; i < size; i++) {\n            input[i] = size - i;\n            expected[i] = i + 1;\n        }\n        SortingAlgorithm.sort(input);\n        assertArrayEquals(expected, input);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```swift\nimport Foundation\n\n// Struct to model a polynomial equation\nstruct Polynomial {\n    // Coefficients of the polynomial, starting from the constant term\n    var coefficients: [Double]\n    \n    // Evaluate the polynomial at a given value of x\n    func evaluate(at x: Double) -> Double {\n        var result: Double = 0.0\n        for (index, coefficient) in coefficients.enumerated() {\n            result += coefficient * pow(x, Double(index))\n        }\n        return result\n    }\n    \n    // Description of the polynomial as a string\n    func description() -> String {\n        var terms: [String] = []\n        for (index, coefficient) in coefficients.enumerated() {\n            if coefficient != 0 {\n                let term = \"\\(coefficient)x^\\(index)\"\n                terms.append(index == 0 ? \"\\(coefficient)\" : term)\n            }\n        }\n        return terms.reversed().joined(separator: \" + \")\n    }\n}\n\n// Example usage\nlet polynomial = Polynomial(coefficients: [3, 2, 5]) // 5x^2 + 2x + 3\nlet value = polynomial.evaluate(at: 1.0) // Evaluates the polynomial at x = 1.0\nprint(value)\nprint(polynomial.description())\n```\n",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <opencv2/opencv.hpp>\n\nvoid preprocessFrame(cv::Mat& frame, cv::Mat& processedFrame) {\n    // Resize the frame to the desired input size for the model\n    cv::resize(frame, processedFrame, cv::Size(224, 224));\n\n    // Convert frame to grayscale\n    cv::cvtColor(processedFrame, processedFrame, cv::COLOR_BGR2GRAY);\n\n    // Normalize pixel values to range [0, 1]\n    processedFrame.convertTo(processedFrame, CV_32F, 1.0 / 255.0);\n\n    // Apply a Gaussian blur to reduce noise\n    cv::GaussianBlur(processedFrame, processedFrame, cv::Size(5, 5), 0);\n}\n\nint main(int argc, char** argv) {\n    cv::VideoCapture capture(\"input_video.mp4\");\n    cv::Mat frame, preprocessedFrame;\n\n    if (!capture.isOpened()) {\n        std::cerr << \"Error opening video.\" << std::endl;\n        return -1;\n    }\n\n    while (capture.read(frame)) {\n        preprocessFrame(frame, preprocessedFrame);\n\n        // Display the preprocessed frame\n        cv::imshow(\"Preprocessed Frame\", preprocessedFrame);\n        \n        // Press 'q' to exit the loop\n        if (cv::waitKey(30) == 'q') break;\n    }\n\n    capture.release();\n    cv::destroyAllWindows();\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport time\nfrom functools import wraps\n\ndef measure_execution_time(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        # Record start time in nanoseconds\n        start_time = time.perf_counter_ns()\n        \n        # Execute the function\n        result = func(*args, **kwargs)\n        \n        # Record end time in nanoseconds\n        end_time = time.perf_counter_ns()\n        \n        # Calculate and print execution time\n        execution_time = end_time - start_time\n        print(f\"{func.__name__} executed in {execution_time} nanoseconds\")\n        \n        return result\n    return wrapper\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<Page<IotHubSkuDescriptionInner>>> getValidSkusWithServiceResponseAsync(final String resourceGroupName, final String resourceName) {\n        return getValidSkusSinglePageAsync(resourceGroupName, resourceName)\n            .concatMap(new Func1<ServiceResponse<Page<IotHubSkuDescriptionInner>>, Observable<ServiceResponse<Page<IotHubSkuDescriptionInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<IotHubSkuDescriptionInner>>> call(ServiceResponse<Page<IotHubSkuDescriptionInner>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(getValidSkusNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def __Calc_HSL_to_RGB_Components(var_q, var_p, C):\n    \"\"\"\n    This is used in HSL_to_RGB conversions on R, G, and B.\n    \"\"\"\n    if C < 0:\n        C += 1.0\n    if C > 1:\n        C -= 1.0\n\n    # Computing C of vector (Color R, Color G, Color B)\n    if C < (1.0 / 6.0):\n        return var_p + ((var_q - var_p) * 6.0 * C)\n    elif (1.0 / 6.0) <= C < 0.5:\n        return var_q\n    elif 0.5 <= C < (2.0 / 3.0):\n        return var_p + ((var_q - var_p) * 6.0 * ((2.0 / 3.0) - C))\n    else:\n        return var_p",
    "label": 0
  },
  {
    "codes": "function findMultipleBaseTextures(item, queue)\n{\n    let result = false;\n\n    // Objects with multiple textures\n    if (item && item._textures && item._textures.length)\n    {\n        for (let i = 0; i < item._textures.length; i++)\n        {\n            if (item._textures[i] instanceof Texture)\n            {\n                const baseTexture = item._textures[i].baseTexture;\n\n                if (queue.indexOf(baseTexture) === -1)\n                {\n                    queue.push(baseTexture);\n                    result = true;\n                }\n            }\n        }\n    }\n\n    return result;\n}",
    "label": 0
  },
  {
    "codes": "func (s *JobDescription) SetCompletionDate(v string) *JobDescription {\n\ts.CompletionDate = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public TaskListSubtasksHeaders withLastModified(DateTime lastModified) {\n        if (lastModified == null) {\n            this.lastModified = null;\n        } else {\n            this.lastModified = new DateTimeRfc1123(lastModified);\n        }\n        return this;\n    }",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <Servo.h>\n#include <SoftwareSerial.h>\n\n// Define Bluetooth module pins\n#define BT_RX 10\n#define BT_TX 11\n\n// Create SoftwareSerial object for Bluetooth communication\nSoftwareSerial bluetooth(BT_RX, BT_TX);\n\n// Create Servo objects\nServo servo1;\nServo servo2;\n\nvoid setup() {\n  // Attach servos to pins\n  servo1.attach(9);  // Servo 1 on pin 9\n  servo2.attach(8);  // Servo 2 on pin 8\n\n  // Initialize Bluetooth communication\n  bluetooth.begin(9600);\n}\n\nvoid loop() {\n  // Check if data is available from Bluetooth\n  if (bluetooth.available()) {\n    char command = bluetooth.read();  // Read the incoming command\n\n    // Control servo1 based on the command\n    if (command == 'A') {\n      servo1.write(0);  // Move servo1 to 0 degrees\n    } else if (command == 'B') {\n      servo1.write(90);  // Move servo1 to 90 degrees\n    } else if (command == 'C') {\n      servo1.write(180);  // Move servo1 to 180 degrees\n    }\n\n    // Control servo2 based on the command\n    if (command == 'D') {\n      servo2.write(0);  // Move servo2 to 0 degrees\n    } else if (command == 'E') {\n      servo2.write(90);  // Move servo2 to 90 degrees\n    } else if (command == 'F') {\n      servo2.write(180);  // Move servo2 to 180 degrees\n    }\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "function parse_BrtWbProp(data, length) {\n\tvar o = ({});\n\tvar flags = data.read_shift(4);\n\to.defaultThemeVersion = data.read_shift(4);\n\tvar strName = (length > 8) ? parse_XLWideString(data) : \"\";\n\tif(strName.length > 0) o.CodeName = strName;\n\to.autoCompressPictures = !!(flags & 0x10000);\n\to.backupFile = !!(flags & 0x40);\n\to.checkCompatibility = !!(flags & 0x1000);\n\to.date1904 = !!(flags & 0x01);\n\to.filterPrivacy = !!(flags & 0x08);\n\to.hidePivotFieldList = !!(flags & 0x400);\n\to.promptedSolutions = !!(flags & 0x10);\n\to.publishItems = !!(flags & 0x800);\n\to.refreshAllConnections = !!(flags & 0x40000);\n\to.saveExternalLinkValues = !!(flags & 0x80);\n\to.showBorderUnselectedTables = !!(flags & 0x04);\n\to.showInkAnnotation = !!(flags & 0x20);\n\to.showObjects = [\"all\", \"placeholders\", \"none\"][(flags >> 13) & 0x03];\n\to.showPivotChartFilter = !!(flags & 0x8000);\n\to.updateLinks = [\"userSet\", \"never\", \"always\"][(flags >> 8) & 0x03];\n\treturn o;\n}",
    "label": 0
  },
  {
    "codes": "```python\n# Import required libraries\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model from a file\nkeras_model_path = 'path_to_your_keras_model.h5'  # Replace with your Keras model file path\nkeras_model = load_model(keras_model_path)\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model)\n\n# Save the CoreML model to a file\ncoreml_model_path = 'your_model.mlmodel'  # Define the path to save the CoreML model\ncoreml_model.save(coreml_model_path)\n```\n\nPlease ensure you have `coremltools` and `keras` installed in your environment to execute this script. You can install them using pip if needed.",
    "label": 1
  },
  {
    "codes": "def declare(self, queue='', virtual_host='/', passive=False, durable=False,\n                auto_delete=False, arguments=None):\n        \"\"\"Declare a Queue.\n\n        :param str queue: Queue name\n        :param str virtual_host: Virtual host name\n        :param bool passive: Do not create\n        :param bool durable: Durable queue\n        :param bool auto_delete: Automatically delete when not in use\n        :param dict|None arguments: Queue key/value arguments\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict\n        \"\"\"\n        if passive:\n            return self.get(queue, virtual_host=virtual_host)\n\n        queue_payload = json.dumps(\n            {\n                'durable': durable,\n                'auto_delete': auto_delete,\n                'arguments': arguments or {},\n                'vhost': virtual_host\n            }\n        )\n        return self.http_client.put(\n            API_QUEUE % (\n                quote(virtual_host, ''),\n                queue\n            ),\n            payload=queue_payload)",
    "label": 0
  },
  {
    "codes": "func (c *Glacier) UploadArchive(input *UploadArchiveInput) (*ArchiveCreationOutput, error) {\n\treq, out := c.UploadArchiveRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "func (s *AbortVaultLockInput) SetVaultName(v string) *AbortVaultLockInput {\n\ts.VaultName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```kotlin\nimport kotlinx.coroutines.*\nimport java.net.HttpURLConnection\nimport java.net.URL\nimport kotlin.math.pow\n\nsuspend fun <T> retryRequest(\n    maxRetries: Int = 5,\n    initialDelay: Long = 1000L,\n    maxDelay: Long = 32000L,\n    factor: Double = 2.0,\n    request: suspend () -> T\n): T {\n    var currentDelay = initialDelay\n    repeat(maxRetries) { attempt ->\n        try {\n            return request()\n        } catch (e: Exception) {\n            if (attempt == maxRetries - 1) {\n                throw e // Rethrow after the last attempt\n            }\n            delay(currentDelay)\n            currentDelay = (currentDelay * factor).toLong().coerceAtMost(maxDelay)\n        }\n    }\n    throw IllegalStateException(\"Reached unreachable code\")\n}\n\nsuspend fun fetchUrl(url: String): String {\n    return withContext(Dispatchers.IO) {\n        val connection = URL(url).openConnection() as HttpURLConnection\n        try {\n            connection.inputStream.bufferedReader().use { it.readText() }\n        } finally {\n            connection.disconnect()\n        }\n    }\n}\n\nfun main() = runBlocking {\n    val url = \"http://example.com\"\n    try {\n        // Execute HTTP request with retry logic\n        val result = retryRequest(\n            maxRetries = 5,\n            request = { fetchUrl(url) }\n        )\n        println(\"Success: $result\")\n    } catch (e: Exception) {\n        println(\"Failed: ${e.message}\")\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function parse_PtgExtraArray(blob, length, opts) {\n\tvar rows = 0, cols = 0;\n\tif(opts.biff == 12) {\n\t\trows = blob.read_shift(4); // DRw\n\t\tcols = blob.read_shift(4); // DCol\n\t} else {\n\t\tcols = 1 + blob.read_shift(1); //DColByteU\n\t\trows = 1 + blob.read_shift(2); //DRw\n\t}\n\tif(opts.biff >= 2 && opts.biff < 8) { --rows; if(--cols == 0) cols = 0x100; }\n\t// $FlowIgnore\n\tfor(var i = 0, o/*:Array<Array<any>>*/ = []; i != rows && (o[i] = []); ++i)\n\t\tfor(var j = 0; j != cols; ++j) o[i][j] = parse_SerAr(blob, opts.biff);\n\treturn o;\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport uuid\n\ndef generate_uuid_without_hyphens():\n    # Generate a UUID v4 and remove hyphens\n    return uuid.uuid4().hex\n\n# Example usage:\n# print(generate_uuid_without_hyphens())\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n)\n\n// handleConnection processes client requests\nfunc handleConnection(conn net.Conn, wg *sync.WaitGroup) {\n\tdefer wg.Done() // Signal that this goroutine is done\n\tdefer conn.Close() // Close the connection when done\n\n\t// Create a buffered reader for the connection\n\treader := bufio.NewReader(conn)\n\tfor {\n\t\t// Read data from the client\n\t\tmessage, err := reader.ReadString('\\n')\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error reading:\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Print the received message\n\t\tfmt.Print(\"Received: \", string(message))\n\n\t\t// Send a response back to the client\n\t\tresponse := fmt.Sprintf(\"Echo: %s\", message)\n\t\t_, err = conn.Write([]byte(response))\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error writing:\", err)\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc main() {\n\t// Create a wait group to manage goroutines\n\tvar wg sync.WaitGroup\n\n\t// Start listening on port 8080\n\tlistener, err := net.Listen(\"tcp\", \":8080\")\n\tif err != nil {\n\t\tfmt.Println(\"Error listening:\", err)\n\t\treturn\n\t}\n\tdefer listener.Close() // Close the listener when done\n\n\tfmt.Println(\"Server is listening on port 8080\")\n\n\tfor {\n\t\t// Accept incoming connections\n\t\tconn, err := listener.Accept()\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error accepting connection:\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Increment wait group counter for the new goroutine\n\t\twg.Add(1)\n\n\t\t// Handle the connection in a new goroutine\n\t\tgo handleConnection(conn, &wg)\n\t}\n\n\t// Wait for all goroutines to finish\n\twg.Wait()\n}\n```",
    "label": 1
  },
  {
    "codes": "function createPackage(tutorialName) {\n\n  const tutorialFilePath = `${CONTENTS_PATH}/getting-started/${tutorialName}.md`;\n  const tutorialFile = readFileSync(tutorialFilePath, 'utf8');\n  const examples = [];\n  tutorialFile.replace(/<code-(?:pane|example) [^>]*path=\"([^\"]+)\"/g, (_, path) => examples.push('examples/' + path));\n\n  if (examples.length) {\n    console.log('The following example files are referenced in this getting-started:');\n    console.log(examples.map(example => ' - ' + example).join('\\n'));\n  }\n\n  return new Package('author-getting-started', [contentPackage])\n    .config(function(readFilesProcessor) {\n      readFilesProcessor.sourceFiles = [\n        {\n          basePath: CONTENTS_PATH,\n          include: tutorialFilePath,\n          fileReader: 'contentFileReader'\n        },\n        {\n          basePath: CONTENTS_PATH,\n          include: examples.map(example => resolve(CONTENTS_PATH, example)),\n          fileReader: 'exampleFileReader'\n        }\n      ];\n    });\n}",
    "label": 0
  },
  {
    "codes": "def eval_bool(self, expr):\n        \"\"\"\n        Evaluate the expression `expr` and return the truthness of its result.\n        A result of an expression is said to be true if it contains at least\n        one value. It has the same semantics as :func:`bool` on sequences.s\n        \"\"\"\n        result = expr.eval(self)\n        iterator = iter(result)\n        try:\n            next(iterator)\n        except StopIteration:\n            return False\n        else:\n            return True\n        finally:\n            if hasattr(iterator, \"close\"):\n                iterator.close()",
    "label": 0
  },
  {
    "codes": "function getUID(prefix) {\n  do {\n    // eslint-disable-next-line no-bitwise\n    prefix += ~~(Math.random() * MAX_UID); // \"~~\" acts like a faster Math.floor() here\n  } while (document.getElementById(prefix));\n\n  return prefix;\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<ManagedClusterAccessProfileInner> getAccessProfileAsync(String resourceGroupName, String resourceName, String roleName, final ServiceCallback<ManagedClusterAccessProfileInner> serviceCallback) {\n        return ServiceFuture.fromResponse(getAccessProfileWithServiceResponseAsync(resourceGroupName, resourceName, roleName), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```java\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.security.KeyStore;\nimport java.security.cert.Certificate;\nimport java.security.cert.CertificateFactory;\n\npublic class KeyStoreWrapper {\n\n    private KeyStore keyStore;\n    private String keyStorePath;\n    private char[] password;\n\n    public KeyStoreWrapper(String keyStorePath, char[] password) throws Exception {\n        this.keyStorePath = keyStorePath;\n        this.password = password;\n        keyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n\n        // Load the keystore from the file, or create a new one if it doesn't exist\n        try (FileInputStream fis = new FileInputStream(keyStorePath)) {\n            keyStore.load(fis, password);\n        } catch (Exception e) {\n            keyStore.load(null, password);\n        }\n    }\n\n    public void addCertificate(String alias, String certPath) throws Exception {\n        try (InputStream certStream = new FileInputStream(certPath)) {\n            CertificateFactory certFactory = CertificateFactory.getInstance(\"X.509\");\n            Certificate cert = certFactory.generateCertificate(certStream);\n            keyStore.setCertificateEntry(alias, cert);\n            saveKeyStore();\n        }\n    }\n\n    public Certificate getCertificate(String alias) throws Exception {\n        return keyStore.getCertificate(alias);\n    }\n\n    public void deleteCertificate(String alias) throws Exception {\n        keyStore.deleteEntry(alias);\n        saveKeyStore();\n    }\n\n    public boolean containsAlias(String alias) throws Exception {\n        return keyStore.containsAlias(alias);\n    }\n\n    private void saveKeyStore() throws Exception {\n        try (FileOutputStream fos = new FileOutputStream(keyStorePath)) {\n            keyStore.store(fos, password);\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "function overwriteFile (filename, contents) {\n    // log.cyan ('Overwriting \u2192 ' + filename.yellow)\n    fs.closeSync (fs.openSync (filename, 'a'));\n    fs.truncateSync (filename)\n    fs.writeFileSync (filename, contents)\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport functools\nimport traceback\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.ERROR, \n                    format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef exception_logger(func):\n    \"\"\"A decorator that wraps the function to catch and log exceptions with traceback.\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            logging.error(\"Exception occurred in function %s: %s\", func.__name__, str(e))\n            logging.error(\"Exception traceback:\\n%s\", traceback.format_exc())\n            raise  # Re-raise the exception after logging it\n    return wrapper\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Collections.Concurrent;\nusing System.Linq;\nusing System.Threading;\nusing System.Threading.Tasks;\n\nclass Program\n{\n    static void Main()\n    {\n        var items = Enumerable.Range(1, 100);\n        var cts = new CancellationTokenSource();\n\n        // Handle cancellation from another thread after a delay\n        Task.Run(() =>\n        {\n            Thread.Sleep(1000); // Wait for a second before cancelling\n            cts.Cancel();\n        });\n\n        try\n        {\n            Parallel.ForEach(items, new ParallelOptions { CancellationToken = cts.Token }, item =>\n            {\n                // Simulating work\n                Console.WriteLine($\"Processing item {item}\");\n                Thread.Sleep(100); // Simulate some computation time\n\n                // Check for cancellation request\n                cts.Token.ThrowIfCancellationRequested();\n            });\n        }\n        catch (OperationCanceledException)\n        {\n            Console.WriteLine(\"Operation was canceled.\");\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```typescript\nimport { gql } from 'apollo-server';\n\nconst typeDefs = gql`\n  # Root Query\n  type Query {\n    user(id: ID!): User\n    post(id: ID!): Post\n    feed(limit: Int, offset: Int): [Post]\n    searchUsers(searchTerm: String!): [User]\n  }\n\n  # User Type\n  type User {\n    id: ID!\n    username: String!\n    fullName: String\n    bio: String\n    avatarUrl: String\n    posts(limit: Int, offset: Int): [Post]\n    followers(limit: Int, offset: Int): [User]\n    following(limit: Int, offset: Int): [User]\n  }\n\n  # Post Type\n  type Post {\n    id: ID!\n    content: String!\n    createdAt: String!\n    author: User!\n    comments(limit: Int, offset: Int): [Comment]\n    likes: [User]\n  }\n\n  # Comment Type\n  type Comment {\n    id: ID!\n    content: String!\n    createdAt: String!\n    author: User!\n    post: Post!\n  }\n\n  # Mutation Type\n  type Mutation {\n    createPost(content: String!): Post\n    updatePost(id: ID!, content: String!): Post\n    deletePost(id: ID!): Boolean\n    createComment(postId: ID!, content: String!): Comment\n    followUser(userId: ID!): Boolean\n    unfollowUser(userId: ID!): Boolean\n  }\n`;\n\nexport default typeDefs;\n```\n",
    "label": 1
  },
  {
    "codes": "function (element, options) {\n    this.$element      = $(element)\n    this.options       = $.extend({}, Collapse.DEFAULTS, options)\n    this.$trigger      = $('[data-toggle=\"collapse\"][href=\"#' + element.id + '\"],' +\n                           '[data-toggle=\"collapse\"][data-target=\"#' + element.id + '\"]')\n    this.transitioning = null\n\n    if (this.options.parent) {\n      this.$parent = this.getParent()\n    } else {\n      this.addAriaAndCollapsedClass(this.$element, this.$trigger)\n    }\n\n    if (this.options.toggle) this.toggle()\n  }",
    "label": 0
  },
  {
    "codes": "```python\nfrom fastapi import FastAPI, HTTPException, Request, Depends\nfrom fastapi.security.api_key import APIKeyHeader, APIKey\nfrom starlette.status import HTTP_403_FORBIDDEN, HTTP_429_TOO_MANY_REQUESTS\nfrom slowapi import Limiter\nfrom slowapi.util import get_remote_address\n\napp = FastAPI()\n\n# Initialize the rate limiter\nlimiter = Limiter(key_func=get_remote_address)\napp.state.limiter = limiter\n\n# API Key Header configuration\nAPI_KEY_NAME = \"api_key\"\napi_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)\n\n# Dummy set of valid API keys\nVALID_API_KEYS = {\"your_valid_api_key_here\"}\n\n# Dependency to get API Key from headers and validate\nasync def get_api_key(api_key_header: str = Depends(api_key_header)):\n    if api_key_header in VALID_API_KEYS:\n        return api_key_header\n    raise HTTPException(\n        status_code=HTTP_403_FORBIDDEN, detail=\"Could not validate credentials\"\n    )\n\n@app.get(\"/items/\")\n@limiter.limit(\"5/minute\")  # Rate limit: max 5 requests per minute per client\nasync def read_items(request: Request, api_key: APIKey = Depends(get_api_key)):\n    # Your logic here\n    return {\"message\": \"Success\"}\n\n@app.exception_handler(HTTP_403_FORBIDDEN)\nasync def api_key_exception_handler(request: Request, exc: HTTPException):\n    # Handle forbidden error: API key not valid\n    return JSONResponse(\n        status_code=HTTP_403_FORBIDDEN,\n        content={\"detail\": exc.detail},\n    )\n\n@app.exception_handler(HTTP_429_TOO_MANY_REQUESTS)\nasync def rate_limit_exception_handler(request: Request, exc: HTTPException):\n    # Handle rate limit error\n    return JSONResponse(\n        status_code=HTTP_429_TOO_MANY_REQUESTS,\n        content={\"detail\": \"Rate limit exceeded\"},\n    )\n```\n\n- **HTTP_403_FORBIDDEN**: Returned when the API key is invalid or missing.\n- **HTTP_429_TOO_MANY_REQUESTS**: Returned when the client exceeds the allowed rate limit.\n- Rate Limit: The endpoint is limited to 5 requests per minute per client.\n- API Key Check: The endpoint requires a valid API key provided in the headers.",
    "label": 1
  },
  {
    "codes": "```javascript\nfunction mergeSortedArrays(arr1, arr2) {\n  let i = 0;\n  let j = 0;\n  let mergedArray = [];\n  \n  // Traverse both arrays and insert smaller of both elements into mergedArray\n  while (i < arr1.length && j < arr2.length) {\n    if (arr1[i] < arr2[j]) {\n      mergedArray.push(arr1[i]);\n      i++;\n    } else {\n      mergedArray.push(arr2[j]);\n      j++;\n    }\n  }\n  \n  // Copy the remaining elements of arr1, if any\n  while (i < arr1.length) {\n    mergedArray.push(arr1[i]);\n    i++;\n  }\n  \n  // Copy the remaining elements of arr2, if any\n  while (j < arr2.length) {\n    mergedArray.push(arr2[j]);\n    j++;\n  }\n  \n  return mergedArray;\n}\n\n// Example usage:\n// const arr1 = [1, 3, 5];\n// const arr2 = [2, 4, 6];\n// console.log(mergeSortedArrays(arr1, arr2)); // Output: [1, 2, 3, 4, 5, 6]\n```",
    "label": 1
  },
  {
    "codes": "```julia\nusing VoronoiDelaunay\nusing Plots\n\n# Function to generate a Voronoi diagram from random points\nfunction generate_voronoi_diagram(n_points::Int; xlim=(0, 1), ylim=(0, 1))\n    # Generate random points within the specified limits\n    points = [rand()*(xlim[2]-xlim[1]) + xlim[1] for _ in 1:n_points], \n             [rand()*(ylim[2]-ylim[1]) + ylim[1] for _ in 1:n_points]\n\n    # Create a Voronoi diagram from the generated points\n    vd = voronoi_delaunay(points[1], points[2])\n\n    # Plot the Voronoi diagram\n    plot()\n    plot!(vd, voronoi=true, xlim=(xlim[1], xlim[2]), ylim=(ylim[1], ylim[2]), \n          legend=false, linecolor=:blue)\n    \n    # Display the plot\n    display(plot!)\nend\n\n# Example usage: generate and plot a Voronoi diagram with 10 random points\ngenerate_voronoi_diagram(10)\n```\n",
    "label": 1
  },
  {
    "codes": "function aboutSessionRestore_getTabs(window) {\n    var tabs = [ ];\n    var tree = this.tabList.getNode();\n\n    // Add entries when they are tabs (no container)\n    var ii = window.listIndex + 1;\n    while (ii < tree.view.rowCount && !tree.view.isContainer(ii)) {\n      tabs.push({\n                 index: tabs.length,\n                 listIndex : ii,\n                 restore: tree.view.getCellValue(ii, tree.columns.getColumnAt(0)),\n                 title: tree.view.getCellText(ii, tree.columns.getColumnAt(2))\n                });\n      ii++;\n    }\n\n    return tabs;\n  }",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <vector>\n#include <deque>\n#include <thread>\n#include <future>\n#include <atomic>\n#include <condition_variable>\n#include <mutex>\n#include <functional>\n#include <optional>\n\nclass ThreadPool {\npublic:\n    explicit ThreadPool(size_t numThreads);\n    ~ThreadPool();\n\n    template<class F, class... Args>\n    auto enqueue(F&& f, Args&&... args) -> std::future<decltype(f(args...))>;\n\nprivate:\n    // Worker thread function\n    void workerThread(size_t index);\n\n    std::vector<std::thread> workers;\n    std::vector<std::deque<std::function<void()>>> tasks;\n    std::mutex mutex;\n    std::condition_variable condition;\n    std::atomic<bool> stop;\n};\n\nThreadPool::ThreadPool(size_t numThreads) : stop(false) {\n    tasks.resize(numThreads);\n    for (size_t i = 0; i < numThreads; ++i) {\n        workers.emplace_back([this, i]() { workerThread(i); });\n    }\n}\n\nThreadPool::~ThreadPool() {\n    stop.store(true);\n    condition.notify_all();\n    for (auto& worker : workers) {\n        worker.join();\n    }\n}\n\ntemplate<class F, class... Args>\nauto ThreadPool::enqueue(F&& f, Args&&... args) -> std::future<decltype(f(args...))> {\n    using returnType = decltype(f(args...));\n    auto task = std::make_shared<std::packaged_task<returnType()>>(\n        std::bind(std::forward<F>(f), std::forward<Args>(args)...)\n    );\n\n    std::future<returnType> result = task->get_future();\n    {\n        std::lock_guard<std::mutex> lock(mutex);\n        size_t idx = std::hash<std::thread::id>()(std::this_thread::get_id()) % tasks.size();\n        tasks[idx].emplace_back([task]() { (*task)(); });\n    }\n    condition.notify_one();\n    return result;\n}\n\nvoid ThreadPool::workerThread(size_t index) {\n    while (!stop) {\n        std::function<void()> task;\n        {\n            std::unique_lock<std::mutex> lock(mutex);\n            condition.wait(lock, [this, index] {\n                return stop || !tasks[index].empty();\n            });\n\n            if (stop && tasks[index].empty()) {\n                return;\n            }\n\n            task = std::move(tasks[index].front());\n            tasks[index].pop_front();\n        }\n\n        // Execute task\n        task();\n\n        // Attempt to steal a task from other threads\n        for (size_t i = 0; i < tasks.size(); ++i) {\n            if (i == index) continue;\n            std::lock_guard<std::mutex> lock(mutex);\n            if (!tasks[i].empty()) {\n                task = std::move(tasks[i].front());\n                tasks[i].pop_front();\n                task(); // Execute stolen task\n                break;\n            }\n        }\n    }\n}\n\n// Example usage:\nint main() {\n    ThreadPool pool(4);\n    auto future = pool.enqueue([] { return 42; });\n    std::cout << future.get() << std::endl; // outputs 42\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```dart\n// Import necessary packages\nimport 'dart:async';\nimport 'package:flutter/services.dart';\n\n// Define the plugin class\nclass MLAcceleratorPlugin {\n  // Channel to communicate with native code\n  static const MethodChannel _channel = MethodChannel('ml_accelerator_plugin');\n\n  // Method to check if ML accelerator is available\n  static Future<bool> isMLAcceleratorAvailable() async {\n    try {\n      final bool isAvailable = await _channel.invokeMethod('isMLAcceleratorAvailable');\n      return isAvailable;\n    } on PlatformException catch (e) {\n      print(\"Failed to check ML accelerator availability: ${e.message}\");\n      return false;\n    }\n  }\n\n  // Method to perform inference using the ML accelerator\n  static Future<List<double>> performInference(List<double> inputData) async {\n    try {\n      final List<dynamic> result = await _channel.invokeMethod('performInference', {'inputData': inputData});\n      return result.cast<double>();\n    } on PlatformException catch (e) {\n      print(\"Failed to perform inference: ${e.message}\");\n      return [];\n    }\n  }\n}\n```\n\n```kotlin\n// Android native implementation (MainActivity.kt)\npackage com.example.mlacceleratorplugin\n\nimport io.flutter.embedding.android.FlutterActivity\nimport io.flutter.embedding.engine.FlutterEngine\nimport io.flutter.plugin.common.MethodChannel\n\nclass MainActivity: FlutterActivity() {\n    private val CHANNEL = \"ml_accelerator_plugin\"\n\n    override fun configureFlutterEngine(flutterEngine: FlutterEngine) {\n        super.configureFlutterEngine(flutterEngine)\n        MethodChannel(flutterEngine.dartExecutor.binaryMessenger, CHANNEL).setMethodCallHandler { call, result ->\n            when (call.method) {\n                \"isMLAcceleratorAvailable\" -> {\n                    val isAvailable = checkMLAcceleratorAvailability()\n                    result.success(isAvailable)\n                }\n                \"performInference\" -> {\n                    val inputData = call.argument<List<Double>>(\"inputData\")\n                    val inferenceResult = performInference(inputData)\n                    result.success(inferenceResult)\n                }\n                else -> result.notImplemented()\n            }\n        }\n    }\n\n    private fun checkMLAcceleratorAvailability(): Boolean {\n        // Implement logic to check if ML accelerator is available\n        return true\n    }\n\n    private fun performInference(inputData: List<Double>?): List<Double> {\n        // Implement logic to perform inference using ML accelerator\n        return inputData?.map { it * 2 } ?: emptyList()\n    }\n}\n```\n\n```swift\n// iOS native implementation (AppDelegate.swift)\nimport UIKit\nimport Flutter\n\n@UIApplicationMain\n@objc class AppDelegate: FlutterAppDelegate {\n    override func application(\n        _ application: UIApplication,\n        didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?\n    ) -> Bool {\n        let controller = window?.rootViewController as! FlutterViewController\n        let mlAcceleratorChannel = FlutterMethodChannel(name: \"ml_accelerator_plugin\",\n                                                      binaryMessenger: controller.binaryMessenger)\n        mlAcceleratorChannel.setMethodCallHandler { (call, result) in\n            switch call.method {\n            case \"isMLAcceleratorAvailable\":\n                let isAvailable = self.checkMLAcceleratorAvailability()\n                result(isAvailable)\n            case \"performInference\":\n                guard let inputData = call.arguments as? [Double] else {\n                    result(FlutterError(code: \"INVALID_ARGUMENT\", message: \"Invalid input data\", details: nil))\n                    return\n                }\n                let inferenceResult = self.performInference(inputData: inputData)\n                result(inferenceResult)\n            default:\n                result(FlutterMethodNotImplemented)\n            }\n        }\n        GeneratedPluginRegistrant.register(with: self)\n        return super.application(application, didFinishLaunchingWithOptions: launchOptions)\n    }\n\n    private func checkMLAcceleratorAvailability() -> Bool {\n        // Implement logic to check if ML accelerator is available\n        return true\n    }\n\n    private func performInference(inputData: [Double]) -> [Double] {\n        // Implement logic to perform inference using ML accelerator\n        return inputData.map { $0 * 2 }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```go\n// Package main implements a Go service to batch process predictions using GPU workers.\npackage main\n\nimport (\n    \"log\"\n    \"net/http\"\n    \"sync\"\n)\n\n// Task represents a prediction task.\ntype Task struct {\n    ID     string\n    Input  interface{}\n    Output interface{}\n}\n\n// TaskQueue is a channel that holds prediction tasks.\nvar TaskQueue = make(chan Task, 100)\n\n// Worker represents a GPU worker for processing tasks.\ntype Worker struct {\n    ID int\n}\n\n// StartWorker launches the worker to process tasks using GPU.\nfunc (w *Worker) StartWorker(wg *sync.WaitGroup) {\n    defer wg.Done()\n    for task := range TaskQueue {\n        // Process the task using GPU.\n        processTask(task)\n        log.Printf(\"Worker %d processed task: %s\\n\", w.ID, task.ID)\n    }\n}\n\n// processTask simulates processing a task with a GPU.\nfunc processTask(task Task) {\n    // Dummy processing logic (replace with actual GPU processing code)\n    task.Output = \"Processed: \" + task.ID\n}\n\n// scheduleTasks simulates adding tasks to the queue for processing.\nfunc scheduleTasks() {\n    for i := 0; i < 20; i++ {\n        TaskQueue <- Task{\n            ID:    fmt.Sprintf(\"Task-%d\", i),\n            Input: fmt.Sprintf(\"Data-%d\", i),\n        }\n    }\n    close(TaskQueue)\n}\n\n// main launches the service with GPU workers.\nfunc main() {\n    // Initialize worker pool\n    numWorkers := 4\n    var wg sync.WaitGroup\n    for i := 0; i < numWorkers; i++ {\n        wg.Add(1)\n        worker := Worker{ID: i}\n        go worker.StartWorker(&wg)\n    }\n\n    // Schedule tasks for processing\n    go scheduleTasks()\n\n    // Block until all tasks are processed\n    wg.Wait()\n    log.Println(\"All tasks processed\")\n}\n```",
    "label": 1
  },
  {
    "codes": "def hset(self, hashkey, attribute, value):\n        \"\"\"Emulate hset.\"\"\"\n\n        redis_hash = self._get_hash(hashkey, 'HSET', create=True)\n        attribute = self._encode(attribute)\n        attribute_present = attribute in redis_hash\n        redis_hash[attribute] = self._encode(value)\n        return long(0) if attribute_present else long(1)",
    "label": 0
  },
  {
    "codes": "function Link(props) {\n  const { activeClassName, router, className: classNameProps, naked, ...other } = props;\n\n  const className = clsx(classNameProps, {\n    [activeClassName]: router.pathname === props.href && activeClassName,\n  });\n\n  if (naked) {\n    return <NextComposed className={className} {...other} />;\n  }\n\n  return <MuiLink component={NextComposed} className={className} {...other} />;\n}",
    "label": 0
  },
  {
    "codes": "```csharp\nusing Microsoft.AspNetCore.Mvc;\nusing Microsoft.AspNetCore.Mvc.Filters;\nusing System.IdentityModel.Tokens.Jwt;\nusing System.Linq;\nusing System.Security.Claims;\n\npublic class ValidateJwtClaimAttribute : ActionFilterAttribute\n{\n    private readonly string _claimType;\n    private readonly string _claimValue;\n\n    public ValidateJwtClaimAttribute(string claimType, string claimValue)\n    {\n        _claimType = claimType;\n        _claimValue = claimValue;\n    }\n\n    public override void OnActionExecuting(ActionExecutingContext context)\n    {\n        // Retrieve the Authorization header\n        var authorizationHeader = context.HttpContext.Request.Headers[\"Authorization\"].FirstOrDefault();\n        \n        // Check if the Authorization header contains a Bearer token\n        if (authorizationHeader != null && authorizationHeader.StartsWith(\"Bearer \"))\n        {\n            // Extract token\n            var token = authorizationHeader.Substring(\"Bearer \".Length).Trim();\n            \n            // Read and validate JWT token claims\n            var jwtHandler = new JwtSecurityTokenHandler();\n            if (jwtHandler.CanReadToken(token))\n            {\n                var jwtToken = jwtHandler.ReadJwtToken(token);\n\n                // Validate the claim\n                if (jwtToken.Claims.Any(c => c.Type == _claimType && c.Value == _claimValue))\n                {\n                    base.OnActionExecuting(context);\n                    return; // Valid claim found, continue processing\n                }\n            }\n        }\n\n        // If this point is reached, the claim is not valid or absent\n        context.Result = new UnauthorizedResult();\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "function( time, type ) {\n\t\ttime = jQuery.fx ? jQuery.fx.speeds[time] || time : time;\n\t\ttype = type || \"fx\";\n\n\t\treturn this.queue( type, function() {\n\t\t\tvar elem = this;\n\t\t\tsetTimeout(function() {\n\t\t\t\tjQuery.dequeue( elem, type );\n\t\t\t}, time );\n\t\t});\n\t}",
    "label": 0
  },
  {
    "codes": "def ltrim(self, key, start, stop):\n        \"\"\"Emulate ltrim.\"\"\"\n        redis_list = self._get_list(key, 'LTRIM')\n        if redis_list:\n            start, stop = self._translate_range(len(redis_list), start, stop)\n            self.redis[self._encode(key)] = redis_list[start:stop + 1]\n        return True",
    "label": 0
  },
  {
    "codes": "function( event ) {\n\tevent.type = event.data;\n\tjQuery.event.handle.apply( this, arguments );\n}",
    "label": 0
  },
  {
    "codes": "```lua\n-- Define the size and position of the destructible terrain\nlocal terrainSize = Vector3.new(100, 10, 100)\nlocal terrainPosition = Vector3.new(0, 0, 0)\n\n-- Create the terrain using parts\nlocal function createTerrain()\n    for x = 1, terrainSize.X do\n        for y = 1, terrainSize.Y do\n            for z = 1, terrainSize.Z do\n                local part = Instance.new(\"Part\")\n                part.Size = Vector3.new(1, 1, 1)\n                part.Position = terrainPosition + Vector3.new(x, y, z)\n                part.Anchored = true\n                part.BrickColor = BrickColor.new(\"Earth green\")\n                part.Parent = workspace\n            end\n        end\n    end\nend\n\n-- Function to destroy a part of the terrain\nlocal function destroyTerrainPart(part)\n    part:Destroy()\nend\n\n-- Mouse click event to destroy parts of the terrain\nlocal player = game.Players.LocalPlayer\nlocal mouse = player:GetMouse()\n\nmouse.Button1Down:Connect(function()\n    local target = mouse.Target\n    if target and target:IsA(\"Part\") and target.BrickColor == BrickColor.new(\"Earth green\") then\n        destroyTerrainPart(target)\n    end\nend)\n\n-- Initialize the destructible terrain\ncreateTerrain()\n```\n",
    "label": 1
  },
  {
    "codes": "func (s *GetTableVersionInput) SetTableName(v string) *GetTableVersionInput {\n\ts.TableName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "def callproc(self, name, args=None):\n        \"\"\"Call a stored procedure on the server, returning the results in a\n        :py:class:`queries.Results` instance.\n\n        :param str name: The procedure name\n        :param list args: The list of arguments to pass in\n        :rtype: queries.Results\n        :raises: queries.DataError\n        :raises: queries.DatabaseError\n        :raises: queries.IntegrityError\n        :raises: queries.InternalError\n        :raises: queries.InterfaceError\n        :raises: queries.NotSupportedError\n        :raises: queries.OperationalError\n        :raises: queries.ProgrammingError\n\n        \"\"\"\n        try:\n            self._cursor.callproc(name, args)\n        except psycopg2.Error as err:\n            self._incr_exceptions()\n            raise err\n        finally:\n            self._incr_executions()\n        return results.Results(self._cursor)",
    "label": 0
  },
  {
    "codes": "function find(cfb, path) {\n\tvar UCFullPaths = cfb.FullPaths.map(function(x) { return x.toUpperCase(); });\n\tvar UCPaths = UCFullPaths.map(function(x) { var y = x.split(\"/\"); return y[y.length - (x.slice(-1) == \"/\" ? 2 : 1)]; });\n\tvar k = false;\n\tif(path.charCodeAt(0) === 47 /* \"/\" */) { k = true; path = UCFullPaths[0].slice(0, -1) + path; }\n\telse k = path.indexOf(\"/\") !== -1;\n\tvar UCPath = path.toUpperCase();\n\tvar w = k === true ? UCFullPaths.indexOf(UCPath) : UCPaths.indexOf(UCPath);\n\tif(w !== -1) return cfb.FileIndex[w];\n\n\tvar m = !UCPath.match(chr1);\n\tUCPath = UCPath.replace(chr0,'');\n\tif(m) UCPath = UCPath.replace(chr1,'!');\n\tfor(w = 0; w < UCFullPaths.length; ++w) {\n\t\tif((m ? UCFullPaths[w].replace(chr1,'!') : UCFullPaths[w]).replace(chr0,'') == UCPath) return cfb.FileIndex[w];\n\t\tif((m ? UCPaths[w].replace(chr1,'!') : UCPaths[w]).replace(chr0,'') == UCPath) return cfb.FileIndex[w];\n\t}\n\treturn null;\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport pandas as pd\n\n# Load the CSV file into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Display initial number of rows\nprint(f'Initial number of rows: {len(df)}')\n\n# Remove rows with any null values\ndf_cleaned = df.dropna()\n\n# Display number of rows after cleaning\nprint(f'Number of rows after removing nulls: {len(df_cleaned)}')\n\n# Save the cleaned data back to a CSV file\ndf_cleaned.to_csv('data_cleaned.csv', index=False)\n```\n",
    "label": 1
  },
  {
    "codes": "public List<QueueDescription> getQueues(int count, int skip) throws ServiceBusException, InterruptedException {\n        return Utils.completeFuture(this.asyncClient.getQueuesAsync(count, skip));\n    }",
    "label": 0
  },
  {
    "codes": "def load(self, filename):\n        \"\"\"Load a FoLiA XML file.\n\n        Argument:\n            filename (str): The file to load\n        \"\"\"\n        #if LXE and self.mode != Mode.XPATH:\n        #    #workaround for xml:id problem (disabled)\n        #    #f = open(filename)\n        #    #s = f.read().replace(' xml:id=', ' id=')\n        #    #f.close()\n        #    self.tree = ElementTree.parse(filename)\n        #else:\n        self.tree = xmltreefromfile(filename)\n        self.parsexml(self.tree.getroot())\n        if self.mode != Mode.XPATH:\n            #XML Tree is now obsolete (only needed when partially loaded for xpath queries)\n            self.tree = None",
    "label": 0
  },
  {
    "codes": "```typescript\n// Define the User type\ntype User = {\n  id: ID!\n  username: String!\n  email: String!\n  posts: [Post!]! // A user can have multiple posts\n  followers: [User!]! // A user can have multiple followers\n  following: [User!]! // A user can follow multiple users\n}\n\n// Define the Post type\ntype Post = {\n  id: ID!\n  content: String!\n  author: User! // Each post has an author\n  likes: [User!]! // A post can have multiple likes\n  comments: [Comment!]! // A post can have multiple comments\n}\n\n// Define the Comment type\ntype Comment = {\n  id: ID!\n  content: String!\n  author: User! // Each comment has an author\n  post: Post! // Each comment belongs to a post\n}\n\n// Define the Query type\ntype Query = {\n  getUser(id: ID!): User // Query to get a user by ID\n  getPost(id: ID!): Post // Query to get a post by ID\n  getAllUsers: [User!]! // Query to get all users\n  getAllPosts: [Post!]! // Query to get all posts\n}\n\n// Define the Mutation type\ntype Mutation = {\n  createUser(username: String!, email: String!): User // Mutation to create a new user\n  createPost(content: String!, authorId: ID!): Post // Mutation to create a new post\n  addComment(content: String!, authorId: ID!, postId: ID!): Comment // Mutation to add a comment to a post\n  likePost(userId: ID!, postId: ID!): Post // Mutation to like a post\n  followUser(followerId: ID!, followeeId: ID!): User // Mutation to follow another user\n}\n\n// Define the Subscription type\ntype Subscription = {\n  postLiked(postId: ID!): Post // Subscription to get notified when a post is liked\n  newComment(postId: ID!): Comment // Subscription to get notified when a new comment is added\n}\n```",
    "label": 1
  },
  {
    "codes": "def free(self):\n        \"\"\"Release the results and connection lock from the TornadoSession\n        object. This **must** be called after you finish processing the results\n        from :py:meth:`TornadoSession.query <queries.TornadoSession.query>` or\n        :py:meth:`TornadoSession.callproc <queries.TornadoSession.callproc>`\n        or the connection will not be able to be reused by other asynchronous\n        requests.\n\n        \"\"\"\n        self._freed = True\n        self._cleanup(self.cursor, self._fd)",
    "label": 0
  },
  {
    "codes": "```javascript\nconst xss = require('xss');\n\nfunction sanitizeInput(req, res, next) {\n  // Recursive function to sanitize all string properties in the object\n  function sanitize(obj) {\n    for (let key in obj) {\n      if (typeof obj[key] === 'string') {\n        obj[key] = xss(obj[key]);  // Sanitize string fields\n      } else if (typeof obj[key] === 'object' && obj[key] !== null) {\n        sanitize(obj[key]);  // Recursively sanitize objects\n      }\n    }\n  }\n  \n  // Sanitize query parameters\n  sanitize(req.query);\n  \n  // Sanitize request body\n  sanitize(req.body);\n  \n  // Sanitize URL parameters\n  sanitize(req.params);\n  \n  // Proceed to the next middleware\n  next();\n}\n\nmodule.exports = sanitizeInput;\n```",
    "label": 1
  },
  {
    "codes": "func (s *Condition) SetJobName(v string) *Condition {\n\ts.JobName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"crypto/rand\"\n\t\"fmt\"\n\t\"log\"\n\t\"math/big\"\n\t\"strings\"\n)\n\nconst (\n\tpasswordLength = 16 // Length of the password\n)\n\nvar (\n\t// Define character sets: upper, lower, digits and special\n\tlowerCharset   = \"abcdefghijklmnopqrstuvwxyz\"\n\tupperCharset   = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\tdigitCharset   = \"0123456789\"\n\tspecialCharset = \"!@#$%^&*()-_=+[]{}|;:,.<>?/~`\"\n)\n\nfunc main() {\n\tpassword, err := generateSecurePassword(passwordLength)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error generating password: %v\", err)\n\t}\n\tfmt.Printf(\"Generated password: %s\\n\", password)\n\tif checkPasswordEntropy(password) {\n\t\tfmt.Println(\"The password meets entropy requirements.\")\n\t} else {\n\t\tfmt.Println(\"The password does not meet entropy requirements.\")\n\t}\n}\n\n// generateSecurePassword creates a password that contains at least one character \n// from each character set and ensures the total entropy meets a threshold.\nfunc generateSecurePassword(length int) (string, error) {\n\tif length < 4 {\n\t\treturn \"\", fmt.Errorf(\"length must be at least 4 to include all character types\")\n\t}\n\n\t// Collect at least one char from each character set\n\tvar password strings.Builder\n\tcharsets := []string{lowerCharset, upperCharset, digitCharset, specialCharset}\n\n\t// Add at least one character from each required charset\n\tfor _, charset := range charsets {\n\t\tchar, err := randomCharFromCharset(charset)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tpassword.WriteByte(char)\n\t}\n\n\t// Fill the rest of the password length with random chars from all charsets combined\n\tallCharsets := strings.Join(charsets, \"\")\n\tfor i := password.Len(); i < length; i++ {\n\t\tchar, err := randomCharFromCharset(allCharsets)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tpassword.WriteByte(char)\n\t}\n\n\treturn password.String(), nil\n}\n\n// randomCharFromCharset selects a random character from the given charset\nfunc randomCharFromCharset(charset string) (byte, error) {\n\tindex, err := rand.Int(rand.Reader, big.NewInt(int64(len(charset))))\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn charset[index.Int64()], nil\n}\n\n// checkPasswordEntropy evaluates the password for acceptable entropy\nfunc checkPasswordEntropy(password string) bool {\n\t// Example simple entropy check based on password length and variation.\n\t// In a real scenario, use more sophisticated entropy measurement.\n\tvariation := map[rune]bool{}\n\tfor _, char := range password {\n\t\tvariation[char] = true\n\t}\n\tentropy := len(variation) * len(password)\n\treturn entropy >= 60 // Example threshold; adjust according to requirements\n}\n```",
    "label": 1
  },
  {
    "codes": "```csharp\npublic static class StringExtensions\n{\n    /// <summary>\n    /// Truncates the string to the specified maximum length and appends an ellipsis (\"...\") if necessary.\n    /// </summary>\n    /// <param name=\"value\">The string to truncate.</param>\n    /// <param name=\"maxLength\">The maximum allowed length for the string, including the ellipsis.</param>\n    /// <returns>The truncated string with ellipsis if it exceeds the maximum length.</returns>\n    public static string TruncateWithEllipsis(this string value, int maxLength)\n    {\n        const string ellipsis = \"...\";\n        if (string.IsNullOrEmpty(value) || maxLength <= 0) return string.Empty;\n        if (value.Length <= maxLength) return value;\n        int truncatedLength = maxLength - ellipsis.Length;\n        return truncatedLength > 0 ? value.Substring(0, truncatedLength) + ellipsis : ellipsis;\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "func (s *Connection) SetConnectionType(v string) *Connection {\n\ts.ConnectionType = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function() {\n                    var args = arguments;\n                    var sc = args[0], so = args[1], ec = sc, eo = so;\n\n                    switch (args.length) {\n                        case 3:\n                            eo = args[2];\n                            break;\n                        case 4:\n                            ec = args[2];\n                            eo = args[3];\n                            break;\n                    }\n\n                    boundaryUpdater(this, sc, so, ec, eo);\n                }",
    "label": 0
  },
  {
    "codes": "```csharp\n// Extension method to truncate a string to a specified max length with ellipsis\npublic static class StringExtensions\n{\n    public static string TruncateWithEllipsis(this string input, int maxLength)\n    {\n        // Check if the input string is null or shorter than the max length\n        if (string.IsNullOrEmpty(input) || input.Length <= maxLength)\n        {\n            return input;\n        }\n\n        // Truncate the string and append ellipsis\n        return input.Substring(0, maxLength - 3) + \"...\";\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define log file path\nLOG_FILE=\"/var/log/apache2/access.log\"\n\n# Parse log file and count 5xx errors per minute\ngrep $(date +\"%d/%b/%Y:%H:%M\") \"$LOG_FILE\" | awk '{print $9}' | grep '^5' | sort | uniq -c | while read -r count code; do\n    # Check if error count exceeds threshold (e.g., 10 errors per minute)\n    if [ \"$count\" -gt 10 ]; then\n        echo \"Spike detected: $count 5xx errors (Code: $code) in the last minute.\"\n    fi\ndone\n\n# Add a comment to explain the script's purpose\n# This script monitors Apache logs for spikes in 5xx server errors and alerts if the count exceeds a threshold.\n```",
    "label": 1
  },
  {
    "codes": "```typescript\ndescribe('E-commerce Checkout Functionality', () => {\n  beforeEach(() => {\n    // Visit the base URL of the application\n    cy.visit('/');\n    \n    // Clear any existing sessions or cookies\n    cy.clearCookies();\n    cy.clearLocalStorage();\n  });\n\n  it('should allow a user to complete the checkout process', () => {\n    // Add an item to the cart\n    cy.get('[data-test=product-add-button]').first().click();\n    \n    // Proceed to checkout\n    cy.get('[data-test=cart-icon]').click();\n    cy.get('[data-test=checkout-button]').click();\n    \n    // Fill in shipping information\n    cy.get('[data-test=shipping-address]').type('123 Cypress Lane');\n    cy.get('[data-test=shipping-city]').type('Cypressville');\n    cy.get('[data-test=shipping-zip]').type('12345');\n    \n    // Fill in payment information\n    cy.get('[data-test=payment-card-number]').type('4111111111111111');\n    cy.get('[data-test=payment-expiry-date]').type('1023');\n    cy.get('[data-test=payment-cvv]').type('123');\n    \n    // Submit the order\n    cy.get('[data-test=submit-order-button]').click();\n    \n    // Verify the order confirmation\n    cy.get('[data-test=order-confirmation]').should('be.visible');\n    cy.get('[data-test=order-confirmation]').should('contain', 'Thank you for your purchase');\n  });\n});\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.security.KeyStore;\nimport java.security.cert.Certificate;\nimport java.security.cert.X509Certificate;\n\npublic class KeyStoreWrapper {\n\n    private KeyStore keyStore;\n    private String keyStorePath;\n    private char[] password;\n\n    // Constructor to initialize the KeyStore\n    public KeyStoreWrapper(String keyStorePath, char[] password) throws Exception {\n        this.keyStorePath = keyStorePath;\n        this.password = password;\n        this.keyStore = KeyStore.getInstance(\"JKS\");\n        loadKeyStore();\n    }\n\n    // Load the KeyStore from the file\n    private void loadKeyStore() throws Exception {\n        try (FileInputStream fis = new FileInputStream(keyStorePath)) {\n            keyStore.load(fis, password);\n        } catch (Exception e) {\n            // If the file does not exist, create a new KeyStore\n            keyStore.load(null, password);\n        }\n    }\n\n    // Save the KeyStore to the file\n    public void saveKeyStore() throws Exception {\n        try (FileOutputStream fos = new FileOutputStream(keyStorePath)) {\n            keyStore.store(fos, password);\n        }\n    }\n\n    // Add a certificate to the KeyStore\n    public void addCertificate(String alias, X509Certificate certificate) throws Exception {\n        keyStore.setCertificateEntry(alias, certificate);\n    }\n\n    // Retrieve a certificate from the KeyStore\n    public X509Certificate getCertificate(String alias) throws Exception {\n        Certificate cert = keyStore.getCertificate(alias);\n        if (cert instanceof X509Certificate) {\n            return (X509Certificate) cert;\n        }\n        return null;\n    }\n\n    // Delete a certificate from the KeyStore\n    public void deleteCertificate(String alias) throws Exception {\n        keyStore.deleteEntry(alias);\n    }\n\n    // Check if a certificate exists in the KeyStore\n    public boolean containsCertificate(String alias) throws Exception {\n        return keyStore.containsAlias(alias);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<Void> beginPurgeDeletedAsync(String vaultName, String location, final ServiceCallback<Void> serviceCallback) {\n        return ServiceFuture.fromResponse(beginPurgeDeletedWithServiceResponseAsync(vaultName, location), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "func (s *Schedule) SetState(v string) *Schedule {\n\ts.State = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```sql\n-- Using a Common Table Expression (CTE) to calculate total purchase amounts per customer\nWITH CustomerTotals AS (\n    SELECT \n        customer_id, \n        SUM(purchase_amount) AS total_purchase_amount\n    FROM \n        purchases\n    GROUP BY \n        customer_id\n)\n\n-- Select the top 5 customers by total purchase amount\nSELECT \n    customer_id, \n    total_purchase_amount\nFROM \n    CustomerTotals\nORDER BY \n    total_purchase_amount DESC\nLIMIT 5;\n```",
    "label": 1
  },
  {
    "codes": "```typescript\n// Define the RGB tuple type\ntype RGB = [number, number, number];\n\n// Type guard to check if a value is a valid RGB tuple\nfunction isRGB(value: unknown): value is RGB {\n    return (\n        Array.isArray(value) && \n        value.length === 3 && \n        value.every((v) => typeof v === 'number' && v >= 0 && v <= 255)\n    );\n}\n```",
    "label": 1
  },
  {
    "codes": "def set_room_config(self, mucjid, data):\n        \"\"\"\n        Set the room configuration using a :xep:`4` data form.\n\n        :param mucjid: JID of the room to query\n        :type mucjid: bare :class:`~.JID`\n        :param data: Filled-out configuration form\n        :type data: :class:`aioxmpp.forms.Data`\n\n        .. seealso::\n\n           :class:`~.ConfigurationForm`\n              for a form template to generate the required form\n\n        A sensible workflow to, for example, set a room to be moderated, could\n        be this::\n\n          form = aioxmpp.muc.ConfigurationForm.from_xso(\n              (await muc_service.get_room_config(mucjid))\n          )\n          form.moderatedroom = True\n          await muc_service.set_rooom_config(mucjid, form.render_reply())\n\n        .. versionadded:: 0.7\n        \"\"\"\n\n        iq = aioxmpp.stanza.IQ(\n            type_=aioxmpp.structs.IQType.SET,\n            to=mucjid,\n            payload=muc_xso.OwnerQuery(form=data),\n        )\n\n        yield from self.client.send(iq)",
    "label": 0
  },
  {
    "codes": "```csharp\n// Extension method to truncate string with ellipsis\npublic static class StringExtensions\n{\n    public static string TruncateWithEllipsis(this string input, int maxLength)\n    {\n        // Check if input is null or shorter than maxLength\n        if (string.IsNullOrEmpty(input) || input.Length <= maxLength)\n        {\n            return input;\n        }\n\n        // Truncate string and append ellipsis\n        return input.Substring(0, maxLength - 3) + \"...\";\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\ndef greedy_tsp(distance_matrix):\n    # Get the number of cities\n    num_cities = len(distance_matrix)\n    # Start from the first city (index 0)\n    current_city = 0\n    # Keep track of the path to be returned\n    path = [current_city]\n    # Track visited cities\n    visited = set(path)\n\n    # Iterate until all cities are visited\n    while len(visited) < num_cities:\n        # Set an initial value for the next city and minimum distance\n        next_city = None\n        min_distance = float('inf')\n\n        # Examine all possible next cities\n        for city in range(num_cities):\n            if city not in visited:\n                # If this city is closer, update the next city and min distance\n                if distance_matrix[current_city][city] < min_distance:\n                    min_distance = distance_matrix[current_city][city]\n                    next_city = city\n\n        # Add the chosen city to the path and mark it as visited\n        visited.add(next_city)\n        path.append(next_city)\n        # Move to the next city\n        current_city = next_city\n\n    # Return to the starting city to complete the tour\n    path.append(path[0])\n    return path\n\n# Example usage:\n# The distance matrix represents the distances between cities\n# distance_matrix[i][j] is the distance from city i to city j\ndistance_matrix = [\n    [0, 10, 15, 20],\n    [10, 0, 35, 25],\n    [15, 35, 0, 30],\n    [20, 25, 30, 0]\n]\n\nprint(greedy_tsp(distance_matrix))\n```\n\nThis code provides a greedy solution to the traveling salesman problem, starting with the first city and iteratively visiting the nearest unvisited city until all cities are visited, finally returning to the starting city.",
    "label": 1
  },
  {
    "codes": "func (s *StorageDescriptor) SetSerdeInfo(v *SerDeInfo) *StorageDescriptor {\n\ts.SerdeInfo = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport pygame\n\n# Constants for the game\nSCREEN_WIDTH = 800\nSCREEN_HEIGHT = 600\nGRAVITY = 0.5\nJUMP_STRENGTH = 10\nDOUBLE_JUMP_STRENGTH = 8\n\nclass Player(pygame.sprite.Sprite):\n    def __init__(self):\n        super().__init__()\n        self.image = pygame.Surface((50, 50))\n        self.image.fill((0, 128, 255))\n        self.rect = self.image.get_rect()\n        self.rect.x = 100\n        self.rect.y = SCREEN_HEIGHT - 150\n        \n        self.velocity_y = 0\n        self.on_ground = False\n        self.jump_count = 0\n\n    def update(self):\n        self.velocity_y += GRAVITY\n        self.rect.y += self.velocity_y\n        \n        # Check for ground collision\n        if self.rect.y >= SCREEN_HEIGHT - 50:\n            self.rect.y = SCREEN_HEIGHT - 50\n            self.on_ground = True\n            self.jump_count = 0  # Reset jump count on ground\n            self.velocity_y = 0\n        else:\n            self.on_ground = False\n\n    def jump(self):\n        if self.on_ground or self.jump_count < 1:  # Allow one double jump\n            if self.on_ground:\n                self.velocity_y = -JUMP_STRENGTH\n            else:  # Double jump\n                self.velocity_y = -DOUBLE_JUMP_STRENGTH\n            self.jump_count += 1\n\ndef main():\n    pygame.init()\n    screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n    clock = pygame.time.Clock()\n    \n    player = Player()\n    all_sprites = pygame.sprite.Group(player)\n\n    running = True\n    while running:\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                running = False\n\n        # Key presses\n        keys = pygame.key.get_pressed()\n        if keys[pygame.K_SPACE]:\n            player.jump()\n\n        all_sprites.update()\n\n        screen.fill((255, 255, 255))\n        all_sprites.draw(screen)\n        pygame.display.flip()\n        clock.tick(60)\n\n    pygame.quit()\n\nif __name__ == \"__main__\":\n    main()\n```",
    "label": 1
  },
  {
    "codes": "```powershell\n# PowerShell script to bulk rename files using regex pattern matching\n\n# Define the directory containing the files\n$directory = \"C:\\path\\to\\your\\files\"\n\n# Define the regex pattern to match the filenames and the replacement pattern\n$pattern = '^(.*?)(\\d+)(.*?)$'\n$replacement = '${1}NewText${3}'\n\n# Get all files in the directory\nGet-ChildItem -Path $directory -File | ForEach-Object {\n    # Check if the filename matches the regex pattern\n    if ($_ -match $pattern) {\n        # Get the new filename using the replacement pattern\n        $newFileName = [Regex]::Replace($_.Name, $pattern, $replacement)\n\n        # Get the full path for the current and new filenames\n        $newFilePath = Join-Path -Path $_.DirectoryName -ChildPath $newFileName\n\n        # Rename the file\n        Rename-Item -Path $_.FullName -NewName $newFilePath\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public PagedList<SecretItem> getSecretsNext(final String nextPageLink) {\n        ServiceResponse<Page<SecretItem>> response = getSecretsNextSinglePageAsync(nextPageLink).toBlocking().single();\n        return new PagedList<SecretItem>(response.body()) {\n            @Override\n            public Page<SecretItem> nextPage(String nextPageLink) {\n                return getSecretsNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "```java\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n\npublic class AsyncTaskWithFallbacks {\n    public static void main(String[] args) {\n        ExecutorService executor = Executors.newFixedThreadPool(3);\n\n        // Primary async task\n        CompletableFuture<Void> primaryTask = CompletableFuture.supplyAsync(() -> {\n            // Simulate a task - replace this with actual task logic\n            if (Math.random() > 0.7) {\n                throw new RuntimeException(\"Primary task failed\");\n            }\n            System.out.println(\"Primary task completed successfully\");\n            return \"Primary Result\";\n        }, executor).thenAccept(result -> {\n            System.out.println(\"Processing result: \" + result);\n        });\n\n        // Fallback task 1\n        CompletableFuture<Void> fallbackTask1 = primaryTask.exceptionally(ex -> {\n            System.out.println(\"Primary task failed, executing fallback task 1\");\n            return null;\n        }).thenRunAsync(() -> {\n            // Simulate a fallback task\n            if (Math.random() > 0.7) {\n                throw new RuntimeException(\"Fallback task 1 failed\");\n            }\n            System.out.println(\"Fallback task 1 completed successfully\");\n        }, executor);\n\n        // Fallback task 2\n        CompletableFuture<Void> fallbackTask2 = fallbackTask1.exceptionally(ex -> {\n            System.out.println(\"Fallback task 1 failed, executing fallback task 2\");\n            return null;\n        }).thenRunAsync(() -> {\n            // Simulate another fallback task\n            System.out.println(\"Fallback task 2 completed successfully\");\n        }, executor);\n\n        fallbackTask2.join(); // Wait for the chain to complete\n        executor.shutdown();\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *GetPlanInput) SetLanguage(v string) *GetPlanInput {\n\ts.Language = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function createResizer(handler) {\n\tvar maxSize = 1000000;\n\n\t// NOTE(SB) Don't use innerHTML because it could be considered unsafe.\n\t// https://github.com/chartjs/Chart.js/issues/5902\n\tvar resizer = createDiv(CSS_SIZE_MONITOR);\n\tvar expand = createDiv(CSS_SIZE_MONITOR + '-expand');\n\tvar shrink = createDiv(CSS_SIZE_MONITOR + '-shrink');\n\n\texpand.appendChild(createDiv());\n\tshrink.appendChild(createDiv());\n\n\tresizer.appendChild(expand);\n\tresizer.appendChild(shrink);\n\tresizer._reset = function() {\n\t\texpand.scrollLeft = maxSize;\n\t\texpand.scrollTop = maxSize;\n\t\tshrink.scrollLeft = maxSize;\n\t\tshrink.scrollTop = maxSize;\n\t};\n\n\tvar onScroll = function() {\n\t\tresizer._reset();\n\t\thandler();\n\t};\n\n\taddListener(expand, 'scroll', onScroll.bind(expand, 'expand'));\n\taddListener(shrink, 'scroll', onScroll.bind(shrink, 'shrink'));\n\n\treturn resizer;\n}",
    "label": 0
  },
  {
    "codes": "public Observable<Void> deleteFromComputeNodeAsync(String poolId, String nodeId, String filePath) {\n        return deleteFromComputeNodeWithServiceResponseAsync(poolId, nodeId, filePath).map(new Func1<ServiceResponseWithHeaders<Void, FileDeleteFromComputeNodeHeaders>, Void>() {\n            @Override\n            public Void call(ServiceResponseWithHeaders<Void, FileDeleteFromComputeNodeHeaders> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "function getMediaDevices () {\n  if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {\n    return navigator.mediaDevices\n  }\n\n  const getUserMedia = navigator.mozGetUserMedia || navigator.webkitGetUserMedia\n  if (!getUserMedia) {\n    return null\n  }\n\n  return {\n    getUserMedia (opts) {\n      return new Promise((resolve, reject) => {\n        getUserMedia.call(navigator, opts, resolve, reject)\n      })\n    }\n  }\n}",
    "label": 0
  },
  {
    "codes": "def unbuild_month(self, dt):\n        \"\"\"\n        Deletes the directory at self.get_build_path.\n        \"\"\"\n        self.year = str(dt.year)\n        self.month = str(dt.month)\n        logger.debug(\"Building %s-%s\" % (self.year, self.month))\n        target_path = os.path.split(self.get_build_path())[0]\n        if self.fs.exists(target_path):\n            logger.debug(\"Removing {}\".format(target_path))\n            self.fs.removetree(target_path)",
    "label": 0
  },
  {
    "codes": "```elixir\ndefmodule DistributedStateServer do\n  use GenServer\n\n  # Client API\n\n  def start_link(initial_state \\\\ %{}) do\n    GenServer.start_link(__MODULE__, initial_state, name: __MODULE__)\n  end\n\n  def get_state() do\n    GenServer.call(__MODULE__, :get_state)\n  end\n\n  def update_state(key, value) do\n    GenServer.cast(__MODULE__, {:update_state, key, value})\n  end\n\n  # Server Callbacks\n\n  def init(initial_state) do\n    {:ok, initial_state}\n  end\n\n  def handle_call(:get_state, _from, state) do\n    {:reply, state, state}\n  end\n\n  def handle_cast({:update_state, key, value}, state) do\n    new_state = Map.put(state, key, value)\n    {:noreply, new_state}\n  end\nend\n\n# Usage example:\n# Start the server\n{:ok, _} = DistributedStateServer.start_link(%{count: 0})\n\n# Update state\nDistributedStateServer.update_state(:count, 1)\n\n# Get current state\nstate = DistributedStateServer.get_state()\nIO.inspect(state)\n```",
    "label": 1
  },
  {
    "codes": "```typescript\nfunction Retry(retries: number, delay: number) {\n  return function (_target: Object, _propertyKey: string, descriptor: PropertyDescriptor) {\n    const originalMethod = descriptor.value;\n\n    descriptor.value = async function (...args: any[]) {\n      let attempts = 0;\n      while (attempts < retries) {\n        try {\n          return await originalMethod.apply(this, args);\n        } catch (err) {\n          attempts++;\n          if (attempts >= retries) throw err;\n          await new Promise(res => setTimeout(res, delay));\n        }\n      }\n    };\n\n    return descriptor;\n  };\n}\n\n// Example usage:\nclass Example {\n  @Retry(3, 1000)\n  async unreliableMethod() {\n    // An async function that may fail\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "function formatArgs(firstArg, ...args) {\n  if (typeof firstArg === 'function') {\n    firstArg = firstArg();\n  }\n  if (typeof firstArg === 'string') {\n    args.unshift(`deck.gl ${firstArg}`);\n  } else {\n    args.unshift(firstArg);\n    args.unshift('deck.gl');\n  }\n  return args;\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.springframework.stereotype.Component;\n\n@Aspect\n@Component\npublic class CircuitBreakerAspect {\n\n    private static final int MAX_FAILURES = 3;\n    private int failureCount = 0;\n    private boolean circuitOpen = false;\n\n    @Around(\"execution(* com.example.service.*.*(..))\") // Adjust the pointcut to match your service methods\n    public Object circuitBreaker(ProceedingJoinPoint joinPoint) throws Throwable {\n        if (circuitOpen) {\n            throw new RuntimeException(\"Circuit is open. Service unavailable.\");\n        }\n\n        try {\n            Object result = joinPoint.proceed(); // Proceed with the original method call\n            failureCount = 0; // Reset failure count on success\n            return result;\n        } catch (Exception e) {\n            failureCount++; // Increment failure count on exception\n            if (failureCount >= MAX_FAILURES) {\n                circuitOpen = true; // Open the circuit if max failures reached\n            }\n            throw e; // Re-throw the exception\n        }\n    }\n}\n```\n\n### Comments:\n- The aspect uses the `@Around` advice to wrap methods with the circuit breaker pattern.\n- `MAX_FAILURES` defines the threshold for opening the circuit.\n- `failureCount` tracks the number of consecutive failures.\n- `circuitOpen` indicates whether the circuit is open or closed.\n- The aspect resets the failure count on successful method execution and opens the circuit if the failure threshold is exceeded.",
    "label": 1
  },
  {
    "codes": "```javascript\nfunction mergeSortedArrays(arr1, arr2) {\n    let merged = [];\n    let i = 0, j = 0;\n\n    // Traverse both arrays and push the smaller element to the merged array\n    while (i < arr1.length && j < arr2.length) {\n        if (arr1[i] < arr2[j]) {\n            merged.push(arr1[i]);\n            i++;\n        } else {\n            merged.push(arr2[j]);\n            j++;\n        }\n    }\n\n    // If there are remaining elements in arr1, add them to merged\n    while (i < arr1.length) {\n        merged.push(arr1[i]);\n        i++;\n    }\n\n    // If there are remaining elements in arr2, add them to merged\n    while (j < arr2.length) {\n        merged.push(arr2[j]);\n        j++;\n    }\n\n    return merged;\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\n@RestController\n@RequestMapping(\"/api/inventory\")\npublic class InventoryController {\n\n    @Autowired\n    private InventoryService inventoryService;\n\n    // Get all inventory items\n    @GetMapping\n    public ResponseEntity<List<InventoryItem>> getAllItems() {\n        return ResponseEntity.ok(inventoryService.getAllItems());\n    }\n\n    // Get a single inventory item by ID\n    @GetMapping(\"/{id}\")\n    public ResponseEntity<InventoryItem> getItemById(@PathVariable Long id) {\n        return inventoryService.getItemById(id)\n                .map(ResponseEntity::ok)\n                .orElse(ResponseEntity.notFound().build());\n    }\n\n    // Create a new inventory item\n    @PostMapping\n    public ResponseEntity<InventoryItem> createItem(@RequestBody InventoryItem item) {\n        InventoryItem createdItem = inventoryService.createItem(item);\n        return ResponseEntity.status(HttpStatus.CREATED).body(createdItem);\n    }\n\n    // Update an existing inventory item\n    @PutMapping(\"/{id}\")\n    public ResponseEntity<InventoryItem> updateItem(@PathVariable Long id, @RequestBody InventoryItem item) {\n        return inventoryService.updateItem(id, item)\n                .map(ResponseEntity::ok)\n                .orElse(ResponseEntity.notFound().build());\n    }\n\n    // Delete an inventory item by ID\n    @DeleteMapping(\"/{id}\")\n    public ResponseEntity<Void> deleteItem(@PathVariable Long id) {\n        if (inventoryService.deleteItem(id)) {\n            return ResponseEntity.noContent().build();\n        } else {\n            return ResponseEntity.notFound().build();\n        }\n    }\n\n    // Search inventory items by name\n    @GetMapping(\"/search\")\n    public ResponseEntity<List<InventoryItem>> searchItemsByName(@RequestParam String name) {\n        return ResponseEntity.ok(inventoryService.searchItemsByName(name));\n    }\n\n    // Update inventory item quantity\n    @PatchMapping(\"/{id}/quantity\")\n    public ResponseEntity<InventoryItem> updateItemQuantity(@PathVariable Long id, @RequestParam int quantity) {\n        return inventoryService.updateItemQuantity(id, quantity)\n                .map(ResponseEntity::ok)\n                .orElse(ResponseEntity.notFound().build());\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *CreateTableInput) SetDatabaseName(v string) *CreateTableInput {\n\ts.DatabaseName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "def set_logging_config(log_level, handlers):\n    \"\"\"Set python logging library config.\n\n    Run this ONCE at the start of your process. It formats the python logging\n    module's output.\n    Defaults logging level to INFO = 20)\n    \"\"\"\n    logging.basicConfig(\n        format='%(asctime)s %(levelname)s:%(name)s:%(funcName)s: %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S',\n        level=log_level,\n        handlers=handlers)",
    "label": 0
  },
  {
    "codes": "def _get_zset(self, name, operation, create=False):\n        \"\"\"\n        Get (and maybe create) a sorted set by name.\n        \"\"\"\n        return self._get_by_type(name, operation, create, b'zset', SortedSet(), return_default=False)",
    "label": 0
  },
  {
    "codes": "```python\nfrom locust import HttpUser, task, between\n\nclass ApiUser(HttpUser):\n    wait_time = between(1, 3)  # Random wait time between tasks\n\n    @task\n    def get_users(self):\n        # GET request to fetch users\n        response = self.client.get(\"/api/users\")\n        assert response.status_code == 200, \"Failed to fetch users\"\n\n    @task(3)  # Higher weight, more frequent execution\n    def create_user(self):\n        # POST request to create a new user\n        payload = {\"name\": \"John Doe\", \"email\": \"john.doe@example.com\"}\n        response = self.client.post(\"/api/users\", json=payload)\n        assert response.status_code == 201, \"Failed to create user\"\n\n    @task(2)\n    def update_user(self):\n        # PUT request to update an existing user\n        user_id = 1  # Assuming user ID 1 exists\n        payload = {\"name\": \"Jane Doe\"}\n        response = self.client.put(f\"/api/users/{user_id}\", json=payload)\n        assert response.status_code == 200, \"Failed to update user\"\n\n    @task(1)\n    def delete_user(self):\n        # DELETE request to delete a user\n        user_id = 1  # Assuming user ID 1 exists\n        response = self.client.delete(f\"/api/users/{user_id}\")\n        assert response.status_code == 204, \"Failed to delete user\"\n```",
    "label": 1
  },
  {
    "codes": "func (s *GetDataflowGraphOutput) SetDagEdges(v []*CodeGenEdge) *GetDataflowGraphOutput {\n\ts.DagEdges = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <sys/inotify.h>\n#include <unistd.h>\n#include <limits.h>\n#include <cstdlib>\n#include <cstring>\n\n// Function to handle events\nvoid handleEvent(struct inotify_event* event) {\n    if (event->len) {\n        if (event->mask & IN_CREATE) {\n            std::cout << \"New file/directory created: \" << event->name << std::endl;\n        } else if (event->mask & IN_DELETE) {\n            std::cout << \"File/directory deleted: \" << event->name << std::endl;\n        } else if (event->mask & IN_MODIFY) {\n            std::cout << \"File/directory modified: \" << event->name << std::endl;\n        }\n        // Add additional event handling as needed\n    }\n}\n\n// Function to initialize inotify and start watching a directory\nvoid watchDirectory(const char* path) {\n    int inotifyFd = inotify_init();\n    if (inotifyFd < 0) {\n        std::cerr << \"Failed to initialize inotify\" << std::endl;\n        exit(EXIT_FAILURE);\n    }\n\n    int wd = inotify_add_watch(inotifyFd, path, IN_CREATE | IN_DELETE | IN_MODIFY);\n    if (wd < 0) {\n        std::cerr << \"Failed to add watch for directory: \" << path << std::endl;\n        close(inotifyFd);\n        exit(EXIT_FAILURE);\n    }\n\n    char buffer[4096];\n    while (true) {\n        ssize_t numRead = read(inotifyFd, buffer, sizeof(buffer));\n        if (numRead <= 0) {\n            std::cerr << \"Error reading from inotify file descriptor\" << std::endl;\n            close(inotifyFd);\n            exit(EXIT_FAILURE);\n        }\n\n        ssize_t i = 0;\n        while (i < numRead) {\n            struct inotify_event* event = reinterpret_cast<struct inotify_event*>(&buffer[i]);\n            handleEvent(event);\n            i += sizeof(struct inotify_event) + event->len;\n        }\n    }\n\n    close(wd);\n    close(inotifyFd);\n}\n\nint main() {\n    // Daemonize process by forking\n    if (fork() > 0) {\n        exit(EXIT_SUCCESS);\n    }\n\n    // Start watching the specified directory\n    const char* directoryToWatch = \"/path/to/directory\";\n    watchDirectory(directoryToWatch);\n\n    return 0;\n}\n```\n",
    "label": 1
  },
  {
    "codes": "public static Creator create(String accessPolicyId, String assetId,\n            LocatorType locatorType) {\n        return new Creator(accessPolicyId, assetId, locatorType);\n    }",
    "label": 0
  },
  {
    "codes": "func (s *LookupEventsInput) SetStartTime(v time.Time) *LookupEventsInput {\n\ts.StartTime = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Directory containing the artifacts and signatures\nARTIFACTS_DIR=\"./artifacts\"\nSIGNATURES_DIR=\"./signatures\"\n\n# Loop through each artifact\nfor artifact in \"$ARTIFACTS_DIR\"/*; do\n    # Extract the base name of the artifact\n    filename=$(basename \"$artifact\")\n    \n    # Construct the expected signature file name\n    signature_file=\"$SIGNATURES_DIR/$filename.sig\"\n    \n    # Check if the signature file exists\n    if [[ -f \"$signature_file\" ]]; then\n        # Verify the signature\n        gpg --verify \"$signature_file\" \"$artifact\"\n        \n        # Check the exit status of gpg\n        if [[ $? -eq 0 ]]; then\n            echo \"Signature for '$filename' is valid.\"\n        else\n            echo \"Signature for '$filename' is invalid.\"\n        fi\n    else\n        echo \"No signature file found for '$filename'. Skipping...\"\n    fi\ndone\n```\n",
    "label": 1
  },
  {
    "codes": "```rust\nuse reqwest::blocking::Client;\nuse serde::Deserialize;\nuse std::collections::HashMap;\n\n#[derive(Deserialize)]\nstruct TokenResponse {\n    access_token: String,\n    token_type: String,\n    expires_in: Option<u64>,\n}\n\nfn get_access_token(client_id: &str, client_secret: &str, token_url: &str) -> Result<String, reqwest::Error> {\n    let client = Client::new();\n    let mut params = HashMap::new();\n    params.insert(\"grant_type\", \"client_credentials\");\n    params.insert(\"client_id\", client_id);\n    params.insert(\"client_secret\", client_secret);\n\n    let response: TokenResponse = client\n        .post(token_url)\n        .form(&params)\n        .send()?\n        .json()?;\n\n    Ok(response.access_token)\n}\n\nfn main() {\n    let client_id = \"your_client_id\";\n    let client_secret = \"your_client_secret\";\n    let token_url = \"https://your.auth.server/token\";\n\n    match get_access_token(client_id, client_secret, token_url) {\n        Ok(token) => println!(\"Access Token: {}\", token),\n        Err(err) => eprintln!(\"Error fetching access token: {:?}\", err),\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glue) GetClassifiersRequest(input *GetClassifiersInput) (req *request.Request, output *GetClassifiersOutput) {\n\top := &request.Operation{\n\t\tName:       opGetClassifiers,\n\t\tHTTPMethod: \"POST\",\n\t\tHTTPPath:   \"/\",\n\t\tPaginator: &request.Paginator{\n\t\t\tInputTokens:     []string{\"NextToken\"},\n\t\t\tOutputTokens:    []string{\"NextToken\"},\n\t\t\tLimitToken:      \"MaxResults\",\n\t\t\tTruncationToken: \"\",\n\t\t},\n\t}\n\n\tif input == nil {\n\t\tinput = &GetClassifiersInput{}\n\t}\n\n\toutput = &GetClassifiersOutput{}\n\treq = c.newRequest(op, input, output)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "func (s *Job) SetCommand(v *JobCommand) *Job {\n\ts.Command = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```javascript\n// Import D3.js library (ensure it's included in your HTML file)\n// <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n\n// Set up SVG dimensions\nconst width = 800;\nconst height = 400;\nconst margin = { top: 20, right: 30, bottom: 30, left: 40 };\n\n// Create SVG container\nconst svg = d3.select(\"body\")\n  .append(\"svg\")\n  .attr(\"width\", width)\n  .attr(\"height\", height);\n\n// Define scales\nconst xScale = d3.scaleTime().range([margin.left, width - margin.right]);\nconst yScale = d3.scaleLinear().range([height - margin.bottom, margin.top]);\n\n// Create axes\nconst xAxis = d3.axisBottom(xScale);\nconst yAxis = d3.axisLeft(yScale);\n\n// Append axes to SVG\nsvg.append(\"g\")\n  .attr(\"transform\", `translate(0,${height - margin.bottom})`)\n  .call(xAxis);\n\nsvg.append(\"g\")\n  .attr(\"transform\", `translate(${margin.left},0)`)\n  .call(yAxis);\n\n// Line generator\nconst line = d3.line()\n  .x(d => xScale(d.timestamp))\n  .y(d => yScale(d.price));\n\n// Initialize data array\nlet data = [];\n\n// Function to fetch real-time cryptocurrency price\nasync function fetchPrice() {\n  const response = await fetch(\"https://api.coingecko.com/api/v3/simple/price?ids=bitcoin&vs_currencies=usd\");\n  const json = await response.json();\n  const price = json.bitcoin.usd;\n  const timestamp = new Date();\n  return { timestamp, price };\n}\n\n// Function to update the chart\nfunction updateChart(newData) {\n  // Update scales\n  xScale.domain(d3.extent(newData, d => d.timestamp));\n  yScale.domain([0, d3.max(newData, d => d.price)]);\n\n  // Update axes\n  svg.select(\".x-axis\").call(xAxis);\n  svg.select(\".y-axis\").call(yAxis);\n\n  // Bind data to the line\n  const path = svg.selectAll(\".line\").data([newData]);\n\n  // Enter new line\n  path.enter()\n    .append(\"path\")\n    .attr(\"class\", \"line\")\n    .merge(path)\n    .attr(\"d\", line);\n\n  // Exit old line\n  path.exit().remove();\n}\n\n// Function to continuously update data and chart\nasync function run() {\n  while (true) {\n    const newPoint = await fetchPrice();\n    data.push(newPoint);\n    if (data.length > 50) data.shift(); // Keep last 50 data points\n    updateChart(data);\n    await new Promise(resolve => setTimeout(resolve, 5000)); // Update every 5 seconds\n  }\n}\n\n// Start the real-time visualization\nrun();\n```",
    "label": 1
  },
  {
    "codes": "func (s *DescribeVaultOutput) SetLastInventoryDate(v string) *DescribeVaultOutput {\n\ts.LastInventoryDate = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public Observable<Page<IntegrationAccountSchemaInner>> listByIntegrationAccountsAsync(final String resourceGroupName, final String integrationAccountName) {\n        return listByIntegrationAccountsWithServiceResponseAsync(resourceGroupName, integrationAccountName)\n            .map(new Func1<ServiceResponse<Page<IntegrationAccountSchemaInner>>, Page<IntegrationAccountSchemaInner>>() {\n                @Override\n                public Page<IntegrationAccountSchemaInner> call(ServiceResponse<Page<IntegrationAccountSchemaInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def _find_address_and_connect(self, addresses):\n        \"\"\"Find and connect to the appropriate address.\n\n        :param addresses:\n\n        :raises AMQPConnectionError: Raises if the connection\n                                     encountered an error.\n\n        :rtype: socket.socket\n        \"\"\"\n        error_message = None\n        for address in addresses:\n            sock = self._create_socket(socket_family=address[0])\n            try:\n                sock.connect(address[4])\n            except (IOError, OSError) as why:\n                error_message = why.strerror\n                continue\n            return sock\n        raise AMQPConnectionError(\n            'Could not connect to %s:%d error: %s' % (\n                self._parameters['hostname'], self._parameters['port'],\n                error_message\n            )\n        )",
    "label": 0
  },
  {
    "codes": "```python\ndef greedy_tsp(graph):\n    \"\"\"\n    Solves the Traveling Salesman Problem using a greedy approach.\n    \n    :param graph: A 2D list representing the adjacency matrix of the graph\n    :return: A tuple containing the path and the total cost\n    \"\"\"\n    import sys\n\n    n = len(graph)  # Number of nodes\n    visited = [False] * n  # Track the visited cities\n    path = []  # Store the path of the traveling salesman\n    cost = 0  # Total traveling cost\n    current_node = 0\n    visited[current_node] = True\n    path.append(current_node)\n\n    # Explore each node selecting the shortest path available each time\n    for _ in range(n - 1):\n        min_distance = sys.maxsize\n        next_node = None\n\n        # Look for the nearest unvisited node\n        for neighbor in range(n):\n            if not visited[neighbor] and graph[current_node][neighbor] < min_distance:\n                min_distance = graph[current_node][neighbor]\n                next_node = neighbor\n\n        # Move to the next node\n        if next_node is not None:\n            visited[next_node] = True\n            path.append(next_node)\n            cost += min_distance\n            current_node = next_node\n\n    # Closing the loop by returning to the starting node\n    cost += graph[current_node][path[0]]\n    path.append(path[0])\n\n    return path, cost\n\n\n# Example of usage:\nif __name__ == \"__main__\":\n    adjacency_matrix = [\n        [0, 10, 15, 20],\n        [10, 0, 35, 25],\n        [15, 35, 0, 30],\n        [20, 25, 30, 0]\n    ]\n    \n    tour, total_cost = greedy_tsp(adjacency_matrix)\n    print(f\"Tour: {tour}\")\n    print(f\"Total cost: {total_cost}\")\n```\n",
    "label": 1
  },
  {
    "codes": "func (c *Glue) UpdateUserDefinedFunctionWithContext(ctx aws.Context, input *UpdateUserDefinedFunctionInput, opts ...request.Option) (*UpdateUserDefinedFunctionOutput, error) {\n\treq, out := c.UpdateUserDefinedFunctionRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "function legacyIsContentEditable(e) {\n    if (e.contentEditable == 'inherit') {\n      var parent = bot.dom.getParentElement(e);\n      return parent ? legacyIsContentEditable(parent) : false;\n    } else {\n      return e.contentEditable == 'true';\n    }\n  }",
    "label": 0
  },
  {
    "codes": "public Observable<ImageList> updateAsync(String listId, String contentType, BodyModel bodyParameter) {\n        return updateWithServiceResponseAsync(listId, contentType, bodyParameter).map(new Func1<ServiceResponse<ImageList>, ImageList>() {\n            @Override\n            public ImageList call(ServiceResponse<ImageList> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "async function doRollup() {\n  // Plugins\n  const es5 = babel({ presets: ['@babel/preset-env'] });\n  const min = minify({ comments: false });\n\n  const output = format => file => ({\n    format,\n    file,\n    name: MODULE_NAME\n  });\n\n  const umd = output('umd');\n  const esm = output('es');\n\n  const bundle = await rollup({ input: ROLLUP_INPUT_FILE });\n  const bundleES5 = await rollup({ input: ROLLUP_INPUT_FILE, plugins: [es5] });\n  const bundleES5Min = await rollup({\n    input: ROLLUP_INPUT_FILE,\n    plugins: [es5, min]\n  });\n\n  const baseName = `${DIST_PATH}/${MODULE_NAME}`;\n\n  // UMD ES2018\n  await bundle.write(umd(`${baseName}.js`));\n  // ESM ES2018\n  await bundle.write(esm(`${baseName}.esm.js`));\n  // UMD ES5\n  await bundleES5.write(umd(`${baseName}.es5.js`));\n  // UMD ES5 min\n  await bundleES5Min.write(umd(`${baseName}.es5.min.js`));\n}",
    "label": 0
  },
  {
    "codes": "```scala\nimport java.time.Duration\nimport java.util.{Collections, Properties}\nimport org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}\nimport org.apache.kafka.common.serialization.StringDeserializer\n\nobject KafkaConsumerExample {\n  def main(args: Array[String]): Unit = {\n    // Kafka consumer configuration\n    val props = new Properties()\n    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\")\n    props.put(ConsumerConfig.GROUP_ID_CONFIG, \"scala-consumer-group\")\n    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)\n    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)\n    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\")\n\n    // Create Kafka consumer instance\n    val consumer = new KafkaConsumer[String, String](props)\n\n    // Subscribe to the topic\n    consumer.subscribe(Collections.singletonList(\"real-time-events\"))\n\n    // Poll for new messages\n    try {\n      while (true) {\n        val records = consumer.poll(Duration.ofMillis(1000))\n        records.forEach { record =>\n          println(s\"Received message: key=${record.key()}, value=${record.value()}, partition=${record.partition()}, offset=${record.offset()}\")\n        }\n      }\n    } finally {\n      consumer.close() // Close the consumer gracefully\n    }\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```plaintext\n// Initialize SCPI Interface with Oscilloscope\nVISA Resource Name \u2192 VISA Configure Serial Port\n# Initialize the connection to the oscilloscope through the VISA resource name.\n# Set baud rate, data bits, parity, and termination character according to oscilloscope specifications.\n\n// Clear any previous errors or status messages on the oscilloscope\nVISA Write \u2192 *CLS\n# Send the SCPI command to clear the oscilloscope's status.\n# Ensures no previous errors affect current operation.\n\n// Identify the oscilloscope\nVISA Write \u2192 *IDN?\n# Request the oscilloscope's identification string.\n\n// Read the response containing the oscilloscope's ID\nVISA Read \u2192 IDN Response\n# Capture the response from the oscilloscope.\n# Display or log the identification for confirmation.\n\n// Set up Oscilloscope's Measurement\nVISA Write \u2192 :AUTOSCALE\n# Automatically adjust the oscilloscope settings for optimal signal viewing.\n\n// Reading measurement from a specific channel, e.g., CH1\nVISA Write \u2192 :MEASure:VPP? CHANnel1\n# Request the peak-to-peak voltage measurement from Channel 1.\n\n// Read the measurement data\nVISA Read \u2192 Measurement Data\n# Capture the measurement result.\n# Parse and log/display the peak-to-peak voltage.\n\n// Close the VISA session to free resources\nVISA Close\n# Properly close the VISA session to release communication resources.\n```\n\n# Note: Each block of code should be implemented using LabVIEW VI blocks.\n# Ensure proper error handling is included for all VISA communication operations.\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <Servo.h>\n#include <SoftwareSerial.h>\n\n// Define Bluetooth RX and TX pins\n#define BT_RX 10\n#define BT_TX 11\n\n// Create a SoftwareSerial object for Bluetooth communication\nSoftwareSerial bluetooth(BT_RX, BT_TX);\n\n// Create a Servo object\nServo myServo;\n\nvoid setup() {\n  // Initialize the servo motor on pin 9\n  myServo.attach(9);\n  \n  // Start serial communication for debugging\n  Serial.begin(9600);\n  \n  // Start Bluetooth communication at 9600 baud rate\n  bluetooth.begin(9600);\n  \n  Serial.println(\"Bluetooth and Servo initialized.\");\n}\n\nvoid loop() {\n  // Check if data is available from Bluetooth\n  if (bluetooth.available()) {\n    // Read the incoming byte\n    int angle = bluetooth.parseInt();\n    \n    // Ensure the angle is within valid range (0 to 180)\n    if (angle >= 0 && angle <= 180) {\n      // Move the servo to the specified angle\n      myServo.write(angle);\n      \n      // Print the angle to the Serial Monitor for debugging\n      Serial.print(\"Servo moved to: \");\n      Serial.println(angle);\n    } else {\n      Serial.println(\"Invalid angle received.\");\n    }\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```zig\nconst std = @import(\"std\");\nconst nrf = @import(\"nrf\");\n\nfn enterLowPowerSleep() !void {\n    // Configure the power management module to enter low-power sleep mode\n    const pwr = try nrf.getPwr();\n\n    // Set the system to low power mode\n    pwr.setLowPowerMode(true);\n\n    // Ensure peripherals are in sleep friendly state\n    try configurePeripheralsForSleep();\n\n    // Enter system-on sleep mode\n    nrf.power_sys_off();\n}\n\nfn configurePeripheralsForSleep() !void {\n    // Configure GPIO pins to low power state if needed\n    const gpio = try nrf.getGPIO();\n\n    // Example: Set all unused pins to input with pull-down to minimize power\n    inline for (0..nrf.Gpio.pin_count) |pin| {\n        gpio.pinMode(pin, nrf.Gpio.PinMode.inputPulldown);\n    }\n\n    // Optionally disable peripherals or set them to power-saving mode\n    disableUnnecessaryPeripherals();\n}\n\nfn disableUnnecessaryPeripherals() void {\n    // Example implementation: Turn off UART, SPI, etc. not needed during sleep\n    const uart = nrf.getUART();\n    if (uart) |u| u.enable(false);\n\n    const spi = nrf.getSPI();\n    if (spi) |s| s.enable(false);\n\n    // Add other peripherals as necessary\n}\n\npub fn main() void {\n    const result = enterLowPowerSleep();\n    if (result) |err| {\n        std.debug.print(\"Failed to enter low power sleep: {}\\n\", .{err});\n        return;\n    }\n\n    // After wakeup, the execution resumes here\n    std.debug.print(\"System has woken up from sleep\\n\", .{});\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```python\nimport keras\nfrom tensorflow import keras as tf_keras\nfrom coremltools import convert\n\n# Load your Keras model\nmodel = tf_keras.models.load_model('path_to_your_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = convert(model)\n\n# Save the CoreML model\ncoreml_model.save('YourModel.mlmodel')\n```",
    "label": 1
  },
  {
    "codes": "```lua\n-- Destructible Terrain Script for Roblox\nlocal Players = game:GetService(\"Players\")\nlocal Workspace = game:GetService(\"Workspace\")\nlocal Debris = game:GetService(\"Debris\")\n\nlocal function createDestructionPart(position, size)\n    local part = Instance.new(\"Part\")\n    part.Size = size\n    part.Position = position\n    part.Anchored = true\n    part.CanCollide = false\n    part.BrickColor = BrickColor.new(\"Dark green\")\n    part.Material = Enum.Material.Grass\n    part.Parent = Workspace\n    return part\nend\n\nlocal function destroyTerrain(player, position)\n    local size = Vector3.new(5, 5, 5) -- Size of the destructible area\n    local explosionRadius = 5 -- Radius for the destruction effect\n\n    -- Create the destruction area\n    for x = -explosionRadius, explosionRadius, size.X do\n        for z = -explosionRadius, explosionRadius, size.Z do\n            local newPos = position + Vector3.new(x, 0, z)\n            local part = createDestructionPart(newPos, size)\n            \n            -- Optionally add effects or particles here\n            Debris:AddItem(part, 2) -- Remove part after 2 seconds\n        end\n    end\nend\n\nlocal function onPlayerActivated(player)\n    player:GetMouse().Button1Down:Connect(function()\n        local mouse = player:GetMouse()\n        local targetPosition = mouse.Hit.Position\n\n        destroyTerrain(player, targetPosition)\n    end)\nend\n\nPlayers.PlayerAdded:Connect(onPlayerActivated)\n```",
    "label": 1
  },
  {
    "codes": "```python\nfrom transformers import pipeline\n\n# Create a text summarization pipeline\nsummarizer = pipeline(\"summarization\")\n\n# Example text to summarize\ntext = \"\"\"\nHugging Face is a company that provides state-of-the-art NLP models and tools. \nTheir Transformers library has gained significant popularity due to its ease of use \nand the wide range of models available for various tasks, including text summarization, \ntranslation, and more. The community has grown rapidly, with many contributors sharing \ntheir own models and datasets. This has made Hugging Face a key player in the field of NLP.\n\"\"\"\n\n# Perform summarization\nsummary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n\n# Output the summary\nprint(summary[0]['summary_text'])\n```",
    "label": 1
  },
  {
    "codes": "```typescript\nimport { GraphQLObjectType, GraphQLString, GraphQLID, GraphQLList, GraphQLSchema, GraphQLInt } from 'graphql';\n\n// UserType represents a user in the social media network\nconst UserType = new GraphQLObjectType({\n  name: 'User',\n  fields: () => ({\n    id: { type: GraphQLID }, // Unique identifier for the user\n    name: { type: GraphQLString }, // User's name\n    email: { type: GraphQLString }, // User's email\n    posts: {\n      type: new GraphQLList(PostType), // List of posts made by the user\n      resolve(parent, args) {\n        // Resolve function to fetch posts by user ID\n      }\n    },\n    friends: {\n      type: new GraphQLList(UserType), // List of user's friends\n      resolve(parent, args) {\n        // Resolve function to fetch friends by user ID\n      }\n    }\n  })\n});\n\n// PostType represents a post made by a user\nconst PostType = new GraphQLObjectType({\n  name: 'Post',\n  fields: () => ({\n    id: { type: GraphQLID }, // Unique identifier for the post\n    content: { type: GraphQLString }, // Content of the post\n    author: {\n      type: UserType, // The user who created the post\n      resolve(parent, args) {\n        // Resolve function to fetch author by post ID\n      }\n    },\n    comments: {\n      type: new GraphQLList(CommentType), // List of comments on the post\n      resolve(parent, args) {\n        // Resolve function to fetch comments by post ID\n      }\n    },\n    likesCount: { type: GraphQLInt } // Number of likes on the post\n  })\n});\n\n// CommentType represents a comment on a post\nconst CommentType = new GraphQLObjectType({\n  name: 'Comment',\n  fields: () => ({\n    id: { type: GraphQLID }, // Unique identifier for the comment\n    content: { type: GraphQLString }, // Content of the comment\n    user: {\n      type: UserType, // The user who made the comment\n      resolve(parent, args) {\n        // Resolve function to fetch user by comment ID\n      }\n    },\n    post: {\n      type: PostType, // The post the comment is associated with\n      resolve(parent, args) {\n        // Resolve function to fetch post by comment ID\n      }\n    },\n  })\n});\n\n// RootQuery defines the entry point for GraphQL queries\nconst RootQuery = new GraphQLObjectType({\n  name: 'RootQueryType',\n  fields: {\n    user: {\n      type: UserType, // Query a single user by ID\n      args: { id: { type: GraphQLID } },\n      resolve(parent, args) {\n        // Resolve function to fetch user by ID\n      }\n    },\n    post: {\n      type: PostType, // Query a single post by ID\n      args: { id: { type: GraphQLID } },\n      resolve(parent, args) {\n        // Resolve function to fetch post by ID\n      }\n    },\n    users: {\n      type: new GraphQLList(UserType), // Query all users\n      resolve(parent, args) {\n        // Resolve function to fetch all users\n      }\n    },\n    posts: {\n      type: new GraphQLList(PostType), // Query all posts\n      resolve(parent, args) {\n        // Resolve function to fetch all posts\n      }\n    }\n  }\n});\n\n// Export the GraphQL schema\nexport const schema = new GraphQLSchema({\n  query: RootQuery\n});\n```",
    "label": 1
  },
  {
    "codes": "```java\n// Method to batch-insert 10,000 records using Hibernate\npublic void batchInsert(List<MyEntity> entities) {\n    Session session = HibernateUtil.getSessionFactory().openSession();\n    Transaction transaction = null;\n\n    try {\n        transaction = session.beginTransaction();\n\n        // Set batch size for optimal performance\n        session.setJdbcBatchSize(50);\n\n        for (int i = 0; i < entities.size(); i++) {\n            session.save(entities.get(i));\n\n            // Flush and clear session periodically to manage memory\n            if (i % 50 == 0 && i > 0) {\n                session.flush();\n                session.clear();\n            }\n        }\n\n        transaction.commit();\n    } catch (Exception e) {\n        if (transaction != null) {\n            transaction.rollback();\n        }\n        e.printStackTrace();\n    } finally {\n        session.close();\n    }\n}\n```\n\n```java\n// HibernateUtil class for session management\npublic class HibernateUtil {\n    private static final SessionFactory sessionFactory = buildSessionFactory();\n\n    private static SessionFactory buildSessionFactory() {\n        try {\n            // Create the SessionFactory from hibernate.cfg.xml\n            return new Configuration().configure().buildSessionFactory();\n        } catch (Throwable ex) {\n            System.err.println(\"Initial SessionFactory creation failed.\" + ex);\n            throw new ExceptionInInitializerError(ex);\n        }\n    }\n\n    public static SessionFactory getSessionFactory() {\n        return sessionFactory;\n    }\n}\n```\n\n```java\n// Example entity class\n@Entity\n@Table(name = \"my_entity\")\npublic class MyEntity {\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private Long id;\n\n    private String name;\n\n    // Getters and setters\n}\n```\n\n```java\n// Example usage\nList<MyEntity> entities = new ArrayList<>();\nfor (int i = 0; i < 10000; i++) {\n    MyEntity entity = new MyEntity();\n    entity.setName(\"Entity \" + i);\n    entities.add(entity);\n}\n\nbatchInsert(entities);\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport processing.core.PApplet;\nimport minim.AudioPlayer;\nimport minim.Minim;\nimport minim FFT;\n\npublic class AudioSpectrum extends PApplet {\n    Minim minim;\n    AudioPlayer player;\n    FFT fft;\n\n    public static void main(String[] args) {\n        PApplet.main(\"AudioSpectrum\");\n    }\n\n    public void settings() {\n        size(800, 600);\n    }\n\n    public void setup() {\n        minim = new Minim(this);\n        player = minim.loadFile(\"your-audio-file.mp3\"); // Replace with your audio file\n        player.play();\n        fft = new FFT(player.bufferSize());\n    }\n\n    public void draw() {\n        background(0);\n        fft.forward(player.mix); // Calculate the FFT for the current audio frame\n        stroke(255);\n        noFill();\n        \n        // Draw the frequency spectrum\n        for (int i = 0; i < fft.specSize(); i++) {\n            float x = map(i, 0, fft.specSize(), 0, width);\n            float h = map(fft.getBand(i), 0, 0.1, 0, height); // Adjust 0.1 to fit your audio\n            line(x, height, x, height - h);\n        }\n    }\n\n    public void stop() {\n        player.close();\n        minim.stop();\n        super.stop();\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function addOptionMethod( PluginClass ) {\n\t\t\t\t// don't overwrite original option method\n\t\t\t\tif ( PluginClass.prototype.option ) {\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t  // option setter\n\t\t\t  PluginClass.prototype.option = function( opts ) {\n\t\t\t    // bail out if not an object\n\t\t\t    if ( !$.isPlainObject( opts ) ){\n\t\t\t      return;\n\t\t\t    }\n\t\t\t    this.options = $.extend( true, this.options, opts );\n\t\t\t  };\n\t\t\t}",
    "label": 0
  },
  {
    "codes": "```typescript\n// Import Cypress types\n/// <reference types=\"cypress\" />\n\ndescribe('E-commerce Checkout Process', () => {\n  \n  beforeEach(() => {\n    // Presuming user needs to be logged in before checking out\n    cy.login('testuser@example.com', 'password123');\n    // Navigate to homepage\n    cy.visit('/');\n  });\n\n  it('should complete the checkout process successfully', () => {\n    // Add item to cart\n    cy.get('[data-test=add-to-cart-button]').click();\n    \n    // Open cart\n    cy.get('[data-test=cart-icon]').click();\n\n    // Assert item in cart\n    cy.get('[data-test=cart-item]').should('have.length', 1);\n\n    // Proceed to checkout\n    cy.get('[data-test=checkout-button]').click();\n\n    // Fill in checkout form\n    cy.get('[data-test=shipping-address]').type('123 Main St, Anytown, USA');\n    cy.get('[data-test=payment-method]').select('Credit Card');\n    cy.get('[data-test=credit-card-number]').type('4111111111111111');\n    cy.get('[data-test=expiration-date]').type('12/25');\n    cy.get('[data-test=cvv]').type('123');\n\n    // Submit the checkout form\n    cy.get('[data-test=complete-purchase-button]').click();\n\n    // Verify that the order was successful\n    cy.get('[data-test=order-confirmation]')\n      .should('contain', 'Thank you for your purchase!')\n      .and('be.visible');\n  });\n  \n  afterEach(() => {\n    // Log out user after test\n    cy.logout();\n  });\n\n});\n```",
    "label": 1
  },
  {
    "codes": "```dart\nimport 'dart:async';\n\nimport 'package:flutter/services.dart';\n\nclass MLAccelerator {\n  // Channel to communicate with platform-specific code\n  static const MethodChannel _channel =\n      MethodChannel('ml_accelerator');\n\n  // Get an instance of the MLAccelerator\n  static Future<String> get platformVersion async {\n    final String version = await _channel.invokeMethod('getPlatformVersion');\n    return version;\n  }\n\n  // Load a ML model for acceleration\n  static Future<void> loadModel(String modelName) async {\n    try {\n      await _channel.invokeMethod('loadModel', {'modelName': modelName});\n    } on PlatformException catch (e) {\n      // Handle error while loading model\n      print('Error loading model: ${e.message}');\n    }\n  }\n\n  // Run inference using the loaded model\n  static Future<dynamic> runInference(Map<String, dynamic> inputData) async {\n    try {\n      final dynamic result = \n          await _channel.invokeMethod('runInference', {'inputData': inputData});\n      return result;\n    } on PlatformException catch (e) {\n      // Handle error during inference\n      print('Error running inference: ${e.message}');\n      return null;\n    }\n  }\n\n  // Unload the loaded model to free resources\n  static Future<void> unloadModel(String modelName) async {\n    try {\n      await _channel.invokeMethod('unloadModel', {'modelName': modelName});\n    } on PlatformException catch (e) {\n      // Handle error while unloading the model\n      print('Error unloading model: ${e.message}');\n    }\n  }\n}\n```\n\n**Platform-specific Code:**\n\n```java\n// Android (Kotlin example)\npackage com.example.ml_accelerator\n\nimport androidx.annotation.NonNull\nimport io.flutter.embedding.engine.plugins.FlutterPlugin\nimport io.flutter.plugin.common.MethodCall\nimport io.flutter.plugin.common.MethodChannel\nimport io.flutter.plugin.common.MethodChannel.MethodCallHandler\nimport io.flutter.plugin.common.MethodChannel.Result\n\nclass MLAcceleratorPlugin: FlutterPlugin, MethodCallHandler {\n  private lateinit var channel : MethodChannel\n\n  override fun onAttachedToEngine(@NonNull flutterPluginBinding: FlutterPlugin.FlutterPluginBinding) {\n    channel = MethodChannel(flutterPluginBinding.binaryMessenger, \"ml_accelerator\")\n    channel.setMethodCallHandler(this)\n  }\n\n  override fun onMethodCall(@NonNull call: MethodCall, @NonNull result: Result) {\n    when (call.method) {\n      \"getPlatformVersion\" -> result.success(\"Android ${android.os.Build.VERSION.RELEASE}\")\n      \"loadModel\" -> loadModel(call.argument<String>(\"modelName\"), result)\n      \"runInference\" -> runInference(call.argument<Map<String, Any>>(\"inputData\"), result)\n      \"unloadModel\" -> unloadModel(call.argument<String>(\"modelName\"), result)\n      else -> result.notImplemented()\n    }\n  }\n\n  private fun loadModel(modelName: String?, result: Result) {\n    // Implement model loading using ML accelerator APIs\n    try {\n      // Code to load model\n      result.success(null)\n    } catch (e: Exception) {\n      result.error(\"ERROR_LOADING_MODEL\", \"Failed to load model\", null)\n    }\n  }\n\n  private fun runInference(inputData: Map<String, Any>?, result: Result) {\n    // Implement inference logic using loaded model\n    try {\n      // Code to run inference\n      val inferenceResult = mapOf<String, Any>() // Example result\n      result.success(inferenceResult)\n    } catch (e: Exception) {\n      result.error(\"ERROR_RUNNING_INFERENCE\", \"Failed to run inference\", null)\n    }\n  }\n\n  private fun unloadModel(modelName: String?, result: Result) {\n    // Implement model unloading\n    try {\n      // Code to unload model\n      result.success(null)\n    } catch (e: Exception) {\n      result.error(\"ERROR_UNLOADING_MODEL\", \"Failed to unload model\", null)\n    }\n  }\n\n  override fun onDetachedFromEngine(@NonNull binding: FlutterPlugin.FlutterPluginBinding) {\n    channel.setMethodCallHandler(null)\n  }\n}\n```\n\n```swift\n// iOS (Swift example)\nimport Flutter\nimport UIKit\n\npublic class SwiftMLAcceleratorPlugin: NSObject, FlutterPlugin {\n  public static func register(with registrar: FlutterPluginRegistrar) {\n    let channel = FlutterMethodChannel(name: \"ml_accelerator\", binaryMessenger: registrar.messenger())\n    let instance = SwiftMLAcceleratorPlugin()\n    registrar.addMethodCallDelegate(instance, channel: channel)\n  }\n\n  public func handle(_ call: FlutterMethodCall, result: @escaping FlutterResult) {\n    switch call.method {\n    case \"getPlatformVersion\":\n      result(\"iOS \" + UIDevice.current.systemVersion)\n    case \"loadModel\":\n      if let args = call.arguments as? [String: Any],\n         let modelName = args[\"modelName\"] as? String {\n        loadModel(modelName: modelName, result: result)\n      } else {\n        result(FlutterError(code: \"ERROR\", message: \"Invalid arguments\", details: nil))\n      }\n    case \"runInference\":\n      if let args = call.arguments as? [String: Any],\n         let inputData = args[\"inputData\"] as? [String: Any] {\n        runInference(inputData: inputData, result: result)\n      } else {\n        result(FlutterError(code: \"ERROR\", message: \"Invalid arguments\", details: nil))\n      }\n    case \"unloadModel\":\n      if let args = call.arguments as? [String: Any],\n         let modelName = args[\"modelName\"] as? String {\n        unloadModel(modelName: modelName, result: result)\n      } else {\n        result(FlutterError(code: \"ERROR\", message: \"Invalid arguments\", details: nil))\n      }\n    default:\n      result(FlutterMethodNotImplemented)\n    }\n  }\n\n  private func loadModel(modelName: String, result: FlutterResult) {\n    // Implement logic for loading ML model using device-native ML accelerators\n    do {\n      // Code to load model\n      result(nil)\n    } catch {\n      result(FlutterError(code: \"ERROR_LOADING_MODEL\", message: \"Failed to load model\", details: nil))\n    }\n  }\n\n  private func runInference(inputData: [String: Any], result: FlutterResult) {\n    // Implement logic for running inference\n    do {\n      // Code to run inference\n      let inferenceResult = [String: Any]() // Example result\n      result(inferenceResult)\n    } catch {\n      result(FlutterError(code: \"ERROR_RUNNING_INFERENCE\", message: \"Failed to run inference\", details: nil))\n    }\n  }\n\n  private func unloadModel(modelName: String, result: FlutterResult) {\n    // Implement logic for unloading model\n    do {\n      // Code to unload model\n      result(nil)\n    } catch {\n      result(FlutterError(code: \"ERROR_UNLOADING_MODEL\", message: \"Failed to unload model\", details: nil))\n    }\n  }\n}\n```\n\n",
    "label": 1
  },
  {
    "codes": "func (c *Glacier) UploadArchiveRequest(input *UploadArchiveInput) (req *request.Request, output *ArchiveCreationOutput) {\n\top := &request.Operation{\n\t\tName:       opUploadArchive,\n\t\tHTTPMethod: \"POST\",\n\t\tHTTPPath:   \"/{accountId}/vaults/{vaultName}/archives\",\n\t}\n\n\tif input == nil {\n\t\tinput = &UploadArchiveInput{}\n\t}\n\n\toutput = &ArchiveCreationOutput{}\n\treq = c.newRequest(op, input, output)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "def _try_decode_list(content):\n        \"\"\"Decode content of a list.\n\n        :param list|tuple content:\n        :return:\n        \"\"\"\n        result = list()\n        for value in content:\n            result.append(try_utf8_decode(value))\n        return result",
    "label": 0
  },
  {
    "codes": "```java\nimport ddf.minim.*;\nimport ddf.minim.analysis.*;\n\nMinim minim;\nAudioInput in;\nFFT fft;\n\nvoid setup() {\n  size(800, 400);\n  minim = new Minim(this);\n  in = minim.getLineIn(Minim.STEREO, 512);\n  fft = new FFT(in.bufferSize(), in.sampleRate());\n  fft.logAverages(22, 3); // Group frequencies into 22 bands, each 3 octaves wide\n}\n\nvoid draw() {\n  background(0);\n  fft.forward(in.mix);\n\n  stroke(255);\n  noFill();\n  beginShape();\n  for (int i = 0; i < fft.avgSize(); i++) {\n    float x = map(i, 0, fft.avgSize(), 0, width);\n    float y = map(fft.getAvg(i), 0, 100, height, 0);\n    vertex(x, y);\n  }\n  endShape();\n}\n\nvoid stop() {\n  in.close();\n  minim.stop();\n  super.stop();\n}\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/spf13/cobra\"\n\t\"gopkg.in/yaml.v2\"\n\t\"github.com/pelletier/go-toml\"\n\t\"encoding/json\"\n)\n\nfunc main() {\n\t// Initialize the root command\n\tvar rootCmd = &cobra.Command{\n\t\tUse:   \"convert\",\n\t\tShort: \"Convert between YAML, TOML, and JSON formats\",\n\t}\n\n\t// Add subcommands to root command\n\trootCmd.AddCommand(yamlToJsonCmd, yamlToTomlCmd, jsonToYamlCmd, jsonToTomlCmd, tomlToYamlCmd, tomlToJsonCmd)\n\n\t// Execute the root command\n\tif err := rootCmd.Execute(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n\n// Convert YAML to JSON\nvar yamlToJsonCmd = &cobra.Command{\n\tUse:   \"yaml-to-json [file]\",\n\tShort: \"Convert YAML to JSON\",\n\tArgs:  cobra.ExactArgs(1), // Ensure exactly one argument is provided\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\tyamlFile := args[0]\n\t\tyamlData, err := ioutil.ReadFile(yamlFile)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error reading YAML file: %v\", err)\n\t\t}\n\n\t\tvar data map[string]interface{}\n\t\tif err := yaml.Unmarshal(yamlData, &data); err != nil {\n\t\t\tlog.Fatalf(\"Error unmarshaling YAML: %v\", err)\n\t\t}\n\n\t\tjsonData, err := json.MarshalIndent(data, \"\", \"  \")\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error marshaling to JSON: %v\", err)\n\t\t}\n\n\t\tfmt.Println(string(jsonData))\n\t},\n}\n\n// Convert YAML to TOML\nvar yamlToTomlCmd = &cobra.Command{\n\tUse:   \"yaml-to-toml [file]\",\n\tShort: \"Convert YAML to TOML\",\n\tArgs:  cobra.ExactArgs(1), // Ensure exactly one argument is provided\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\tyamlFile := args[0]\n\t\tyamlData, err := ioutil.ReadFile(yamlFile)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error reading YAML file: %v\", err)\n\t\t}\n\n\t\tvar data map[string]interface{}\n\t\tif err := yaml.Unmarshal(yamlData, &data); err != nil {\n\t\t\tlog.Fatalf(\"Error unmarshaling YAML: %v\", err)\n\t\t}\n\n\t\ttomlData, err := toml.Marshal(data)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error marshaling to TOML: %v\", err)\n\t\t}\n\n\t\tfmt.Println(string(tomlData))\n\t},\n}\n\n// Convert JSON to YAML\nvar jsonToYamlCmd = &cobra.Command{\n\tUse:   \"json-to-yaml [file]\",\n\tShort: \"Convert JSON to YAML\",\n\tArgs:  cobra.ExactArgs(1), // Ensure exactly one argument is provided\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\tjsonFile := args[0]\n\t\tjsonData, err := ioutil.ReadFile(jsonFile)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error reading JSON file: %v\", err)\n\t\t}\n\n\t\tvar data map[string]interface{}\n\t\tif err := json.Unmarshal(jsonData, &data); err != nil {\n\t\t\tlog.Fatalf(\"Error unmarshalling JSON: %v\", err)\n\t\t}\n\n\t\tyamlData, err := yaml.Marshal(data)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error marshaling to YAML: %v\", err)\n\t\t}\n\n\t\tfmt.Println(string(yamlData))\n\t},\n}\n\n// Convert JSON to TOML\nvar jsonToTomlCmd = &cobra.Command{\n\tUse:   \"json-to-toml [file]\",\n\tShort: \"Convert JSON to TOML\",\n\tArgs:  cobra.ExactArgs(1), // Ensure exactly one argument is provided\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\tjsonFile := args[0]\n\t\tjsonData, err := ioutil.ReadFile(jsonFile)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error reading JSON file: %v\", err)\n\t\t}\n\n\t\tvar data map[string]interface{}\n\t\tif err := json.Unmarshal(jsonData, &data); err != nil {\n\t\t\tlog.Fatalf(\"Error unmarshalling JSON: %v\", err)\n\t\t}\n\n\t\ttomlData, err := toml.Marshal(data)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error marshaling to TOML: %v\", err)\n\t\t}\n\n\t\tfmt.Println(string(tomlData))\n\t},\n}\n\n// Convert TOML to YAML\nvar tomlToYamlCmd = &cobra.Command{\n\tUse:   \"toml-to-yaml [file]\",\n\tShort: \"Convert TOML to YAML\",\n\tArgs:  cobra.ExactArgs(1), // Ensure exactly one argument is provided\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\ttomlFile := args[0]\n\t\ttomlData, err := ioutil.ReadFile(tomlFile)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error reading TOML file: %v\", err)\n\t\t}\n\n\t\tvar data map[string]interface{}\n\t\tif err := toml.Unmarshal(tomlData, &data); err != nil {\n\t\t\tlog.Fatalf(\"Error unmarshaling TOML: %v\", err)\n\t\t}\n\n\t\tyamlData, err := yaml.Marshal(data)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error marshaling to YAML: %v\", err)\n\t\t}\n\n\t\tfmt.Println(string(yamlData))\n\t},\n}\n\n// Convert TOML to JSON\nvar tomlToJsonCmd = &cobra.Command{\n\tUse:   \"toml-to-json [file]\",\n\tShort: \"Convert TOML to JSON\",\n\tArgs:  cobra.ExactArgs(1), // Ensure exactly one argument is provided\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\ttomlFile := args[0]\n\t\ttomlData, err := ioutil.ReadFile(tomlFile)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error reading TOML file: %v\", err)\n\t\t}\n\n\t\tvar data map[string]interface{}\n\t\tif err := toml.Unmarshal(tomlData, &data); err != nil {\n\t\t\tlog.Fatalf(\"Error unmarshaling TOML: %v\", err)\n\t\t}\n\n\t\tjsonData, err := json.MarshalIndent(data, \"\", \"  \")\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error marshaling to JSON: %v\", err)\n\t\t}\n\n\t\tfmt.Println(string(jsonData))\n\t},\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *DevEndpointCustomLibraries) SetExtraPythonLibsS3Path(v string) *DevEndpointCustomLibraries {\n\ts.ExtraPythonLibsS3Path = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport asyncio\nimport logging\nfrom collections import deque\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Connection pool to manage active connections\nconnection_pool = deque(maxlen=10)  # Adjust maxlen as needed\n\nasync def handle_client(reader, writer):\n    \"\"\"\n    Handle incoming client connections and proxy data to the target server.\n    \"\"\"\n    client_addr = writer.get_extra_info('peername')\n    logger.info(f\"New connection from {client_addr}\")\n\n    try:\n        # Get a connection from the pool or create a new one\n        if connection_pool:\n            target_reader, target_writer = connection_pool.popleft()\n            logger.info(f\"Reusing connection from pool for {client_addr}\")\n        else:\n            target_reader, target_writer = await asyncio.open_connection('target_server', 8080)\n            logger.info(f\"Created new connection for {client_addr}\")\n\n        # Forward data between client and target server\n        async def forward(src_reader, dst_writer):\n            try:\n                while True:\n                    data = await src_reader.read(4096)\n                    if not data:\n                        break\n                    dst_writer.write(data)\n                    await dst_writer.drain()\n            except Exception as e:\n                logger.error(f\"Error forwarding data: {e}\")\n            finally:\n                dst_writer.close()\n                await dst_writer.wait_closed()\n\n        # Start forwarding tasks\n        client_to_target = asyncio.create_task(forward(reader, target_writer))\n        target_to_client = asyncio.create_task(forward(target_reader, writer))\n\n        # Wait for either task to complete\n        await asyncio.wait([client_to_target, target_to_client], return_when=asyncio.FIRST_COMPLETED)\n\n    except Exception as e:\n        logger.error(f\"Error handling client {client_addr}: {e}\")\n    finally:\n        # Return the connection to the pool if it's still open\n        if not target_writer.is_closing():\n            connection_pool.append((target_reader, target_writer))\n            logger.info(f\"Connection returned to pool for {client_addr}\")\n        else:\n            logger.info(f\"Connection closed for {client_addr}\")\n\n        writer.close()\n        await writer.wait_closed()\n\nasync def start_proxy_server():\n    \"\"\"\n    Start the TCP proxy server.\n    \"\"\"\n    server = await asyncio.start_server(handle_client, '0.0.0.0', 8888)\n    logger.info(\"Proxy server started on 0.0.0.0:8888\")\n\n    async with server:\n        await server.serve_forever()\n\nif __name__ == \"__main__\":\n    asyncio.run(start_proxy_server())\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<Page<ResourceMetricInner>>> listWebWorkerMetricsWithServiceResponseAsync(final String resourceGroupName, final String name, final String workerPoolName, final Boolean details, final String filter) {\n        return listWebWorkerMetricsSinglePageAsync(resourceGroupName, name, workerPoolName, details, filter)\n            .concatMap(new Func1<ServiceResponse<Page<ResourceMetricInner>>, Observable<ServiceResponse<Page<ResourceMetricInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<ResourceMetricInner>>> call(ServiceResponse<Page<ResourceMetricInner>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(listWebWorkerMetricsNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def append_track(self, track=None, pianoroll=None, program=0, is_drum=False,\n                     name='unknown'):\n        \"\"\"\n        Append a multitrack.Track instance to the track list or create a new\n        multitrack.Track object and append it to the track list.\n\n        Parameters\n        ----------\n        track : pianoroll.Track\n            A :class:`pypianoroll.Track` instance to be appended to the track\n            list.\n        pianoroll : np.ndarray, shape=(n_time_steps, 128)\n            A pianoroll matrix. The first and second dimension represents time\n            and pitch, respectively. Available datatypes are bool, int and\n            float. Only effective when `track` is None.\n        program: int\n            A program number according to General MIDI specification [1].\n            Available values are 0 to 127. Defaults to 0 (Acoustic Grand Piano).\n            Only effective when `track` is None.\n        is_drum : bool\n            A boolean number that indicates whether it is a percussion track.\n            Defaults to False. Only effective when `track` is None.\n        name : str\n            The name of the track. Defaults to 'unknown'. Only effective when\n            `track` is None.\n\n        References\n        ----------\n        [1] https://www.midi.org/specifications/item/gm-level-1-sound-set\n\n        \"\"\"\n        if track is not None:\n            if not isinstance(track, Track):\n                raise TypeError(\"`track` must be a pypianoroll.Track instance.\")\n            track.check_validity()\n        else:\n            track = Track(pianoroll, program, is_drum, name)\n        self.tracks.append(track)",
    "label": 0
  },
  {
    "codes": "function computeBaseline(data) {\n    var layerNum = data.length;\n    var pointNum = data[0].length;\n    var sums = [];\n    var y0 = [];\n    var max = 0;\n    var temp;\n    var base = {};\n\n    for (var i = 0; i < pointNum; ++i) {\n        for (var j = 0, temp = 0; j < layerNum; ++j) {\n            temp += data[j][i][1];\n        }\n        if (temp > max) {\n            max = temp;\n        }\n        sums.push(temp);\n    }\n\n    for (var k = 0; k < pointNum; ++k) {\n        y0[k] = (max - sums[k]) / 2;\n    }\n    max = 0;\n\n    for (var l = 0; l < pointNum; ++l) {\n        var sum = sums[l] + y0[l];\n        if (sum > max) {\n            max = sum;\n        }\n    }\n    base.y0 = y0;\n    base.max = max;\n\n    return base;\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport pygame\n\n# Initialize Pygame\npygame.init()\n\n# Constants\nWIDTH, HEIGHT = 800, 600\nGRAVITY = 0.5\nJUMP_STRENGTH = -10\nMAX_JUMPS = 2\n\n# Setup the display\nscreen = pygame.display.set_mode((WIDTH, HEIGHT))\npygame.display.set_caption('2D Platformer with Double Jump')\n\n# Player attributes\nplayer = pygame.Rect(100, 300, 50, 50)  # x, y, width, height\nplayer_velocity_y = 0\njumps_left = MAX_JUMPS\non_ground = False\n\n# Clock for controlling the frame rate\nclock = pygame.time.Clock()\n\n# Game loop\nrunning = True\nwhile running:\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            running = False\n    \n    keys = pygame.key.get_pressed()\n\n    # Jumping and Double Jump Logic\n    if keys[pygame.K_SPACE]:\n        if jumps_left > 0:\n            player_velocity_y = JUMP_STRENGTH\n            jumps_left -= 1\n            on_ground = False\n\n    # Gravity\n    player_velocity_y += GRAVITY\n    player.y += player_velocity_y\n\n    # Check if player is on the ground\n    if player.y >= HEIGHT - player.height:  # Assume ground is at bottom of window\n        player.y = HEIGHT - player.height\n        player_velocity_y = 0\n        on_ground = True\n        jumps_left = MAX_JUMPS\n\n    # Rendering\n    screen.fill((0, 0, 0))  # Clear screen with black\n    pygame.draw.rect(screen, (255, 0, 0), player)  # Draw player in red\n    pygame.display.flip()\n\n    # Cap the frame rate\n    clock.tick(60)\n\n# Quit Pygame\npygame.quit()\n```\n\nNote: In this code, the player can perform a double jump. The `on_ground` boolean helps reset the jump counter when the player touches the ground. Make sure to handle collisions if you're integrating into a larger game environment.",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries for I2C and time functions\nfrom machine import Pin, I2C\nimport time\n\n# Define the I2C bus parameters\ni2c = I2C(0, scl=Pin(22), sda=Pin(21), freq=100000)\n\n# Sensor I2C address and register (example for a common temperature sensor like TMP102)\nsensor_address = 0x48\ntemp_register = 0x00\n\n# Function to read temperature from the sensor\ndef read_temperature():\n    # Write to the temperature register to start the reading\n    i2c.writeto(sensor_address, bytearray([temp_register]))\n    \n    # Read two bytes from the temperature register\n    temp_data = i2c.readfrom(sensor_address, 2)\n    \n    # Convert the byte data to temperature (example conversion for TMP102)\n    temp_raw = (temp_data[0] << 4) | (temp_data[1] >> 4)\n    if temp_raw > 0x7FF:\n        temp_raw -= 4096\n    \n    temperature_celsius = temp_raw * 0.0625\n    return temperature_celsius\n\n# Infinite loop to continuously read the temperature\nwhile True:\n    # Read and print the temperature\n    temperature = read_temperature()\n    print(\"Temperature: {:.2f} C\".format(temperature))\n    \n    # Wait before the next reading\n    time.sleep(2)\n```\n",
    "label": 1
  },
  {
    "codes": "```dart\nimport 'dart:math';\n\nString formatNumberCompact(num number) {\n  const suffixes = ['', 'K', 'M', 'B', 'T'];\n  if (number < 1000) return number.toString();\n\n  int i = (log(number) / log(1000)).floor();\n  num rounded = number / pow(1000, i);\n  \n  return '${rounded.toStringAsFixed(1)}${suffixes[i]}';\n}\n\nvoid main() {\n  print(formatNumberCompact(123)); // \"123\"\n  print(formatNumberCompact(1234)); // \"1.2K\"\n  print(formatNumberCompact(1234567)); // \"1.2M\"\n  print(formatNumberCompact(1234567890)); // \"1.2B\"\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport static org.mockito.Mockito.*;\nimport static org.junit.Assert.*;\n\nimport org.junit.Before;\nimport org.junit.Test;\nimport org.mockito.InjectMocks;\nimport org.mockito.Mock;\nimport org.mockito.MockitoAnnotations;\nimport org.springframework.transaction.PlatformTransactionManager;\nimport org.springframework.transaction.TransactionStatus;\nimport org.springframework.transaction.support.DefaultTransactionDefinition;\n\npublic class TransactionRollbackTest {\n\n    @Mock\n    private PlatformTransactionManager transactionManager;\n\n    @Mock\n    private TransactionStatus transactionStatus;\n\n    @InjectMocks\n    private MyService myService; // Replace with your service class\n\n    @Before\n    public void setUp() {\n        MockitoAnnotations.openMocks(this);\n        when(transactionManager.getTransaction(any(DefaultTransactionDefinition.class))).thenReturn(transactionStatus);\n    }\n\n    @Test\n    public void testTransactionRollback() {\n        // Given: Set up conditions for the service method that eventually throws an exception\n        doThrow(new RuntimeException(\"Test Exception\")).when(myService).performDatabaseOperation();\n\n        // When: Call the service method\n        try {\n            myService.performTransactionalOperation(); // Replace with your method\n        } catch (Exception e) {\n            // Expected exception\n        }\n\n        // Then: Verify that the transaction was rolled back\n        verify(transactionStatus).setRollbackOnly();\n        verify(transactionManager).rollback(transactionStatus);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function (opt, ignoreUpdateRangeUsg) {\n        var option = this.option;\n        each([['start', 'startValue'], ['end', 'endValue']], function (names) {\n            // If only one of 'start' and 'startValue' is not null/undefined, the other\n            // should be cleared, which enable clear the option.\n            // If both of them are not set, keep option with the original value, which\n            // enable use only set start but not set end when calling `dispatchAction`.\n            // The same as 'end' and 'endValue'.\n            if (opt[names[0]] != null || opt[names[1]] != null) {\n                option[names[0]] = opt[names[0]];\n                option[names[1]] = opt[names[1]];\n            }\n        }, this);\n\n        !ignoreUpdateRangeUsg && updateRangeUse(this, opt);\n    }",
    "label": 0
  },
  {
    "codes": "func (s *JobDescription) SetStatusMessage(v string) *JobDescription {\n\ts.StatusMessage = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function locationBar_focus(event) {\n    switch (event.type) {\n      case \"click\":\n        this._controller.click(this.urlbar);\n        break;\n      case \"shortcut\":\n        var cmdKey = utils.getEntity(this.getDtds(), \"openCmd.commandkey\");\n        this._controller.keypress(null, cmdKey, {accelKey: true});\n        break;\n      default:\n        throw new Error(arguments.callee.name + \": Unkown event type - \" + event.type);\n    }\n\n    // Wait until the location bar has been focused\n    this._controller.waitForEval(\"subject.getAttribute('focused') == 'true'\",\n                                 TIMEOUT, 100, this.urlbar.getNode());\n  }",
    "label": 0
  },
  {
    "codes": "function(elem, types, handler) {\n\t\t// don't do events on text and comment nodes\n\t\tif ( elem.nodeType == 3 || elem.nodeType == 8 )\n\t\t\treturn;\n\n\t\tvar events = jQuery.data(elem, \"events\"), ret, index;\n\n\t\tif ( events ) {\n\t\t\t// Unbind all events for the element\n\t\t\tif ( types === undefined || (typeof types === \"string\" && types.charAt(0) == \".\") )\n\t\t\t\tfor ( var type in events )\n\t\t\t\t\tthis.remove( elem, type + (types || \"\") );\n\t\t\telse {\n\t\t\t\t// types is actually an event object here\n\t\t\t\tif ( types.type ) {\n\t\t\t\t\thandler = types.handler;\n\t\t\t\t\ttypes = types.type;\n\t\t\t\t}\n\n\t\t\t\t// Handle multiple events seperated by a space\n\t\t\t\t// jQuery(...).unbind(\"mouseover mouseout\", fn);\n\t\t\t\tjQuery.each(types.split(/\\s+/), function(index, type){\n\t\t\t\t\t// Namespaced event handlers\n\t\t\t\t\tvar namespaces = type.split(\".\");\n\t\t\t\t\ttype = namespaces.shift();\n\t\t\t\t\tvar namespace = RegExp(\"(^|\\\\.)\" + namespaces.slice().sort().join(\".*\\\\.\") + \"(\\\\.|$)\");\n\n\t\t\t\t\tif ( events[type] ) {\n\t\t\t\t\t\t// remove the given handler for the given type\n\t\t\t\t\t\tif ( handler )\n\t\t\t\t\t\t\tdelete events[type][handler.guid];\n\n\t\t\t\t\t\t// remove all handlers for the given type\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\tfor ( var handle in events[type] )\n\t\t\t\t\t\t\t\t// Handle the removal of namespaced events\n\t\t\t\t\t\t\t\tif ( namespace.test(events[type][handle].type) )\n\t\t\t\t\t\t\t\t\tdelete events[type][handle];\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\tif ( jQuery.event.specialAll[type] )\n\t\t\t\t\t\t\tjQuery.event.specialAll[type].teardown.call(elem, namespaces);\n\n\t\t\t\t\t\t// remove generic event handler if no more handlers exist\n\t\t\t\t\t\tfor ( ret in events[type] ) break;\n\t\t\t\t\t\tif ( !ret ) {\n\t\t\t\t\t\t\tif ( !jQuery.event.special[type] || jQuery.event.special[type].teardown.call(elem, namespaces) === false ) {\n\t\t\t\t\t\t\t\tif (elem.removeEventListener)\n\t\t\t\t\t\t\t\t\telem.removeEventListener(type, jQuery.data(elem, \"handle\"), false);\n\t\t\t\t\t\t\t\telse if (elem.detachEvent)\n\t\t\t\t\t\t\t\t\telem.detachEvent(\"on\" + type, jQuery.data(elem, \"handle\"));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tret = null;\n\t\t\t\t\t\t\tdelete events[type];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t}\n\n\t\t\t// Remove the expando if it's no longer used\n\t\t\tfor ( ret in events ) break;\n\t\t\tif ( !ret ) {\n\t\t\t\tvar handle = jQuery.data( elem, \"handle\" );\n\t\t\t\tif ( handle ) handle.elem = null;\n\t\t\t\tjQuery.removeData( elem, \"events\" );\n\t\t\t\tjQuery.removeData( elem, \"handle\" );\n\t\t\t}\n\t\t}\n\t}",
    "label": 0
  },
  {
    "codes": "function createURI(spec, originCharset, baseURI)\n{\n  let iosvc = Cc[\"@mozilla.org/network/io-service;1\"].\n              getService(Ci.nsIIOService);\n\n  return iosvc.newURI(spec, originCharset, baseURI);\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport psutil\nimport time\nimport logging\n\n# Configure logging\nlogging.basicConfig(filename='cpu_usage.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n\n# Define CPU usage threshold\nTHRESHOLD = 80.0  # in percentage\n\ndef monitor_cpu_usage(interval=5):\n    while True:\n        # Get current CPU usage\n        cpu_usage = psutil.cpu_percent(interval=interval)\n        \n        # Check if the CPU usage exceeds the threshold\n        if cpu_usage > THRESHOLD:\n            logging.warning(f\"High CPU usage detected: {cpu_usage}%\")\n        \n        # Debug output for monitoring\n        print(f\"Current CPU usage: {cpu_usage}%\")\n\nif __name__ == '__main__':\n    try:\n        monitor_cpu_usage()\n    except KeyboardInterrupt:\n        print(\"Monitoring stopped.\")\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Sample text data\ntexts = [\"This is a sample sentence.\", \"Another example for preprocessing.\", \"TensorFlow is powerful.\"]\n\n# Initialize Tokenizer\ntokenizer = Tokenizer(num_words=10000)  # Limit vocabulary to top 10,000 words\ntokenizer.fit_on_texts(texts)\n\n# Convert text to sequences of integers\nsequences = tokenizer.texts_to_sequences(texts)\n\n# Pad sequences to ensure uniform length\nmax_length = 10  # Define maximum sequence length\npadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n\n# Output the preprocessed data\nprint(padded_sequences)\n```\n\n```python\n# Optional: Convert padded sequences to TensorFlow Dataset for model training\ndataset = tf.data.Dataset.from_tensor_slices((padded_sequences, labels))  # Assuming labels are defined\ndataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)  # Batch and prefetch for performance\n```\n\n```python\n# Optional: Save tokenizer for later use\nimport pickle\n\nwith open('tokenizer.pkl', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse bevy::prelude::*;\n\n// Component to mark entities as selectable units\n#[derive(Component)]\nstruct SelectableUnit;\n\n// Component to store selection state\n#[derive(Component)]\nstruct Selected;\n\n// Resource to store the selection rectangle\n#[derive(Resource)]\nstruct SelectionRectangle {\n    start: Vec2,\n    end: Vec2,\n}\n\n// System to handle selection rectangle input\nfn handle_selection_input(\n    mut selection_rect: ResMut<SelectionRectangle>,\n    mouse_button_input: Res<Input<MouseButton>>,\n    windows: Query<&Window>,\n) {\n    let window = windows.single();\n    if mouse_button_input.just_pressed(MouseButton::Left) {\n        if let Some(cursor_pos) = window.cursor_position() {\n            selection_rect.start = cursor_pos;\n        }\n    }\n\n    if mouse_button_input.pressed(MouseButton::Left) {\n        if let Some(cursor_pos) = window.cursor_position() {\n            selection_rect.end = cursor_pos;\n        }\n    }\n}\n\n// System to select units within the selection rectangle\nfn select_units(\n    mut commands: Commands,\n    selection_rect: Res<SelectionRectangle>,\n    query: Query<(Entity, &Transform), With<SelectableUnit>>,\n) {\n    let rect = selection_rect.into();\n    for (entity, transform) in query.iter() {\n        let position = transform.translation.truncate();\n        if rect.contains(position) {\n            commands.entity(entity).insert(Selected);\n        } else {\n            commands.entity(entity).remove::<Selected>();\n        }\n    }\n}\n\n// System to visualize selected units\nfn visualize_selected_units(\n    selected_query: Query<&Transform, With<Selected>>,\n    mut gizmos: Gizmos,\n) {\n    for transform in selected_query.iter() {\n        gizmos.circle_2d(transform.translation.truncate(), 10.0, Color::GREEN);\n    }\n}\n\n// Plugin to bundle the systems\npub struct UnitSelectionPlugin;\n\nimpl Plugin for UnitSelectionPlugin {\n    fn build(&self, app: &mut App) {\n        app.init_resource::<SelectionRectangle>()\n            .add_systems(Update, (\n                handle_selection_input,\n                select_units,\n                visualize_selected_units,\n            ));\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (c *CloudTrail) PutEventSelectorsRequest(input *PutEventSelectorsInput) (req *request.Request, output *PutEventSelectorsOutput) {\n\top := &request.Operation{\n\t\tName:       opPutEventSelectors,\n\t\tHTTPMethod: \"POST\",\n\t\tHTTPPath:   \"/\",\n\t}\n\n\tif input == nil {\n\t\tinput = &PutEventSelectorsInput{}\n\t}\n\n\toutput = &PutEventSelectorsOutput{}\n\treq = c.newRequest(op, input, output)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "def legends(value):\n        \"\"\"list or KeyedList of ``Legends`` : Legend definitions\n\n        Legends visualize scales, and take one or more scales as their input.\n        They can be customized via a LegendProperty object.\n        \"\"\"\n        for i, entry in enumerate(value):\n            _assert_is_type('legends[{0}]'.format(i), entry, Legend)",
    "label": 0
  },
  {
    "codes": "public Observable<Page<NetworkInterfaceTapConfigurationInner>> listNextAsync(final String nextPageLink) {\n        return listNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<NetworkInterfaceTapConfigurationInner>>, Page<NetworkInterfaceTapConfigurationInner>>() {\n                @Override\n                public Page<NetworkInterfaceTapConfigurationInner> call(ServiceResponse<Page<NetworkInterfaceTapConfigurationInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define variables\nBACKUP_SOURCE=\"/path/to/directory\" # Local directory to back up\nS3_BUCKET=\"s3://your-bucket-name/backups\" # S3 bucket path\nLOG_FILE=\"/var/log/s3_backup.log\" # Path for log file\nCURRENT_DATE=$(date +%Y-%m-%d-%H-%M-%S) # Current timestamp\nBACKUP_FILE=\"backup-$CURRENT_DATE.tar.gz\" # Backup filename\n\n# Check if AWS CLI is installed\nif ! command -v aws &> /dev/null; then\n    echo \"AWS CLI not installed. Please install it to continue.\" >> $LOG_FILE\n    exit 1\nfi\n\n# Create a tar.gz file of the backup source\ntar -czf /tmp/${BACKUP_FILE} ${BACKUP_SOURCE} >> ${LOG_FILE} 2>&1\n\n# Check for errors during backup creation\nif [ $? -ne 0 ]; then\n    echo \"Failed to create backup archive.\" >> ${LOG_FILE}\n    exit 1\nfi\n\n# Upload the backup to S3\naws s3 cp /tmp/${BACKUP_FILE} ${S3_BUCKET}/ >> ${LOG_FILE} 2>&1\n\n# Check if the upload succeeded\nif [ $? -eq 0 ]; then\n    echo \"Backup ${BACKUP_FILE} successfully uploaded to ${S3_BUCKET}.\" >> ${LOG_FILE}\n    rm /tmp/${BACKUP_FILE} # Remove the local backup file after successful upload\nelse\n    echo \"Failed to upload ${BACKUP_FILE} to ${S3_BUCKET}.\" >> ${LOG_FILE}\n    exit 1\nfi\n\nexit 0\n```",
    "label": 1
  },
  {
    "codes": "```go\n// Note: This sample provides a basic structure for a Kubernetes Operator using the Operator SDK in Go.\n// It assumes that you have the Operator SDK installed and a working understanding of Go and Kubernetes.\n\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"sigs.k8s.io/controller-runtime/pkg/manager\"\n\t\"sigs.k8s.io/controller-runtime/pkg/ctrl\"\n\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\tutilruntime \"k8s.io/apimachinery/pkg/util/runtime\"\n\tclientgoscheme \"k8s.io/client-go/kubernetes/scheme\"\n\t\"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\tctrl \"sigs.k8s.io/controller-runtime\"\n\n\tappv1 \"github.com/example-inc/app-operator/api/v1\"\n\t\"github.com/example-inc/app-operator/controllers\"\n)\n\nvar (\n\tscheme   = runtime.NewScheme()\n\tsetupLog = ctrl.Log.WithName(\"setup\")\n)\n\nfunc init() {\n\t// Add client-go schemes\n\tutilruntime.Must(clientgoscheme.AddToScheme(scheme))\n\n\t// Add application v1 scheme\n\tutilruntime.Must(appv1.AddToScheme(scheme))\n\t// Add other schemes here as required\n}\n\nfunc main() {\n\t// Create a new Manager\n\tmgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{\n\t\tScheme: scheme,\n\t})\n\tif err != nil {\n\t\tsetupLog.Error(err, \"unable to start manager\")\n\t\tos.Exit(1)\n\t}\n\n\t// Setup your controller here\n\tif err = (&controllers.AppReconciler{\n\t\tClient: mgr.GetClient(),\n\t\tLog:    ctrl.Log.WithName(\"controllers\").WithName(\"App\"),\n\t\tScheme: mgr.GetScheme(),\n\t}).SetupWithManager(mgr); err != nil {\n\t\tsetupLog.Error(err, \"unable to create controller\", \"controller\", \"App\")\n\t\tos.Exit(1)\n\t}\n\n\t// Start the Manager\n\tsetupLog.Info(\"starting manager\")\n\tif err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {\n\t\tsetupLog.Error(err, \"problem running manager\")\n\t\tos.Exit(1)\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Diagnostics;\nusing System.ServiceProcess;\nusing System.Collections.Generic;\nusing System.Management;\n\nnamespace UsbMonitorService\n{\n    public partial class UsbMonitor : ServiceBase\n    {\n        private ManagementEventWatcher watcher;\n\n        public UsbMonitor()\n        {\n            InitializeComponent();\n            watcher = new ManagementEventWatcher();\n            watcher.EventArrived += new EventArrivedEventHandler(DeviceInsertedEvent);\n        }\n\n        protected override void OnStart(string[] args)\n        {\n            // Query to monitor USB device insertion\n            var queryString = \"SELECT * FROM Win32_DeviceChangeEvent WHERE EventType=2\";\n            watcher.Query = new WqlEventQuery(queryString);\n            watcher.Start();\n        }\n\n        protected override void OnStop()\n        {\n            watcher.Stop();\n            watcher.Dispose();\n        }\n\n        private void DeviceInsertedEvent(object sender, EventArrivedEventArgs e)\n        {\n            EventLog.WriteEntry(\"USB Device Connected\", $\"Connection time: {DateTime.Now}\");\n        }\n    }\n}\n```\n\n```xml\n<!-- App.config -->\n<configuration>\n  <system.serviceModel>\n    <services>\n      <service name=\"UsbMonitorService.UsbMonitor\" >\n        <host>\n          <baseAddresses>\n            <add baseAddress=\"http://localhost:8000/UsbMonitorService\"/>\n          </baseAddresses>\n        </host>\n      </service>\n    </services>\n  </system.serviceModel>\n</configuration>\n```\n\n```csharp\n// Installer class for the service\nusing System.ComponentModel;\nusing System.Configuration.Install;\nusing System.ServiceProcess;\n\n[RunInstaller(true)]\npublic class UsbMonitorInstaller : Installer\n{\n    public UsbMonitorInstaller()\n    {\n        ServiceProcessInstaller processInstaller = new ServiceProcessInstaller();\n        ServiceInstaller serviceInstaller = new ServiceInstaller();\n\n        processInstaller.Account = ServiceAccount.LocalSystem;\n\n        serviceInstaller.StartType = ServiceStartMode.Automatic;\n        serviceInstaller.ServiceName = \"UsbMonitorService\";\n\n        Installers.Add(processInstaller);\n        Installers.Add(serviceInstaller);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nimport React, { useState, useEffect } from 'react';\n\n// RealTimeChat component\nconst RealTimeChat = () => {\n  const [messages, setMessages] = useState([]); // State to store chat messages\n  const [inputValue, setInputValue] = useState(''); // State to store input value\n\n  // Simulate real-time message updates (e.g., from a WebSocket or API)\n  useEffect(() => {\n    const interval = setInterval(() => {\n      setMessages((prevMessages) => [\n        ...prevMessages,\n        { id: prevMessages.length + 1, text: `New message ${prevMessages.length + 1}` },\n      ]);\n    }, 5000); // Add a new message every 5 seconds\n\n    return () => clearInterval(interval); // Cleanup interval on unmount\n  }, []);\n\n  // Handle sending a message\n  const handleSendMessage = () => {\n    if (inputValue.trim()) {\n      setMessages((prevMessages) => [\n        ...prevMessages,\n        { id: prevMessages.length + 1, text: inputValue },\n      ]);\n      setInputValue(''); // Clear input after sending\n    }\n  };\n\n  return (\n    <div style={styles.chatContainer}>\n      <div style={styles.messagesContainer}>\n        {messages.map((message) => (\n          <div key={message.id} style={styles.message}>\n            {message.text}\n          </div>\n        ))}\n      </div>\n      <div style={styles.inputContainer}>\n        <input\n          type=\"text\"\n          value={inputValue}\n          onChange={(e) => setInputValue(e.target.value)}\n          style={styles.input}\n          placeholder=\"Type a message...\"\n        />\n        <button onClick={handleSendMessage} style={styles.sendButton}>\n          Send\n        </button>\n      </div>\n    </div>\n  );\n};\n\n// Styles for the chat interface\nconst styles = {\n  chatContainer: {\n    width: '300px',\n    height: '400px',\n    border: '1px solid #ccc',\n    borderRadius: '8px',\n    display: 'flex',\n    flexDirection: 'column',\n    justifyContent: 'space-between',\n    padding: '10px',\n  },\n  messagesContainer: {\n    flex: 1,\n    overflowY: 'auto',\n    marginBottom: '10px',\n  },\n  message: {\n    padding: '5px',\n    borderBottom: '1px solid #eee',\n  },\n  inputContainer: {\n    display: 'flex',\n  },\n  input: {\n    flex: 1,\n    padding: '5px',\n    borderRadius: '4px',\n    border: '1px solid #ccc',\n  },\n  sendButton: {\n    marginLeft: '10px',\n    padding: '5px 10px',\n    borderRadius: '4px',\n    border: 'none',\n    backgroundColor: '#007bff',\n    color: '#fff',\n    cursor: 'pointer',\n  },\n};\n\nexport default RealTimeChat;\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <cstdlib>\n\n// Function to allocate memory and intentionally create a memory leak\nvoid createMemoryLeak() {\n    int* leakyPointer = new int(42); // Allocate memory\n    // Forget to delete the allocated memory\n}\n\nint main() {\n    // Call the function that creates a memory leak\n    createMemoryLeak();\n\n    // Print a message to indicate the program has finished\n    std::cout << \"Program finished. Check for memory leaks using Valgrind.\" << std::endl;\n\n    return 0;\n}\n```\n\n```bash\n# Compile the program with debugging symbols\ng++ -g -o memory_leak_debug memory_leak_debug.cpp\n\n# Run the program with Valgrind to detect memory leaks\nvalgrind --leak-check=full ./memory_leak_debug\n```\n\n```plaintext\n// Expected Valgrind output:\n// ==12345== HEAP SUMMARY:\n// ==12345==     in use at exit: 4 bytes in 1 blocks\n// ==12345==   total heap usage: 1 allocs, 0 frees, 4 bytes allocated\n// ==12345== \n// ==12345== 4 bytes in 1 blocks are definitely lost in loss record 1 of 1\n// ==12345==    at 0x4C2BBAF: operator new(unsigned long) (vg_replace_malloc.c:344)\n// ==12345==    by 0x4006F6: createMemoryLeak() (memory_leak_debug.cpp:6)\n// ==12345==    by 0x400716: main (memory_leak_debug.cpp:12)\n// ==12345== \n// ==12345== LEAK SUMMARY:\n// ==12345==    definitely lost: 4 bytes in 1 blocks\n// ==12345==    indirectly lost: 0 bytes in 0 blocks\n// ==12345==      possibly lost: 0 bytes in 0 blocks\n// ==12345==    still reachable: 0 bytes in 0 blocks\n// ==12345==         suppressed: 0 bytes in 0 blocks\n// ==12345== \n// ==12345== For counts of detected and suppressed errors, rerun with: -v\n// ==12345== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)\n```",
    "label": 1
  },
  {
    "codes": "```typescript\n// Import necessary Cypress commands\nimport { checkoutPage } from '../pages/checkoutPage';\n\ndescribe('E-commerce Checkout Functionality', () => {\n  beforeEach(() => {\n    // Visit the e-commerce website and navigate to the checkout page\n    cy.visit('/');\n    cy.get('.cart-icon').click();\n    cy.contains('Proceed to Checkout').click();\n  });\n\n  it('should successfully complete the checkout process', () => {\n    // Fill in the checkout form\n    checkoutPage.fillCheckoutForm({\n      firstName: 'John',\n      lastName: 'Doe',\n      email: 'john.doe@example.com',\n      address: '123 Main St',\n      city: 'Anytown',\n      zipCode: '12345',\n      country: 'USA'\n    });\n\n    // Select payment method\n    checkoutPage.selectPaymentMethod('Credit Card');\n\n    // Enter payment details\n    checkoutPage.fillPaymentDetails({\n      cardNumber: '4111111111111111',\n      expirationDate: '12/25',\n      cvv: '123'\n    });\n\n    // Place the order\n    checkoutPage.placeOrder();\n\n    // Verify the order confirmation message\n    cy.get('.order-confirmation').should('contain', 'Thank you for your order!');\n  });\n\n  it('should display an error for invalid payment details', () => {\n    // Fill in the checkout form\n    checkoutPage.fillCheckoutForm({\n      firstName: 'Jane',\n      lastName: 'Doe',\n      email: 'jane.doe@example.com',\n      address: '456 Elm St',\n      city: 'Othertown',\n      zipCode: '67890',\n      country: 'USA'\n    });\n\n    // Select payment method\n    checkoutPage.selectPaymentMethod('Credit Card');\n\n    // Enter invalid payment details\n    checkoutPage.fillPaymentDetails({\n      cardNumber: '1234567890123456',\n      expirationDate: '13/25',\n      cvv: '999'\n    });\n\n    // Attempt to place the order\n    checkoutPage.placeOrder();\n\n    // Verify the error message\n    cy.get('.error-message').should('contain', 'Invalid payment details');\n  });\n});\n```",
    "label": 1
  },
  {
    "codes": "```rust\nextern crate serde_json;\nextern crate avro_rs;\n\nuse serde_json::Value;\nuse avro_rs::types::Value as AvroValue;\nuse avro_rs::{Schema, Writer};\nuse std::fs::File;\nuse std::io::{BufReader, BufWriter, Read, Write};\n\n// Convert JSON Value to Avro Value\nfn json_to_avro(json_value: &Value, schema: &Schema) -> AvroValue {\n    match json_value {\n        Value::Null => AvroValue::Null,\n        Value::Bool(b) => AvroValue::Boolean(*b),\n        Value::Number(num) => {\n            if let Some(n) = num.as_i64() {\n                AvroValue::Long(n)\n            } else if let Some(n) = num.as_f64() {\n                AvroValue::Double(n)\n            } else {\n                AvroValue::Double(0.0) // Default case\n            }\n        }\n        Value::String(s) => AvroValue::String(s.clone()),\n        Value::Array(arr) => {\n            let avro_arr: Vec<AvroValue> = arr.iter()\n                                               .map(|v| json_to_avro(v, schema))\n                                               .collect();\n            AvroValue::Array(avro_arr)\n        }\n        Value::Object(map) => {\n            let mut avro_map = serde_json::Map::new();\n            for (k, v) in map {\n                avro_map.insert(k.clone(), json_to_avro(v, schema));\n            }\n            AvroValue::Record(\n                schema\n                    .lookup_fullname(\"root\")\n                    .unwrap() // Assume root is the name of the record\n                    .clone(),\n                avro_map\n            )\n        }\n    }\n}\n\nfn main() -> std::io::Result<()> {\n    // Define schema\n    let raw_schema = r#\"\n    {\n        \"type\": \"record\",\n        \"name\": \"root\",\n        \"fields\": [\n          {\"name\": \"bool\", \"type\": \"boolean\"},\n          {\"name\": \"num\", \"type\": \"double\"},\n          {\"name\": \"text\", \"type\": \"string\"}\n        ]\n    }\n    \"#;\n    let schema = Schema::parse_str(raw_schema).unwrap();\n\n    // Read JSON file\n    let file = File::open(\"input.json\")?;\n    let reader = BufReader::new(file);\n    let json_value: Value = serde_json::from_reader(reader)?;\n\n    // Convert JSON to Avro\n    let avro_value = json_to_avro(&json_value, &schema);\n\n    // Write Avro file\n    let out_file = File::create(\"output.avro\")?;\n    let out_writer = BufWriter::new(out_file);\n    let mut writer = Writer::new(&schema, out_writer);\n\n    writer.append(avro_value).ok();\n    writer.flush().ok();\n\n    Ok(())\n}\n```\n\nThis program will read JSON from a file named `input.json`, convert it to Avro format using a predefined schema, and then write the Avro data to a file named `output.avro`. Make sure to adjust the schema to match the structure of your JSON data.",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<Page<CertificateItem>>> getCertificateVersionsNextWithServiceResponseAsync(final String nextPageLink) {\n        return getCertificateVersionsNextSinglePageAsync(nextPageLink)\n            .concatMap(new Func1<ServiceResponse<Page<CertificateItem>>, Observable<ServiceResponse<Page<CertificateItem>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<CertificateItem>>> call(ServiceResponse<Page<CertificateItem>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(getCertificateVersionsNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```kotlin\nfun String.reverseWordsPreservingWhitespace(): String {\n    val words = mutableListOf<String>()\n    val currentWord = StringBuilder()\n    \n    for (char in this) {\n        if (!char.isWhitespace()) {\n            currentWord.append(char) // Building the current word until whitespace\n        } else {\n            words.add(currentWord.toString()) // Add completed word to list\n            words.add(char.toString()) // Add the whitespace itself to list\n            currentWord.clear() // Reset for the next word\n        }\n    }\n    \n    words.add(currentWord.toString()) // Add the last word, if there is one\n    \n    return words.map { it.reversed() }.joinToString(\"\")\n}\n\n// Example usage:\n// val original = \"Hello,   world!\"\n// val reversed = original.reverseWordsPreservingWhitespace()\n// println(reversed) // Outputs: \",olleH   !dlrow\"\n```",
    "label": 1
  },
  {
    "codes": "public Observable<Page<ResourceMetricInner>> listMultiRoleMetricsAsync(final String resourceGroupName, final String name, final String startTime, final String endTime, final String timeGrain, final Boolean details, final String filter) {\n        return listMultiRoleMetricsWithServiceResponseAsync(resourceGroupName, name, startTime, endTime, timeGrain, details, filter)\n            .map(new Func1<ServiceResponse<Page<ResourceMetricInner>>, Page<ResourceMetricInner>>() {\n                @Override\n                public Page<ResourceMetricInner> call(ServiceResponse<Page<ResourceMetricInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def subset(self, names):\n        \"\"\"Creates a new, flat EvalEnvironment that contains only\n        the variables specified.\"\"\"\n        vld = VarLookupDict(self._namespaces)\n        new_ns = dict((name, vld[name]) for name in names)\n        return EvalEnvironment([new_ns], self.flags)",
    "label": 0
  },
  {
    "codes": "def _temporal_derivative(data, params, report):\n    \"\"\"\n    Performs mtd method. See func: teneto.derive.derive.\n    \"\"\"\n    # Data should be timexnode\n    report = {}\n\n    # Derivative\n    tdat = data[1:, :] - data[:-1, :]\n    # Normalize\n    tdat = tdat / np.std(tdat, axis=0)\n    # Coupling\n    coupling = np.array([tdat[:, i] * tdat[:, j] for i in np.arange(0,\n                                                                    tdat.shape[1]) for j in np.arange(0, tdat.shape[1])])\n    coupling = np.reshape(\n        coupling, [tdat.shape[1], tdat.shape[1], tdat.shape[0]])\n    # Average over window using strides\n    shape = coupling.shape[:-1] + (coupling.shape[-1] -\n                                   params['windowsize'] + 1, params['windowsize'])\n    strides = coupling.strides + (coupling.strides[-1],)\n    coupling_windowed = np.mean(np.lib.stride_tricks.as_strided(\n        coupling, shape=shape, strides=strides), -1)\n\n    report = {}\n    report['method'] = 'temporalderivative'\n    report['temporalderivative'] = {}\n    report['temporalderivative']['windowsize'] = params['windowsize']\n\n    return coupling_windowed, report",
    "label": 0
  },
  {
    "codes": "```tsx\nimport React from 'react';\n\n// Define a type for TreeNode\ninterface TreeNode {\n  id: string;\n  name: string;\n  children?: TreeNode[];\n}\n\n// Define props for the Tree component\ninterface TreeProps {\n  data: TreeNode;\n}\n\n// Recursive TreeNode component\nconst TreeNodeComponent: React.FC<{ node: TreeNode }> = ({ node }) => {\n  return (\n    <div style={{ marginLeft: '20px', position: 'relative' }}>\n      <div>{node.name}</div>\n      {node.children && node.children.length > 0 && (\n        <div>\n          {node.children.map(child => (\n            <TreeNodeComponent key={child.id} node={child} />\n          ))}\n        </div>\n      )}\n    </div>\n  );\n};\n\n// Main Tree component\nconst Tree: React.FC<TreeProps> = ({ data }) => {\n  return (\n    <div>\n      <TreeNodeComponent node={data} />\n    </div>\n  );\n};\n\nexport default Tree;\n\n// Usage example\n// const treeData: TreeNode = {\n//   id: '1',\n//   name: 'Root',\n//   children: [\n//     {\n//       id: '2',\n//       name: 'Child 1',\n//       children: [\n//         {\n//           id: '3',\n//           name: 'Grandchild 1',\n//         },\n//       ],\n//     },\n//     {\n//       id: '4',\n//       name: 'Child 2',\n//     },\n//   ],\n// };\n\n// <Tree data={treeData} />\n```",
    "label": 1
  },
  {
    "codes": "function emptyDir(dir) {\n  return new Promise((resolve, reject) => {\n    FS.emptyDir(dir, err => {\n      err ? reject(err) : resolve(dir);\n    })\n  });\n}",
    "label": 0
  },
  {
    "codes": "```javascript\nconst xss = require('xss'); // Import xss library for sanitization\n\nfunction sanitizeInput(req, res, next) {\n    // Sanitize query parameters\n    if (req.query) {\n        for (let key in req.query) {\n            req.query[key] = xss(req.query[key]);\n        }\n    }\n\n    // Sanitize request body\n    if (req.body) {\n        for (let key in req.body) {\n            req.body[key] = xss(req.body[key]);\n        }\n    }\n\n    // Sanitize URL parameters\n    if (req.params) {\n        for (let key in req.params) {\n            req.params[key] = xss(req.params[key]);\n        }\n    }\n\n    next(); // Proceed to the next middleware or route handler\n}\n\nmodule.exports = sanitizeInput;\n```",
    "label": 1
  },
  {
    "codes": "function(callback, thisArg) {\n      if (Array.prototype.map) {\n        return arr.map(callback, thisArg);\n      } else {\n        var len = arr.length >>> 0,\n            A = new Array(len),\n            i = 0;\n        for (; i < len; i++) {\n           A[i] = callback.call(thisArg, arr[i], i, arr);\n        }\n        return A;\n      }\n    }",
    "label": 0
  },
  {
    "codes": "def serve(request, path, document_root=None, show_indexes=False, default=''):\n    \"\"\"\n    Serve static files below a given point in the directory structure.\n\n    To use, put a URL pattern such as::\n\n        (r'^(?P<path>.*)$', 'django.views.static.serve',\n            {'document_root' : '/path/to/my/files/'})\n\n    in your URLconf. You must provide the ``document_root`` param. You may\n    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index\n    of the directory.  This index view will use the template hardcoded below,\n    but if you'd like to override it, you can create a template called\n    ``static/directory_index.html``.\n\n     Modified by ticket #1013 to serve index.html files in the same manner\n     as Apache and other web servers.\n\n     https://code.djangoproject.com/ticket/1013\n    \"\"\"\n\n    # Clean up given path to only allow serving files below document_root.\n    path = posixpath.normpath(unquote(path))\n    path = path.lstrip('/')\n    newpath = ''\n    for part in path.split('/'):\n        if not part:\n            # Strip empty path components.\n            continue\n        drive, part = os.path.splitdrive(part)\n        head, part = os.path.split(part)\n        if part in (os.curdir, os.pardir):\n            # Strip '.' and '..' in path.\n            continue\n        newpath = os.path.join(newpath, part).replace('\\\\', '/')\n    if newpath and path != newpath:\n        return HttpResponseRedirect(newpath)\n    fullpath = os.path.join(document_root, newpath)\n    if os.path.isdir(fullpath) and default:\n        defaultpath = os.path.join(fullpath, default)\n        if os.path.exists(defaultpath):\n            fullpath = defaultpath\n    if os.path.isdir(fullpath):\n        if show_indexes:\n            return directory_index(newpath, fullpath)\n        raise Http404(\"Directory indexes are not allowed here.\")\n    if not os.path.exists(fullpath):\n        raise Http404('\"%s\" does not exist' % fullpath)\n    # Respect the If-Modified-Since header.\n    statobj = os.stat(fullpath)\n    mimetype = mimetypes.guess_type(fullpath)[0] or 'application/octet-stream'\n    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\n                              statobj[stat.ST_MTIME], statobj[stat.ST_SIZE]):\n        if django.VERSION > (1, 6):\n            return HttpResponseNotModified(content_type=mimetype)\n        else:\n            return HttpResponseNotModified(mimetype=mimetype)\n    contents = open(fullpath, 'rb').read()\n    if django.VERSION > (1, 6):\n        response = HttpResponse(contents, content_type=mimetype)\n    else:\n        response = HttpResponse(contents, mimetype=mimetype)\n    response[\"Last-Modified\"] = http_date(statobj[stat.ST_MTIME])\n    response[\"Content-Length\"] = len(contents)\n    return response",
    "label": 0
  },
  {
    "codes": "```dart\n// Import necessary Flutter and Dart packages\nimport 'package:flutter/services.dart';\n\n// Define a method channel to communicate with native code\nconst MethodChannel _channel = MethodChannel('ml_accelerator_plugin');\n\n// Class to interact with device-native ML accelerators\nclass MLAcceleratorPlugin {\n  // Method to check if ML accelerator is available\n  static Future<bool> isMLAcceleratorAvailable() async {\n    try {\n      final bool isAvailable = await _channel.invokeMethod('isMLAcceleratorAvailable');\n      return isAvailable;\n    } on PlatformException catch (e) {\n      print(\"Failed to check ML accelerator availability: ${e.message}\");\n      return false;\n    }\n  }\n\n  // Method to perform inference using the ML accelerator\n  static Future<String> performInference(String modelPath, String inputData) async {\n    try {\n      final String result = await _channel.invokeMethod(\n        'performInference',\n        {'modelPath': modelPath, 'inputData': inputData},\n      );\n      return result;\n    } on PlatformException catch (e) {\n      print(\"Failed to perform inference: ${e.message}\");\n      return \"Error: ${e.message}\";\n    }\n  }\n}\n```\n\n```kotlin\n// Android native code (MainActivity.kt)\nimport io.flutter.embedding.android.FlutterActivity\nimport io.flutter.embedding.engine.FlutterEngine\nimport io.flutter.plugin.common.MethodChannel\n\nclass MainActivity: FlutterActivity() {\n    private val CHANNEL = \"ml_accelerator_plugin\"\n\n    override fun configureFlutterEngine(flutterEngine: FlutterEngine) {\n        super.configureFlutterEngine(flutterEngine)\n        MethodChannel(flutterEngine.dartExecutor.binaryMessenger, CHANNEL).setMethodCallHandler { call, result ->\n            when (call.method) {\n                \"isMLAcceleratorAvailable\" -> {\n                    // Check if ML accelerator is available\n                    val isAvailable = checkMLAcceleratorAvailability()\n                    result.success(isAvailable)\n                }\n                \"performInference\" -> {\n                    // Perform inference using ML accelerator\n                    val modelPath = call.argument<String>(\"modelPath\")\n                    val inputData = call.argument<String>(\"inputData\")\n                    val inferenceResult = performInference(modelPath, inputData)\n                    result.success(inferenceResult)\n                }\n                else -> {\n                    result.notImplemented()\n                }\n            }\n        }\n    }\n\n    private fun checkMLAcceleratorAvailability(): Boolean {\n        // Implement logic to check ML accelerator availability\n        return true // Placeholder\n    }\n\n    private fun performInference(modelPath: String?, inputData: String?): String {\n        // Implement logic to perform inference using ML accelerator\n        return \"Inference Result\" // Placeholder\n    }\n}\n```\n\n```swift\n// iOS native code (AppDelegate.swift)\nimport UIKit\nimport Flutter\n\n@UIApplicationMain\n@objc class AppDelegate: FlutterAppDelegate {\n    override func application(\n        _ application: UIApplication,\n        didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?\n    ) -> Bool {\n        let controller = window?.rootViewController as! FlutterViewController\n        let mlAcceleratorChannel = FlutterMethodChannel(name: \"ml_accelerator_plugin\",\n                                                      binaryMessenger: controller.binaryMessenger)\n        mlAcceleratorChannel.setMethodCallHandler({\n            (call: FlutterMethodCall, result: @escaping FlutterResult) -> Void in\n            switch call.method {\n            case \"isMLAcceleratorAvailable\":\n                // Check if ML accelerator is available\n                let isAvailable = self.checkMLAcceleratorAvailability()\n                result(isAvailable)\n            case \"performInference\":\n                // Perform inference using ML accelerator\n                let args = call.arguments as? [String: Any]\n                let modelPath = args?[\"modelPath\"] as? String\n                let inputData = args?[\"inputData\"] as? String\n                let inferenceResult = self.performInference(modelPath: modelPath, inputData: inputData)\n                result(inferenceResult)\n            default:\n                result(FlutterMethodNotImplemented)\n            }\n        })\n        return super.application(application, didFinishLaunchingWithOptions: launchOptions)\n    }\n\n    private func checkMLAcceleratorAvailability() -> Bool {\n        // Implement logic to check ML accelerator availability\n        return true // Placeholder\n    }\n\n    private func performInference(modelPath: String?, inputData: String?) -> String {\n        // Implement logic to perform inference using ML accelerator\n        return \"Inference Result\" // Placeholder\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "def _make_node_str_list(l):\n    \"\"\"Take a list of python objects and make a MPV string node array from it.\n\n    As an example, the python list ``l = [ \"foo\", 23, false ]`` will result in the following MPV node object::\n\n        struct mpv_node {\n            .format = MPV_NODE_ARRAY,\n            .u.list = *(struct mpv_node_array){\n                .num = len(l),\n                .keys = NULL,\n                .values = struct mpv_node[len(l)] {\n                    { .format = MPV_NODE_STRING, .u.string = l[0] },\n                    { .format = MPV_NODE_STRING, .u.string = l[1] },\n                    ...\n                }\n            }\n        }\n    \"\"\"\n    char_ps = [ c_char_p(_mpv_coax_proptype(e, str)) for e in l ]\n    node_list = MpvNodeList(\n        num=len(l),\n        keys=None,\n        values=( MpvNode * len(l))( *[ MpvNode(\n                format=MpvFormat.STRING,\n                val=MpvNodeUnion(string=p))\n            for p in char_ps ]))\n    node = MpvNode(\n        format=MpvFormat.NODE_ARRAY,\n        val=MpvNodeUnion(list=pointer(node_list)))\n    return char_ps, node_list, node, cast(pointer(node), c_void_p)",
    "label": 0
  },
  {
    "codes": "```python\nimport time\nfrom functools import wraps\n\ndef measure_time_ns(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.perf_counter_ns()  # Start time in nanoseconds\n        result = func(*args, **kwargs)       # Execute the function\n        end_time = time.perf_counter_ns()    # End time in nanoseconds\n        print(f\"{func.__name__} executed in {end_time - start_time} ns\")\n        return result\n    return wrapper\n```",
    "label": 1
  },
  {
    "codes": "```javascript\n// Import necessary libraries\nconst xss = require('xss');\n\n// Middleware to sanitize user input\nconst sanitizeInput = (req, res, next) => {\n    // Sanitize request body\n    if (req.body) {\n        for (let key in req.body) {\n            if (typeof req.body[key] === 'string') {\n                req.body[key] = xss(req.body[key]);\n            }\n        }\n    }\n\n    // Sanitize request query parameters\n    if (req.query) {\n        for (let key in req.query) {\n            if (typeof req.query[key] === 'string') {\n                req.query[key] = xss(req.query[key]);\n            }\n        }\n    }\n\n    // Sanitize request params\n    if (req.params) {\n        for (let key in req.params) {\n            if (typeof req.params[key] === 'string') {\n                req.params[key] = xss(req.params[key]);\n            }\n        }\n    }\n\n    // Proceed to the next middleware or route handler\n    next();\n};\n\n// Export the middleware\nmodule.exports = sanitizeInput;\n```",
    "label": 1
  },
  {
    "codes": "```kotlin\nsealed class DomainError {\n    object NetworkError : DomainError() // Represents a network-related error.\n    object NotFound : DomainError() // Represents a resource not found error.\n    data class ValidationError(val message: String) : DomainError() // Represents a validation error with a specific message.\n    object Unauthorized : DomainError() // Represents an unauthorized access error.\n    data class UnknownError(val error: Throwable) : DomainError() // Represents an unknown error with the original throwable.\n}\n```",
    "label": 1
  },
  {
    "codes": "```lua\n-- Define the size of the terrain block\nlocal blockSize = 10\nlocal blockHeight = 10\n\n-- Function to create a destructible block\nlocal function createBlock(position)\n    local block = Instance.new(\"Part\")\n    block.Size = Vector3.new(blockSize, blockHeight, blockSize)\n    block.Position = position\n    block.Anchored = true\n    block.Material = Enum.Material.Concrete\n    block.Parent = workspace\n\n    -- Add a ClickDetector to detect when the block is clicked\n    local clickDetector = Instance.new(\"ClickDetector\")\n    clickDetector.Parent = block\n\n    -- Function to destroy the block when clicked\n    clickDetector.MouseClick:Connect(function()\n        block:Destroy()\n    end)\nend\n\n-- Generate a grid of destructible blocks\nlocal gridSize = 5\nfor x = 1, gridSize do\n    for z = 1, gridSize do\n        local position = Vector3.new(x * blockSize, blockHeight / 2, z * blockSize)\n        createBlock(position)\n    end\nend\n```\n\nThis script creates a grid of destructible blocks in Roblox. Each block is clickable and will be destroyed when clicked. The blocks are arranged in a grid pattern, and their size and position can be adjusted by modifying the `blockSize`, `blockHeight`, and `gridSize` variables.",
    "label": 1
  },
  {
    "codes": "func (s *ConnectionPasswordEncryption) Validate() error {\n\tinvalidParams := request.ErrInvalidParams{Context: \"ConnectionPasswordEncryption\"}\n\tif s.AwsKmsKeyId != nil && len(*s.AwsKmsKeyId) < 1 {\n\t\tinvalidParams.Add(request.NewErrParamMinLen(\"AwsKmsKeyId\", 1))\n\t}\n\tif s.ReturnConnectionPasswordEncrypted == nil {\n\t\tinvalidParams.Add(request.NewErrParamRequired(\"ReturnConnectionPasswordEncrypted\"))\n\t}\n\n\tif invalidParams.Len() > 0 {\n\t\treturn invalidParams\n\t}\n\treturn nil\n}",
    "label": 0
  },
  {
    "codes": "def ttl(self, key):\n        \"\"\"\n        Emulate ttl\n\n        Even though the official redis commands documentation at http://redis.io/commands/ttl\n        states \"Return value: Integer reply: TTL in seconds, -2 when key does not exist or -1\n        when key does not have a timeout.\" the redis-py lib returns None for both these cases.\n        The lib behavior has been emulated here.\n\n        :param key: key for which ttl is requested.\n        :returns: the number of seconds till timeout, None if the key does not exist or if the\n                  key has no timeout(as per the redis-py lib behavior).\n        \"\"\"\n        value = self.pttl(key)\n        if value is None or value < 0:\n            return value\n        return value // 1000",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <Servo.h>\n#include <SoftwareSerial.h>\n\n// Create servo objects\nServo servo1;\nServo servo2;\n\n// Define Bluetooth serial connection\nSoftwareSerial Bluetooth(10, 11);  // RX, TX\n\nvoid setup() {\n  // Attach servos to pins\n  servo1.attach(9);\n  servo2.attach(8);\n  \n  // Start serial communication with Bluetooth at 9600 baud rate\n  Bluetooth.begin(9600);\n}\n\nvoid loop() {\n  if (Bluetooth.available()) {\n    // Read the incoming data via Bluetooth\n    char command = Bluetooth.read();\n\n    switch(command) {\n      case 'A':  // Command to rotate servo1 to 0 degrees\n        servo1.write(0);\n        break;\n      case 'B':  // Command to rotate servo1 to 90 degrees\n        servo1.write(90);\n        break;\n      case 'C':  // Command to rotate servo1 to 180 degrees\n        servo1.write(180);\n        break;\n      case 'D':  // Command to rotate servo2 to 0 degrees\n        servo2.write(0);\n        break;\n      case 'E':  // Command to rotate servo2 to 90 degrees\n        servo2.write(90);\n        break;\n      case 'F':  // Command to rotate servo2 to 180 degrees\n        servo2.write(180);\n        break;\n      default:\n        // If the command is not recognized, do nothing\n        break;\n    }\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\n// MainActivity.java\npackage com.example.tfliteobjectdetection;\n\nimport android.Manifest;\nimport android.content.pm.PackageManager;\nimport android.graphics.Bitmap;\nimport android.graphics.BitmapFactory;\nimport android.graphics.Canvas;\nimport android.graphics.Color;\nimport android.graphics.Paint;\nimport android.graphics.RectF;\nimport android.os.Bundle;\nimport android.util.Log;\nimport android.util.Size;\nimport android.view.SurfaceHolder;\nimport android.view.SurfaceView;\nimport android.widget.Toast;\n\nimport androidx.annotation.NonNull;\nimport androidx.appcompat.app.AppCompatActivity;\nimport androidx.core.app.ActivityCompat;\nimport androidx.core.content.ContextCompat;\n\nimport org.tensorflow.lite.Interpreter;\nimport org.tensorflow.lite.support.common.FileUtil;\nimport org.tensorflow.lite.support.image.TensorImage;\nimport org.tensorflow.lite.support.model.Model;\n\nimport java.io.IOException;\nimport java.nio.ByteBuffer;\nimport java.util.List;\n\npublic class MainActivity extends AppCompatActivity implements SurfaceHolder.Callback {\n\n    private static final int CAMERA_PERMISSION_CODE = 100;\n    private static final String MODEL_FILE = \"model.tflite\";\n    private static final String LABEL_FILE = \"labels.txt\";\n\n    private SurfaceView surfaceView;\n    private SurfaceHolder surfaceHolder;\n    private Interpreter interpreter;\n    private List<String> labels;\n    private Paint paint;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        surfaceView = findViewById(R.id.surfaceView);\n        surfaceHolder = surfaceView.getHolder();\n        surfaceHolder.addCallback(this);\n\n        paint = new Paint();\n        paint.setColor(Color.RED);\n        paint.setStyle(Paint.Style.STROKE);\n        paint.setStrokeWidth(5.0f);\n\n        if (ContextCompat.checkSelfPermission(this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {\n            ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.CAMERA}, CAMERA_PERMISSION_CODE);\n        } else {\n            initializeModel();\n        }\n    }\n\n    private void initializeModel() {\n        try {\n            interpreter = new Interpreter(FileUtil.loadMappedFile(this, MODEL_FILE));\n            labels = FileUtil.loadLabels(this, LABEL_FILE);\n        } catch (IOException e) {\n            Log.e(\"TFLite\", \"Error loading model or labels\", e);\n            Toast.makeText(this, \"Failed to load model\", Toast.LENGTH_SHORT).show();\n        }\n    }\n\n    @Override\n    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults);\n        if (requestCode == CAMERA_PERMISSION_CODE && grantResults.length > 0 && grantResults[0] == PackageManager.PERMISSION_GRANTED) {\n            initializeModel();\n        } else {\n            Toast.makeText(this, \"Camera permission required\", Toast.LENGTH_SHORT).show();\n        }\n    }\n\n    @Override\n    public void surfaceCreated(SurfaceHolder holder) {\n        // Start camera preview or other initialization\n    }\n\n    @Override\n    public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {\n        // Handle surface changes\n    }\n\n    @Override\n    public void surfaceDestroyed(SurfaceHolder holder) {\n        // Clean up resources\n    }\n\n    private void detectObjects(Bitmap bitmap) {\n        TensorImage tensorImage = new TensorImage(DataType.UINT8);\n        tensorImage.load(bitmap);\n\n        ByteBuffer outputBuffer = ByteBuffer.allocateDirect(10 * 4); // Adjust size based on model output\n        interpreter.run(tensorImage.getBuffer(), outputBuffer);\n\n        // Parse output and draw bounding boxes\n        Canvas canvas = surfaceHolder.lockCanvas();\n        if (canvas != null) {\n            canvas.drawBitmap(bitmap, 0, 0, null);\n            for (int i = 0; i < outputBuffer.limit(); i += 4) {\n                float left = outputBuffer.getFloat(i);\n                float top = outputBuffer.getFloat(i + 1);\n                float right = outputBuffer.getFloat(i + 2);\n                float bottom = outputBuffer.getFloat(i + 3);\n                RectF rect = new RectF(left, top, right, bottom);\n                canvas.drawRect(rect, paint);\n            }\n            surfaceHolder.unlockCanvasAndPost(canvas);\n        }\n    }\n}\n```\n\n```xml\n<!-- activity_main.xml -->\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n    <SurfaceView\n        android:id=\"@+id/surfaceView\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\" />\n</RelativeLayout>\n```\n\n```xml\n<!-- AndroidManifest.xml -->\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.example.tfliteobjectdetection\">\n\n    <uses-permission android:name=\"android.permission.CAMERA\" />\n\n    <application\n        android:allowBackup=\"true\"\n        android:icon=\"@mipmap/ic_launcher\"\n        android:label=\"@string/app_name\"\n        android:roundIcon=\"@mipmap/ic_launcher_round\"\n        android:supportsRtl=\"true\"\n        android:theme=\"@style/Theme.TFLiteObjectDetection\">\n        <activity android:name=\".MainActivity\">\n            <intent-filter>\n                <action android:name=\"android.intent.action.MAIN\" />\n                <category android:name=\"android.intent.category.LAUNCHER\" />\n            </intent-filter>\n        </activity>\n    </application>\n</manifest>\n```",
    "label": 1
  },
  {
    "codes": "public Observable<Page<ModuleInner>> listByAutomationAccountNextAsync(final String nextPageLink) {\n        return listByAutomationAccountNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<ModuleInner>>, Page<ModuleInner>>() {\n                @Override\n                public Page<ModuleInner> call(ServiceResponse<Page<ModuleInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *DeleteDatabaseInput) SetName(v string) *DeleteDatabaseInput {\n\ts.Name = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public Observable<StorageBundle> setStorageAccountAsync(String vaultBaseUrl, String storageAccountName, String resourceId, String activeKeyName, boolean autoRegenerateKey, String regenerationPeriod, StorageAccountAttributes storageAccountAttributes, Map<String, String> tags) {\n        return setStorageAccountWithServiceResponseAsync(vaultBaseUrl, storageAccountName, resourceId, activeKeyName, autoRegenerateKey, regenerationPeriod, storageAccountAttributes, tags).map(new Func1<ServiceResponse<StorageBundle>, StorageBundle>() {\n            @Override\n            public StorageBundle call(ServiceResponse<StorageBundle> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```ruby\n# Module to centralize error reporting to Sentry\nmodule SentryErrorReporter\n  # Reports an error to Sentry\n  # @param exception [Exception] The exception to report\n  # @param context [Hash] Additional context to include in the report\n  def self.report_error(exception, context = {})\n    Sentry.with_scope do |scope|\n      scope.set_context('custom_context', context) if context.any?\n      Sentry.capture_exception(exception)\n    end\n  end\n\n  # Reports a message to Sentry\n  # @param message [String] The message to report\n  # @param level [Symbol] The severity level (e.g., :info, :warning, :error)\n  # @param context [Hash] Additional context to include in the report\n  def self.report_message(message, level = :info, context = {})\n    Sentry.with_scope do |scope|\n      scope.set_context('custom_context', context) if context.any?\n      Sentry.capture_message(message, level: level)\n    end\n  end\nend\n```\n\n```ruby\n# Example usage:\nbegin\n  # Some code that might raise an exception\n  raise StandardError, \"Something went wrong\"\nrescue StandardError => e\n  SentryErrorReporter.report_error(e, { user_id: 123, action: \"process_data\" })\nend\n\n# Example for reporting a message\nSentryErrorReporter.report_message(\"User logged in\", :info, { user_id: 456 })\n```\n\n```ruby\n# Ensure Sentry is configured in your application\n# Example configuration in an initializer (config/initializers/sentry.rb):\nSentry.init do |config|\n  config.dsn = 'YOUR_SENTRY_DSN'\n  config.environment = Rails.env\n  config.release = 'your-app-version'\nend\n```",
    "label": 1
  },
  {
    "codes": "public Observable<JobResponseInner> exportDevicesAsync(String resourceGroupName, String resourceName, ExportDevicesRequest exportDevicesParameters) {\n        return exportDevicesWithServiceResponseAsync(resourceGroupName, resourceName, exportDevicesParameters).map(new Func1<ServiceResponse<JobResponseInner>, JobResponseInner>() {\n            @Override\n            public JobResponseInner call(ServiceResponse<JobResponseInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "function parse_DataSpaceMapEntry(blob) {\n\tvar len = blob.read_shift(4);\n\tvar end = blob.l + len - 4;\n\tvar o = {};\n\tvar cnt = blob.read_shift(4);\n\tvar comps = [];\n\t/* [MS-OFFCRYPTO] 2.1.6.2 DataSpaceReferenceComponent Structure */\n\twhile(cnt-- > 0) comps.push({ t: blob.read_shift(4), v: blob.read_shift(0, 'lpp4') });\n\to.name = blob.read_shift(0, 'lpp4');\n\to.comps = comps;\n\tif(blob.l != end) throw new Error(\"Bad DataSpaceMapEntry: \" + blob.l + \" != \" + end);\n\treturn o;\n}",
    "label": 0
  },
  {
    "codes": "```go\n// Import necessary packages\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"sigs.k8s.io/controller-runtime/pkg/client\"\n\tctrl \"sigs.k8s.io/controller-runtime\"\n\t\"sigs.k8s.io/controller-runtime/pkg/log/zap\"\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\t\"k8s.io/apimachinery/pkg/runtime/schema\"\n\t\"k8s.io/apimachinery/pkg/util/runtime\"\n\t\"k8s.io/apimachinery/pkg/util/wait\"\n\t\"k8s.io/client-go/rest\"\n\t\"k8s.io/client-go/util/homedir\"\n\t\"path/filepath\"\n)\n\n// Define the CustomResource struct and add required fields\ntype CustomResource struct {\n\tSpec CustomResourceSpec `json:\"spec,omitempty\"`\n\tStatus CustomResourceStatus `json:\"status,omitempty\"`\n}\n\n// Define spec for the CustomResource\ntype CustomResourceSpec struct {\n\t// Example field\n\tExampleField string `json:\"exampleField\"`\n}\n\n// Define status for the CustomResource\ntype CustomResourceStatus struct {\n\t// Example status\n\tState string `json:\"state\"`\n}\n\n// Define a CustomResourceReconciler struct with client and scheme\ntype CustomResourceReconciler struct {\n\tclient.Client\n\tScheme *runtime.Scheme\n}\n\n// Implement the Reconcile method\nfunc (r *CustomResourceReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n\tlogger := ctrl.Log.WithValues(\"customresource\", req.NamespacedName)\n\n\tvar customResource CustomResource\n\tif err := r.Get(ctx, req.NamespacedName, &customResource); err != nil {\n\t\tlogger.Error(err, \"unable to fetch CustomResource\")\n\t\treturn ctrl.Result{}, client.IgnoreNotFound(err)\n\t}\n\n\t// Example reconciliation logic\n\tcustomResource.Status.State = customResource.Spec.ExampleField\n\terr := r.Status().Update(ctx, &customResource)\n\tif err != nil {\n\t\tlogger.Error(err, \"unable to update CustomResource status\")\n\t\treturn ctrl.Result{}, err\n\t}\n\n\treturn ctrl.Result{}, nil\n}\n\n// Setup the CustomResourceReconciler with the manager\nfunc (r *CustomResourceReconciler) SetupWithManager(mgr ctrl.Manager) error {\n\treturn ctrl.NewControllerManagedBy(mgr).\n\t\tFor(&CustomResource{}).\n\t\tComplete(r)\n}\n\nfunc main() {\n\t// Setup logger\n\tctrl.SetLogger(zap.New(zap.UseDevMode(true)))\n\n\t// Create a new scheme\n\tscheme := runtime.NewScheme()\n\tutilruntime.Must(AddToScheme(scheme))\n\n\t// Start a new manager\n\topts := ctrl.Options{\n\t\tScheme: scheme,\n\t}\n\tmgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), opts)\n\tif err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"unable to create manager: %v\", err)\n\t\tos.Exit(1)\n\t}\n\n\t// Setup the CustomResourceReconciler\n\tif err = (&CustomResourceReconciler{\n\t\tClient: mgr.GetClient(),\n\t\tScheme: mgr.GetScheme(),\n\t}).SetupWithManager(mgr); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"unable to create controller: %v\", err)\n\t\tos.Exit(1)\n\t}\n\n\t// Start the manager\n\tif err := mgr.Start(wait.NeverStop); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"unable to run manager: %v\", err)\n\t\tos.Exit(1)\n\t}\n}\n\n// AddToScheme dummy function for demonstration purposes,\n// you should replace it with actual logic to add your CRD's scheme\nfunc AddToScheme(s *runtime.Scheme) error {\n\treturn nil\n}\n```\n\nNote:  \n- Be sure to implement `AddToScheme` properly based on your specific CRD if needed.\n- This is a simplistic example and may require modifications for production use (e.g., handling watches, events, leader election).",
    "label": 1
  },
  {
    "codes": "function request(url, payload) {\n  return fetch(url, {\n    method: 'POST',\n    // set to 1000 b/c no response needed\n    timeout: 1000,\n    body: JSON.stringify(payload),\n  })\n  .then((response) => {\n    if (response.status === 404) {\n      return BbPromise.reject('404 api not found');\n    }\n    return response.json();\n  })\n  .then((res) => BbPromise.resolve(res))\n  .catch((e) => BbPromise.resolve(e));\n}",
    "label": 0
  },
  {
    "codes": "public Observable<Page<CertificateItem>> getCertificateVersionsAsync(final String vaultBaseUrl, final String certificateName, final Integer maxresults) {\n        return getCertificateVersionsWithServiceResponseAsync(vaultBaseUrl, certificateName, maxresults)\n            .map(new Func1<ServiceResponse<Page<CertificateItem>>, Page<CertificateItem>>() {\n                @Override\n                public Page<CertificateItem> call(ServiceResponse<Page<CertificateItem>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<KeyBundle> importKeyAsync(String vaultBaseUrl, String keyName, JsonWebKey key, Boolean hsm, KeyAttributes keyAttributes, Map<String, String> tags) {\n        return importKeyWithServiceResponseAsync(vaultBaseUrl, keyName, key, hsm, keyAttributes, tags).map(new Func1<ServiceResponse<KeyBundle>, KeyBundle>() {\n            @Override\n            public KeyBundle call(ServiceResponse<KeyBundle> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```javascript\nfunction deepClone(obj, hash = new WeakMap()) {\n    // If obj is null or not an object, return it directly\n    if (obj === null || typeof obj !== 'object') {\n        return obj;\n    }\n\n    // If the object is already cloned, return the cloned reference\n    if (hash.has(obj)) {\n        return hash.get(obj);\n    }\n\n    // Create a new object or array based on the instance type of obj\n    const clonedObj = Array.isArray(obj) ? [] : {};\n\n    // Store the reference in the hash map\n    hash.set(obj, clonedObj);\n\n    // Iterate through each key in the object\n    for (const key in obj) {\n        // Use hasOwnProperty to check key's presence in the object\n        if (obj.hasOwnProperty(key)) {\n            // Recursively clone the value\n            clonedObj[key] = deepClone(obj[key], hash);\n        }\n    }\n\n    return clonedObj;\n}\n\n// Example usage:\n// const circularObj = {};\n// circularObj.myself = circularObj;\n// const clone = deepClone(circularObj);\n// console.log(clone);\n```\n\nThis implementation handles circular references by using a `WeakMap` to keep track of already cloned objects, ensuring that each object is cloned only once.",
    "label": 1
  },
  {
    "codes": "function parse_Font(blob, length, opts) {\n\tvar o = {\n\t\tdyHeight: blob.read_shift(2),\n\t\tfl: blob.read_shift(2)\n\t};\n\tswitch((opts && opts.biff) || 8) {\n\t\tcase 2: break;\n\t\tcase 3: case 4: blob.l += 2; break;\n\t\tdefault: blob.l += 10; break;\n\t}\n\to.name = parse_ShortXLUnicodeString(blob, 0, opts);\n\treturn o;\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport javafx.application.Application;\nimport javafx.scene.Scene;\nimport javafx.scene.canvas.Canvas;\nimport javafx.scene.canvas.GraphicsContext;\nimport javafx.scene.control.Slider;\nimport javafx.scene.layout.BorderPane;\nimport javafx.scene.layout.VBox;\nimport javafx.scene.paint.Color;\nimport javafx.stage.Stage;\n\npublic class FractalApp extends Application {\n    private final int WIDTH = 800;\n    private final int HEIGHT = 800;\n\n    public static void main(String[] args) {\n        launch(args);\n    }\n\n    @Override\n    public void start(Stage primaryStage) {\n        Canvas canvas = new Canvas(WIDTH, HEIGHT);\n        Slider depthSlider = new Slider(1, 10, 1);\n        depthSlider.setShowTickLabels(true);\n        depthSlider.setBlockIncrement(1);\n\n        depthSlider.valueProperty().addListener((observable, oldValue, newValue) -> {\n            drawFractal(canvas.getGraphicsContext2D(), newValue.intValue());\n        });\n\n        VBox controls = new VBox(depthSlider);\n        BorderPane root = new BorderPane();\n        root.setCenter(canvas);\n        root.setBottom(controls);\n\n        drawFractal(canvas.getGraphicsContext2D(), (int) depthSlider.getValue());\n\n        Scene scene = new Scene(root, WIDTH, HEIGHT);\n        primaryStage.setTitle(\"Interactive Fractal Patterns\");\n        primaryStage.setScene(scene);\n        primaryStage.show();\n    }\n    \n    private void drawFractal(GraphicsContext gc, int depth) {\n        gc.setFill(Color.WHITE);\n        gc.fillRect(0, 0, WIDTH, HEIGHT);\n        gc.setStroke(Color.BLACK);\n        gc.setLineWidth(1.0);\n        \n        // Start the recursive drawing process\n        drawKochSnowflake(gc, WIDTH / 2 - 150, HEIGHT / 2 + 100, 300, depth);\n    }\n    \n    private void drawKochSnowflake(GraphicsContext gc, double x, double y, double size, int depth) {\n        double offsetX = -size / 2;\n        double offsetY = size * Math.sqrt(3) / 2 / 3;\n        for (int i = 0; i < 3; i++) {\n            drawKochCurve(gc, x + offsetX, y - offsetY, size, depth, i * 2 * Math.PI / 3);\n        }\n    }\n    \n    private void drawKochCurve(GraphicsContext gc, double x, double y, double size, int depth, double angle) {\n        if (depth == 0) {\n            gc.strokeLine(x, y, x + size * Math.cos(angle), y - size * Math.sin(angle));\n        } else {\n            size /= 3.0;\n            drawKochCurve(gc, x, y, size, depth - 1, angle);\n            x += size * Math.cos(angle);\n            y -= size * Math.sin(angle);\n            \n            drawKochCurve(gc, x, y, size, depth - 1, angle - Math.PI / 3);\n            x += size * Math.cos(angle - Math.PI / 3);\n            y -= size * Math.sin(angle - Math.PI / 3);\n            \n            drawKochCurve(gc, x, y, size, depth - 1, angle + Math.PI / 3);\n            x += size * Math.cos(angle + Math.PI / 3);\n            y -= size * Math.sin(angle + Math.PI / 3);\n            \n            drawKochCurve(gc, x, y, size, depth - 1, angle);\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "function Single(axisModel, ecModel, api) {\n\n    /**\n     * @type {string}\n     * @readOnly\n     */\n    this.dimension = 'single';\n\n    /**\n     * Add it just for draw tooltip.\n     *\n     * @type {Array.<string>}\n     * @readOnly\n     */\n    this.dimensions = ['single'];\n\n    /**\n     * @private\n     * @type {module:echarts/coord/single/SingleAxis}.\n     */\n    this._axis = null;\n\n    /**\n     * @private\n     * @type {module:zrender/core/BoundingRect}\n     */\n    this._rect;\n\n    this._init(axisModel, ecModel, api);\n\n    /**\n     * @type {module:echarts/coord/single/AxisModel}\n     */\n    this.model = axisModel;\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<SecretBundle> getSecretAsync(String vaultBaseUrl, String secretName, String secretVersion, final ServiceCallback<SecretBundle> serviceCallback) {\n        return ServiceFuture.fromResponse(getSecretWithServiceResponseAsync(vaultBaseUrl, secretName, secretVersion), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\n\t\"github.com/rwcarlsen/goexif/exif\"\n)\n\nfunc main() {\n\t// Open the JPEG file\n\timageFile, err := os.Open(\"example.jpeg\")\n\tif err != nil {\n\t\tfmt.Println(\"Error opening file:\", err)\n\t\treturn\n\t}\n\tdefer imageFile.Close()\n\n\t// Decode the EXIF data\n\tx, err := exif.Decode(imageFile)\n\tif err != nil {\n\t\tfmt.Println(\"Error decoding EXIF data:\", err)\n\t\treturn\n\t}\n\n\t// Print all available EXIF tags\n\tx.Walk(exif.WalkFunc(func(name exif.FieldName, tag *exif.Tag) error {\n\t\tfmt.Printf(\"%s: %s\\n\", name, tag.String())\n\t\treturn nil\n\t}))\n}\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glue) BatchStopJobRun(input *BatchStopJobRunInput) (*BatchStopJobRunOutput, error) {\n\treq, out := c.BatchStopJobRunRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport java.util.regex.Pattern;\n\npublic class MarkdownToPlainText {\n\n    public static String markdownToPlainText(String markdown) {\n        if (markdown == null) {\n            return \"\";\n        }\n        \n        // Remove Markdown emphasis, headers, lists, and other syntax\n        String plainText = markdown;\n        \n        // Remove headers\n        plainText = plainText.replaceAll(\"(?m)^#{1,6}\\\\s*\", \"\");\n        \n        // Remove emphasis\n        plainText = plainText.replaceAll(\"\\\\*{1,2}|_{1,2}\", \"\");\n        \n        // Remove code blocks\n        plainText = plainText.replaceAll(\"(?s)```.*?```\", \"\");\n        \n        // Remove inline code\n        plainText = plainText.replaceAll(\"`([^`]*)`\", \"$1\");\n        \n        // Remove images\n        plainText = plainText.replaceAll(\"!\\\\[.*?\\\\]\\\\(.*?\\\\)\", \"\");\n        \n        // Remove links\n        plainText = plainText.replaceAll(\"\\\\[(.*?)\\\\]\\\\(.*?\\\\)\", \"$1\");\n        \n        // Remove lists\n        plainText = plainText.replaceAll(\"(?m)^\\\\s*[-+*]\\\\s+\", \"\");\n        plainText = plainText.replaceAll(\"(?m)^\\\\s*\\\\d+\\\\.\\\\s+\", \"\");\n        \n        // Remove blockquotes\n        plainText = plainText.replaceAll(\"(?m)^>\\\\s*\", \"\");\n        \n        // Remove remaining HTML tags if any\n        plainText = plainText.replaceAll(\"<[^>]*>\", \"\");\n        \n        // Trim surrounding whitespace\n        plainText = plainText.trim();\n        \n        return plainText;\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *DeleteTableVersionInput) SetDatabaseName(v string) *DeleteTableVersionInput {\n\ts.DatabaseName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<List<KeyItem>> getKeysAsync(final String vaultBaseUrl, final Integer maxresults, final ListOperationCallback<KeyItem> serviceCallback) {\n        return AzureServiceFuture.fromPageResponse(\n            getKeysSinglePageAsync(vaultBaseUrl, maxresults),\n            new Func1<String, Observable<ServiceResponse<Page<KeyItem>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<KeyItem>>> call(String nextPageLink) {\n                    return getKeysNextSinglePageAsync(nextPageLink);\n                }\n            },\n            serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport functools\nimport traceback\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.ERROR)\n\ndef log_exceptions(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            # Log the exception with stack trace\n            logging.error(f\"Exception occurred in {func.__name__}: {e}\\n{traceback.format_exc()}\")\n            raise  # Re-raise the exception after logging\n    return wrapper\n```\n\n```python\n# Example usage\n@log_exceptions\ndef faulty_function():\n    return 1 / 0\n\nfaulty_function()  # This will log the exception with stack trace\n```",
    "label": 1
  },
  {
    "codes": "```dart\nString formatNumberCompact(num value) {\n  if (value == null) return '';\n\n  const suffixes = ['K', 'M', 'B', 'T'];\n  int suffixIndex = 0;\n  double compactValue = value.abs().toDouble();\n\n  while (compactValue >= 1000 && suffixIndex < suffixes.length - 1) {\n    compactValue /= 1000;\n    suffixIndex++;\n  }\n\n  return '${compactValue.toStringAsFixed(1)}${suffixes[suffixIndex]}';\n}\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glue) BatchDeleteConnectionWithContext(ctx aws.Context, input *BatchDeleteConnectionInput, opts ...request.Option) (*BatchDeleteConnectionOutput, error) {\n\treq, out := c.BatchDeleteConnectionRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <opencv2/opencv.hpp>\n#include <iostream>\n\nint main() {\n    // Open video file\n    cv::VideoCapture cap(\"input_video.mp4\");\n    if (!cap.isOpened()) {\n        std::cerr << \"Error opening video file\" << std::endl;\n        return -1;\n    }\n\n    cv::Mat frame, resizedFrame, grayFrame, normalizedFrame;\n\n    while (cap.read(frame)) {\n        // Resize frame to model input size (e.g., 224x224)\n        cv::resize(frame, resizedFrame, cv::Size(224, 224));\n\n        // Convert to grayscale\n        cv::cvtColor(resizedFrame, grayFrame, cv::COLOR_BGR2GRAY);\n\n        // Normalize pixel values to [0, 1]\n        grayFrame.convertTo(normalizedFrame, CV_32F, 1.0 / 255.0);\n\n        // Optional: Subtract mean and divide by std deviation\n        // cv::Scalar mean, stddev;\n        // cv::meanStdDev(normalizedFrame, mean, stddev);\n        // normalizedFrame = (normalizedFrame - mean) / stddev;\n\n        // Display processed frame (optional)\n        cv::imshow(\"Processed Frame\", normalizedFrame);\n        if (cv::waitKey(1) == 'q') break;\n    }\n\n    cap.release();\n    cv::destroyAllWindows();\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *GetDevEndpointsInput) SetNextToken(v string) *GetDevEndpointsInput {\n\ts.NextToken = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function openModal () {\n  robodog.pick({\n    restrictions: {\n      allowedFileTypes: ['.png']\n    },\n    waitForEncoding: true,\n    params: {\n      auth: { key: TRANSLOADIT_KEY },\n      template_id: TEMPLATE_ID\n    },\n    providers: [\n      'webcam'\n    ]\n    // if providers need custom config\n    // webcam: {\n    //   option: 'whatever'\n    // }\n  }).then(console.log, console.error)\n}",
    "label": 0
  },
  {
    "codes": "def write_single_xso(x, dest):\n    \"\"\"\n    Write a single XSO `x` to a binary file-like object `dest`.\n    \"\"\"\n    gen = XMPPXMLGenerator(dest,\n                           short_empty_elements=True,\n                           sorted_attributes=True)\n    x.unparse_to_sax(gen)",
    "label": 0
  },
  {
    "codes": "func (c *Glue) GetConnection(input *GetConnectionInput) (*GetConnectionOutput, error) {\n\treq, out := c.GetConnectionRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "func (c *Glue) GetTableVersionWithContext(ctx aws.Context, input *GetTableVersionInput, opts ...request.Option) (*GetTableVersionOutput, error) {\n\treq, out := c.GetTableVersionRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "func (c *Glue) StopCrawlerSchedule(input *StopCrawlerScheduleInput) (*StopCrawlerScheduleOutput, error) {\n\treq, out := c.StopCrawlerScheduleRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```csharp\n// Define a retry policy with exponential backoff\nvar retryPolicy = Policy\n    .Handle<HttpRequestException>() // Handle transient HTTP exceptions\n    .Or<TimeoutException>()       // Handle timeout exceptions\n    .WaitAndRetryAsync(\n        retryCount: 5,             // Retry up to 5 times\n        sleepDurationProvider: retryAttempt => TimeSpan.FromSeconds(Math.Pow(2, retryAttempt)), // Exponential backoff\n        onRetry: (exception, timespan, retryCount, context) =>\n        {\n            // Log retry attempts for monitoring\n            Console.WriteLine($\"Retry {retryCount} due to {exception.Message}. Waiting {timespan} before next retry.\");\n        });\n\n// Define a circuit breaker policy to stop retries after a certain number of failures\nvar circuitBreakerPolicy = Policy\n    .Handle<HttpRequestException>() // Handle transient HTTP exceptions\n    .Or<TimeoutException>()       // Handle timeout exceptions\n    .CircuitBreakerAsync(\n        exceptionsAllowedBeforeBreaking: 3, // Allow 3 failures before breaking the circuit\n        durationOfBreak: TimeSpan.FromMinutes(1), // Break the circuit for 1 minute\n        onBreak: (exception, timespan) =>\n        {\n            // Log circuit break event\n            Console.WriteLine($\"Circuit broken due to {exception.Message}. Circuit will be open for {timespan}.\");\n        },\n        onReset: () =>\n        {\n            // Log circuit reset event\n            Console.WriteLine(\"Circuit reset and closed.\");\n        },\n        onHalfOpen: () =>\n        {\n            // Log circuit half-open event\n            Console.WriteLine(\"Circuit half-open, testing connection.\");\n        });\n\n// Combine retry and circuit breaker policies\nvar resiliencePolicy = Policy.WrapAsync(retryPolicy, circuitBreakerPolicy);\n\n// Example usage in a microservice call\nawait resiliencePolicy.ExecuteAsync(async () =>\n{\n    // Simulate a microservice call\n    var response = await httpClient.GetAsync(\"https://example.com/api/resource\");\n    response.EnsureSuccessStatusCode(); // Ensure the response is successful\n});\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse bevy::prelude::*;\nuse bevy::ecs::query::With;\nuse bevy::input::mouse::{MouseButtonInput, MouseMotion, MouseWheel};\nuse bevy::render::camera::Camera;\n\nstruct Unit;\nstruct Selected;\n\n// Marks if a unit is selected or not\n#[derive(Default)]\nstruct SelectionBox {\n    start: Vec2,\n    end: Vec2,\n    active: bool,\n}\n\nfn main() {\n    App::build()\n        .add_plugins(DefaultPlugins)\n        .add_startup_system(setup.system())\n        .add_system(select_units.system())\n        .add_system(update_selection_box.system())\n        .add_system(draw_selection_box.system())\n        .run();\n}\n\n// System to setup initial state, like spawning a camera and some units\nfn setup(mut commands: Commands) {\n    commands.spawn_bundle(OrthographicCameraBundle::new_2d());\n    for i in 0..10 {\n        commands.spawn().insert(Unit).insert(Transform::from_translation(Vec3::new(i as f32 * 50.0, 0.0, 0.0)));\n    }\n}\n\n// Handles the input for starting, updating, and ending the selection box\nfn update_selection_box(\n    mut commands: Commands,\n    mouse_button_input: Res<Input<MouseButton>>,\n    windows: Res<Windows>,\n    mut query: Query<&mut SelectionBox>,\n    camera_query: Query<(&Camera, &GlobalTransform)>,\n) {\n    let window = windows.get_primary().unwrap();\n    let (camera, camera_transform) = camera_query.single().unwrap();\n\n    if let Some(cursor_position) = window.cursor_position() {\n        let mut selection_box = query.single_mut().unwrap();\n\n        if mouse_button_input.pressed(MouseButton::Left) {\n            // Start or update the selection box\n            if !selection_box.active {\n                selection_box.start = cursor_position;\n                selection_box.active = true;\n            }\n            selection_box.end = cursor_position;\n        } else if selection_box.active {\n            // Finalize the selection box\n            selection_box.active = false;\n            commands.insert_resource(selection_box.clone());\n        }\n    }\n}\n\n// System to select units based on the selection box\nfn select_units(\n    mut commands: Commands,\n    selection_box: Res<SelectionBox>,\n    mut query: Query<(Entity, &mut Transform), With<Unit>>,\n    selected_query: Query<Entity, With<Selected>>,\n) {\n    // Clear previous selections\n    for entity in selected_query.iter() {\n        commands.entity(entity).remove::<Selected>();\n    }\n\n    // Create a rectangle from the selection box\n    let min_x = selection_box.start.x.min(selection_box.end.x);\n    let max_x = selection_box.start.x.max(selection_box.end.x);\n    let min_y = selection_box.start.y.min(selection_box.end.y);\n    let max_y = selection_box.start.y.max(selection_box.end.y);\n\n    for (entity, transform) in query.iter() {\n        // Consider the unit selected if it falls within the AABB of the selection box\n        let position = transform.translation;\n        if position.x >= min_x && position.x <= max_x && position.y >= min_y && position.y <= max_y {\n            commands.entity(entity).insert(Selected);\n        }\n    }\n}\n\n// (Optional) System to draw the selection box on the screen\nfn draw_selection_box(mut gizmos: ResMut<Gizmos>, selection_box: Res<SelectionBox>) {\n    if selection_box.active {\n        let min_x = selection_box.start.x.min(selection_box.end.x);\n        let max_x = selection_box.start.x.max(selection_box.end.x);\n        let min_y = selection_box.start.y.min(selection_box.end.y);\n        let max_y = selection_box.start.y.max(selection_box.end.y);\n\n        gizmos.rect(Vec2::new(min_x, min_y), Vec2::new(max_x, max_y), 0.0, Color::rgba(0.0, 1.0, 0.0, 0.1));\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<TaskInner> getDetailsAsync(String resourceGroupName, String registryName, String taskName, final ServiceCallback<TaskInner> serviceCallback) {\n        return ServiceFuture.fromResponse(getDetailsWithServiceResponseAsync(resourceGroupName, registryName, taskName), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "function getCleanedElOption(elOption) {\n    elOption = zrUtil.extend({}, elOption);\n    zrUtil.each(\n        ['id', 'parentId', '$action', 'hv', 'bounding'].concat(layoutUtil.LOCATION_PARAMS),\n        function (name) {\n            delete elOption[name];\n        }\n    );\n    return elOption;\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport org.junit.runner.RunWith;\nimport org.junit.runners.Suite;\n\n// Specify a runner class: Suite.class\n@RunWith(Suite.class)\n\n// Specify an array of test classes\n@Suite.SuiteClasses({\n    SortingAlgorithmTest.class,\n    SortingAlgorithmEmptyTest.class,\n    SortingAlgorithmSingleElementTest.class,\n    SortingAlgorithmIdenticalElementsTest.class,\n    SortingAlgorithmReverseSortedTest.class,\n    SortingAlgorithmLargeNumbersTest.class\n})\n\npublic class SortingAlgorithmTestSuite {\n    // This class remains empty, it is used only as a holder for the above annotations\n}\n\n// Individual test case for a normal array of integers\nclass SortingAlgorithmTest {\n    @Test\n    public void testNormalCase() {\n        int[] input = {3, 1, 4, 1, 5, 9, 2};\n        int[] expected = {1, 1, 2, 3, 4, 5, 9};\n        assertArrayEquals(expected, sortAlgorithm(input));\n    }\n}\n\n// Edge case for an empty array\nclass SortingAlgorithmEmptyTest {\n    @Test\n    public void testEmptyArray() {\n        int[] input = {};\n        int[] expected = {};\n        assertArrayEquals(expected, sortAlgorithm(input));\n    }\n}\n\n// Edge case for a single element array\nclass SortingAlgorithmSingleElementTest {\n    @Test\n    public void testSingleElementArray() {\n        int[] input = {42};\n        int[] expected = {42};\n        assertArrayEquals(expected, sortAlgorithm(input));\n    }\n}\n\n// Edge case for an array with all identical elements\nclass SortingAlgorithmIdenticalElementsTest {\n    @Test\n    public void testIdenticalElementsArray() {\n        int[] input = {7, 7, 7, 7, 7};\n        int[] expected = {7, 7, 7, 7, 7};\n        assertArrayEquals(expected, sortAlgorithm(input));\n    }\n}\n\n// Edge case for an array that is already sorted in reverse order\nclass SortingAlgorithmReverseSortedTest {\n    @Test\n    public void testReverseSortedArray() {\n        int[] input = {9, 5, 4, 3, 2, 1, 1};\n        int[] expected = {1, 1, 2, 3, 4, 5, 9};\n        assertArrayEquals(expected, sortAlgorithm(input));\n    }\n}\n\n// Edge case for an array with large numbers\nclass SortingAlgorithmLargeNumbersTest {\n    @Test\n    public void testLargeNumbers() {\n        int[] input = {Integer.MAX_VALUE, Integer.MIN_VALUE, 0, -1, 1};\n        int[] expected = {Integer.MIN_VALUE, -1, 0, 1, Integer.MAX_VALUE};\n        assertArrayEquals(expected, sortAlgorithm(input));\n    }\n}\n\n// Dummy sortAlgorithm method to avoid compilation errors; replace with the actual method\nprivate int[] sortAlgorithm(int[] array) {\n    return array; // Replace with actual sorting algorithm implementation\n}\n```",
    "label": 1
  },
  {
    "codes": "public PagedList<CertificateIssuerItem> getCertificateIssuersNext(final String nextPageLink) {\n        ServiceResponse<Page<CertificateIssuerItem>> response = getCertificateIssuersNextSinglePageAsync(nextPageLink).toBlocking().single();\n        return new PagedList<CertificateIssuerItem>(response.body()) {\n            @Override\n            public Page<CertificateIssuerItem> nextPage(String nextPageLink) {\n                return getCertificateIssuersNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "```swift\nimport SpriteKit\n\nfunc createPuzzlePieceAnimation(for piece: SKSpriteNode) -> SKAction {\n    // Fade in the puzzle piece\n    let fadeIn = SKAction.fadeIn(withDuration: 0.5)\n    \n    // Scale up slightly (zoom effect)\n    let scaleUp = SKAction.scale(to: 1.1, duration: 0.5)\n    \n    // Wait for a moment\n    let wait = SKAction.wait(forDuration: 0.3)\n    \n    // Rotate slightly to the left\n    let rotateLeft = SKAction.rotate(byAngle: -.pi / 16, duration: 0.25)\n    \n    // Rotate back to the original position\n    let rotateRight = SKAction.rotate(byAngle: .pi / 16, duration: 0.25)\n    \n    // Scale back down to the original size\n    let scaleDown = SKAction.scale(to: 1.0, duration: 0.5)\n    \n    // Group rotation animations for a rocking effect\n    let rocking = SKAction.sequence([rotateLeft, rotateRight])\n    \n    // Repeat the rocking animation a few times\n    let repeatRocking = SKAction.repeat(rocking, count: 2)\n    \n    // Combine all animations in a sequence\n    let animationSequence = SKAction.sequence([fadeIn, scaleUp, wait, repeatRocking, scaleDown])\n    \n    return animationSequence\n}\n\n// Usage example:\n// let puzzlePieceNode = SKSpriteNode(imageNamed: \"puzzlePiece\")\n// puzzlePieceNode.run(createPuzzlePieceAnimation(for: puzzlePieceNode))\n```",
    "label": 1
  },
  {
    "codes": "def get_parsed_context(context_arg):\n    \"\"\"Parse input context string and returns context as dictionary.\"\"\"\n    assert context_arg, (\"pipeline must be invoked with context arg set. For \"\n                         \"this json parser you're looking for something \"\n                         \"like: \"\n                         \"pypyr pipelinename './myjsonfile.json'\")\n    logger.debug(\"starting\")\n    # open the json file on disk so that you can initialize the dictionary\n    logger.debug(f\"attempting to open file: {context_arg}\")\n    with open(context_arg) as json_file:\n        payload = json.load(json_file)\n\n    logger.debug(f\"json file loaded into context. Count: {len(payload)}\")\n    logger.debug(\"done\")\n    return payload",
    "label": 0
  },
  {
    "codes": "```python\nimport functools\nimport logging\nimport traceback\n\n# Configure logging\nlogging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef log_exceptions(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            # Log the exception with stack trace\n            logging.error(f\"Exception occurred in {func.__name__}: {str(e)}\")\n            logging.error(traceback.format_exc())\n            raise  # Re-raise the exception after logging\n    return wrapper\n```",
    "label": 1
  },
  {
    "codes": "def rpoplpush(self, source, destination):\n        \"\"\"Emulate rpoplpush\"\"\"\n        transfer_item = self.rpop(source)\n        if transfer_item is not None:\n            self.lpush(destination, transfer_item)\n        return transfer_item",
    "label": 0
  },
  {
    "codes": "```c\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <openssl/err.h>\n#include <stdio.h>\n#include <string.h>\n\n// Wrapper for AES-GCM encryption\nint aes_gcm_encrypt(const unsigned char *plaintext, size_t plaintext_len,\n                    const unsigned char *key, const unsigned char *iv,\n                    unsigned char *ciphertext, unsigned char *tag) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int ciphertext_len;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        ERR_print_errors_fp(stderr);\n        return -1;\n    }\n\n    // Initialize the encryption operation\n    if (1 != EVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL)) {\n        ERR_print_errors_fp(stderr);\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Set the key and IV\n    if (1 != EVP_EncryptInit_ex(ctx, NULL, NULL, key, iv)) {\n        ERR_print_errors_fp(stderr);\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Encrypt the plaintext\n    if (1 != EVP_EncryptUpdate(ctx, ciphertext, &len, plaintext, plaintext_len)) {\n        ERR_print_errors_fp(stderr);\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    ciphertext_len = len;\n\n    // Finalize the encryption\n    if (1 != EVP_EncryptFinal_ex(ctx, ciphertext + len, &len)) {\n        ERR_print_errors_fp(stderr);\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    ciphertext_len += len;\n\n    // Get the tag\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_GET_TAG, 16, tag)) {\n        ERR_print_errors_fp(stderr);\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    return ciphertext_len;\n}\n\n// Wrapper for AES-GCM decryption\nint aes_gcm_decrypt(const unsigned char *ciphertext, size_t ciphertext_len,\n                    const unsigned char *key, const unsigned char *iv,\n                    const unsigned char *tag, unsigned char *plaintext) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int plaintext_len;\n    int ret;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        ERR_print_errors_fp(stderr);\n        return -1;\n    }\n\n    // Initialize the decryption operation\n    if (1 != EVP_DecryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL)) {\n        ERR_print_errors_fp(stderr);\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Set the key and IV\n    if (1 != EVP_DecryptInit_ex(ctx, NULL, NULL, key, iv)) {\n        ERR_print_errors_fp(stderr);\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Decrypt the ciphertext\n    if (1 != EVP_DecryptUpdate(ctx, plaintext, &len, ciphertext, ciphertext_len)) {\n        ERR_print_errors_fp(stderr);\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    plaintext_len = len;\n\n    // Set the expected tag value\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_TAG, 16, (void *)tag)) {\n        ERR_print_errors_fp(stderr);\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Finalize the decryption\n    ret = EVP_DecryptFinal_ex(ctx, plaintext + len, &len);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    if (ret > 0) {\n        plaintext_len += len;\n        return plaintext_len;\n    } else {\n        // Verification failed\n        ERR_print_errors_fp(stderr);\n        return -1;\n    }\n}\n\n// Example usage\nint main() {\n    unsigned char key[32]; // 256-bit key\n    unsigned char iv[12];  // 96-bit IV\n    unsigned char plaintext[] = \"This is a secret message.\";\n    unsigned char ciphertext[128];\n    unsigned char decryptedtext[128];\n    unsigned char tag[16];\n\n    // Generate random key and IV\n    if (!RAND_bytes(key, sizeof(key)) || !RAND_bytes(iv, sizeof(iv))) {\n        fprintf(stderr, \"Error generating key or IV\\n\");\n        return 1;\n    }\n\n    // Encrypt\n    int ciphertext_len = aes_gcm_encrypt(plaintext, strlen((char *)plaintext), key, iv, ciphertext, tag);\n    if (ciphertext_len == -1) {\n        fprintf(stderr, \"Encryption failed\\n\");\n        return 1;\n    }\n\n    // Decrypt\n    int decryptedtext_len = aes_gcm_decrypt(ciphertext, ciphertext_len, key, iv, tag, decryptedtext);\n    if (decryptedtext_len == -1) {\n        fprintf(stderr, \"Decryption failed\\n\");\n        return 1;\n    }\n\n    // Null-terminate the decrypted text\n    decryptedtext[decryptedtext_len] = '\\0';\n\n    printf(\"Decrypted text: %s\\n\", decryptedtext);\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <fstream>\n#include <vector>\n#include <iostream>\n\n// Function to encrypt data using AES-256-CBC\nbool encryptFile(const std::string& inputFile, const std::string& outputFile, const std::vector<unsigned char>& key) {\n    // Initialize OpenSSL cipher context\n    EVP_CIPHER_CTX* ctx = EVP_CIPHER_CTX_new();\n    if (!ctx) {\n        std::cerr << \"Error creating cipher context.\" << std::endl;\n        return false;\n    }\n\n    // Generate a random IV (Initialization Vector)\n    unsigned char iv[EVP_MAX_IV_LENGTH];\n    if (RAND_bytes(iv, EVP_MAX_IV_LENGTH) != 1) {\n        std::cerr << \"Error generating IV.\" << std::endl;\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n\n    // Initialize encryption operation\n    if (EVP_EncryptInit_ex(ctx, EVP_aes_256_cbc(), nullptr, key.data(), iv) != 1) {\n        std::cerr << \"Error initializing encryption.\" << std::endl;\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n\n    // Open input and output files\n    std::ifstream inFile(inputFile, std::ios::binary);\n    std::ofstream outFile(outputFile, std::ios::binary);\n    if (!inFile || !outFile) {\n        std::cerr << \"Error opening files.\" << std::endl;\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n\n    // Write IV to the output file\n    outFile.write(reinterpret_cast<char*>(iv), EVP_MAX_IV_LENGTH);\n\n    // Buffer for reading and encrypting data\n    std::vector<unsigned char> inBuffer(4096);\n    std::vector<unsigned char> outBuffer(4096 + EVP_MAX_BLOCK_LENGTH);\n    int bytesRead, bytesEncrypted;\n\n    // Encrypt the file data\n    while ((bytesRead = inFile.read(reinterpret_cast<char*>(inBuffer.data()), inBuffer.size()).gcount()) > 0) {\n        if (EVP_EncryptUpdate(ctx, outBuffer.data(), &bytesEncrypted, inBuffer.data(), bytesRead) != 1) {\n            std::cerr << \"Error encrypting data.\" << std::endl;\n            EVP_CIPHER_CTX_free(ctx);\n            return false;\n        }\n        outFile.write(reinterpret_cast<char*>(outBuffer.data()), bytesEncrypted);\n    }\n\n    // Finalize encryption\n    if (EVP_EncryptFinal_ex(ctx, outBuffer.data(), &bytesEncrypted) != 1) {\n        std::cerr << \"Error finalizing encryption.\" << std::endl;\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n    outFile.write(reinterpret_cast<char*>(outBuffer.data()), bytesEncrypted);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n    return true;\n}\n\nint main() {\n    // Example key (32 bytes for AES-256)\n    std::vector<unsigned char> key = {\n        0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,\n        0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,\n        0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17,\n        0x18, 0x19, 0x1A, 0x1B, 0x1C, 0x1D, 0x1E, 0x1F\n    };\n\n    // Encrypt the file\n    if (encryptFile(\"input.txt\", \"output.enc\", key)) {\n        std::cout << \"File encrypted successfully.\" << std::endl;\n    } else {\n        std::cerr << \"File encryption failed.\" << std::endl;\n    }\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport time\n\ndef execution_time_decorator(func):\n    def wrapper(*args, **kwargs):\n        start_time = time.perf_counter_ns()  # Start time in nanoseconds\n        result = func(*args, **kwargs)        # Call the original function\n        end_time = time.perf_counter_ns()    # End time in nanoseconds\n        execution_time = end_time - start_time\n        print(f\"Execution time: {execution_time} ns\")  # Output the execution time\n        return result\n    return wrapper\n\n# Example usage\n@execution_time_decorator\ndef example_function(n):\n    return sum(range(n))\n\n# Call the decorated function\nexample_function(1000000)\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\n\n// Sample Product class\npublic class Product\n{\n    public string Name { get; set; }\n    public string Category { get; set; }\n    public decimal Price { get; set; }\n}\n\n// Method demonstrating the LINQ query\npublic class Program\n{\n    public static void Main()\n    {\n        // Sample product list\n        List<Product> products = new List<Product>\n        {\n            new Product { Name = \"Product1\", Category = \"Electronics\", Price = 100 },\n            new Product { Name = \"Product2\", Category = \"Electronics\", Price = 200 },\n            new Product { Name = \"Product3\", Category = \"Furniture\", Price = 300 },\n            new Product { Name = \"Product4\", Category = \"Furniture\", Price = 400 },\n        };\n\n        // Define filter criteria\n        decimal minPrice = 150;\n        decimal maxPrice = 350;\n        string category = \"Electronics\";\n\n        // LINQ query to filter products by category and price range\n        var filteredProducts = products\n            .Where(p => p.Category == category && p.Price >= minPrice && p.Price <= maxPrice)\n            .Select(p => new { p.Name, p.Price });\n\n        // Output the filtered products\n        foreach (var product in filteredProducts)\n        {\n            Console.WriteLine($\"{product.Name}: ${product.Price}\");\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```python\nimport psutil\nimport time\nimport logging\n\n# Configure logging\nlogging.basicConfig(filename='cpu_anomalies.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n\n# Threshold for CPU usage percentage considered as an anomaly\nCPU_USAGE_THRESHOLD = 90.0 \n\ndef log_anomaly(usage):\n    \"\"\"Logs the CPU usage anomaly\"\"\"\n    logging.info(f\"High CPU usage detected: {usage}%\")\n\ndef monitor_cpu_usage(interval=5):\n    \"\"\"Monitors the CPU usage and logs anomalies\"\"\"\n    while True:\n        # Get the current CPU usage\n        cpu_usage = psutil.cpu_percent(interval=1)\n        \n        # Check if the CPU usage exceeds the defined threshold\n        if cpu_usage > CPU_USAGE_THRESHOLD:\n            log_anomaly(cpu_usage)\n        \n        # Wait for the specified interval before checking again\n        time.sleep(interval)\n\nif __name__ == '__main__':\n    monitor_cpu_usage()\n```\n",
    "label": 1
  },
  {
    "codes": "def is_present(p):\n    \"\"\"\n    Given a parser or string, make a parser that returns\n    True if the parser matches, False otherwise\n    \"\"\"\n    return lexeme(p).optional().map(lambda v: False if v is None else True)",
    "label": 0
  },
  {
    "codes": "function BMapCoordSys(bmap, api) {\n  this._bmap = bmap;\n  this.dimensions = ['lng', 'lat'];\n  this._mapOffset = [0, 0];\n  this._api = api;\n  this._projection = new BMap.MercatorProjection();\n}",
    "label": 0
  },
  {
    "codes": "def free(self):\n        \"\"\"Remove the lock on the connection if the connection is not active\n\n        :raises: ConnectionBusyError\n\n        \"\"\"\n        LOGGER.debug('Connection %s freeing', self.id)\n        if self.handle.isexecuting():\n            raise ConnectionBusyError(self)\n        with self._lock:\n            self.used_by = None\n        LOGGER.debug('Connection %s freed', self.id)",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<MetricDefinitionInner> listMetricDefinitionsAsync(String resourceGroupName, String name, final ServiceCallback<MetricDefinitionInner> serviceCallback) {\n        return ServiceFuture.fromResponse(listMetricDefinitionsWithServiceResponseAsync(resourceGroupName, name), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```csharp\n// Custom attribute to validate JWT claims\n[AttributeUsage(AttributeTargets.Method | AttributeTargets.Class, AllowMultiple = false)]\npublic class ValidateJwtClaimsAttribute : ActionFilterAttribute\n{\n    private readonly string[] _requiredClaims;\n\n    // Constructor to accept required claims\n    public ValidateJwtClaimsAttribute(params string[] requiredClaims)\n    {\n        _requiredClaims = requiredClaims;\n    }\n\n    public override void OnActionExecuting(ActionExecutingContext context)\n    {\n        var user = context.HttpContext.User;\n\n        // Check if user is authenticated\n        if (!user.Identity.IsAuthenticated)\n        {\n            context.Result = new UnauthorizedResult();\n            return;\n        }\n\n        // Validate required claims\n        foreach (var claim in _requiredClaims)\n        {\n            if (!user.HasClaim(c => c.Type == claim))\n            {\n                context.Result = new ForbidResult();\n                return;\n            }\n        }\n\n        base.OnActionExecuting(context);\n    }\n}\n```\n\n```csharp\n// Example usage in a controller\n[ApiController]\n[Route(\"api/[controller]\")]\npublic class SecureController : ControllerBase\n{\n    [HttpGet(\"secure-endpoint\")]\n    [ValidateJwtClaims(\"role:admin\", \"permission:read\")] // Validate specific claims\n    public IActionResult SecureEndpoint()\n    {\n        return Ok(\"Access granted!\");\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ResourceGroupExportResultInner> exportTemplateAsync(String resourceGroupName, ExportTemplateRequest parameters) {\n        return exportTemplateWithServiceResponseAsync(resourceGroupName, parameters).map(new Func1<ServiceResponse<ResourceGroupExportResultInner>, ResourceGroupExportResultInner>() {\n            @Override\n            public ResourceGroupExportResultInner call(ServiceResponse<ResourceGroupExportResultInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "def from_events(self, instance, ev_args, ctx):\n        \"\"\"\n        Starting with the element to which the start event information in\n        `ev_args` belongs, parse text data. If any children are encountered,\n        :attr:`child_policy` is enforced (see\n        :class:`UnknownChildPolicy`). Likewise, if the start event contains\n        attributes, :attr:`attr_policy` is enforced\n        (c.f. :class:`UnknownAttrPolicy`).\n\n        The extracted text is passed through :attr:`type_` and\n        :attr:`validator` and if it passes, stored in the attribute on the\n        `instance` with which the property is associated.\n\n        This method is suspendable.\n        \"\"\"\n\n        # goal: take all text inside the child element and collect it as\n        # attribute value\n\n        attrs = ev_args[2]\n        if attrs and self.attr_policy == UnknownAttrPolicy.FAIL:\n            raise ValueError(\"unexpected attribute (at text only node)\")\n        parts = []\n        while True:\n            ev_type, *ev_args = yield\n            if ev_type == \"text\":\n                # collect ALL TEH TEXT!\n                parts.append(ev_args[0])\n            elif ev_type == \"start\":\n                # ok, a child inside the child was found, we look at our policy\n                # to see what to do\n                yield from enforce_unknown_child_policy(\n                    self.child_policy,\n                    ev_args)\n            elif ev_type == \"end\":\n                # end of our element, return\n                break\n\n        joined = \"\".join(parts)\n        try:\n            parsed = self.type_.parse(joined)\n        except (ValueError, TypeError):\n            if self.erroneous_as_absent:\n                return\n            raise\n        self._set_from_recv(instance, parsed)",
    "label": 0
  },
  {
    "codes": "T unsafeGetIfOpened() {\n\n        if (innerObject != null && innerObject.getState() == IOObject.IOObjectState.OPENED) {\n            return innerObject;\n        }\n        return null;\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport psutil\nimport time\nimport logging\n\n# Configure logging\nlogging.basicConfig(filename='cpu_anomalies.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n\n# Define threshold for CPU usage in percentage\nCPU_USAGE_THRESHOLD = 80.0  # Example threshold\n\ndef log_anomaly(cpu_usage):\n    \"\"\"Log the CPU usage anomaly.\"\"\"\n    logging.info(f\"High CPU usage detected: {cpu_usage}%\")\n\ndef monitor_cpu():\n    \"\"\"Monitor the system's CPU usage at regular intervals and log anomalies.\"\"\"\n    while True:\n        # Get the current CPU usage\n        cpu_usage = psutil.cpu_percent(interval=1)\n        \n        # Check if usage exceeds the threshold\n        if cpu_usage > CPU_USAGE_THRESHOLD:\n            log_anomaly(cpu_usage)\n        \n        # Wait before checking again to reduce resource use\n        time.sleep(5)\n\nif __name__ == \"__main__\":\n    # Start monitoring the CPU usage\n    monitor_cpu()\n```",
    "label": 1
  },
  {
    "codes": "public CompletableFuture<Void> deleteTopicAsync(String path) {\n        EntityNameHelper.checkValidTopicName(path);\n        return deleteEntityAsync(path);\n    }",
    "label": 0
  },
  {
    "codes": "```java\nimport javafx.application.Application;\nimport javafx.scene.Scene;\nimport javafx.scene.input.MouseEvent;\nimport javafx.scene.layout.Pane;\nimport javafx.scene.canvas.Canvas;\nimport javafx.scene.canvas.GraphicsContext;\nimport javafx.scene.paint.Color;\nimport javafx.stage.Stage;\n\npublic class FractalViewer extends Application {\n\n    private final int CANVAS_WIDTH = 800;\n    private final int CANVAS_HEIGHT = 600;\n    \n    private GraphicsContext gc;\n    private double xOffset = 0;\n    private double yOffset = 0;\n    private double zoomFactor = 1;\n    \n    public static void main(String[] args) {\n        launch(args);\n    }\n    \n    @Override\n    public void start(Stage primaryStage) {\n        Pane pane = new Pane();\n        Canvas canvas = new Canvas(CANVAS_WIDTH, CANVAS_HEIGHT);\n        gc = canvas.getGraphicsContext2D();\n        \n        // Initial render\n        drawFractal();\n        \n        // Mouse controls for interaction\n        canvas.addEventHandler(MouseEvent.MOUSE_DRAGGED, this::handleMouseDrag);\n        canvas.setOnScroll(event -> handleZoom(event.getDeltaY()));\n        \n        pane.getChildren().add(canvas);\n        Scene scene = new Scene(pane, CANVAS_WIDTH, CANVAS_HEIGHT);\n        \n        primaryStage.setTitle(\"Interactive Fractal Patterns\");\n        primaryStage.setScene(scene);\n        primaryStage.show();\n    }\n    \n    // Draw the fractal on the canvas\n    private void drawFractal() {\n        gc.clearRect(0, 0, CANVAS_WIDTH, CANVAS_HEIGHT);\n\n        for (int x = 0; x < CANVAS_WIDTH; x++) {\n            for (int y = 0; y < CANVAS_HEIGHT; y++) {\n                double c_re = (x - CANVAS_WIDTH/2) * 4.0 / (zoomFactor * CANVAS_WIDTH) + xOffset;\n                double c_im = (y - CANVAS_HEIGHT/2) * 4.0 / (zoomFactor * CANVAS_HEIGHT) + yOffset;\n                double X = 0, Y = 0;\n                int iterations = 0;\n                int maxIter = 100;\n                \n                while (X*X + Y*Y < 4 && iterations < maxIter) {\n                    double x_new = X*X - Y*Y + c_re;\n                    Y = 2*X*Y + c_im;\n                    X = x_new;\n                    iterations++;\n                }\n                \n                if (iterations < maxIter) gc.getPixelWriter().setColor(x, y, Color.color(iterations / (float)maxIter, 0, 0));\n                else gc.getPixelWriter().setColor(x, y, Color.BLACK);\n            }\n        }\n    }\n    \n    // Handle mouse drag for panning\n    private void handleMouseDrag(MouseEvent event) {\n        double prevX = xOffset;\n        double prevY = yOffset;\n        xOffset += (event.getX() - CANVAS_WIDTH / 2) / (zoomFactor * CANVAS_WIDTH / 4.0);\n        yOffset -= (event.getY() - CANVAS_HEIGHT / 2) / (zoomFactor * CANVAS_HEIGHT / 4.0);\n\n        if(prevX != xOffset || prevY != yOffset) {\n            drawFractal();\n        }\n    }\n    \n    // Handle zooming in and out\n    private void handleZoom(double deltaY) {\n        double prevZoom = zoomFactor;\n        zoomFactor = (deltaY > 0) ? zoomFactor * 1.1 : zoomFactor / 1.1;\n\n        if (prevZoom != zoomFactor) {\n            drawFractal();\n        }\n    }\n}\n```\n\nThis code sets up a basic JavaFX application to display and interact with fractals. It renders the Mandelbrot set and supports zooming and panning through mouse events.",
    "label": 1
  },
  {
    "codes": "```swift\nimport SwiftUI\nimport HealthKit\n\nstruct HeartRateView: View {\n    @StateObject private var heartRateManager = HeartRateManager()\n    \n    var body: some View {\n        VStack {\n            Text(\"Live Heart Rate\")\n                .font(.title)\n                .padding()\n            \n            Text(\"\\(heartRateManager.currentHeartRate) BPM\")\n                .font(.largeTitle)\n                .fontWeight(.bold)\n                .padding()\n        }\n        .onAppear {\n            heartRateManager.startHeartRateUpdates()\n        }\n        .onDisappear {\n            heartRateManager.stopHeartRateUpdates()\n        }\n    }\n}\n\nfinal class HeartRateManager: NSObject, ObservableObject {\n    private let healthStore = HKHealthStore()\n    private var heartRateQuery: HKAnchoredObjectQuery?\n    @Published var currentHeartRate: Int = 0\n    \n    override init() {\n        super.init()\n        authorizeHealthKit()\n    }\n    \n    private func authorizeHealthKit() {\n        if HKHealthStore.isHealthDataAvailable() {\n            let heartRateType = HKObjectType.quantityType(forIdentifier: .heartRate)!\n            healthStore.requestAuthorization(toShare: nil, read: Set([heartRateType])) { _, _ in }\n        }\n    }\n    \n    func startHeartRateUpdates() {\n        let heartRateType = HKObjectType.quantityType(forIdentifier: .heartRate)!\n        let sortDescriptor = NSSortDescriptor(key: HKSampleSortIdentifierStartDate, ascending: false)\n        let query = HKAnchoredObjectQuery(type: heartRateType,\n                                          predicate: nil,\n                                          anchor: nil,\n                                          limit: HKObjectQueryNoLimit,\n                                          resultsHandler: updateHandler)\n        \n        query.updateHandler = updateHandler\n        healthStore.execute(query)\n        heartRateQuery = query\n    }\n    \n    private func updateHandler(query: HKAnchoredObjectQuery, samples: [HKSample]?, deletedObjects: [HKDeletedObject]?, newAnchor: HKQueryAnchor?, error: Error?) {\n        guard error == nil, let samples = samples as? [HKQuantitySample] else { return }\n        \n        DispatchQueue.main.async {\n            if let heartRateSample = samples.first {\n                let heartRateUnit = HKUnit.count().unitDivided(by: HKUnit.minute())\n                let heartRate = heartRateSample.quantity.doubleValue(for: heartRateUnit)\n                self.currentHeartRate = Int(heartRate)\n            }\n        }\n    }\n    \n    func stopHeartRateUpdates() {\n        if let query = heartRateQuery {\n            healthStore.stop(query)\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Check if both source and destination directories are provided\nif [ \"$#\" -ne 2 ]; then\n    echo \"Usage: $0 <source_directory> <destination_directory>\"\n    exit 1\nfi\n\nSOURCE_DIR=$1\nDEST_DIR=$2\n\n# Check if rsync is installed\nif ! command -v rsync &> /dev/null; then\n    echo \"Error: rsync is not installed. Please install it and try again.\"\n    exit 1\nfi\n\n# Perform rsync operation\nrsync -av --delete \"$SOURCE_DIR/\" \"$DEST_DIR/\"\n\n# Exit with the last command's status\nexit $?\n```",
    "label": 1
  },
  {
    "codes": "```python\nfrom flask import Flask, request, jsonify\nfrom werkzeug.utils import secure_filename\nfrom PIL import Image\nimport os\nimport asyncio\nimport aiofiles\n\napp = Flask(__name__)\napp.config['UPLOAD_FOLDER'] = 'uploads/'\napp.config['RESIZED_FOLDER'] = 'resized/'\n\n# Ensure upload and resized folders exist\nos.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\nos.makedirs(app.config['RESIZED_FOLDER'], exist_ok=True)\n\ndef resize_image(image_path, output_path, size=(100, 100)):\n    with Image.open(image_path) as img:\n        img.thumbnail(size)\n        img.save(output_path)\n\nasync def save_and_resize(file):\n    filename = secure_filename(file.filename)\n    upload_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    \n    async with aiofiles.open(upload_path, 'wb') as out_file:\n        content = await file.read()\n        await out_file.write(content)\n\n    resized_path = os.path.join(app.config['RESIZED_FOLDER'], filename)\n    resize_image(upload_path, resized_path)\n\n@app.route('/upload', methods=['POST'])\nasync def upload_image():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file part'}), 400\n\n    file = request.files['file']\n\n    if file.filename == '':\n        return jsonify({'error': 'No selected file'}), 400\n\n    await save_and_resize(file)\n    \n    return jsonify({'message': 'File successfully uploaded and resized'}), 201\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<ClusterInner> createOrUpdateAsync(String resourceGroupName, String clusterName, ClusterInner parameters, final ServiceCallback<ClusterInner> serviceCallback) {\n        return ServiceFuture.fromResponse(createOrUpdateWithServiceResponseAsync(resourceGroupName, clusterName, parameters), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```rust\nuse bevy::prelude::*;\nuse bevy_prototype_lyon::prelude::*;\n\n#[derive(Component)]\nstruct Selectable;\n\n#[derive(Resource)]\nstruct Selection {\n    entities: Vec<Entity>,\n}\n\nfn main() {\n    App::build()\n        .add_plugins(DefaultPlugins)\n        .add_plugin(ShapePlugin)\n        .insert_resource(Selection { entities: Vec::new() })\n        .add_startup_system(setup_camera)\n        .add_system(selection_system)\n        .run();\n}\n\nfn setup_camera(mut commands: Commands) {\n    commands.spawn(Camera2dBundle::default());\n}\n\nfn selection_system(\n    mut commands: Commands,\n    mouse_button_input: Res<Input<MouseButton>>,\n    windows: Res<Windows>,\n    query: Query<(Entity, &GlobalTransform), With<Selectable>>,\n    mut selection: ResMut<Selection>,\n) {\n    let window = windows.get_primary().unwrap();\n\n    if mouse_button_input.just_pressed(MouseButton::Left) {\n        let cursor_position = window.cursor_position().unwrap();\n        let world_position = cursor_position_to_world_position(window, cursor_position);\n\n        // Deselect all currently selected units\n        for &entity in &selection.entities {\n            commands.entity(entity).remove::<Selected>();\n        }\n        selection.entities.clear();\n\n        // Select units within a certain radius from the cursor\n        for (entity, transform) in query.iter() {\n            let unit_position = transform.translation.truncate();\n            if unit_position.distance(world_position) < SELECTION_RADIUS {\n                commands.entity(entity).insert(Selected);\n                selection.entities.push(entity);\n            }\n        }\n    }\n}\n\nfn cursor_position_to_world_position(window: &Window, position: Vec2) -> Vec2 {\n    Vec2::new(\n        position.x - window.width() / 2.0,\n        position.y - window.height() / 2.0,\n    )\n}\n\n#[derive(Component)]\nstruct Selected;\n\n// Constants\nconst SELECTION_RADIUS: f32 = 50.0;\n```\n",
    "label": 1
  },
  {
    "codes": "function superCall(context, methodName) {\n    var args = zrUtil.slice(arguments, 2);\n    return this.superClass.prototype[methodName].apply(context, args);\n}",
    "label": 0
  },
  {
    "codes": "function prepareAnimationWhenNoOld(lasts, element, storageName) {\n        var lastCfg = lasts[thisRawIndex] = {};\n        var parentNode = thisNode.parentNode;\n\n        if (parentNode && (!reRoot || reRoot.direction === 'drillDown')) {\n            var parentOldX = 0;\n            var parentOldY = 0;\n\n            // New nodes appear from right-bottom corner in 'zoomToNode' animation.\n            // For convenience, get old bounding rect from background.\n            var parentOldBg = lastsForAnimation.background[parentNode.getRawIndex()];\n            if (!reRoot && parentOldBg && parentOldBg.old) {\n                parentOldX = parentOldBg.old.width;\n                parentOldY = parentOldBg.old.height;\n            }\n\n            // When no parent old shape found, its parent is new too,\n            // so we can just use {x:0, y:0}.\n            lastCfg.old = storageName === 'nodeGroup'\n                ? [0, parentOldY]\n                : {x: parentOldX, y: parentOldY, width: 0, height: 0};\n        }\n\n        // Fade in, user can be aware that these nodes are new.\n        lastCfg.fadein = storageName !== 'nodeGroup';\n    }",
    "label": 0
  },
  {
    "codes": "```kotlin\n// Create a Flow of sensor data\nval sensorDataFlow: Flow<SensorData> = sensorDataSource.getSensorData()\n\n// Batch process the sensor data in chunks of 10\nsensorDataFlow\n    .buffer(10) // Buffer to handle backpressure\n    .chunked(10) // Group data into batches of 10\n    .map { batch -> \n        // Process each batch\n        processBatch(batch)\n    }\n    .flowOn(Dispatchers.IO) // Perform processing on IO thread\n    .collect { processedBatch ->\n        // Handle the processed batch\n        handleProcessedBatch(processedBatch)\n    }\n\n// Function to process a batch of sensor data\nfun processBatch(batch: List<SensorData>): ProcessedBatch {\n    // Perform batch processing logic here\n    return ProcessedBatch(batch)\n}\n\n// Function to handle the processed batch\nfun handleProcessedBatch(processedBatch: ProcessedBatch) {\n    // Handle the processed batch (e.g., save to database, send to server, etc.)\n}\n```",
    "label": 1
  },
  {
    "codes": "```swift\nimport Security\n\n/// Function to securely store a key in the iOS Keychain\nfunc storeKeyInKeychain(key: String, value: String) -> Bool {\n    let keychainQuery: [String: Any] = [\n        kSecClass as String: kSecClassGenericPassword,\n        kSecAttrAccount as String: key,\n        kSecValueData as String: value.data(using: .utf8)!\n    ]\n    \n    // Remove existing item\n    SecItemDelete(keychainQuery as CFDictionary)\n\n    // Add new item to the Keychain\n    let status = SecItemAdd(keychainQuery as CFDictionary, nil)\n    \n    return status == errSecSuccess\n}\n```",
    "label": 1
  },
  {
    "codes": "function scaleNodeBreadths(nodes, kx, orient) {\n    zrUtil.each(nodes, function (node) {\n        var nodeDepth = node.getLayout().depth * kx;\n        orient === 'vertical'\n            ? node.setLayout({y: nodeDepth}, true)\n            : node.setLayout({x: nodeDepth}, true);\n    });\n}",
    "label": 0
  },
  {
    "codes": "func (c *Glacier) ListPartsRequest(input *ListPartsInput) (req *request.Request, output *ListPartsOutput) {\n\top := &request.Operation{\n\t\tName:       opListParts,\n\t\tHTTPMethod: \"GET\",\n\t\tHTTPPath:   \"/{accountId}/vaults/{vaultName}/multipart-uploads/{uploadId}\",\n\t\tPaginator: &request.Paginator{\n\t\t\tInputTokens:     []string{\"marker\"},\n\t\t\tOutputTokens:    []string{\"Marker\"},\n\t\t\tLimitToken:      \"limit\",\n\t\t\tTruncationToken: \"\",\n\t\t},\n\t}\n\n\tif input == nil {\n\t\tinput = &ListPartsInput{}\n\t}\n\n\toutput = &ListPartsOutput{}\n\treq = c.newRequest(op, input, output)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "def send_tracked(self, stanza, tracker):\n        \"\"\"\n        Send a message stanza with tracking.\n\n        :param stanza: Message stanza to send.\n        :type stanza: :class:`aioxmpp.Message`\n        :param tracker: Message tracker to use.\n        :type tracker: :class:`~.MessageTracker`\n        :rtype: :class:`~.StanzaToken`\n        :return: The token used to send the stanza.\n\n        If `tracker` is :data:`None`, a new :class:`~.MessageTracker` is\n        created.\n\n        This configures tracking for the stanza as if by calling\n        :meth:`attach_tracker` with a `token` and sends the stanza through the\n        stream.\n\n        .. seealso::\n\n           :meth:`attach_tracker`\n              can be used if the stanza cannot be sent (e.g. because it is a\n              carbon-copy) or has already been sent.\n        \"\"\"\n        token = self.client.enqueue(stanza)\n        self.attach_tracker(stanza, tracker, token)\n        return token",
    "label": 0
  },
  {
    "codes": "def RGB_to_HSV(cobj, *args, **kwargs):\n    \"\"\"\n    Converts from RGB to HSV.\n\n    H values are in degrees and are 0 to 360.\n    S values are a percentage, 0.0 to 1.0.\n    V values are a percentage, 0.0 to 1.0.\n    \"\"\"\n    var_R = cobj.rgb_r\n    var_G = cobj.rgb_g\n    var_B = cobj.rgb_b\n\n    var_max = max(var_R, var_G, var_B)\n    var_min = min(var_R, var_G, var_B)\n\n    var_H = __RGB_to_Hue(var_R, var_G, var_B, var_min, var_max)\n\n    if var_max == 0:\n        var_S = 0\n    else:\n        var_S = 1.0 - (var_min / var_max)\n\n    var_V = var_max\n\n    return HSVColor(\n        var_H, var_S, var_V)",
    "label": 0
  },
  {
    "codes": "function parse_xlsxcfb(cfb, _opts) {\n\tvar opts = _opts || {};\n\tvar f = 'Workbook', data = CFB.find(cfb, f);\n\ttry {\n\tf = '/!DataSpaces/Version';\n\tdata = CFB.find(cfb, f); if(!data || !data.content) throw new Error(\"ECMA-376 Encrypted file missing \" + f);\n\t/*var version = */parse_DataSpaceVersionInfo(data.content);\n\n\t/* 2.3.4.1 */\n\tf = '/!DataSpaces/DataSpaceMap';\n\tdata = CFB.find(cfb, f); if(!data || !data.content) throw new Error(\"ECMA-376 Encrypted file missing \" + f);\n\tvar dsm = parse_DataSpaceMap(data.content);\n\tif(dsm.length !== 1 || dsm[0].comps.length !== 1 || dsm[0].comps[0].t !== 0 || dsm[0].name !== \"StrongEncryptionDataSpace\" || dsm[0].comps[0].v !== \"EncryptedPackage\")\n\t\tthrow new Error(\"ECMA-376 Encrypted file bad \" + f);\n\n\t/* 2.3.4.2 */\n\tf = '/!DataSpaces/DataSpaceInfo/StrongEncryptionDataSpace';\n\tdata = CFB.find(cfb, f); if(!data || !data.content) throw new Error(\"ECMA-376 Encrypted file missing \" + f);\n\tvar seds = parse_DataSpaceDefinition(data.content);\n\tif(seds.length != 1 || seds[0] != \"StrongEncryptionTransform\")\n\t\tthrow new Error(\"ECMA-376 Encrypted file bad \" + f);\n\n\t/* 2.3.4.3 */\n\tf = '/!DataSpaces/TransformInfo/StrongEncryptionTransform/!Primary';\n\tdata = CFB.find(cfb, f); if(!data || !data.content) throw new Error(\"ECMA-376 Encrypted file missing \" + f);\n\t/*var hdr = */parse_Primary(data.content);\n\t} catch(e) {}\n\n\tf = '/EncryptionInfo';\n\tdata = CFB.find(cfb, f); if(!data || !data.content) throw new Error(\"ECMA-376 Encrypted file missing \" + f);\n\tvar einfo = parse_EncryptionInfo(data.content);\n\n\t/* 2.3.4.4 */\n\tf = '/EncryptedPackage';\n\tdata = CFB.find(cfb, f); if(!data || !data.content) throw new Error(\"ECMA-376 Encrypted file missing \" + f);\n\n/*global decrypt_agile */\nif(einfo[0] == 0x04 && typeof decrypt_agile !== 'undefined') return decrypt_agile(einfo[1], data.content, opts.password || \"\", opts);\n/*global decrypt_std76 */\nif(einfo[0] == 0x02 && typeof decrypt_std76 !== 'undefined') return decrypt_std76(einfo[1], data.content, opts.password || \"\", opts);\n\tthrow new Error(\"File is password-protected\");\n}",
    "label": 0
  },
  {
    "codes": "public Observable<Void> disableAsync(String resourceGroupName, String workflowName) {\n        return disableWithServiceResponseAsync(resourceGroupName, workflowName).map(new Func1<ServiceResponse<Void>, Void>() {\n            @Override\n            public Void call(ServiceResponse<Void> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans\n\n# Create Spark session\nspark = SparkSession.builder.appName(\"CustomerClustering\").getOrCreate()\n\n# Load data\n# Assuming data is in a CSV file with relevant features for clustering\ndata = spark.read.csv(\"customer_data.csv\", header=True, inferSchema=True)\n\n# Define feature columns\nfeature_columns = ['feature1', 'feature2', 'feature3']  # Specify your feature columns here\n\n# Assemble features into a feature vector\nvector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\ndata_with_features = vector_assembler.transform(data)\n\n# Create KMeans model\nkmeans = KMeans().setK(5).setSeed(1)  # Use 5 clusters, for example\n\n# Fit the model\nmodel = kmeans.fit(data_with_features)\n\n# Make predictions\npredictions = model.transform(data_with_features)\n\n# Show results\npredictions.select(\"customer_id\", \"prediction\").show()  # Assuming customer_id is a column in the original data\n\n# Stop the Spark session\nspark.stop()\n```",
    "label": 1
  },
  {
    "codes": "```javascript\ndb.sales.aggregate([\n  {\n    $group: {\n      _id: {\n        year: { $year: \"$date\" },\n        month: { $month: \"$date\" }\n      },\n      totalSales: { $sum: \"$amount\" }\n    }\n  },\n  {\n    $sort: {\n      \"_id.year\": 1,\n      \"_id.month\": 1\n    }\n  },\n  {\n    $project: {\n      _id: 0,\n      year: \"$_id.year\",\n      month: \"$_id.month\",\n      totalSales: 1\n    }\n  }\n])\n```",
    "label": 1
  },
  {
    "codes": "```java\n// MainActivity.java\npackage com.example.objectdetection;\n\nimport android.Manifest;\nimport android.content.pm.PackageManager;\nimport android.graphics.Bitmap;\nimport android.graphics.BitmapFactory;\nimport android.os.Bundle;\nimport android.util.Log;\nimport android.widget.ImageView;\nimport android.widget.Toast;\n\nimport androidx.annotation.NonNull;\nimport androidx.appcompat.app.AppCompatActivity;\nimport androidx.core.app.ActivityCompat;\nimport androidx.core.content.ContextCompat;\n\nimport org.tensorflow.lite.Interpreter;\nimport org.tensorflow.lite.support.image.TensorImage;\nimport org.tensorflow.lite.support.label.Category;\nimport org.tensorflow.lite.task.vision.detector.ObjectDetector;\n\nimport java.io.IOException;\nimport java.nio.MappedByteBuffer;\nimport java.nio.channels.FileChannel;\nimport java.util.List;\n\npublic class MainActivity extends AppCompatActivity {\n\n    private static final int CAMERA_PERMISSION_CODE = 100;\n    private Interpreter tflite;\n    private ImageView imageView;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        imageView = findViewById(R.id.imageView);\n\n        // Check and request camera permission\n        if (ContextCompat.checkSelfPermission(this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {\n            ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.CAMERA}, CAMERA_PERMISSION_CODE);\n        } else {\n            initializeTFLite();\n        }\n    }\n\n    private void initializeTFLite() {\n        try {\n            tflite = new Interpreter(loadModelFile());\n        } catch (IOException e) {\n            Log.e(\"TFLite\", \"Error loading model\", e);\n        }\n    }\n\n    private MappedByteBuffer loadModelFile() throws IOException {\n        FileChannel fileChannel = (FileChannel) getAssets().openFd(\"model.tflite\").getFileDescriptor().getChannel();\n        return fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, fileChannel.size());\n    }\n\n    private void detectObjects(Bitmap bitmap) {\n        TensorImage tensorImage = TensorImage.fromBitmap(bitmap);\n        ObjectDetector.ObjectDetectorOptions options = ObjectDetector.ObjectDetectorOptions.builder()\n                .setMaxResults(5)\n                .setScoreThreshold(0.5f)\n                .build();\n\n        try {\n            ObjectDetector detector = ObjectDetector.createFromFileAndOptions(this, \"model.tflite\", options);\n            List<Category> results = detector.detect(tensorImage);\n\n            for (Category result : results) {\n                Log.d(\"ObjectDetection\", \"Label: \" + result.getLabel() + \", Score: \" + result.getScore());\n            }\n        } catch (IOException e) {\n            Log.e(\"ObjectDetection\", \"Error detecting objects\", e);\n        }\n    }\n\n    @Override\n    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults);\n        if (requestCode == CAMERA_PERMISSION_CODE) {\n            if (grantResults.length > 0 && grantResults[0] == PackageManager.PERMISSION_GRANTED) {\n                initializeTFLite();\n            } else {\n                Toast.makeText(this, \"Camera permission denied\", Toast.LENGTH_SHORT).show();\n            }\n        }\n    }\n\n    @Override\n    protected void onDestroy() {\n        super.onDestroy();\n        if (tflite != null) {\n            tflite.close();\n        }\n    }\n}\n```\n\n```xml\n<!-- activity_main.xml -->\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n    <ImageView\n        android:id=\"@+id/imageView\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:layout_centerInParent=\"true\"\n        android:src=\"@drawable/sample_image\" />\n\n</RelativeLayout>\n```\n\n```xml\n<!-- AndroidManifest.xml -->\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.example.objectdetection\">\n\n    <uses-permission android:name=\"android.permission.CAMERA\" />\n\n    <application\n        android:allowBackup=\"true\"\n        android:icon=\"@mipmap/ic_launcher\"\n        android:label=\"@string/app_name\"\n        android:roundIcon=\"@mipmap/ic_launcher_round\"\n        android:supportsRtl=\"true\"\n        android:theme=\"@style/Theme.ObjectDetection\">\n        <activity android:name=\".MainActivity\">\n            <intent-filter>\n                <action android:name=\"android.intent.action.MAIN\" />\n                <category android:name=\"android.intent.category.LAUNCHER\" />\n            </intent-filter>\n        </activity>\n    </application>\n\n</manifest>\n```\n\n```gradle\n// build.gradle (Module: app)\ndependencies {\n    implementation 'org.tensorflow:tensorflow-lite-task-vision:0.3.1'\n    implementation 'org.tensorflow:tensorflow-lite-support:0.3.1'\n}\n```\n\n```gradle\n// build.gradle (Project: ObjectDetection)\nallprojects {\n    repositories {\n        google()\n        mavenCentral()\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport logging\nimport traceback\n\n# Configure logging\nlogging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef log_exceptions(func):\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            # Log the exception with traceback\n            logging.error(f\"Exception occurred in {func.__name__}: {str(e)}\")\n            logging.error(traceback.format_exc())\n            # Re-raise the exception to propagate it\n            raise\n    return wrapper\n\n# Example usage\n@log_exceptions\ndef risky_function(x, y):\n    return x / y\n\n# Test the decorator\nrisky_function(10, 0)\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport uuid\n\ndef generate_uuid_v4():\n    # Generate a UUID v4 and remove hyphens\n    return str(uuid.uuid4()).replace('-', '')\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nconst express = require('express');\nconst app = express();\napp.use(express.json());\n\n// Mock user data\nconst users = [\n    { id: 1, username: 'user1', password: 'pass1' },\n    { id: 2, username: 'user2', password: 'pass2' }\n];\n\n// Login endpoint\napp.post('/login', (req, res) => {\n    const { username, password } = req.body;\n\n    // Check if username and password are provided\n    if (!username || !password) {\n        return res.status(400).json({ message: 'Username and password are required' });\n    }\n\n    // Find user in mock data\n    const user = users.find(u => u.username === username && u.password === password);\n\n    // If user not found, return 401 Unauthorized\n    if (!user) {\n        return res.status(401).json({ message: 'Invalid credentials' });\n    }\n\n    // If user found, return 200 OK with user data\n    res.status(200).json({ message: 'Login successful', user });\n});\n\n// Start the server\napp.listen(3000, () => {\n    console.log('Server is running on port 3000');\n});\n```",
    "label": 1
  },
  {
    "codes": "public DeletedVaultInner getDeleted(String vaultName, String location) {\n        return getDeletedWithServiceResponseAsync(vaultName, location).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "```python\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nimport time\n\n# Set up the WebDriver\ndriver = webdriver.Chrome()  # Make sure the ChromeDriver is installed and in PATH\ndriver.get(\"https://example.com/login\")  # Replace with the actual URL of the login page\n\n# Wait for the page to load\ntime.sleep(2)  # Sleep is used for demonstration; use WebDriverWait for better handling\n\n# Locate the username and password fields and the login button\nusername_field = driver.find_element(By.NAME, \"username\")  # Adjust according to the actual name attribute\npassword_field = driver.find_element(By.NAME, \"password\")  # Adjust according to the actual name attribute\nlogin_button = driver.find_element(By.XPATH, '//button[@type=\"submit\"]')  # Adjust if needed\n\n# Simulate user input\nusername_field.send_keys(\"your_username\")  # Replace with test username\npassword_field.send_keys(\"your_password\")  # Replace with test password\n\n# Click the login button\nlogin_button.click()\n\n# Allow some time for the login process to complete\ntime.sleep(3)\n\n# Check if login was successful by looking for a logout button or user profile element\ntry:\n    logout_button = driver.find_element(By.XPATH, '//a[contains(text(), \"Logout\")]')  # Adjust if needed\n    print(\"Login successful!\")\nexcept:\n    print(\"Login failed.\")\n\n# Clean up\ndriver.quit()\n```",
    "label": 1
  },
  {
    "codes": "public WorkerPoolResourceInner createOrUpdateMultiRolePool(String resourceGroupName, String name, WorkerPoolResourceInner multiRolePoolEnvelope) {\n        return createOrUpdateMultiRolePoolWithServiceResponseAsync(resourceGroupName, name, multiRolePoolEnvelope).toBlocking().last().body();\n    }",
    "label": 0
  },
  {
    "codes": "```csharp\n// Assuming 'products' is a collection of Product objects with properties 'Price' and 'Category'\n\n// Filter products by price range (minPrice to maxPrice) and specific category\nvar filteredProducts = products\n    .Where(p => p.Price >= minPrice && p.Price <= maxPrice && p.Category == categoryName)\n    .ToList();\n\n// Example usage:\n// var minPrice = 50.0;\n// var maxPrice = 200.0;\n// var categoryName = \"Electronics\";\n```",
    "label": 1
  },
  {
    "codes": "public Observable<Page<DatabaseInner>> listByServerNextAsync(final String nextPageLink) {\n        return listByServerNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<DatabaseInner>>, Page<DatabaseInner>>() {\n                @Override\n                public Page<DatabaseInner> call(ServiceResponse<Page<DatabaseInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "function getNative(object, key) {\n  var value = getValue(object, key)\n  return baseIsNative(value) ? value : undefined\n}",
    "label": 0
  },
  {
    "codes": "```jsx\n// pages/data.js\nimport { useEffect, useState } from 'react';\nimport { Pool } from 'pg';\n\nconst pool = new Pool({\n  user: 'yourusername',\n  host: 'localhost',\n  database: 'yourdatabase',\n  password: 'yourpassword',\n  port: 5432,\n});\n\nexport async function getServerSideProps() {\n  let res = [];\n  try {\n    const client = await pool.connect();\n    const result = await client.query('SELECT * FROM your_table');\n    res = result.rows;\n    client.release();\n  } catch (err) {\n    console.error(err);\n  }\n\n  return {\n    props: {\n      data: res,\n    },\n  };\n}\n\nconst DataPage = ({ data }) => {\n  return (\n    <div>\n      <h1>Data from PostgreSQL</h1>\n      <ul>\n        {data.map((item) => (\n          <li key={item.id}>{item.name}</li>\n        ))}\n      </ul>\n    </div>\n  );\n};\n\nexport default DataPage;\n```\n\n```bash\n# Ensure to install necessary packages\nnpm install pg\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define source and destination directories\nSOURCE_DIR=\"/path/to/source\"\nDEST_DIR=\"/path/to/destination\"\n\n# Compare directories using rsync in dry-run mode\necho \"Comparing directories...\"\nrsync -avn --delete \"$SOURCE_DIR/\" \"$DEST_DIR/\"\n\n# Prompt user to confirm sync\nread -p \"Do you want to sync changes? (y/n): \" CONFIRM\n\n# Sync changes if user confirms\nif [[ $CONFIRM == \"y\" ]]; then\n    echo \"Syncing directories...\"\n    rsync -av --delete \"$SOURCE_DIR/\" \"$DEST_DIR/\"\n    echo \"Sync complete.\"\nelse\n    echo \"Sync cancelled.\"\nfi\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com/rwcarlsen/goexif/exif\"\n\t\"os\"\n)\n\nfunc main() {\n\t// Open the JPEG file\n\tfile, err := os.Open(\"image.jpg\")\n\tif err != nil {\n\t\tlog.Fatal(\"Error opening file:\", err)\n\t}\n\tdefer file.Close()\n\n\t// Decode EXIF data\n\tx, err := exif.Decode(file)\n\tif err != nil {\n\t\tlog.Fatal(\"Error decoding EXIF data:\", err)\n\t}\n\n\t// Iterate over EXIF tags and print them\n\tx.Walk(exif.WalkerFunc(func(name exif.FieldName, tag *exif.Tag) error {\n\t\tfmt.Printf(\"%s: %v\\n\", name, tag)\n\t\treturn nil\n\t}))\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.chrome.service import Service as ChromeService\nfrom webdriver_manager.chrome import ChromeDriverManager\nimport time\n\n# Initialize the Chrome driver\ndriver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n\ntry:\n    # Open the login page\n    driver.get('https://example.com/login')\n\n    # Find and fill in username\n    username_input = driver.find_element(By.ID, 'username')  # Adjust locator as necessary\n    username_input.send_keys('your_username')  # Replace with the actual username\n\n    # Find and fill in password\n    password_input = driver.find_element(By.ID, 'password')  # Adjust locator as necessary\n    password_input.send_keys('your_password')  # Replace with the actual password\n\n    # Submit the form\n    password_input.send_keys(Keys.RETURN)\n\n    # Wait for some time to see the logged-in page\n    time.sleep(5)\n    \n    # Check if login was successful by looking for a known element on the landing page\n    assert \"Dashboard\" in driver.title  # This is an example, adjust to your website\n\nfinally:\n    # Close the driver regardless of the result\n    driver.quit()\n```",
    "label": 1
  },
  {
    "codes": "def convert_color(color, target_cs, through_rgb_type=sRGBColor,\n                  target_illuminant=None, *args, **kwargs):\n    \"\"\"\n    Converts the color to the designated color space.\n\n    :param color: A Color instance to convert.\n    :param target_cs: The Color class to convert to. Note that this is not\n        an instance, but a class.\n    :keyword BaseRGBColor through_rgb_type: If during your conversion between\n        your original and target color spaces you have to pass through RGB,\n        this determines which kind of RGB to use. For example, XYZ->HSL.\n        You probably don't need to specify this unless you have a special\n        usage case.\n    :type target_illuminant: None or str\n    :keyword target_illuminant: If during conversion from RGB to a reflective\n        color space you want to explicitly end up with a certain illuminant,\n        pass this here. Otherwise the RGB space's native illuminant\n        will be used.\n    :returns: An instance of the type passed in as ``target_cs``.\n    :raises: :py:exc:`colormath.color_exceptions.UndefinedConversionError`\n        if conversion between the two color spaces isn't possible.\n    \"\"\"\n    if isinstance(target_cs, str):\n        raise ValueError(\"target_cs parameter must be a Color object.\")\n    if not issubclass(target_cs, ColorBase):\n        raise ValueError(\"target_cs parameter must be a Color object.\")\n\n    conversions = _conversion_manager.get_conversion_path(color.__class__, target_cs)\n\n    logger.debug('Converting %s to %s', color, target_cs)\n    logger.debug(' @ Conversion path: %s', conversions)\n\n    # Start with original color in case we convert to the same color space.\n    new_color = color\n\n    if issubclass(target_cs, BaseRGBColor):\n        # If the target_cs is an RGB color space of some sort, then we\n        # have to set our through_rgb_type to make sure the conversion returns\n        # the expected RGB colorspace (instead of defaulting to sRGBColor).\n        through_rgb_type = target_cs\n\n    # We have to be careful to use the same RGB color space that created\n    # an object (if it was created by a conversion) in order to get correct\n    # results. For example, XYZ->HSL via Adobe RGB should default to Adobe\n    # RGB when taking that generated HSL object back to XYZ.\n    # noinspection PyProtectedMember\n    if through_rgb_type != sRGBColor:\n        # User overrides take priority over everything.\n        # noinspection PyProtectedMember\n        target_rgb = through_rgb_type\n    elif color._through_rgb_type:\n        # Otherwise, a value on the color object is the next best thing,\n        # when available.\n        # noinspection PyProtectedMember\n        target_rgb = color._through_rgb_type\n    else:\n        # We could collapse this into a single if statement above,\n        # but I think this reads better.\n        target_rgb = through_rgb_type\n\n    # Iterate through the list of functions for the conversion path, storing\n    # the results in a dictionary via update(). This way the user has access\n    # to all of the variables involved in the conversion.\n    for func in conversions:\n        # Execute the function in this conversion step and store the resulting\n        # Color object.\n        logger.debug(' * Conversion: %s passed to %s()',\n                     new_color.__class__.__name__, func)\n        logger.debug(' |->  in %s', new_color)\n\n        if func:\n            # This can be None if you try to convert a color to the color\n            # space that is already in. IE: XYZ->XYZ.\n            new_color = func(\n                new_color,\n                target_rgb=target_rgb,\n                target_illuminant=target_illuminant,\n                *args, **kwargs)\n\n        logger.debug(' |-< out %s', new_color)\n\n    # If this conversion had something other than the default sRGB color space\n    # requested,\n    if through_rgb_type != sRGBColor:\n        new_color._through_rgb_type = through_rgb_type\n\n    return new_color",
    "label": 0
  },
  {
    "codes": "```elixir\ndefmodule DistributedState do\n  use GenServer\n\n  # Client API\n\n  def start_link(opts \\\\ []) do\n    GenServer.start_link(__MODULE__, :ok, opts)\n  end\n\n  def get_state(pid, key) do\n    GenServer.call(pid, {:get_state, key})\n  end\n\n  def set_state(pid, key, value) do\n    GenServer.cast(pid, {:set_state, key, value})\n  end\n\n  # Server Callbacks\n\n  @impl true\n  def init(:ok) do\n    {:ok, %{}}\n  end\n\n  @impl true\n  def handle_call({:get_state, key}, _from, state) do\n    {:reply, Map.get(state, key), state}\n  end\n\n  @impl true\n  def handle_cast({:set_state, key, value}, state) do\n    {:noreply, Map.put(state, key, value)}\n  end\nend\n```\n\n```erlang\n-module(distributed_state).\n-behaviour(gen_server).\n\n-export([start_link/0, get_state/2, set_state/3]).\n-export([init/1, handle_call/3, handle_cast/2]).\n\n% Client API\n\nstart_link() ->\n    gen_server:start_link(?MODULE, [], []).\n\nget_state(Pid, Key) ->\n    gen_server:call(Pid, {get_state, Key}).\n\nset_state(Pid, Key, Value) ->\n    gen_server:cast(Pid, {set_state, Key, Value}).\n\n% Server Callbacks\n\ninit([]) ->\n    {ok, #{}}.\n\nhandle_call({get_state, Key}, _From, State) ->\n    {reply, maps:get(Key, State, undefined), State}.\n\nhandle_cast({set_state, Key, Value}, State) ->\n    {noreply, maps:put(Key, Value, State)}.\n```",
    "label": 1
  },
  {
    "codes": "```ruby\nrequire 'fiber'\n\n# A basic Fiber-based Scheduler class to handle lightweight concurrency\nclass FiberScheduler\n  def initialize\n    @queue = []\n  end\n\n  # Method to add tasks (Fibers) to the queue\n  def schedule(&block)\n    @queue << Fiber.new(&block)\n  end\n\n  # Method to run the scheduler and execute scheduled tasks\n  def run\n    while fiber = @queue.shift\n      fiber.resume\n      @queue << fiber if fiber.alive?\n    end\n  end\nend\n\n# Create an instance of FiberScheduler\nscheduler = FiberScheduler.new\n\n# Schedule two tasks for demonstration\nscheduler.schedule do\n  3.times do |i|\n    puts \"Task 1, step #{i}\"\n    Fiber.yield # Suspend execution and give control back to the scheduler\n  end\nend\n\nscheduler.schedule do\n  3.times do |i|\n    puts \"Task 2, step #{i}\"\n    Fiber.yield # Suspend execution and give control back to the scheduler\n  end\nend\n\n# Run the scheduler\nscheduler.run\n```\n",
    "label": 1
  },
  {
    "codes": "function write_ws_bin_cell(ba, cell, R, C, opts, ws) {\n\tif(cell.v === undefined) return \"\";\n\tvar vv = \"\";\n\tswitch(cell.t) {\n\t\tcase 'b': vv = cell.v ? \"1\" : \"0\"; break;\n\t\tcase 'd': // no BrtCellDate :(\n\t\t\tcell = dup(cell);\n\t\t\tcell.z = cell.z || SSF._table[14];\n\t\t\tcell.v = datenum(parseDate(cell.v)); cell.t = 'n';\n\t\t\tbreak;\n\t\t/* falls through */\n\t\tcase 'n': case 'e': vv = ''+cell.v; break;\n\t\tdefault: vv = cell.v; break;\n\t}\n\tvar o = ({r:R, c:C});\n\t/* TODO: cell style */\n\to.s = get_cell_style(opts.cellXfs, cell, opts);\n\tif(cell.l) ws['!links'].push([encode_cell(o), cell.l]);\n\tif(cell.c) ws['!comments'].push([encode_cell(o), cell.c]);\n\tswitch(cell.t) {\n\t\tcase 's': case 'str':\n\t\t\tif(opts.bookSST) {\n\t\t\t\tvv = get_sst_id(opts.Strings, (cell.v), opts.revStrings);\n\t\t\t\to.t = \"s\"; o.v = vv;\n\t\t\t\twrite_record(ba, \"BrtCellIsst\", write_BrtCellIsst(cell, o));\n\t\t\t} else {\n\t\t\t\to.t = \"str\";\n\t\t\t\twrite_record(ba, \"BrtCellSt\", write_BrtCellSt(cell, o));\n\t\t\t}\n\t\t\treturn;\n\t\tcase 'n':\n\t\t\t/* TODO: determine threshold for Real vs RK */\n\t\t\tif(cell.v == (cell.v | 0) && cell.v > -1000 && cell.v < 1000) write_record(ba, \"BrtCellRk\", write_BrtCellRk(cell, o));\n\t\t\telse write_record(ba, \"BrtCellReal\", write_BrtCellReal(cell, o));\n\t\t\treturn;\n\t\tcase 'b':\n\t\t\to.t = \"b\";\n\t\t\twrite_record(ba, \"BrtCellBool\", write_BrtCellBool(cell, o));\n\t\t\treturn;\n\t\tcase 'e': /* TODO: error */ o.t = \"e\"; break;\n\t}\n\twrite_record(ba, \"BrtCellBlank\", write_BrtCellBlank(cell, o));\n}",
    "label": 0
  },
  {
    "codes": "function (chrs, buffer, pos, strict, opts) {\n                        var isValid = opts.regex.hrs.test(chrs);\n                        if (!strict && !isValid) {\n                            if (chrs.charAt(1) == opts.timeseparator || \"-.:\".indexOf(chrs.charAt(1)) != -1) {\n                                isValid = opts.regex.hrs.test(\"0\" + chrs.charAt(0));\n                                if (isValid) {\n                                    buffer[pos - 1] = \"0\";\n                                    buffer[pos] = chrs.charAt(0);\n                                    pos++;\n                                    return { \"pos\": pos };\n                                }\n                            }\n                        }\n\n                        if (isValid && opts.hourFormat !== \"24\" && opts.regex.hrs24.test(chrs)) {\n\n                            var tmp = parseInt(chrs, 10);\n\n                            if (tmp == 24) {\n                                buffer[pos + 5] = \"a\";\n                                buffer[pos + 6] = \"m\";\n                            } else {\n                                buffer[pos + 5] = \"p\";\n                                buffer[pos + 6] = \"m\";\n                            }\n\n                            tmp = tmp - 12;\n\n                            if (tmp < 10) {\n                                buffer[pos] = tmp.toString();\n                                buffer[pos - 1] = \"0\";\n                            } else {\n                                buffer[pos] = tmp.toString().charAt(1);\n                                buffer[pos - 1] = tmp.toString().charAt(0);\n                            }\n\n                            return { \"pos\": pos, \"c\": buffer[pos] };\n                        }\n\n                        return isValid;\n                    }",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define source and destination directories\nSOURCE_DIR=\"/path/to/source\"\nDEST_DIR=\"/path/to/destination\"\n\n# Check if directories exist\nif [ ! -d \"$SOURCE_DIR\" ]; then\n    echo \"Source directory does not exist.\"\n    exit 1\nfi\n\nif [ ! -d \"$DEST_DIR\" ]; then\n    echo \"Destination directory does not exist.\"\n    exit 1\nfi\n\n# Compare directories using rsync in dry-run mode\necho \"Comparing directories...\"\nrsync -avun --delete \"$SOURCE_DIR/\" \"$DEST_DIR/\"\n\n# Prompt user to confirm sync\nread -p \"Do you want to sync changes? (y/n): \" CONFIRM\n\n# Sync changes if confirmed\nif [ \"$CONFIRM\" == \"y\" ]; then\n    echo \"Syncing directories...\"\n    rsync -avu --delete \"$SOURCE_DIR/\" \"$DEST_DIR/\"\n    echo \"Sync completed.\"\nelse\n    echo \"Sync aborted.\"\nfi\n```",
    "label": 1
  },
  {
    "codes": "def set_private_xml(self, xso):\n        \"\"\"\n        Store the serialization of `xso` on the server as the private XML\n        data for the namespace of `xso`.\n\n        :param xso: the XSO whose serialization is send as private XML data.\n        \"\"\"\n        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            payload=private_xml_xso.Query(xso)\n        )\n        yield from self.client.send(iq)",
    "label": 0
  },
  {
    "codes": "```java\n// IsometricTilemapRenderer.java\npackage com.mygdx.game;\n\nimport com.badlogic.gdx.graphics.OrthographicCamera;\nimport com.badlogic.gdx.graphics.g2d.SpriteBatch;\nimport com.badlogic.gdx.maps.tiled.TiledMap;\nimport com.badlogic.gdx.maps.tiled.TiledMapTileLayer;\nimport com.badlogic.gdx.maps.tiled.renderers.IsometricTiledMapRenderer;\n\npublic class IsometricTilemapRenderer extends IsometricTiledMapRenderer {\n\n    public IsometricTilemapRenderer(TiledMap map, float unitScale, SpriteBatch batch) {\n        super(map, unitScale, batch);\n    }\n\n    @Override\n    public void renderTileLayer(TiledMapTileLayer layer) {\n        // Custom rendering logic for isometric tile layers\n        super.renderTileLayer(layer);\n    }\n\n    @Override\n    public void setView(OrthographicCamera camera) {\n        // Adjust the camera view for isometric projection\n        super.setView(camera);\n    }\n\n    @Override\n    public void render() {\n        // Render the entire map with isometric projection\n        super.render();\n    }\n}\n```\n\n```java\n// GameScreen.java\npackage com.mygdx.game;\n\nimport com.badlogic.gdx.Gdx;\nimport com.badlogic.gdx.Screen;\nimport com.badlogic.gdx.graphics.GL20;\nimport com.badlogic.gdx.graphics.OrthographicCamera;\nimport com.badlogic.gdx.maps.tiled.TiledMap;\nimport com.badlogic.gdx.maps.tiled.TmxMapLoader;\nimport com.badlogic.gdx.maps.tiled.renderers.IsometricTiledMapRenderer;\nimport com.badlogic.gdx.utils.viewport.FitViewport;\nimport com.badlogic.gdx.utils.viewport.Viewport;\n\npublic class GameScreen implements Screen {\n\n    private TiledMap map;\n    private IsometricTilemapRenderer renderer;\n    private OrthographicCamera camera;\n    private Viewport viewport;\n\n    @Override\n    public void show() {\n        // Load the isometric map\n        map = new TmxMapLoader().load(\"isometric_map.tmx\");\n\n        // Initialize the camera and viewport\n        camera = new OrthographicCamera();\n        viewport = new FitViewport(800, 480, camera);\n        viewport.apply();\n\n        // Initialize the custom isometric renderer\n        renderer = new IsometricTilemapRenderer(map, 1f, new SpriteBatch());\n\n        // Position the camera\n        camera.position.set(viewport.getWorldWidth() / 2, viewport.getWorldHeight() / 2, 0);\n        camera.update();\n    }\n\n    @Override\n    public void render(float delta) {\n        // Clear the screen\n        Gdx.gl.glClearColor(0, 0, 0, 1);\n        Gdx.gl.glClear(GL20.GL_COLOR_BUFFER_BIT);\n\n        // Set the renderer's view and render the map\n        renderer.setView(camera);\n        renderer.render();\n    }\n\n    @Override\n    public void resize(int width, int height) {\n        viewport.update(width, height);\n        camera.update();\n    }\n\n    @Override\n    public void pause() {}\n\n    @Override\n    public void resume() {}\n\n    @Override\n    public void hide() {}\n\n    @Override\n    public void dispose() {\n        map.dispose();\n        renderer.dispose();\n    }\n}\n```\n\n```java\n// MyGdxGame.java\npackage com.mygdx.game;\n\nimport com.badlogic.gdx.Game;\n\npublic class MyGdxGame extends Game {\n\n    @Override\n    public void create() {\n        setScreen(new GameScreen());\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: custom-metrics-config\n  namespace: custom-metrics\ndata:\n  config.yaml: |\n    rules:\n    - seriesQuery: 'queue_depth'\n      resources:\n        overrides:\n          instance:\n            resource: pod\n      name:\n        matches: \"^(.*)_total$\"\n        as: \"${1}_per_pod\"\n      metricsQuery: 'sum(rate(queue_depth{<<.LabelMatchers>>}[5m])) by (<<.GroupBy>>)'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pytorch-inference\n  labels:\n    app: pytorch-inference\nspec:\n  selector:\n    matchLabels:\n      app: pytorch-inference\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: pytorch-inference\n    spec:\n      containers:\n      - name: pytorch-inference\n        image: pytorch/image:latest\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: pytorch-inference-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: pytorch-inference\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Pods\n    pods:\n      metric:\n        name: queue_depth_per_pod\n      target:\n        type: AverageValue\n        averageValue: \"10\"\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: pytorch-inference-monitor\n  labels:\n    release: prometheus-operator\nspec:\n  selector:\n    matchLabels:\n      app: pytorch-inference\n  endpoints:\n  - port: metrics\n```\n\n",
    "label": 1
  },
  {
    "codes": "```go\n// main.go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"os\"\n    \"time\"\n\n    \"k8s.io/apimachinery/pkg/runtime\"\n    clientgoscheme \"k8s.io/client-go/kubernetes/scheme\"\n    _ \"k8s.io/client-go/plugin/pkg/client/auth/gcp\"\n    ctrl \"sigs.k8s.io/controller-runtime\"\n    \"sigs.k8s.io/controller-runtime/pkg/log/zap\"\n\n    mygroupv1alpha1 \"github.com/example/my-operator/api/v1alpha1\"\n    \"github.com/example/my-operator/controllers\"\n)\n\nvar (\n    scheme   = runtime.NewScheme()\n    setupLog = ctrl.Log.WithName(\"setup\")\n)\n\nfunc init() {\n    _ = clientgoscheme.AddToScheme(scheme)\n    _ = mygroupv1alpha1.AddToScheme(scheme)\n}\n\nfunc main() {\n    ctrl.SetLogger(zap.New(zap.UseDevMode(true)))\n\n    mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{\n        Scheme:             scheme,\n        MetricsBindAddress: \"0\",\n        Port:               9443,\n    })\n    if err != nil {\n        setupLog.Error(err, \"unable to start manager\")\n        os.Exit(1)\n    }\n\n    if err = (&controllers.MyResourceReconciler{\n        Client: mgr.GetClient(),\n        Log:    ctrl.Log.WithName(\"controllers\").WithName(\"MyResource\"),\n        Scheme: mgr.GetScheme(),\n    }).SetupWithManager(mgr); err != nil {\n        setupLog.Error(err, \"unable to create controller\", \"controller\", \"MyResource\")\n        os.Exit(1)\n    }\n\n    setupLog.Info(\"starting manager\")\n    if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {\n        setupLog.Error(err, \"problem running manager\")\n        os.Exit(1)\n    }\n}\n\n// api/v1alpha1/myresource_types.go\npackage v1alpha1\n\nimport (\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n)\n\n// MyResourceSpec defines the desired state of MyResource\ntype MyResourceSpec struct {\n    // Add fields here\n    Replicas int32 `json:\"replicas\"`\n}\n\n// MyResourceStatus defines the observed state of MyResource\ntype MyResourceStatus struct {\n    // Add fields here\n    AvailableReplicas int32 `json:\"availableReplicas\"`\n}\n\n// +kubebuilder:object:root=true\n// +kubebuilder:subresource:status\n\n// MyResource is the Schema for the myresources API\ntype MyResource struct {\n    metav1.TypeMeta   `json:\",inline\"`\n    metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n    Spec   MyResourceSpec   `json:\"spec,omitempty\"`\n    Status MyResourceStatus `json:\"status,omitempty\"`\n}\n\n// +kubebuilder:object:root=true\n\n// MyResourceList contains a list of MyResource\ntype MyResourceList struct {\n    metav1.TypeMeta `json:\",inline\"`\n    metav1.ListMeta `json:\"metadata,omitempty\"`\n    Items           []MyResource `json:\"items\"`\n}\n\nfunc init() {\n    SchemeBuilder.Register(&MyResource{}, &MyResourceList{})\n}\n\n// controllers/myresource_controller.go\npackage controllers\n\nimport (\n    \"context\"\n    \"fmt\"\n\n    \"github.com/go-logr/logr\"\n    \"k8s.io/apimachinery/pkg/runtime\"\n    ctrl \"sigs.k8s.io/controller-runtime\"\n    \"sigs.k8s.io/controller-runtime/pkg/client\"\n\n    mygroupv1alpha1 \"github.com/example/my-operator/api/v1alpha1\"\n)\n\n// MyResourceReconciler reconciles a MyResource object\ntype MyResourceReconciler struct {\n    client.Client\n    Log    logr.Logger\n    Scheme *runtime.Scheme\n}\n\n// +kubebuilder:rbac:groups=mygroup.example.com,resources=myresources,verbs=get;list;watch;create;update;patch;delete\n// +kubebuilder:rbac:groups=mygroup.example.com,resources=myresources/status,verbs=get;update;patch\n\nfunc (r *MyResourceReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n    log := r.Log.WithValues(\"myresource\", req.NamespacedName)\n\n    // Fetch the MyResource instance\n    myResource := &mygroupv1alpha1.MyResource{}\n    if err := r.Get(ctx, req.NamespacedName, myResource); err != nil {\n        log.Error(err, \"unable to fetch MyResource\")\n        return ctrl.Result{}, client.IgnoreNotFound(err)\n    }\n\n    // Implement your reconciliation logic here\n    log.Info(\"Reconciling MyResource\", \"name\", myResource.Name)\n\n    // Example: Update status\n    myResource.Status.AvailableReplicas = myResource.Spec.Replicas\n    if err := r.Status().Update(ctx, myResource); err != nil {\n        log.Error(err, \"unable to update MyResource status\")\n        return ctrl.Result{}, err\n    }\n\n    return ctrl.Result{}, nil\n}\n\nfunc (r *MyResourceReconciler) SetupWithManager(mgr ctrl.Manager) error {\n    return ctrl.NewControllerManagedBy(mgr).\n        For(&mygroupv1alpha1.MyResource{}).\n        Complete(r)\n}\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse bevy::prelude::*;\nuse bevy::render::camera::Camera;\nuse bevy::math::Vec2;\n\nstruct Selected;\n\nstruct Selectable;\n\nstruct MousePosition(Vec2); // Stores current mouse position\n\nfn setup(mut commands: Commands) {\n    // Set up your entities and systems here\n    commands.spawn_bundle(OrthographicCameraBundle::new_2d());\n    // Example entity\n    commands.spawn().insert(Selectable).insert_bundle(SpriteBundle::default());\n}\n\nfn update_mouse_position(\n    windows: Res<Windows>,\n    mut mouse_position: ResMut<MousePosition>,\n) {\n    let window = windows.get_primary().unwrap();\n    if let Some(position) = window.cursor_position() {\n        mouse_position.0 = position;\n    }\n}\n\nfn unit_selection_system(\n    mouse_input: Res<Input<MouseButton>>,\n    mouse_position: Res<MousePosition>,\n    camera_query: Query<&Transform, With<Camera>>,\n    mut query: Query<(&mut Transform, &mut Option<Selected>), With<Selectable>>,\n) {\n    if mouse_input.just_released(MouseButton::Left) {\n        let camera_transform = camera_query.single();\n        let window_pos = mouse_position.0;\n\n        let world_pos = camera_transform\n            .compute_matrix()\n            .inverse()\n            * Vec3::new(window_pos.x, window_pos.y, 0.0);\n\n        for (mut transform, mut selected) in query.iter_mut() {\n            let distance = transform.translation - world_pos;\n            if distance.truncate().length() < 15.0 { // 15.0 is a placeholder for selection radius\n                if selected.is_none() {\n                    transform.translation.z = 1.0; // Bring to front for visibility\n                    *selected = Some(Selected);\n                }\n            } else {\n                transform.translation.z = 0.0; // Reset position\n                *selected = None;\n            }\n        }\n    }\n}\n\nfn main() {\n    App::build()\n        .insert_resource(WindowDescriptor {\n            title: \"RTS Unit Selection\".into(),\n            width: 800.0,\n            height: 600.0,\n            ..Default::default()\n        })\n        .insert_resource(MousePosition(Vec2::ZERO))\n        .add_startup_system(setup.system())\n        .add_system(update_mouse_position.system())\n        .add_system(unit_selection_system.system())\n        .add_plugins(DefaultPlugins)\n        .run();\n}\n``` \n\n",
    "label": 1
  },
  {
    "codes": "public SecretBundle restoreSecret(String vaultBaseUrl, byte[] secretBundleBackup) {\n        return restoreSecretWithServiceResponseAsync(vaultBaseUrl, secretBundleBackup).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "func (c *CloudTrail) LookupEventsPages(input *LookupEventsInput, fn func(*LookupEventsOutput, bool) bool) error {\n\treturn c.LookupEventsPagesWithContext(aws.BackgroundContext(), input, fn)\n}",
    "label": 0
  },
  {
    "codes": "function getOverflowStyles(e) {\n    // When the <html> element has an overflow style of 'visible', it assumes\n    // the overflow style of the body, and the body is really overflow:visible.\n    var overflowElem = e;\n    if (htmlOverflowStyle == 'visible') {\n      // Note: bodyElem will be null/undefined in SVG documents.\n      if (e == htmlElem && bodyElem) {\n        overflowElem = bodyElem;\n      } else if (e == bodyElem) {\n        return {x: 'visible', y: 'visible'};\n      }\n    }\n    var overflow = {\n      x: bot.dom.getEffectiveStyle(overflowElem, 'overflow-x'),\n      y: bot.dom.getEffectiveStyle(overflowElem, 'overflow-y')\n    };\n    // The <html> element cannot have a genuine 'visible' overflow style,\n    // because the viewport can't expand; 'visible' is really 'auto'.\n    if (e == htmlElem) {\n      overflow.x = overflow.x == 'visible' ? 'auto' : overflow.x;\n      overflow.y = overflow.y == 'visible' ? 'auto' : overflow.y;\n    }\n    return overflow;\n  }",
    "label": 0
  },
  {
    "codes": "func (s *CodeGenNode) SetLineNumber(v int64) *CodeGenNode {\n\ts.LineNumber = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<KeyOperationResult> encryptAsync(String vaultBaseUrl, String keyName, String keyVersion, JsonWebKeyEncryptionAlgorithm algorithm, byte[] value, final ServiceCallback<KeyOperationResult> serviceCallback) {\n        return ServiceFuture.fromResponse(encryptWithServiceResponseAsync(vaultBaseUrl, keyName, keyVersion, algorithm, value), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom transformers import pipeline\n\n# Create a text summarization pipeline using a different model\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n# Example text to summarize\ntext = \"\"\"\nThe Hugging Face Transformers library provides an easy-to-use interface for various NLP tasks, \nincluding text summarization. It supports multiple pre-trained models that can be used out-of-the-box \nfor generating summaries of long documents. The library is widely used in the NLP community due to \nits flexibility and ease of integration with other machine learning frameworks.\n\"\"\"\n\n# Generate the summary\nsummary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n\n# Print the summary\nprint(summary)\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<SecretBundle> restoreSecretAsync(String vaultBaseUrl, byte[] secretBundleBackup, final ServiceCallback<SecretBundle> serviceCallback) {\n        return ServiceFuture.fromResponse(restoreSecretWithServiceResponseAsync(vaultBaseUrl, secretBundleBackup), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "function searchBar_search(data)\n  {\n    var searchBar = this.getElement({type: \"searchBar\"});\n    this.type(data.text);\n\n    switch (data.action) {\n      case \"returnKey\":\n        this._controller.keypress(searchBar, 'VK_RETURN', {});\n        break;\n      case \"goButton\":\n      default:\n        this._controller.click(this.getElement({type: \"searchBar_goButton\"}));\n        break;\n    }\n\n    this._controller.waitForPageLoad();\n    this.checkSearchResultPage(data.text);\n  }",
    "label": 0
  },
  {
    "codes": "function( elem ) {\n\t\tif ( !elem || typeof elem === \"string\" ) {\n\t\t\treturn jQuery.inArray( this[0],\n\t\t\t\t// If it receives a string, the selector is used\n\t\t\t\t// If it receives nothing, the siblings are used\n\t\t\t\telem ? jQuery( elem ) : this.parent().children() );\n\t\t}\n\t\t// Locate the position of the desired element\n\t\treturn jQuery.inArray(\n\t\t\t// If it receives a jQuery object, the first element is used\n\t\t\telem.jquery ? elem[0] : elem, this );\n\t}",
    "label": 0
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model)\n\n# Save the CoreML model to a file\ncoreml_model.save('your_coreml_model.mlmodel')\n\n# Optional: Add metadata to the CoreML model\ncoreml_model.author = 'Your Name'\ncoreml_model.license = 'MIT'\ncoreml_model.short_description = 'A CoreML model converted from Keras.'\n\n# Save the updated CoreML model\ncoreml_model.save('your_coreml_model_with_metadata.mlmodel')\n```\n\n```python\n# Alternative method using TensorFlow 2.x and tf.keras\nimport tensorflow as tf\nimport coremltools as ct\n\n# Load the Keras model\nkeras_model = tf.keras.models.load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.converters.tensorflow.convert(keras_model)\n\n# Save the CoreML model to a file\ncoreml_model.save('your_coreml_model.mlmodel')\n\n# Optional: Add metadata to the CoreML model\ncoreml_model.author = 'Your Name'\ncoreml_model.license = 'MIT'\ncoreml_model.short_description = 'A CoreML model converted from Keras.'\n\n# Save the updated CoreML model\ncoreml_model.save('your_coreml_model_with_metadata.mlmodel')\n```\n\n```python\n# Another approach using a custom input shape and output labels\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Define input and output descriptions\ninput_shape = (1, 224, 224, 3)  # Example input shape\noutput_labels = ['class1', 'class2', 'class3']  # Example output labels\n\n# Convert the Keras model to CoreML format with custom input/output\ncoreml_model = ct.convert(\n    keras_model,\n    inputs=[ct.ImageType(shape=input_shape)],\n    classifier_config=ct.ClassifierConfig(output_labels)\n)\n\n# Save the CoreML model to a file\ncoreml_model.save('your_coreml_model_with_custom_io.mlmodel')\n```",
    "label": 1
  },
  {
    "codes": "```rust\n// Import necessary crates\nuse embedded_hal::blocking::i2c::{Write, WriteRead};\nuse stm32f4xx_hal::i2c::I2c;\nuse stm32f4xx_hal::pac::I2C1;\n\n// Define IMU driver structure\npub struct Imu<I2C> {\n    i2c: I2C,\n    address: u8,\n}\n\nimpl<I2C> Imu<I2C>\nwhere\n    I2C: WriteRead + Write,\n{\n    // Constructor for the IMU driver\n    pub fn new(i2c: I2C, address: u8) -> Self {\n        Imu { i2c, address }\n    }\n\n    // Read a single byte from a register\n    pub fn read_register(&mut self, reg: u8) -> Result<u8, I2C::Error> {\n        let mut data = [0u8; 1];\n        self.i2c.write_read(self.address, &[reg], &mut data)?;\n        Ok(data[0])\n    }\n\n    // Write a single byte to a register\n    pub fn write_register(&mut self, reg: u8, value: u8) -> Result<(), I2C::Error> {\n        self.i2c.write(self.address, &[reg, value])\n    }\n\n    // Read multiple bytes from a register\n    pub fn read_multiple_registers(&mut self, reg: u8, buffer: &mut [u8]) -> Result<(), I2C::Error> {\n        self.i2c.write_read(self.address, &[reg], buffer)\n    }\n\n    // Example function to read accelerometer data\n    pub fn read_accelerometer(&mut self) -> Result<(i16, i16, i16), I2C::Error> {\n        let mut data = [0u8; 6];\n        self.read_multiple_registers(0x3B, &mut data)?;\n\n        let x = ((data[0] as i16) << 8) | data[1] as i16;\n        let y = ((data[2] as i16) << 8) | data[3] as i16;\n        let z = ((data[4] as i16) << 8) | data[5] as i16;\n\n        Ok((x, y, z))\n    }\n\n    // Example function to read gyroscope data\n    pub fn read_gyroscope(&mut self) -> Result<(i16, i16, i16), I2C::Error> {\n        let mut data = [0u8; 6];\n        self.read_multiple_registers(0x43, &mut data)?;\n\n        let x = ((data[0] as i16) << 8) | data[1] as i16;\n        let y = ((data[2] as i16) << 8) | data[3] as i16;\n        let z = ((data[4] as i16) << 8) | data[5] as i16;\n\n        Ok((x, y, z))\n    }\n}\n\n// Example usage with STM32 I2C\nfn main() {\n    let dp = stm32f4xx_hal::pac::Peripherals::take().unwrap();\n    let rcc = dp.RCC.constrain();\n    let clocks = rcc.cfgr.freeze();\n    let gpiob = dp.GPIOB.split();\n    let scl = gpiob.pb6.into_alternate_af4().set_open_drain();\n    let sda = gpiob.pb7.into_alternate_af4().set_open_drain();\n    let i2c = I2c::i2c1(dp.I2C1, (scl, sda), 100.khz(), clocks);\n\n    let mut imu = Imu::new(i2c, 0x68); // 0x68 is the default I2C address for MPU6050\n\n    // Read accelerometer data\n    let accel_data = imu.read_accelerometer().unwrap();\n    println!(\"Accelerometer: X: {}, Y: {}, Z: {}\", accel_data.0, accel_data.1, accel_data.2);\n\n    // Read gyroscope data\n    let gyro_data = imu.read_gyroscope().unwrap();\n    println!(\"Gyroscope: X: {}, Y: {}, Z: {}\", gyro_data.0, gyro_data.1, gyro_data.2);\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *InitiateJobInput) SetAccountId(v string) *InitiateJobInput {\n\ts.AccountId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```powershell\n# Define registry paths to audit\n$registryPaths = @(\n    \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run\",\n    \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\RunOnce\",\n    \"HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run\",\n    \"HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\RunOnce\"\n)\n\n# Function to check registry permissions\nfunction Check-RegistryPermissions {\n    param (\n        [string]$path\n    )\n    try {\n        # Get ACL for the registry key\n        $acl = Get-Acl -Path $path\n        $accessRules = $acl.Access\n\n        # Check for weak permissions\n        foreach ($rule in $accessRules) {\n            if ($rule.IdentityReference -eq \"BUILTIN\\Users\" -or $rule.IdentityReference -eq \"Everyone\") {\n                if ($rule.RegistryRights -match \"Write|FullControl\") {\n                    Write-Host \"Weak permission found: $path - $($rule.IdentityReference) has $($rule.RegistryRights) access.\"\n                }\n            }\n        }\n    } catch {\n        Write-Host \"Error accessing registry path: $path\"\n    }\n}\n\n# Iterate through registry paths and check permissions\nforeach ($path in $registryPaths) {\n    Check-RegistryPermissions -path $path\n}\n```",
    "label": 1
  },
  {
    "codes": "def load(cls, path, encoding=\"utf-8\", format_=None, fps=None, **kwargs):\n        \"\"\"\n        Load subtitle file from given path.\n\n        Arguments:\n            path (str): Path to subtitle file.\n            encoding (str): Character encoding of input file.\n                Defaults to UTF-8, you may need to change this.\n            format_ (str): Optional, forces use of specific parser\n                (eg. `\"srt\"`, `\"ass\"`). Otherwise, format is detected\n                automatically from file contents. This argument should\n                be rarely needed.\n            fps (float): Framerate for frame-based formats (MicroDVD),\n                for other formats this argument is ignored. Framerate might\n                be detected from the file, in which case you don't need\n                to specify it here (when given, this argument overrides\n                autodetection).\n            kwargs: Extra options for the parser.\n\n        Returns:\n            SSAFile\n\n        Raises:\n            IOError\n            UnicodeDecodeError\n            pysubs2.exceptions.UnknownFPSError\n            pysubs2.exceptions.UnknownFormatIdentifierError\n            pysubs2.exceptions.FormatAutodetectionError\n\n        Note:\n            pysubs2 may autodetect subtitle format and/or framerate. These\n            values are set as :attr:`SSAFile.format` and :attr:`SSAFile.fps`\n            attributes.\n\n        Example:\n            >>> subs1 = pysubs2.load(\"subrip-subtitles.srt\")\n            >>> subs2 = pysubs2.load(\"microdvd-subtitles.sub\", fps=23.976)\n\n        \"\"\"\n        with open(path, encoding=encoding) as fp:\n            return cls.from_file(fp, format_, fps=fps, **kwargs)",
    "label": 0
  },
  {
    "codes": "function internalSearchObject (obj, searchTerm, seen, depth) {\n  if (depth > SEARCH_MAX_DEPTH) {\n    return false\n  }\n  let match = false\n  const keys = Object.keys(obj)\n  let key, value\n  for (let i = 0; i < keys.length; i++) {\n    key = keys[i]\n    value = obj[key]\n    match = internalSearchCheck(searchTerm, key, value, seen, depth + 1)\n    if (match) {\n      break\n    }\n  }\n  return match\n}",
    "label": 0
  },
  {
    "codes": "function positionToOffset(str, desiredLine, desiredCol) {\n    if (desiredLine === 1) {\n        return desiredCol - 1;\n    }\n    let line = 1;\n    let lastNewLineOffset = -1;\n    do {\n        if (desiredLine === line) {\n            return lastNewLineOffset + 1 + desiredCol - 1;\n        }\n        lastNewLineOffset = str.indexOf('\\n', lastNewLineOffset + 1);\n        line++;\n    } while (lastNewLineOffset >= 0);\n    return -1;\n}",
    "label": 0
  },
  {
    "codes": "```python\n# Import necessary modules\nfrom fastapi import FastAPI, Request, HTTPException, status, Depends\nfrom fastapi.security.api_key import APIKeyHeader\nfrom starlette.responses import JSONResponse\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\nfrom slowapi.middleware import SlowAPIMiddleware\n\n# Initialize the FastAPI application\napp = FastAPI()\n\n# Configure rate limiter\nlimiter = Limiter(key_func=get_remote_address, default_limits=[\"5/minute\"])\napp.state.limiter = limiter\napp.add_exception_handler(HTTPException, _rate_limit_exceeded_handler)\n\n# Add middleware for rate limiting\napp.add_middleware(SlowAPIMiddleware)\n\n# Setup API key authentication\nAPI_KEY = \"your-secure-api-key\"\napi_key_header = APIKeyHeader(name=\"X-API-KEY\", auto_error=False)\n\n# Dependency to check API key\nasync def get_api_key(api_key: str = Depends(api_key_header)):\n    if api_key != API_KEY:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid API Key\",\n        )\n    return api_key\n\n# Define an endpoint with rate limiting and API key authentication\n@app.get(\"/secure-data\")\n@limiter.limit(\"5/minute\")  # Rate limit this endpoint\nasync def get_secure_data(api_key: str = Depends(get_api_key)):\n    # Successful response\n    return JSONResponse(content={\"data\": \"Sensitive Data\"}, status_code=status.HTTP_200_OK)\n\n# Run FastAPI app using any server like Uvicorn\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n",
    "label": 1
  },
  {
    "codes": "function filterCroppedNodes(node) {\n  if (!node.boxObject) {\n    return domUtils.DOMWalker.FILTER_SKIP;\n  } else {\n    if (!node.disabled && !node.collapsed && !node.hidden) {\n      // Code specific to the preferences panes to reject out not visible nodes\n      // in the panes.\n      if (node.parentNode && (node.parentNode.localName == \"prefwindow\" &&\n                              node.parentNode.currentPane.id != node.id) ||\n          ((node.parentNode.localName == \"tabpanels\" ||\n            node.parentNode.localName == \"deck\") &&\n           node.parentNode.selectedPanel.id != node.id)) {\n        return domUtils.DOMWalker.FILTER_REJECT;\n        // end of the specific code\n      } else {\n        return domUtils.DOMWalker.FILTER_ACCEPT;\n      }\n    } else {\n      // we don't want to test not visible elements\n      return domUtils.DOMWalker.FILTER_REJECT;\n    }\n  }\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport hmac\nfrom secrets import compare_digest\n\ndef secure_compare_hmac(digest1: bytes, digest2: bytes, key: bytes, msg: bytes) -> bool:\n    # Generate HMAC digests using the provided key and message\n    hmac1 = hmac.new(key, msg, digestmod='sha256').digest()\n    hmac2 = hmac.new(key, msg, digestmod='sha256').digest()\n    \n    # Compare the generated HMAC digests with the provided ones\n    return compare_digest(digest1, hmac1) and compare_digest(digest2, hmac2)\n```",
    "label": 1
  },
  {
    "codes": "def XYZ_to_Luv(cobj, *args, **kwargs):\n    \"\"\"\n    Convert from XYZ to Luv\n    \"\"\"\n    temp_x = cobj.xyz_x\n    temp_y = cobj.xyz_y\n    temp_z = cobj.xyz_z\n    denom = temp_x + (15.0 * temp_y) + (3.0 * temp_z)\n    # avoid division by zero\n    if denom == 0.0:\n        luv_u = 0.0\n        luv_v = 0.0\n    else:\n        luv_u = (4.0 * temp_x) / denom\n        luv_v = (9.0 * temp_y) / denom\n\n    illum = cobj.get_illuminant_xyz()\n    temp_y = temp_y / illum[\"Y\"]\n    if temp_y > color_constants.CIE_E:\n        temp_y = math.pow(temp_y, (1.0 / 3.0))\n    else:\n        temp_y = (7.787 * temp_y) + (16.0 / 116.0)\n\n    ref_U = (4.0 * illum[\"X\"]) / (illum[\"X\"] + (15.0 * illum[\"Y\"]) + (3.0 * illum[\"Z\"]))\n    ref_V = (9.0 * illum[\"Y\"]) / (illum[\"X\"] + (15.0 * illum[\"Y\"]) + (3.0 * illum[\"Z\"]))\n\n    luv_l = (116.0 * temp_y) - 16.0\n    luv_u = 13.0 * luv_l * (luv_u - ref_U)\n    luv_v = 13.0 * luv_l * (luv_v - ref_V)\n\n    return LuvColor(\n        luv_l, luv_u, luv_v, observer=cobj.observer, illuminant=cobj.illuminant)",
    "label": 0
  },
  {
    "codes": "public CompletableFuture<Void> deleteRuleAsync(String topicPath, String subscriptionName, String ruleName) {\n        EntityNameHelper.checkValidTopicName(topicPath);\n        EntityNameHelper.checkValidSubscriptionName(subscriptionName);\n        EntityNameHelper.checkValidRuleName(ruleName);\n        String path = EntityNameHelper.formatRulePath(topicPath, subscriptionName, ruleName);\n        return deleteEntityAsync(path);\n    }",
    "label": 0
  },
  {
    "codes": "func (s *Event) SetResources(v []*Resource) *Event {\n\ts.Resources = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```rust\n// Define a custom macro for generating error types\nmacro_rules! generate_error {\n    ( $name:ident, $( $variant:ident ),* ) => {\n        #[derive(Debug, thiserror::Error)]\n        pub enum $name {\n            $( \n                #[error(\"{:?}\", stringify!($variant))]\n                $variant,\n            )*\n        }\n        \n        impl $name {\n            // Example: method to create an instance of each variant\n            $( \n                pub fn $variant() -> Self {\n                    $name::$variant\n                }\n            )*\n        }\n    };\n}\n\n// Usage example of the macro to generate an error type\ngenerate_error! {\n    MyError,\n    NotFound,\n    InvalidInput,\n    Unauthorized\n}\n```",
    "label": 1
  },
  {
    "codes": "```c\n#include <openssl/evp.h>\n#include <openssl/err.h>\n#include <string.h>\n#include <stdio.h>\n\n// Function to handle errors\nvoid handleErrors(void)\n{\n    ERR_print_errors_fp(stderr);\n    abort();\n}\n\n// AES-GCM encrypt function\nint aes_gcm_encrypt(const unsigned char *plaintext, int plaintext_len,\n                    const unsigned char *aad, int aad_len,\n                    const unsigned char *key,\n                    const unsigned char *iv, int iv_len,\n                    unsigned char *ciphertext,\n                    unsigned char *tag)\n{\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int ciphertext_len;\n\n    // Create and initialize the context\n    if(!(ctx = EVP_CIPHER_CTX_new())) handleErrors();\n\n    // Initialize the encryption operation.\n    if(1 != EVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL))\n        handleErrors();\n\n    // Set IV length if default 12 bytes is not appropriate\n    if(1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_IVLEN, iv_len, NULL))\n        handleErrors();\n\n    // Initialize key and IV\n    if(1 != EVP_EncryptInit_ex(ctx, NULL, NULL, key, iv)) handleErrors();\n\n    // Provide any AAD data if present.\n    if(aad && aad_len > 0) {\n        if(1 != EVP_EncryptUpdate(ctx, NULL, &len, aad, aad_len))\n            handleErrors();\n    }\n\n    // Provide the message to be encrypted, and obtain the encrypted output\n    if(1 != EVP_EncryptUpdate(ctx, ciphertext, &len, plaintext, plaintext_len))\n        handleErrors();\n    ciphertext_len = len;\n\n    // Finalize the encryption. Normally ciphertext bytes may be written at\n    // this stage, but this does not occur in GCM mode.\n    if(1 != EVP_EncryptFinal_ex(ctx, ciphertext + len, &len)) handleErrors();\n    ciphertext_len += len;\n\n    // Get the tag\n    if(1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_GET_TAG, 16, tag))\n        handleErrors();\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    return ciphertext_len;\n}\n\n// AES-GCM decrypt function\nint aes_gcm_decrypt(const unsigned char *ciphertext, int ciphertext_len,\n                    const unsigned char *aad, int aad_len,\n                    const unsigned char *tag,\n                    const unsigned char *key,\n                    const unsigned char *iv, int iv_len,\n                    unsigned char *plaintext)\n{\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int plaintext_len;\n    int ret;\n\n    // Create and initialize the context\n    if(!(ctx = EVP_CIPHER_CTX_new())) handleErrors();\n\n    // Initialize the decryption operation.\n    if(!EVP_DecryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL))\n        handleErrors();\n\n    // Set IV length if default 12 bytes is not appropriate\n    if(!EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_IVLEN, iv_len, NULL))\n        handleErrors();\n\n    // Initialize key and IV\n    if(!EVP_DecryptInit_ex(ctx, NULL, NULL, key, iv)) handleErrors();\n\n    // Provide any AAD data if present.\n    if(aad && aad_len > 0) {\n        if(!EVP_DecryptUpdate(ctx, NULL, &len, aad, aad_len))\n            handleErrors();\n    }\n\n    // Provide the message to be decrypted, and obtain the plaintext output\n    if(!EVP_DecryptUpdate(ctx, plaintext, &len, ciphertext, ciphertext_len))\n        handleErrors();\n    plaintext_len = len;\n\n    // Set expected tag value. Works in OpenSSL 1.0.1d and later\n    if(!EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_TAG, 16, (void *)tag))\n        handleErrors();\n\n    // Finalize the decryption.\n    ret = EVP_DecryptFinal_ex(ctx, plaintext + len, &len);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    if(ret > 0) {\n        // Success\n        plaintext_len += len;\n        return plaintext_len;\n    } else {\n        // Verify failed\n        return -1;\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "def _open_for_csv(name, mode):\n    \"\"\" Deal with Python 2/3 open API differences \"\"\"\n    if sys.version_info[0] < 3:\n        return open_rw(name, mode + 'b')\n    return open_rw(name, mode, newline='', encoding='utf-8')",
    "label": 0
  },
  {
    "codes": "def approve(self, peer_jid):\n        \"\"\"\n        (Pre-)approve a subscription request from `peer_jid`.\n\n        :param peer_jid: The peer to (pre-)approve.\n\n        This sends a ``\"subscribed\"`` presence to the peer; if the peer has\n        previously asked for a subscription, this will seal the deal and create\n        the subscription.\n\n        If the peer has not requested a subscription (yet), it is marked as\n        pre-approved by the server. A future subscription request by the peer\n        will then be confirmed by the server automatically.\n\n        .. note::\n\n            Pre-approval is an OPTIONAL feature in :rfc:`6121`. It is announced\n            as a stream feature.\n        \"\"\"\n        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBED,\n                            to=peer_jid)\n        )",
    "label": 0
  },
  {
    "codes": "public Observable<Page<SiteInner>> beginResumeNextAsync(final String nextPageLink) {\n        return beginResumeNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<SiteInner>>, Page<SiteInner>>() {\n                @Override\n                public Page<SiteInner> call(ServiceResponse<Page<SiteInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *PartitionError) SetPartitionValues(v []*string) *PartitionError {\n\ts.PartitionValues = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (c *Glue) GetPartitions(input *GetPartitionsInput) (*GetPartitionsOutput, error) {\n\treq, out := c.GetPartitionsRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "func (c *Glue) CreatePartition(input *CreatePartitionInput) (*CreatePartitionOutput, error) {\n\treq, out := c.CreatePartitionRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```java\npublic class LongestPalindromicSubstring {\n\n    // Method to find the longest palindromic substring\n    public static String longestPalindrome(String s) {\n        if (s == null || s.length() < 1) return \"\";\n\n        int start = 0, end = 0;\n\n        // Iterate through each character in the string\n        for (int i = 0; i < s.length(); i++) {\n            // Check for odd-length palindromes\n            int len1 = expandAroundCenter(s, i, i);\n            // Check for even-length palindromes\n            int len2 = expandAroundCenter(s, i, i + 1);\n            // Get the maximum length between the two\n            int len = Math.max(len1, len2);\n            // Update the start and end indices if a longer palindrome is found\n            if (len > end - start) {\n                start = i - (len - 1) / 2;\n                end = i + len / 2;\n            }\n        }\n\n        // Return the longest palindromic substring\n        return s.substring(start, end + 1);\n    }\n\n    // Helper method to expand around the center and find the length of the palindrome\n    private static int expandAroundCenter(String s, int left, int right) {\n        while (left >= 0 && right < s.length() && s.charAt(left) == s.charAt(right)) {\n            left--;\n            right++;\n        }\n        // Return the length of the palindrome\n        return right - left - 1;\n    }\n\n    // Main method to test the function\n    public static void main(String[] args) {\n        String input = \"babad\";\n        System.out.println(\"Longest Palindromic Substring: \" + longestPalindrome(input));\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse reqwest::blocking::Client;\nuse serde::{Deserialize, Serialize};\nuse std::error::Error;\n\n// Define the structure for the token response\n#[derive(Debug, Serialize, Deserialize)]\nstruct TokenResponse {\n    access_token: String,\n    token_type: String,\n    expires_in: u64,\n}\n\n// Function to request an access token using client credentials flow\nfn get_access_token(client_id: &str, client_secret: &str, token_url: &str) -> Result<TokenResponse, Box<dyn Error>> {\n    let client = Client::new();\n\n    // Prepare the request body\n    let params = [\n        (\"grant_type\", \"client_credentials\"),\n        (\"client_id\", client_id),\n        (\"client_secret\", client_secret),\n    ];\n\n    // Send the POST request to the token endpoint\n    let response = client\n        .post(token_url)\n        .form(&params)\n        .send()?;\n\n    // Check if the request was successful\n    if response.status().is_success() {\n        // Parse the JSON response into the TokenResponse struct\n        let token_response: TokenResponse = response.json()?;\n        Ok(token_response)\n    } else {\n        // Handle error cases\n        Err(format!(\"Failed to get access token: {}\", response.status()).into())\n    }\n}\n\nfn main() {\n    // Example usage\n    let client_id = \"your_client_id\";\n    let client_secret = \"your_client_secret\";\n    let token_url = \"https://example.com/oauth2/token\";\n\n    match get_access_token(client_id, client_secret, token_url) {\n        Ok(token) => println!(\"Access Token: {}\", token.access_token),\n        Err(e) => eprintln!(\"Error: {}\", e),\n    }\n}\n```\n\n### Comments:\n- **TokenResponse**: Struct to deserialize the JSON response from the OAuth2 token endpoint.\n- **get_access_token**: Function to request an access token using the client credentials flow.\n- **Client**: Reqwest blocking client for making HTTP requests.\n- **params**: Form data to be sent in the POST request.\n- **response.json()**: Deserializes the JSON response into the `TokenResponse` struct.\n- **Error Handling**: Checks if the request was successful and handles errors accordingly.\n\nThis implementation uses the `reqwest` crate for HTTP requests and `serde` for JSON serialization/deserialization. It follows the OAuth2 client credentials flow by sending a POST request to the token endpoint with the necessary parameters.",
    "label": 1
  },
  {
    "codes": "def search(keyword, type=1, offset=0, limit=30):\n    \"\"\"\u641c\u7d22\u6b4c\u66f2\uff0c\u652f\u6301\u641c\u7d22\u6b4c\u66f2\u3001\u6b4c\u624b\u3001\u4e13\u8f91\u7b49\n\n    :param keyword: \u5173\u952e\u8bcd\n    :param type: (optional) \u641c\u7d22\u7c7b\u578b\uff0c1: \u5355\u66f2, 100: \u6b4c\u624b, 1000: \u6b4c\u5355, 1002: \u7528\u6237\n    :param offset: (optional) \u5206\u6bb5\u8d77\u59cb\u4f4d\u7f6e\uff0c\u9ed8\u8ba4 0\n    :param limit: (optional) \u6570\u636e\u4e0a\u9650\u591a\u5c11\u884c\uff0c\u9ed8\u8ba4 30\n    \"\"\"\n    if keyword is None:\n        raise ParamsError()\n    r = NCloudBot()\n    r.method = 'SEARCH'\n    r.data = {\n        's': keyword,\n        'limit': str(limit),\n        'type': str(type),\n        'offset': str(offset)\n    }\n    r.send()\n\n    return r.response",
    "label": 0
  },
  {
    "codes": "```python\n# CircuitPython NMEA GPS Parser Library\n\nimport re\n\nclass NMEAParser:\n    def __init__(self):\n        self.sentence_pattern = re.compile(r'^\\$([A-Z]{2}[A-Z]{3}),(.*)\\*([0-9A-F]{2})$')\n        self.checksum_pattern = re.compile(r'^\\$.*\\*[0-9A-F]{2}$')\n        self.data = {}\n\n    def checksum(self, sentence):\n        \"\"\"Calculate NMEA checksum for a sentence.\"\"\"\n        checksum = 0\n        for char in sentence[1:]:\n            if char == '*':\n                break\n            checksum ^= ord(char)\n        return f\"{checksum:02X}\"\n\n    def validate_checksum(self, sentence):\n        \"\"\"Validate the checksum of an NMEA sentence.\"\"\"\n        if not self.checksum_pattern.match(sentence):\n            return False\n        calculated_checksum = self.checksum(sentence)\n        received_checksum = sentence[-2:]\n        return calculated_checksum == received_checksum\n\n    def parse(self, sentence):\n        \"\"\"Parse an NMEA sentence into a dictionary.\"\"\"\n        if not self.validate_checksum(sentence):\n            return None\n\n        match = self.sentence_pattern.match(sentence)\n        if not match:\n            return None\n\n        talker_id = match.group(1)[:2]\n        sentence_type = match.group(1)[2:]\n        data_fields = match.group(2).split(',')\n\n        self.data = {\n            'talker_id': talker_id,\n            'sentence_type': sentence_type,\n            'data_fields': data_fields\n        }\n\n        return self.data\n\n    def get_latitude(self):\n        \"\"\"Extract latitude from GGA sentence.\"\"\"\n        if self.data.get('sentence_type') == 'GGA':\n            lat = self.data['data_fields'][1]\n            lat_dir = self.data['data_fields'][2]\n            if lat and lat_dir:\n                degrees = float(lat[:2])\n                minutes = float(lat[2:])\n                latitude = degrees + (minutes / 60)\n                if lat_dir == 'S':\n                    latitude = -latitude\n                return latitude\n        return None\n\n    def get_longitude(self):\n        \"\"\"Extract longitude from GGA sentence.\"\"\"\n        if self.data.get('sentence_type') == 'GGA':\n            lon = self.data['data_fields'][3]\n            lon_dir = self.data['data_fields'][4]\n            if lon and lon_dir:\n                degrees = float(lon[:3])\n                minutes = float(lon[3:])\n                longitude = degrees + (minutes / 60)\n                if lon_dir == 'W':\n                    longitude = -longitude\n                return longitude\n        return None\n\n    def get_altitude(self):\n        \"\"\"Extract altitude from GGA sentence.\"\"\"\n        if self.data.get('sentence_type') == 'GGA':\n            alt = self.data['data_fields'][8]\n            if alt:\n                return float(alt)\n        return None\n\n    def get_speed(self):\n        \"\"\"Extract speed from RMC sentence.\"\"\"\n        if self.data.get('sentence_type') == 'RMC':\n            speed = self.data['data_fields'][6]\n            if speed:\n                return float(speed)\n        return None\n\n    def get_course(self):\n        \"\"\"Extract course from RMC sentence.\"\"\"\n        if self.data.get('sentence_type') == 'RMC':\n            course = self.data['data_fields'][7]\n            if course:\n                return float(course)\n        return None\n\n    def get_time(self):\n        \"\"\"Extract time from GGA or RMC sentence.\"\"\"\n        if self.data.get('sentence_type') in ['GGA', 'RMC']:\n            time_str = self.data['data_fields'][0]\n            if time_str:\n                return f\"{time_str[:2]}:{time_str[2:4]}:{time_str[4:6]}\"\n        return None\n\n    def get_date(self):\n        \"\"\"Extract date from RMC sentence.\"\"\"\n        if self.data.get('sentence_type') == 'RMC':\n            date_str = self.data['data_fields'][8]\n            if date_str:\n                return f\"{date_str[4:6]}/{date_str[2:4]}/{date_str[:2]}\"\n        return None\n```",
    "label": 1
  },
  {
    "codes": "public Observable<CertificateOperation> createCertificateAsync(String vaultBaseUrl, String certificateName, CertificatePolicy certificatePolicy, CertificateAttributes certificateAttributes, Map<String, String> tags) {\n        return createCertificateWithServiceResponseAsync(vaultBaseUrl, certificateName, certificatePolicy, certificateAttributes, tags).map(new Func1<ServiceResponse<CertificateOperation>, CertificateOperation>() {\n            @Override\n            public CertificateOperation call(ServiceResponse<CertificateOperation> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Collections.Concurrent;\nusing System.Threading;\nusing System.Threading.Tasks;\n\nclass Program\n{\n    static void Main(string[] args)\n    {\n        // Create a cancellation token source\n        var cts = new CancellationTokenSource();\n\n        // Create a collection of items to process\n        var items = new ConcurrentBag<int> { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\n\n        try\n        {\n            // Start a task to cancel the operation after 2 seconds\n            Task.Run(() =>\n            {\n                Thread.Sleep(2000); // Simulate a delay\n                cts.Cancel(); // Cancel the operation\n            });\n\n            // Process items in parallel with cancellation support\n            Parallel.ForEach(items, new ParallelOptions { CancellationToken = cts.Token }, item =>\n            {\n                // Simulate work\n                Thread.Sleep(500); // Simulate processing time\n                Console.WriteLine($\"Processed item: {item}\");\n\n                // Check for cancellation\n                cts.Token.ThrowIfCancellationRequested();\n            });\n        }\n        catch (OperationCanceledException)\n        {\n            Console.WriteLine(\"Operation was canceled.\");\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nimport React, { useState, useEffect } from 'react';\n\n// RealTimeChat component\nconst RealTimeChat = () => {\n  const [messages, setMessages] = useState([]); // State to store chat messages\n  const [inputValue, setInputValue] = useState(''); // State to store input value\n\n  // Simulate real-time message updates (e.g., from a WebSocket or API)\n  useEffect(() => {\n    const interval = setInterval(() => {\n      // Simulate receiving a new message\n      const newMessage = { id: messages.length + 1, text: `Message ${messages.length + 1}` };\n      setMessages((prevMessages) => [...prevMessages, newMessage]);\n    }, 5000); // Add a new message every 5 seconds\n\n    return () => clearInterval(interval); // Cleanup interval on unmount\n  }, [messages]);\n\n  // Handle sending a new message\n  const handleSendMessage = () => {\n    if (inputValue.trim()) {\n      const newMessage = { id: messages.length + 1, text: inputValue };\n      setMessages((prevMessages) => [...prevMessages, newMessage]);\n      setInputValue(''); // Clear input after sending\n    }\n  };\n\n  return (\n    <div style={styles.chatContainer}>\n      <div style={styles.messagesContainer}>\n        {messages.map((message) => (\n          <div key={message.id} style={styles.message}>\n            {message.text}\n          </div>\n        ))}\n      </div>\n      <div style={styles.inputContainer}>\n        <input\n          type=\"text\"\n          value={inputValue}\n          onChange={(e) => setInputValue(e.target.value)}\n          style={styles.input}\n          placeholder=\"Type a message...\"\n        />\n        <button onClick={handleSendMessage} style={styles.sendButton}>\n          Send\n        </button>\n      </div>\n    </div>\n  );\n};\n\n// Styles for the chat interface\nconst styles = {\n  chatContainer: {\n    width: '300px',\n    height: '400px',\n    border: '1px solid #ccc',\n    borderRadius: '8px',\n    display: 'flex',\n    flexDirection: 'column',\n    justifyContent: 'space-between',\n    padding: '10px',\n  },\n  messagesContainer: {\n    flex: 1,\n    overflowY: 'auto',\n    marginBottom: '10px',\n  },\n  message: {\n    padding: '5px',\n    borderBottom: '1px solid #eee',\n  },\n  inputContainer: {\n    display: 'flex',\n  },\n  input: {\n    flex: 1,\n    padding: '5px',\n    borderRadius: '4px',\n    border: '1px solid #ccc',\n  },\n  sendButton: {\n    marginLeft: '10px',\n    padding: '5px 10px',\n    borderRadius: '4px',\n    border: 'none',\n    backgroundColor: '#007bff',\n    color: '#fff',\n    cursor: 'pointer',\n  },\n};\n\nexport default RealTimeChat;\n```",
    "label": 1
  },
  {
    "codes": "public Observable<Page<SiteInner>> suspendAsync(final String resourceGroupName, final String name) {\n        return suspendWithServiceResponseAsync(resourceGroupName, name)\n            .map(new Func1<ServiceResponse<Page<SiteInner>>, Page<SiteInner>>() {\n                @Override\n                public Page<SiteInner> call(ServiceResponse<Page<SiteInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<Page<DenyAssignmentInner>> listForResourceNextAsync(final String nextPageLink) {\n        return listForResourceNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<DenyAssignmentInner>>, Page<DenyAssignmentInner>>() {\n                @Override\n                public Page<DenyAssignmentInner> call(ServiceResponse<Page<DenyAssignmentInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def mset(self, *args, **kwargs):\n        \"\"\"\n        Sets key/values based on a mapping. Mapping can be supplied as a single\n        dictionary argument or as kwargs.\n        \"\"\"\n        mapping = kwargs\n        if args:\n            if len(args) != 1 or not isinstance(args[0], dict):\n                raise RedisError('MSET requires **kwargs or a single dict arg')\n            mapping.update(args[0])\n\n        if len(mapping) == 0:\n            raise ResponseError(\"wrong number of arguments for 'mset' command\")\n\n        for key, value in mapping.items():\n            self.set(key, value)\n        return True",
    "label": 0
  },
  {
    "codes": "func (c *Glue) GetTrigger(input *GetTriggerInput) (*GetTriggerOutput, error) {\n\treq, out := c.GetTriggerRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\n# Create a Spark session\nspark = SparkSession.builder \\\n    .appName(\"CustomerClustering\") \\\n    .getOrCreate()\n\n# Load the customer dataset\ndata = spark.read.csv(\"path_to_customer_data.csv\", header=True, inferSchema=True)\n\n# Select features for clustering\nfeature_columns = [\"feature1\", \"feature2\", \"feature3\"]  # specify your feature columns\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\nfeature_data = assembler.transform(data)\n\n# Train a KMeans model\nkmeans = KMeans().setK(4).setSeed(1)  # set the number of clusters\nmodel = kmeans.fit(feature_data)\n\n# Make predictions\npredictions = model.transform(feature_data)\n\n# Evaluate the clustering result\nevaluator = ClusteringEvaluator()\nsilhouette_score = evaluator.evaluate(predictions)\nprint(f\"Silhouette Score: {silhouette_score}\")\n\n# Show cluster centers\ncenters = model.clusterCenters()\nprint(\"Cluster Centers:\")\nfor center in centers:\n    print(center)\n\n# Stop the Spark session\nspark.stop()\n```",
    "label": 1
  },
  {
    "codes": "func (s *DevEndpoint) SetAvailabilityZone(v string) *DevEndpoint {\n\ts.AvailabilityZone = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "def add_cache_entry(self, key, entry):\n        \"\"\"\n        Add the given `entry` (which must be a :class:`~.disco.xso.InfoQuery`\n        instance) to the user-level database keyed with the hash function type\n        `hash_` and the `node` URL. The `entry` is **not** validated to\n        actually map to `node` with the given `hash_` function, it is expected\n        that the caller perfoms the validation.\n        \"\"\"\n        copied_entry = copy.copy(entry)\n        self._memory_overlay[key] = copied_entry\n        if self._user_db_path is not None:\n            asyncio.ensure_future(asyncio.get_event_loop().run_in_executor(\n                None,\n                writeback,\n                self._user_db_path / key.path,\n                entry.captured_events))",
    "label": 0
  },
  {
    "codes": "```powershell\n# Define the event IDs to audit (common security event IDs)\n$eventIds = @(4624, 4625, 4672, 4688, 4702, 4720, 4722, 4724, 4725, 4726, 4732, 4738, 4740, 4768, 4776, 4798, 4799, 1102)\n\n# Define the log name (Security log)\n$logName = \"Security\"\n\n# Define the time range (last 7 days)\n$startTime = (Get-Date).AddDays(-7)\n$endTime = Get-Date\n\n# Query the event logs for the specified event IDs\n$events = Get-WinEvent -FilterHashtable @{\n    LogName = $logName\n    ID = $eventIds\n    StartTime = $startTime\n    EndTime = $endTime\n} -ErrorAction SilentlyContinue\n\n# Output the results to a CSV file\n$events | Select-Object TimeCreated, Id, ProviderName, Message | Export-Csv -Path \"SecurityAuditReport.csv\" -NoTypeInformation\n\n# Output a summary of the audit\nWrite-Host \"Audit completed. Total events found: $($events.Count)\"\nWrite-Host \"Report saved to SecurityAuditReport.csv\"\n```",
    "label": 1
  },
  {
    "codes": "```python\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\nfrom werkzeug.utils import secure_filename\nfrom PIL import Image\nimport os\nimport uuid\nfrom concurrent.futures import ThreadPoolExecutor\n\napp = Flask(__name__)\nCORS(app)\nexecutor = ThreadPoolExecutor(2)  # Adjust the number of threads as needed\n\n# Configuration for file uploads\nUPLOAD_FOLDER = 'uploads'\nALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\n# Ensure the upload folder exists\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\n\ndef allowed_file(filename):\n    \"\"\"Check if the file has an allowed extension.\"\"\"\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef resize_image(image_path, output_path, size=(128, 128)):\n    \"\"\"Resize the image to the specified size.\"\"\"\n    with Image.open(image_path) as img:\n        img.thumbnail(size)\n        img.save(output_path)\n\n@app.route('/upload', methods=['POST'])\ndef upload_image():\n    \"\"\"Endpoint to upload and resize images asynchronously.\"\"\"\n    if 'file' not in request.files:\n        return jsonify({\"error\": \"No file part\"}), 400\n\n    file = request.files['file']\n\n    if file.filename == '':\n        return jsonify({\"error\": \"No selected file\"}), 400\n\n    if file and allowed_file(file.filename):\n        filename = secure_filename(file.filename)\n        unique_id = str(uuid.uuid4())\n        original_path = os.path.join(app.config['UPLOAD_FOLDER'], f\"{unique_id}_{filename}\")\n        resized_path = os.path.join(app.config['UPLOAD_FOLDER'], f\"resized_{unique_id}_{filename}\")\n\n        # Save the original file\n        file.save(original_path)\n\n        # Resize the image asynchronously\n        executor.submit(resize_image, original_path, resized_path)\n\n        return jsonify({\"message\": \"File uploaded and resizing started\", \"file_id\": unique_id}), 202\n\n    return jsonify({\"error\": \"File type not allowed\"}), 400\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```",
    "label": 1
  },
  {
    "codes": "function getAncestorOrSelfWithJavascriptHref(element) {\n    if (hasJavascriptHref(element)) {\n        return element;\n    }\n    if (element.parentNode == null) {\n        return null;\n    }\n    return getAncestorOrSelfWithJavascriptHref(element.parentNode);\n}",
    "label": 0
  },
  {
    "codes": "```typescript\nimport { Injectable } from '@angular/core';\nimport { HttpClient, HttpParams } from '@angular/common/http';\nimport { Observable } from 'rxjs';\n\n@Injectable({\n  providedIn: 'root'\n})\nexport class DataService {\n\n  private apiUrl = 'https://api.example.com/data'; // Base URL for API\n\n  constructor(private http: HttpClient) { }\n\n  /**\n   * Fetch paginated data from the API\n   * @param page Number of the page to retrieve\n   * @param pageSize Number of items per page\n   * @returns An Observable with the paginated data\n   */\n  getPaginatedData(page: number, pageSize: number): Observable<any> {\n    // Set HTTP params for page and page size\n    const params = new HttpParams()\n      .set('page', page.toString())\n      .set('pageSize', pageSize.toString());\n\n    // Make GET request to the API with parameters\n    return this.http.get(this.apiUrl, { params });\n  }\n\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport uuid\n\ndef generate_uuid4():\n    # Creates a UUID v4 and removes hyphens\n    return uuid.uuid4().hex\n\n# Example usage\nuuid_no_hyphens = generate_uuid4()\nprint(uuid_no_hyphens)\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nimport React, { useState, useEffect } from 'react';\n\n// RealTimeChat component\nconst RealTimeChat = () => {\n  const [messages, setMessages] = useState([]); // State to store chat messages\n  const [inputValue, setInputValue] = useState(''); // State to store input value\n\n  // Simulate real-time message updates (e.g., from a WebSocket)\n  useEffect(() => {\n    const interval = setInterval(() => {\n      setMessages((prevMessages) => [\n        ...prevMessages,\n        { id: prevMessages.length + 1, text: `New message ${prevMessages.length + 1}` },\n      ]);\n    }, 5000); // Add a new message every 5 seconds\n\n    return () => clearInterval(interval); // Cleanup interval on unmount\n  }, []);\n\n  // Handle sending a new message\n  const handleSendMessage = () => {\n    if (inputValue.trim()) {\n      setMessages((prevMessages) => [\n        ...prevMessages,\n        { id: prevMessages.length + 1, text: inputValue },\n      ]);\n      setInputValue(''); // Clear input after sending\n    }\n  };\n\n  return (\n    <div style={styles.chatContainer}>\n      <div style={styles.messagesContainer}>\n        {messages.map((message) => (\n          <div key={message.id} style={styles.message}>\n            {message.text}\n          </div>\n        ))}\n      </div>\n      <div style={styles.inputContainer}>\n        <input\n          type=\"text\"\n          value={inputValue}\n          onChange={(e) => setInputValue(e.target.value)}\n          style={styles.input}\n          placeholder=\"Type a message...\"\n        />\n        <button onClick={handleSendMessage} style={styles.sendButton}>\n          Send\n        </button>\n      </div>\n    </div>\n  );\n};\n\n// Styles for the chat interface\nconst styles = {\n  chatContainer: {\n    width: '300px',\n    height: '400px',\n    border: '1px solid #ccc',\n    borderRadius: '8px',\n    display: 'flex',\n    flexDirection: 'column',\n    justifyContent: 'space-between',\n    padding: '10px',\n  },\n  messagesContainer: {\n    flex: 1,\n    overflowY: 'auto',\n    marginBottom: '10px',\n  },\n  message: {\n    padding: '5px',\n    borderBottom: '1px solid #eee',\n  },\n  inputContainer: {\n    display: 'flex',\n  },\n  input: {\n    flex: 1,\n    padding: '5px',\n    borderRadius: '4px',\n    border: '1px solid #ccc',\n  },\n  sendButton: {\n    marginLeft: '10px',\n    padding: '5px 10px',\n    borderRadius: '4px',\n    border: 'none',\n    backgroundColor: '#007bff',\n    color: '#fff',\n    cursor: 'pointer',\n  },\n};\n\nexport default RealTimeChat;\n```",
    "label": 1
  },
  {
    "codes": "function splitNewlines(str) {\n\tif ((typeof str === 'string' || str instanceof String) && str.indexOf('\\n') > -1) {\n\t\treturn str.split('\\n');\n\t}\n\treturn str;\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport \"fmt\"\n\n// Function to multiply two matrices\nfunc multiplyMatrices(A, B [][]int) [][]int {\n    n := len(A)\n    result := make([][]int, n)\n    for i := range result {\n        result[i] = make([]int, n)\n    }\n    for i := 0; i < n; i++ {\n        for j := 0; j < n; j++ {\n            result[i][j] = 0\n            for k := 0; k < n; k++ {\n                result[i][j] += A[i][k] * B[k][j]\n            }\n        }\n    }\n    return result\n}\n\n// Function to perform matrix exponentiation\nfunc matrixExponentiation(A [][]int, exponent int) [][]int {\n    n := len(A)\n    result := make([][]int, n)\n    for i := range result {\n        result[i] = make([]int, n)\n        result[i][i] = 1 // Initialize result as identity matrix\n    }\n    base := A\n    for exponent > 0 {\n        if exponent%2 == 1 {\n            result = multiplyMatrices(result, base)\n        }\n        base = multiplyMatrices(base, base)\n        exponent /= 2\n    }\n    return result\n}\n\nfunc main() {\n    matrix := [][]int{\n        {1, 1},\n        {1, 0},\n    }\n    exponent := 5\n\n    result := matrixExponentiation(matrix, exponent)\n    for _, row := range result {\n        fmt.Println(row)\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "def start_sm(self, request_resumption=True, resumption_timeout=None):\n        \"\"\"\n        Start stream management (version 3).\n\n        :param request_resumption: Request that the stream shall be resumable.\n        :type request_resumption: :class:`bool`\n        :param resumption_timeout: Maximum time in seconds for a stream to be\n            resumable.\n        :type resumption_timeout: :class:`int`\n        :raises aioxmpp.errors.StreamNegotiationFailure: if the server rejects\n            the attempt to enable stream management.\n\n        This method attempts to starts stream management on the stream.\n\n        `resumption_timeout` is the ``max`` attribute on\n        :class:`.nonza.SMEnabled`; it can be used to set a maximum time for\n        which the server shall consider the stream to still be alive after the\n        underlying transport (TCP) has failed. The server may impose its own\n        maximum or ignore the request, so there are no guarentees that the\n        session will stay alive for at most or at least `resumption_timeout`\n        seconds. Passing a `resumption_timeout` of 0 is equivalent to passing\n        false to `request_resumption` and takes precedence over\n        `request_resumption`.\n\n        .. note::\n\n            In addition to server implementation details, it is very well\n            possible that the server does not even detect that the underlying\n            transport has failed for quite some time for various reasons\n            (including high TCP timeouts).\n\n        If the server rejects the attempt to enable stream management, a\n        :class:`.errors.StreamNegotiationFailure` is raised. The stream is\n        still running in that case.\n\n        .. warning::\n\n           This method cannot and does not check whether the server advertised\n           support for stream management. Attempting to negotiate stream\n           management without server support might lead to termination of the\n           stream.\n\n        If an XML stream error occurs during the negotiation, the result\n        depends on a few factors. In any case, the stream is not running\n        afterwards. If the :class:`.nonza.SMEnabled` response was not received\n        before the XML stream died, SM is also disabled and the exception which\n        caused the stream to die is re-raised (this is due to the\n        implementation of :func:`~.protocol.send_and_wait_for`). If the\n        :class:`.nonza.SMEnabled` response was received and annonuced support\n        for resumption, SM is enabled. Otherwise, it is disabled. No exception\n        is raised if :class:`.nonza.SMEnabled` was received, as this method has\n        no way to determine that the stream failed.\n\n        If negotiation succeeds, this coroutine initializes a new stream\n        management session. The stream management state attributes become\n        available and :attr:`sm_enabled` becomes :data:`True`.\n        \"\"\"\n        if not self.running:\n            raise RuntimeError(\"cannot start Stream Management while\"\n                               \" StanzaStream is not running\")\n        if self.sm_enabled:\n            raise RuntimeError(\"Stream Management already enabled\")\n\n        if resumption_timeout == 0:\n            request_resumption = False\n            resumption_timeout = None\n\n        # sorry for the callback spaghetti code\n        # we have to handle the response synchronously, so we have to use a\n        # callback.\n        # otherwise, it is possible that an SM related nonza (e.g. <r/>) is\n        # received (and attempted to be deserialized) before the handlers are\n        # registered\n        # see tests/test_highlevel.py:TestProtocoltest_sm_bootstrap_race\n        def handle_response(response):\n            if isinstance(response, nonza.SMFailed):\n                # we handle the error down below\n                return\n\n            self._sm_outbound_base = 0\n            self._sm_inbound_ctr = 0\n            self._sm_unacked_list = []\n            self._sm_enabled = True\n            self._sm_id = response.id_\n            self._sm_resumable = response.resume\n            self._sm_max = response.max_\n            self._sm_location = response.location\n\n            self._logger.info(\"SM started: resumable=%s, stream id=%r\",\n                              self._sm_resumable,\n                              self._sm_id)\n\n            # if not self._xmlstream:\n            #     # stream died in the meantime...\n            #     if self._xmlstream_exception:\n            #         raise self._xmlstream_exception\n\n            self._xmlstream.stanza_parser.add_class(\n                nonza.SMRequest,\n                self.recv_stanza)\n            self._xmlstream.stanza_parser.add_class(\n                nonza.SMAcknowledgement,\n                self.recv_stanza)\n\n        with (yield from self._broker_lock):\n            response = yield from protocol.send_and_wait_for(\n                self._xmlstream,\n                [\n                    nonza.SMEnable(resume=bool(request_resumption),\n                                   max_=resumption_timeout),\n                ],\n                [\n                    nonza.SMEnabled,\n                    nonza.SMFailed\n                ],\n                cb=handle_response,\n            )\n\n            if isinstance(response, nonza.SMFailed):\n                raise errors.StreamNegotiationFailure(\n                    \"Server rejected SM request\")",
    "label": 0
  },
  {
    "codes": "```yaml\n---\n# Ansible playbook to deploy a web app across 100+ servers\n- name: Deploy Web Application\n  hosts: web_servers  # Define the group of servers in your inventory\n  become: yes  # Run tasks with elevated privileges\n  vars:\n    app_version: \"1.0.0\"  # Define the version of the web app\n    app_source: \"https://example.com/app/{{ app_version }}.tar.gz\"  # Source URL for the app\n    deploy_dir: \"/var/www/myapp\"  # Directory to deploy the app\n\n  tasks:\n    - name: Ensure deployment directory exists\n      ansible.builtin.file:\n        path: \"{{ deploy_dir }}\"\n        state: directory\n        owner: www-data\n        group: www-data\n        mode: '0755'\n\n    - name: Download the web application\n      ansible.builtin.get_url:\n        url: \"{{ app_source }}\"\n        dest: \"/tmp/app.tar.gz\"\n\n    - name: Extract the application\n      ansible.builtin.unarchive:\n        src: \"/tmp/app.tar.gz\"\n        dest: \"{{ deploy_dir }}\"\n        remote_src: yes\n        owner: www-data\n        group: www-data\n\n    - name: Install dependencies\n      ansible.builtin.apt:\n        name:\n          - python3\n          - python3-pip\n        state: present\n        update_cache: yes\n\n    - name: Install Python requirements\n      ansible.builtin.pip:\n        requirements: \"{{ deploy_dir }}/requirements.txt\"\n        executable: pip3\n\n    - name: Configure the web server\n      ansible.builtin.template:\n        src: \"templates/nginx.conf.j2\"\n        dest: \"/etc/nginx/sites-available/myapp\"\n        owner: root\n        group: root\n        mode: '0644'\n      notify: Restart Nginx\n\n    - name: Enable the site\n      ansible.builtin.file:\n        src: \"/etc/nginx/sites-available/myapp\"\n        dest: \"/etc/nginx/sites-enabled/myapp\"\n        state: link\n\n    - name: Restart Nginx\n      ansible.builtin.service:\n        name: nginx\n        state: restarted\n      when: nginx_restart_needed  # Conditional restart based on changes\n\n  handlers:\n    - name: Restart Nginx\n      ansible.builtin.service:\n        name: nginx\n        state: restarted\n```",
    "label": 1
  },
  {
    "codes": "func (s *OutputSerialization) SetCsv(v *CSVOutput) *OutputSerialization {\n\ts.Csv = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport org.junit.jupiter.api.Test;\nimport org.junit.jupiter.api.BeforeEach;\nimport static org.junit.jupiter.api.Assertions.*;\n\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\n\nclass SortingAlgorithmTest {\n\n    private SortingAlgorithm sorter;\n\n    @BeforeEach\n    void setUp() {\n        sorter = new SortingAlgorithm(); // Assume SortingAlgorithm is the class with the sorting method\n    }\n\n    @Test\n    void testEmptyList() {\n        List<Integer> emptyList = Collections.emptyList();\n        List<Integer> sortedList = sorter.sort(emptyList);\n        assertTrue(sortedList.isEmpty(), \"Sorting an empty list should return an empty list\");\n    }\n\n    @Test\n    void testSingleElementList() {\n        List<Integer> singleElementList = Collections.singletonList(42);\n        List<Integer> sortedList = sorter.sort(singleElementList);\n        assertEquals(singleElementList, sortedList, \"Sorting a single-element list should return the same list\");\n    }\n\n    @Test\n    void testAlreadySortedList() {\n        List<Integer> sortedList = Arrays.asList(1, 2, 3, 4, 5);\n        List<Integer> result = sorter.sort(sortedList);\n        assertEquals(sortedList, result, \"Sorting an already sorted list should return the same list\");\n    }\n\n    @Test\n    void testReverseSortedList() {\n        List<Integer> reverseSortedList = Arrays.asList(5, 4, 3, 2, 1);\n        List<Integer> expectedSortedList = Arrays.asList(1, 2, 3, 4, 5);\n        List<Integer> result = sorter.sort(reverseSortedList);\n        assertEquals(expectedSortedList, result, \"Sorting a reverse-sorted list should return a sorted list\");\n    }\n\n    @Test\n    void testListWithDuplicates() {\n        List<Integer> listWithDuplicates = Arrays.asList(3, 1, 2, 3, 1);\n        List<Integer> expectedSortedList = Arrays.asList(1, 1, 2, 3, 3);\n        List<Integer> result = sorter.sort(listWithDuplicates);\n        assertEquals(expectedSortedList, result, \"Sorting a list with duplicates should return a sorted list with duplicates\");\n    }\n\n    @Test\n    void testListWithNegativeNumbers() {\n        List<Integer> listWithNegatives = Arrays.asList(-3, -1, -2, 0, 2);\n        List<Integer> expectedSortedList = Arrays.asList(-3, -2, -1, 0, 2);\n        List<Integer> result = sorter.sort(listWithNegatives);\n        assertEquals(expectedSortedList, result, \"Sorting a list with negative numbers should return a sorted list\");\n    }\n\n    @Test\n    void testListWithAllSameElements() {\n        List<Integer> listWithSameElements = Arrays.asList(7, 7, 7, 7);\n        List<Integer> result = sorter.sort(listWithSameElements);\n        assertEquals(listWithSameElements, result, \"Sorting a list with all same elements should return the same list\");\n    }\n\n    @Test\n    void testLargeList() {\n        List<Integer> largeList = Arrays.asList(100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90);\n        List<Integer> expectedSortedList = Arrays.asList(90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100);\n        List<Integer> result = sorter.sort(largeList);\n        assertEquals(expectedSortedList, result, \"Sorting a large list should return a sorted list\");\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function findComponents(directory, components = []) {\n  const items = fs.readdirSync(directory);\n\n  items.forEach(item => {\n    const itemPath = path.resolve(directory, item);\n\n    if (fs.statSync(itemPath).isDirectory()) {\n      findComponents(itemPath, components);\n      return;\n    }\n\n    if (!componentRegex.test(item)) {\n      return;\n    }\n\n    components.push({\n      filename: itemPath,\n    });\n  });\n\n  return components;\n}",
    "label": 0
  },
  {
    "codes": "function(node) {\n      var range = rangy.createRange(this.doc);\n      range.setStartBefore(node);\n      range.setEndBefore(node);\n      return this.setSelection(range);\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport functools\nimport logging\nimport traceback\n\n# Configure logging\nlogging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef log_exceptions(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            # Log the exception with traceback\n            logging.error(f\"Exception occurred in {func.__name__}: {str(e)}\")\n            logging.error(traceback.format_exc())\n            raise  # Re-raise the exception after logging\n    return wrapper\n```\n\n```python\n# Example usage\n@log_exceptions\ndef risky_function(x, y):\n    return x / y\n\n# Test the decorator\nrisky_function(10, 0)  # This will log the exception and traceback\n```",
    "label": 1
  },
  {
    "codes": "public JobStreamListResultInner listByJob(String resourceGroupName, String automationAccountName, UUID jobId) {\n        return listByJobWithServiceResponseAsync(resourceGroupName, automationAccountName, jobId).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <fstream>\n#include <iostream>\n#include <vector>\n\n// Function to encrypt data using AES-256-CBC\nbool encryptFile(const std::string& inputFile, const std::string& outputFile, const std::vector<unsigned char>& key) {\n    // Initialize OpenSSL cipher context\n    EVP_CIPHER_CTX* ctx = EVP_CIPHER_CTX_new();\n    if (!ctx) {\n        std::cerr << \"Error creating cipher context\" << std::endl;\n        return false;\n    }\n\n    // Generate a random IV (Initialization Vector)\n    unsigned char iv[EVP_MAX_IV_LENGTH];\n    if (!RAND_bytes(iv, EVP_MAX_IV_LENGTH)) {\n        std::cerr << \"Error generating IV\" << std::endl;\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n\n    // Initialize encryption operation\n    if (EVP_EncryptInit_ex(ctx, EVP_aes_256_cbc(), nullptr, key.data(), iv) != 1) {\n        std::cerr << \"Error initializing encryption\" << std::endl;\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n\n    // Open input and output files\n    std::ifstream inFile(inputFile, std::ios::binary);\n    std::ofstream outFile(outputFile, std::ios::binary);\n    if (!inFile || !outFile) {\n        std::cerr << \"Error opening files\" << std::endl;\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n\n    // Write IV to the output file\n    outFile.write(reinterpret_cast<char*>(iv), EVP_MAX_IV_LENGTH);\n\n    // Buffer for reading and encrypting data\n    const size_t bufferSize = 4096;\n    std::vector<unsigned char> inBuffer(bufferSize);\n    std::vector<unsigned char> outBuffer(bufferSize + EVP_MAX_BLOCK_LENGTH);\n\n    int bytesRead, outLen;\n    while ((bytesRead = inFile.read(reinterpret_cast<char*>(inBuffer.data()), bufferSize).gcount()) > 0) {\n        // Encrypt the data\n        if (EVP_EncryptUpdate(ctx, outBuffer.data(), &outLen, inBuffer.data(), bytesRead) != 1) {\n            std::cerr << \"Error encrypting data\" << std::endl;\n            EVP_CIPHER_CTX_free(ctx);\n            return false;\n        }\n        // Write encrypted data to output file\n        outFile.write(reinterpret_cast<char*>(outBuffer.data()), outLen);\n    }\n\n    // Finalize encryption\n    if (EVP_EncryptFinal_ex(ctx, outBuffer.data(), &outLen) != 1) {\n        std::cerr << \"Error finalizing encryption\" << std::endl;\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n    // Write final encrypted data to output file\n    outFile.write(reinterpret_cast<char*>(outBuffer.data()), outLen);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n    return true;\n}\n\nint main() {\n    // Example key (32 bytes for AES-256)\n    std::vector<unsigned char> key = {\n        0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,\n        0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,\n        0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17,\n        0x18, 0x19, 0x1A, 0x1B, 0x1C, 0x1D, 0x1E, 0x1F\n    };\n\n    // Encrypt the file\n    if (encryptFile(\"input.txt\", \"output.enc\", key)) {\n        std::cout << \"File encrypted successfully!\" << std::endl;\n    } else {\n        std::cerr << \"File encryption failed!\" << std::endl;\n    }\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<KeyBundle> createKeyAsync(String vaultBaseUrl, String keyName, JsonWebKeyType kty) {\n        return createKeyWithServiceResponseAsync(vaultBaseUrl, keyName, kty).map(new Func1<ServiceResponse<KeyBundle>, KeyBundle>() {\n            @Override\n            public KeyBundle call(ServiceResponse<KeyBundle> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```go\n// Import necessary packages\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\t\"k8s.io/apimachinery/pkg/runtime/schema\"\n\t\"k8s.io/client-go/kubernetes/scheme\"\n\t\"k8s.io/client-go/rest\"\n\tctrl \"sigs.k8s.io/controller-runtime\"\n\t\"sigs.k8s.io/controller-runtime/pkg/client\"\n\t\"sigs.k8s.io/controller-runtime/pkg/log/zap\"\n\t\"sigs.k8s.io/controller-runtime/pkg/manager\"\n)\n\n// Define the custom resource type\ntype MyCustomResource struct {\n\truntime.TypeMeta\n\tMetadata struct {\n\t\tName      string `json:\"name\"`\n\t\tNamespace string `json:\"namespace\"`\n\t} `json:\"metadata\"`\n\tSpec struct {\n\t\tMessage string `json:\"message\"`\n\t} `json:\"spec\"`\n}\n\n// Implement the controller logic\ntype MyCustomResourceReconciler struct {\n\tclient.Client\n\tScheme *runtime.Scheme\n}\n\nfunc (r *MyCustomResourceReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n\tlog := ctrl.Log.WithValues(\"mycustomresource\", req.NamespacedName)\n\n\t// Fetch the custom resource instance\n\tinstance := &MyCustomResource{}\n\tif err := r.Get(ctx, req.NamespacedName, instance); err != nil {\n\t\tlog.Error(err, \"unable to fetch MyCustomResource\")\n\t\treturn ctrl.Result{}, client.IgnoreNotFound(err)\n\t}\n\n\t// Log the custom resource's message\n\tlog.Info(\"Reconciling MyCustomResource\", \"message\", instance.Spec.Message)\n\n\t// Add your custom logic here\n\n\treturn ctrl.Result{}, nil\n}\n\nfunc main() {\n\t// Set up logging\n\tctrl.SetLogger(zap.New())\n\n\t// Create a new manager\n\tmgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{\n\t\tScheme:             scheme.Scheme,\n\t\tMetricsBindAddress: \":8080\",\n\t})\n\tif err != nil {\n\t\tfmt.Println(\"unable to start manager:\", err)\n\t\tos.Exit(1)\n\t}\n\n\t// Register the custom resource with the scheme\n\tscheme := mgr.GetScheme()\n\tscheme.AddKnownTypes(schema.GroupVersion{Group: \"mygroup\", Version: \"v1\"}, &MyCustomResource{})\n\n\t// Create and register the reconciler\n\treconciler := &MyCustomResourceReconciler{\n\t\tClient: mgr.GetClient(),\n\t\tScheme: mgr.GetScheme(),\n\t}\n\tif err := reconciler.SetupWithManager(mgr); err != nil {\n\t\tfmt.Println(\"unable to create controller:\", err)\n\t\tos.Exit(1)\n\t}\n\n\t// Start the manager\n\tif err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {\n\t\tfmt.Println(\"problem running manager:\", err)\n\t\tos.Exit(1)\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "function (items) {\n        this.items = [];\n        this.length = 0;\n        this.type = \"set\";\n        if (items) {\n            for (var i = 0, ii = items.length; i < ii; i++) {\n                if (items[i] && (items[i].constructor == elproto.constructor || items[i].constructor == Set)) {\n                    this[this.items.length] = this.items[this.items.length] = items[i];\n                    this.length++;\n                }\n            }\n        }\n    }",
    "label": 0
  },
  {
    "codes": "function UIMap()\n{\n    // the singleton pattern, split into two parts so that \"new\" can still\n    // be used, in addition to \"getInstance()\"\n    UIMap.self = this;\n    \n    // need to attach variables directly to the Editor object in order for them\n    // to be in scope for Editor methods\n    if (is_IDE()) {\n        Editor.uiMap = this;\n        Editor.UI_PREFIX = UI_GLOBAL.UI_PREFIX;\n    }\n    \n    this.pagesets = new Object();\n    \n    \n    \n    /**\n     * pageset[pagesetName]\n     *   regexp\n     *   elements[elementName]\n     *     UIElement\n     */\n    this.addPageset = function(pagesetShorthand)\n    {\n        try {\n            var pageset = new Pageset(pagesetShorthand);\n        }\n        catch (e) {\n            safe_alert(\"Could not create pageset from shorthand:\\n\"\n                + print_r(pagesetShorthand) + \"\\n\" + e.message);\n            return false;\n        }\n        \n        if (this.pagesets[pageset.name]) {\n            safe_alert('Could not add pageset \"' + pageset.name\n                + '\": a pageset with that name already exists!');\n            return false;\n        }\n        \n        this.pagesets[pageset.name] = pageset;\n        return true;\n    };\n    \n    \n    \n    /**\n     * @param pagesetName\n     * @param uiElementShorthand  a representation of a UIElement object in\n     *                            shorthand JSON.\n     */\n    this.addElement = function(pagesetName, uiElementShorthand)\n    {\n        try {\n            var uiElement = new UIElement(uiElementShorthand);\n        }\n        catch (e) {\n            safe_alert(\"Could not create UI element from shorthand:\\n\"\n                + print_r(uiElementShorthand) + \"\\n\" + e.message);\n            return false;\n        }\n        \n        // run the element's unit tests only for the IDE, and only when the\n        // IDE is starting. Make a rough guess as to the latter condition.\n        if (is_IDE() && !editor.selDebugger && !uiElement.test()) {\n            safe_alert('Could not add UI element \"' + uiElement.name\n                + '\": failed testcases!');\n            return false;\n        }\n        \n        try {\n            this.pagesets[pagesetName].uiElements[uiElement.name] = uiElement;\n        }\n        catch (e) {\n            safe_alert(\"Could not add UI element '\" + uiElement.name\n                + \"' to pageset '\" + pagesetName + \"':\\n\" + e.message);\n            return false;\n        }\n        \n        return true;\n    };\n    \n    \n    \n    /**\n     * Returns the pageset for a given UI specifier string.\n     *\n     * @param uiSpecifierString\n     * @return  a pageset object\n     */\n    this.getPageset = function(uiSpecifierString)\n    {\n        try {\n            var uiSpecifier = new UISpecifier(uiSpecifierString);\n            return this.pagesets[uiSpecifier.pagesetName];\n        }\n        catch (e) {\n            return null;\n        }\n    }\n    \n    \n    \n    /**\n     * Returns the UIElement that a UISpecifierString or pageset and element\n     * pair refer to.\n     *\n     * @param pagesetNameOrUISpecifierString\n     * @return  a UIElement, or null if none is found associated with\n     *          uiSpecifierString\n     */\n    this.getUIElement = function(pagesetNameOrUISpecifierString, uiElementName)\n    {\n        var pagesetName = pagesetNameOrUISpecifierString;\n        if (arguments.length == 1) {\n            var uiSpecifierString = pagesetNameOrUISpecifierString;\n            try {\n                var uiSpecifier = new UISpecifier(uiSpecifierString);\n                pagesetName = uiSpecifier.pagesetName;\n                var uiElementName = uiSpecifier.elementName;\n            }\n            catch (e) {\n                return null;\n            }\n        }\n        try {\n            return this.pagesets[pagesetName].uiElements[uiElementName];\n        }\n        catch (e) {\n            return null;\n        }\n    };\n    \n    \n    \n    /**\n     * Returns a list of pagesets that \"contains\" the provided page,\n     * represented as a document object. Containership is defined by the\n     * Pageset object's contain() method.\n     *\n     * @param inDocument  the page to get pagesets for\n     * @return            a list of pagesets\n     */\n    this.getPagesetsForPage = function(inDocument)\n    {\n        var pagesets = [];\n        for (var pagesetName in this.pagesets) {\n            var pageset = this.pagesets[pagesetName];\n            if (pageset.contains(inDocument)) {\n                pagesets.push(pageset);\n            }\n        }\n        return pagesets;\n    };\n    \n    \n    \n    /**\n     * Returns a list of all pagesets.\n     *\n     * @return  a list of pagesets\n     */\n    this.getPagesets = function()\n    {\n        var pagesets = [];\n        for (var pagesetName in this.pagesets) {\n            pagesets.push(this.pagesets[pagesetName]);\n        }\n        return pagesets;\n    };\n    \n    \n    \n    /**\n     * Returns a list of elements on a page that a given UI specifier string,\n     * maps to. If no elements are mapped to, returns an empty list..\n     *\n     * @param   uiSpecifierString  a String that specifies a UI element with\n     *                             attendant argument values\n     * @param   inDocument         the document object the specified UI element\n     *                             appears in\n     * @return                     a potentially-empty list of elements\n     *                             specified by uiSpecifierString\n     */\n    this.getPageElements = function(uiSpecifierString, inDocument)\n    {\n        var locator = this.getLocator(uiSpecifierString);\n        var results = locator ? eval_locator(locator, inDocument) : [];\n        return results;\n    };\n    \n    \n    \n    /**\n     * Returns the locator string that a given UI specifier string maps to, or\n     * null if it cannot be mapped.\n     *\n     * @param uiSpecifierString\n     */\n    this.getLocator = function(uiSpecifierString)\n    {\n        try {\n            var uiSpecifier = new UISpecifier(uiSpecifierString);\n        }\n        catch (e) {\n            safe_alert('Could not create UISpecifier for string \"'\n                + uiSpecifierString + '\": ' + e.message);\n            return null;\n        }\n        \n        var uiElement = this.getUIElement(uiSpecifier.pagesetName,\n            uiSpecifier.elementName);\n        try {\n            return uiElement.getLocator(uiSpecifier.args);\n        }\n        catch (e) {\n            return null;\n        }\n    }\n    \n    \n    \n    /**\n     * Finds and returns a UI specifier string given an element and the page\n     * that it appears on.\n     *\n     * @param pageElement  the document element to map to a UI specifier\n     * @param inDocument   the document the element appears in\n     * @return             a UI specifier string, or false if one cannot be\n     *                     constructed\n     */\n    this.getUISpecifierString = function(pageElement, inDocument)\n    {\n        var is_fuzzy_match =\n            BrowserBot.prototype.locateElementByUIElement.is_fuzzy_match;\n        var pagesets = this.getPagesetsForPage(inDocument);\n        \n        for (var i = 0; i < pagesets.length; ++i) {\n            var pageset = pagesets[i];\n            var uiElements = pageset.getUIElements();\n            \n            for (var j = 0; j < uiElements.length; ++j) {\n                var uiElement = uiElements[j];\n                \n                // first test against the generic locator, if there is one.\n                // This should net some performance benefit when recording on\n                // more complicated pages.\n                if (uiElement.getGenericLocator) {\n                    var passedTest = false;\n                    var results =\n                        eval_locator(uiElement.getGenericLocator(), inDocument);\n                    for (var i = 0; i < results.length; ++i) {\n                        if (results[i] == pageElement) {\n                            passedTest = true;\n                            break;\n                        }\n                    }\n                    if (!passedTest) {\n                        continue;\n                    }\n                }\n                \n                var defaultLocators;\n                if (uiElement.isDefaultLocatorConstructionDeferred) {\n                    defaultLocators = uiElement.getDefaultLocators(inDocument);\n                }\n                else {\n                    defaultLocators = uiElement.defaultLocators;\n                }\n                \n                //safe_alert(print_r(uiElement.defaultLocators));\n                for (var locator in defaultLocators) {\n                    var locatedElements = eval_locator(locator, inDocument);\n                    if (locatedElements.length) {\n                        var locatedElement = locatedElements[0];\n                    }\n                    else {\n                        continue;\n                    }\n                    \n                    // use a heuristic to determine whether the element\n                    // specified is the \"same\" as the element we're matching\n                    if (is_fuzzy_match) {\n                        if (is_fuzzy_match(locatedElement, pageElement)) {\n                            return UI_GLOBAL.UI_PREFIX + '=' +\n                                new UISpecifier(pageset.name, uiElement.name,\n                                    defaultLocators[locator]);\n                        }\n                    }\n                    else {\n                        if (locatedElement == pageElement) {\n                            return UI_GLOBAL.UI_PREFIX + '=' +\n                                new UISpecifier(pageset.name, uiElement.name,\n                                    defaultLocators[locator]);\n                        }\n                    }\n                    \n                    // ok, matching the element failed. See if an offset\n                    // locator can complete the match.\n                    if (uiElement.getOffsetLocator) {\n                        for (var k = 0; k < locatedElements.length; ++k) {\n                            var offsetLocator = uiElement\n                                .getOffsetLocator(locatedElements[k], pageElement);\n                            if (offsetLocator) {\n                                return UI_GLOBAL.UI_PREFIX + '=' +\n                                    new UISpecifier(pageset.name,\n                                        uiElement.name,\n                                        defaultLocators[locator])\n                                    + '->' + offsetLocator;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        return false;\n    };\n    \n    \n    \n    /**\n     * Returns a sorted list of UI specifier string stubs representing possible\n     * UI elements for all pagesets, paired the their descriptions. Stubs\n     * contain all required arguments, but leave argument values blank.\n     *\n     * @return  a list of UI specifier string stubs\n     */\n    this.getUISpecifierStringStubs = function() {\n        var stubs = [];\n        var pagesets = this.getPagesets();\n        for (var i = 0; i < pagesets.length; ++i) {\n            stubs = stubs.concat(pagesets[i].getUISpecifierStringStubs());\n        }\n        stubs.sort(function(a, b) {\n            if (a[0] < b[0]) {\n                return -1;\n            }\n            return a[0] == b[0] ? 0 : 1;\n        });\n        return stubs;\n    }\n}",
    "label": 0
  },
  {
    "codes": "public Observable<Page<DenyAssignmentInner>> listByResourceGroupNextAsync(final String nextPageLink) {\n        return listByResourceGroupNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<DenyAssignmentInner>>, Page<DenyAssignmentInner>>() {\n                @Override\n                public Page<DenyAssignmentInner> call(ServiceResponse<Page<DenyAssignmentInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def register_request(self, valid_responses):\n        \"\"\"Register a RPC request.\n\n        :param list valid_responses: List of possible Responses that\n                                     we should be waiting for.\n        :return:\n        \"\"\"\n        uuid = str(uuid4())\n        self._response[uuid] = []\n        for action in valid_responses:\n            self._request[action] = uuid\n        return uuid",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"context\"\n\t\"log\"\n\t\"net/http\"\n\t\"time\"\n)\n\n// PredictionRequest represents a prediction task\ntype PredictionRequest struct {\n\tInputData string `json:\"input_data\"`\n}\n\n// PredictionResponse represents the result of a prediction\ntype PredictionResponse struct {\n\tOutputData string `json:\"output_data\"`\n}\n\n// Worker represents a GPU worker\ntype Worker struct {\n\tID int\n}\n\n// NewWorker creates a new Worker\nfunc NewWorker(id int) *Worker {\n\treturn &Worker{ID: id}\n}\n\n// Process performs prediction on GPU\nfunc (w *Worker) Process(ctx context.Context, req PredictionRequest) (PredictionResponse, error) {\n\t// Simulate GPU processing\n\ttime.Sleep(100 * time.Millisecond)\n\treturn PredictionResponse{OutputData: \"processed_\" + req.InputData}, nil\n}\n\n// WorkerPool manages a pool of workers\ntype WorkerPool struct {\n\tworkers []*Worker\n}\n\n// NewWorkerPool initializes a pool with a given number of workers\nfunc NewWorkerPool(workerCount int) *WorkerPool {\n\tworkers := make([]*Worker, workerCount)\n\tfor i := 0; i < workerCount; i++ {\n\t\tworkers[i] = NewWorker(i)\n\t}\n\treturn &WorkerPool{workers: workers}\n}\n\n// ProcessBatch dispatches prediction requests among available workers\nfunc (p *WorkerPool) ProcessBatch(ctx context.Context, requests []PredictionRequest) []PredictionResponse {\n\tresults := make([]PredictionResponse, len(requests))\n\tworkerCount := len(p.workers)\n\n\tfor i, req := range requests {\n\t\tworker := p.workers[i%workerCount]\n\t\tif res, err := worker.Process(ctx, req); err == nil {\n\t\t\tresults[i] = res\n\t\t}\n\t}\n\treturn results\n}\n\nfunc main() {\n\tpool := NewWorkerPool(4)\n\n\thttp.HandleFunc(\"/predict\", func(w http.ResponseWriter, r *http.Request) {\n\t\t// Simulating incoming batch requests\n\t\trequests := []PredictionRequest{\n\t\t\t{\"data1\"}, {\"data2\"}, {\"data3\"}, {\"data4\"},\n\t\t}\n\n\t\tctx, cancel := context.WithTimeout(r.Context(), time.Second)\n\t\tdefer cancel()\n\n\t\tresponses := pool.ProcessBatch(ctx, requests)\n\n\t\tfor _, res := range responses {\n\t\t\tlog.Println(res.OutputData)\n\t\t}\n\t\tw.WriteHeader(http.StatusOK)\n\t})\n\n\tlog.Fatal(http.ListenAndServe(\":8080\", nil))\n}\n```\n\n// This Go service includes a `Worker` representing a GPU worker to process prediction requests.\n// The `WorkerPool` manages multiple workers, distributing incoming prediction requests to be processed concurrently.\n// Prediction tasks are simulated as batch processing via an HTTP handler running on port 8080.",
    "label": 1
  },
  {
    "codes": "```java\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.springframework.stereotype.Component;\n\n@Aspect\n@Component\npublic class CircuitBreakerAspect {\n\n    private static final int MAX_FAILURES = 3; // Maximum allowed failures\n    private int failureCount = 0; // Current failure count\n    private boolean circuitOpen = false; // Circuit breaker state\n\n    @Around(\"execution(* com.example.service.*.*(..))\") // Pointcut for service methods\n    public Object circuitBreaker(ProceedingJoinPoint joinPoint) throws Throwable {\n        if (circuitOpen) {\n            throw new RuntimeException(\"Circuit is open. Service unavailable.\"); // Fail fast if circuit is open\n        }\n\n        try {\n            Object result = joinPoint.proceed(); // Proceed with the method execution\n            failureCount = 0; // Reset failure count on success\n            return result;\n        } catch (Exception e) {\n            failureCount++; // Increment failure count on exception\n            if (failureCount >= MAX_FAILURES) {\n                circuitOpen = true; // Open the circuit if max failures reached\n            }\n            throw e; // Re-throw the exception\n        }\n    }\n\n    // Method to manually reset the circuit breaker (optional)\n    public void resetCircuit() {\n        circuitOpen = false;\n        failureCount = 0;\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\n# Define the Generator\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, img_channels, feature_map_size):\n        super(Generator, self).__init__()\n        self.net = nn.Sequential(\n            # Input is latent vector (z)\n            nn.ConvTranspose2d(latent_dim, feature_map_size * 8, 4, 1, 0),\n            nn.BatchNorm2d(feature_map_size * 8),\n            nn.ReLU(True),\n            # State size: (feature_map_size*8) x 4 x 4\n            nn.ConvTranspose2d(feature_map_size * 8, feature_map_size * 4, 4, 2, 1),\n            nn.BatchNorm2d(feature_map_size * 4),\n            nn.ReLU(True),\n            # State size: (feature_map_size*4) x 8 x 8\n            nn.ConvTranspose2d(feature_map_size * 4, feature_map_size * 2, 4, 2, 1),\n            nn.BatchNorm2d(feature_map_size * 2),\n            nn.ReLU(True),\n            # State size: (feature_map_size*2) x 16 x 16\n            nn.ConvTranspose2d(feature_map_size * 2, img_channels, 4, 2, 1),\n            nn.Tanh()\n            # Output size: img_channels x 64 x 64\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# Define the Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self, img_channels, feature_map_size):\n        super(Discriminator, self).__init__()\n        self.net = nn.Sequential(\n            # Input size: img_channels x 64 x 64\n            nn.Conv2d(img_channels, feature_map_size * 2, 4, 2, 1),\n            nn.LeakyReLU(0.2, inplace=True),\n            # State size: (feature_map_size*2) x 32 x 32\n            nn.Conv2d(feature_map_size * 2, feature_map_size * 4, 4, 2, 1),\n            nn.BatchNorm2d(feature_map_size * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # State size: (feature_map_size*4) x 16 x 16\n            nn.Conv2d(feature_map_size * 4, feature_map_size * 8, 4, 2, 1),\n            nn.BatchNorm2d(feature_map_size * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # State size: (feature_map_size*8) x 8 x 8\n            nn.Conv2d(feature_map_size * 8, 1, 4, 1, 0),\n            nn.Sigmoid()\n            # Output size: 1 (probability)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# Hyperparameters\nlatent_dim = 100\nimg_channels = 3\nfeature_map_size = 64\nbatch_size = 128\nepochs = 50\nlr = 0.0002\nbeta1 = 0.5\n\n# Initialize Generator and Discriminator\ngenerator = Generator(latent_dim, img_channels, feature_map_size)\ndiscriminator = Discriminator(img_channels, feature_map_size)\n\n# Optimizers\noptimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n\n# Loss function\ncriterion = nn.BCELoss()\n\n# Data loading\ntransform = transforms.Compose([\n    transforms.Resize(64),\n    transforms.CenterCrop(64),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndataset = datasets.ImageFolder(root='path_to_face_dataset', transform=transform)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Training loop\nfor epoch in range(epochs):\n    for i, (real_images, _) in enumerate(dataloader):\n        # Adversarial ground truths\n        real_labels = torch.ones(real_images.size(0), 1)\n        fake_labels = torch.zeros(real_images.size(0), 1)\n\n        # Train Discriminator\n        optimizer_D.zero_grad()\n        # Real images\n        real_output = discriminator(real_images)\n        d_loss_real = criterion(real_output, real_labels)\n        # Fake images\n        z = torch.randn(real_images.size(0), latent_dim, 1, 1)\n        fake_images = generator(z)\n        fake_output = discriminator(fake_images.detach())\n        d_loss_fake = criterion(fake_output, fake_labels)\n        # Total loss\n        d_loss = d_loss_real + d_loss_fake\n        d_loss.backward()\n        optimizer_D.step()\n\n        # Train Generator\n        optimizer_G.zero_grad()\n        fake_output = discriminator(fake_images)\n        g_loss = criterion(fake_output, real_labels)\n        g_loss.backward()\n        optimizer_G.step()\n\n        # Print losses\n        if i % 100 == 0:\n            print(f'Epoch [{epoch}/{epochs}] Batch {i}/{len(dataloader)} '\n                  f'Loss_D: {d_loss.item():.4f} Loss_G: {g_loss.item():.4f}')\n\n    # Save generated images\n    with torch.no_grad():\n        fake = generator(torch.randn(64, latent_dim, 1, 1)).detach().cpu()\n        plt.figure(figsize=(8, 8))\n        plt.imshow(torchvision.utils.make_grid(fake, padding=2, normalize=True).permute(1, 2, 0))\n        plt.axis('off')\n        plt.show()\n\n# Save models\ntorch.save(generator.state_dict(), 'generator.pth')\ntorch.save(discriminator.state_dict(), 'discriminator.pth')\n```",
    "label": 1
  },
  {
    "codes": "func (s *ListPublicKeysInput) SetNextToken(v string) *ListPublicKeysInput {\n\ts.NextToken = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function setText(element, text) {\n    if (element.textContent != null) {\n        element.textContent = text;\n    } else if (element.innerText != null) {\n        element.innerText = text;\n    }\n}",
    "label": 0
  },
  {
    "codes": "func (s *GetTableVersionInput) SetCatalogId(v string) *GetTableVersionInput {\n\ts.CatalogId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```javascript\n// Define a class to represent a 2D vector\nclass Vector2D {\n    constructor(x, y) {\n        this.x = x; // x-component of the vector\n        this.y = y; // y-component of the vector\n    }\n\n    // Method to add another vector to this vector\n    add(otherVector) {\n        return new Vector2D(this.x + otherVector.x, this.y + otherVector.y);\n    }\n\n    // Method to calculate the magnitude (length) of the vector\n    magnitude() {\n        return Math.sqrt(this.x * this.x + this.y * this.y);\n    }\n}\n\n// Example usage:\n// const v1 = new Vector2D(3, 4);\n// const v2 = new Vector2D(1, 2);\n// const v3 = v1.add(v2);\n// console.log(v3.magnitude()); // Output: 7.0710678118654755\n```",
    "label": 1
  },
  {
    "codes": "func (s *CrawlerMetrics) SetLastRuntimeSeconds(v float64) *CrawlerMetrics {\n\ts.LastRuntimeSeconds = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```verilog\n// PWM Generator Module\nmodule pwm_generator #(\n    parameter WIDTH = 8 // Bit width for PWM resolution\n)(\n    input wire clk,          // Clock signal\n    input wire rst,          // Reset signal\n    input wire [WIDTH-1:0] duty_cycle, // Duty cycle input\n    output reg pwm_out      // PWM output signal\n);\n\n    reg [WIDTH-1:0] counter; // Counter for PWM generation\n\n    always @(posedge clk or posedge rst) begin\n        if (rst) begin\n            counter <= 0;       // Reset counter\n            pwm_out <= 0;       // Reset PWM output\n        end else begin\n            counter <= counter + 1; // Increment counter\n            if (counter < duty_cycle) begin\n                pwm_out <= 1;   // Set PWM high\n            end else begin\n                pwm_out <= 0;   // Set PWM low\n            end\n        end\n    end\n\nendmodule\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport plotly.express as px\nimport pandas as pd\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom rdkit.Chem import Draw\nfrom rdkit.Geometry import Point3D\n\n# Function to extract atomic positions and types from a molecule\ndef get_atom_positions(mol):\n    atoms_data = []\n    AllChem.EmbedMolecule(mol, randomSeed=42)  # Generate 3D coordinates\n    conf = mol.GetConformer()\n    for atom in mol.GetAtoms():\n        pos = conf.GetAtomPosition(atom.GetIdx())\n        atoms_data.append([atom.GetSymbol(), pos.x, pos.y, pos.z])\n    return pd.DataFrame(atoms_data, columns=[\"Element\", \"X\", \"Y\", \"Z\"])\n\n# Define a simple molecule using SMILES (e.g., \"CCO\" for ethanol)\nsmiles = \"CCO\"\nmolecule = Chem.MolFromSmiles(smiles)\natom_df = get_atom_positions(molecule)\n\n# Create a 3D scatter plot using Plotly Express\nfig = px.scatter_3d(atom_df, x='X', y='Y', z='Z', color='Element', symbol='Element',\n                     title='3D Molecular Structure', labels={'X': 'X', 'Y': 'Y', 'Z': 'Z'})\n\n# Add bonds by connecting atoms with lines\nbond_indices = [(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()) for bond in molecule.GetBonds()]\nfor start, end in bond_indices:\n    x_coords = [atom_df.iloc[start].X, atom_df.iloc[end].X]\n    y_coords = [atom_df.iloc[start].Y, atom_df.iloc[end].Y]\n    z_coords = [atom_df.iloc[start].Z, atom_df.iloc[end].Z]\n    fig.add_trace(px.line_3d(x=x_coords, y=y_coords, z=z_coords, line=dict(color='gray')).data[0])\n\n# Show the figure\nfig.show()\n```\n",
    "label": 1
  },
  {
    "codes": "func (s *UploadArchiveInput) SetAccountId(v string) *UploadArchiveInput {\n\ts.AccountId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "def _get_set(self, key, operation, create=False):\n        \"\"\"\n        Get (and maybe create) a set by name.\n        \"\"\"\n        return self._get_by_type(key, operation, create, b'set', set())",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define the log directory and archive location\nLOG_DIR=\"/path/to/logs\"\nARCHIVE_DIR=\"/path/to/archive\"\n\n# Create the archive directory if it doesn't exist\nmkdir -p \"$ARCHIVE_DIR\"\n\n# Find log files older than 30 days and move them to the archive directory\nfind \"$LOG_DIR\" -type f -name \"*.log\" -mtime +30 -exec mv {} \"$ARCHIVE_DIR\" \\;\n\n# Optional: Compress the archived logs\ntar -czf \"$ARCHIVE_DIR/logs_$(date +%Y%m%d).tar.gz\" -C \"$ARCHIVE_DIR\" .\n\n# Optional: Remove the original uncompressed archived logs after compressing\nfind \"$ARCHIVE_DIR\" -type f -name \"*.log\" -mtime +30 -delete\n```",
    "label": 1
  },
  {
    "codes": "```typescript\nfunction shuffleArray<T>(array: T[]): void {\n    // Start from the last element and swap one by one. We don't need to run for the first element.\n    for (let i = array.length - 1; i > 0; i--) {\n        // Pick a random index from 0 to i\n        const j = Math.floor(Math.random() * (i + 1));\n\n        // Swap the current element with the element at the random index\n        [array[i], array[j]] = [array[j], array[i]];\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *SecurityConfiguration) SetCreatedTimeStamp(v time.Time) *SecurityConfiguration {\n\ts.CreatedTimeStamp = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport time\nimport functools\n\ndef measure_time_ns(func):\n    \"\"\"Decorator to measure the execution time of a function in nanoseconds.\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.perf_counter_ns()  # Start time in nanoseconds\n        result = func(*args, **kwargs)\n        end_time = time.perf_counter_ns()    # End time in nanoseconds\n        execution_time = end_time - start_time  # Calculate execution time\n        print(f\"Function {func.__name__} executed in {execution_time} nanoseconds\")\n        return result\n\n    return wrapper\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <stdexcept>\n\nclass DatabaseConnection {\npublic:\n    // Constructor: Establishes the database connection\n    DatabaseConnection(const std::string& connectionString) {\n        std::cout << \"Connecting to database: \" << connectionString << std::endl;\n        // Simulate connection logic\n        if (connectionString.empty()) {\n            throw std::runtime_error(\"Invalid connection string\");\n        }\n        // Assume connection is successful\n        isConnected = true;\n    }\n\n    // Destructor: Ensures the connection is closed\n    ~DatabaseConnection() {\n        if (isConnected) {\n            std::cout << \"Closing database connection\" << std::endl;\n            // Simulate disconnection logic\n            isConnected = false;\n        }\n    }\n\n    // Execute a query (example method)\n    void executeQuery(const std::string& query) {\n        if (!isConnected) {\n            throw std::runtime_error(\"Database not connected\");\n        }\n        std::cout << \"Executing query: \" << query << std::endl;\n    }\n\n    // Disable copy constructor and assignment operator\n    DatabaseConnection(const DatabaseConnection&) = delete;\n    DatabaseConnection& operator=(const DatabaseConnection&) = delete;\n\nprivate:\n    bool isConnected = false; // Tracks connection status\n};\n\nint main() {\n    try {\n        DatabaseConnection db(\"valid_connection_string\");\n        db.executeQuery(\"SELECT * FROM users\");\n    } catch (const std::exception& e) {\n        std::cerr << \"Error: \" << e.what() << std::endl;\n    }\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport psutil\nimport time\n\n# Function to log messages to a file\ndef log_to_file(message):\n    with open(\"cpu_anomalies.log\", \"a\") as log_file:\n        log_file.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - {message}\\n\")\n\n# Define what is considered as an anomaly\nCPU_USAGE_THRESHOLD = 85.0  # in percentage\n\nwhile True:\n    # Get the current CPU usage\n    current_cpu_usage = psutil.cpu_percent(interval=1)\n    \n    # Check if the current usage exceeds the threshold\n    if current_cpu_usage > CPU_USAGE_THRESHOLD:\n        anomaly_message = f\"Anomaly detected: CPU usage at {current_cpu_usage}%\"\n        \n        # Log the anomaly to a file\n        log_to_file(anomaly_message)\n    \n    # Pause before the next check\n    time.sleep(5)\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<Void> beginDeleteAsync(String locationName, String longTermRetentionServerName, String longTermRetentionDatabaseName, String backupName, final ServiceCallback<Void> serviceCallback) {\n        return ServiceFuture.fromResponse(beginDeleteWithServiceResponseAsync(locationName, longTermRetentionServerName, longTermRetentionDatabaseName, backupName), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "function xpathCollectDescendants(nodelist, node, opt_tagName) {\n  if (opt_tagName && node.getElementsByTagName) {\n    copyArray(nodelist, node.getElementsByTagName(opt_tagName));\n    return;\n  }\n  for (var n = node.firstChild; n; n = n.nextSibling) {\n    nodelist.push(n);\n    xpathCollectDescendants(nodelist, n);\n  }\n}",
    "label": 0
  },
  {
    "codes": "def validate(self, *args):\n        \"\"\"Validate contents of class\n        \"\"\"\n        super(self.__class__, self).validate(*args)\n        if not self.name:\n            raise ValidationError('name is required for Data')",
    "label": 0
  },
  {
    "codes": "```R\n# Load necessary libraries\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(plotly)\n\n# Load COVID-19 vaccination data\nvaccination_data <- read.csv(\"covid_vaccination_data.csv\")\n\n# Define UI for the Shiny app\nui <- fluidPage(\n  titlePanel(\"COVID-19 Vaccination Rates by Country\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"country\", \"Select Country:\", choices = unique(vaccination_data$Country)),\n      dateRangeInput(\"date_range\", \"Select Date Range:\", start = min(vaccination_data$Date), end = max(vaccination_data$Date))\n    ),\n    \n    mainPanel(\n      plotlyOutput(\"vaccination_plot\"),\n      dataTableOutput(\"vaccination_table\")\n    )\n  )\n)\n\n# Define server logic for the Shiny app\nserver <- function(input, output) {\n  \n  # Filter data based on user input\n  filtered_data <- reactive({\n    vaccination_data %>%\n      filter(Country == input$country,\n             Date >= input$date_range[1] & Date <= input$date_range[2])\n  })\n  \n  # Render the vaccination plot\n  output$vaccination_plot <- renderPlotly({\n    ggplot(filtered_data(), aes(x = Date, y = VaccinationRate)) +\n      geom_line(color = \"blue\") +\n      labs(title = paste(\"Vaccination Rate in\", input$country),\n           x = \"Date\",\n           y = \"Vaccination Rate (%)\") +\n      theme_minimal()\n  })\n  \n  # Render the vaccination table\n  output$vaccination_table <- renderDataTable({\n    filtered_data()\n  })\n}\n\n# Run the Shiny app\nshinyApp(ui = ui, server = server)\n```\n\n### Comments:\n- The code above creates a Shiny dashboard to explore COVID-19 vaccination rates by country.\n- Users can select a country and a date range to view the vaccination rates over time.\n- The dashboard includes an interactive plot and a data table displaying the filtered data.\n- The `plotly` package is used to make the plot interactive.\n- The `dplyr` package is used for data manipulation and filtering.",
    "label": 1
  },
  {
    "codes": "```java\nimport ddf.minim.*;\nimport ddf.minim.analysis.*;\n\nMinim minim;\nAudioInput in;\nFFT fft;\n\nvoid setup() {\n  size(1024, 512);\n  minim = new Minim(this);\n  \n  // Using getLineIn to receive real-time audio input\n  in = minim.getLineIn(Minim.STEREO, 1024);\n  \n  // Create an FFT object for analyzing the frequency spectrum\n  fft = new FFT(in.bufferSize(), in.sampleRate());\n}\n\nvoid draw() {\n  background(0);\n  \n  // Perform a forward FFT to analyze the frequency spectrum\n  fft.forward(in.mix);\n  \n  // Visualize the frequency spectrum\n  for (int i = 0; i < fft.specSize(); i++) {\n    float x = map(i, 0, fft.specSize(), 0, width);\n    float h = -fft.getBand(i) * 4; // Scale the height for better visualization\n    line(x, height, x, height + h);\n  }\n}\n\n// Don't forget to clean up\nvoid stop() {\n  in.close();\n  minim.stop();\n  super.stop();\n}\n```",
    "label": 1
  },
  {
    "codes": "def from_events(self, instance, ev_args, ctx):\n        \"\"\"\n        Detect the object to instanciate from the arguments `ev_args` of the\n        ``\"start\"`` event. The new object is stored at the corresponding\n        descriptor attribute on `instance`.\n\n        This method is suspendable.\n        \"\"\"\n        obj = yield from self._process(instance, ev_args, ctx)\n        self.__set__(instance, obj)\n        return obj",
    "label": 0
  },
  {
    "codes": "function getDatasetModel(seriesModel) {\n    var option = seriesModel.option;\n    // Caution: consider the scenario:\n    // A dataset is declared and a series is not expected to use the dataset,\n    // and at the beginning `setOption({series: { noData })` (just prepare other\n    // option but no data), then `setOption({series: {data: [...]}); In this case,\n    // the user should set an empty array to avoid that dataset is used by default.\n    var thisData = option.data;\n    if (!thisData) {\n        return seriesModel.ecModel.getComponent('dataset', option.datasetIndex || 0);\n    }\n}",
    "label": 0
  },
  {
    "codes": "function addonsManager_open(aSpec) {\n    var spec = aSpec || { };\n    var type = (spec.type == undefined) ? \"menu\" : spec.type;\n    var waitFor = (spec.waitFor == undefined) ? true : spec.waitFor;\n\n    switch (type) {\n      case \"menu\":\n        var menuItem = new elementslib.Elem(this._controller.\n                                            menus[\"tools-menu\"].menu_openAddons);\n        this._controller.click(menuItem);\n        break;\n      case \"shortcut\":\n        var cmdKey = utils.getEntity(this.dtds, \"addons.commandkey\");\n        this._controller.keypress(null, cmdKey, {accelKey: true, shiftKey: true});\n        break;\n      default:\n        throw new Error(arguments.callee.name + \": Unknown event type - \" +\n                        event.type);\n    }\n\n    return waitFor ? this.waitForOpened() : null;\n  }",
    "label": 0
  },
  {
    "codes": "```rust\n// Import necessary crates\nuse embedded_hal::blocking::i2c::{Write, WriteRead};\nuse stm32f4xx_hal::i2c::I2c;\nuse stm32f4xx_hal::pac::I2C1;\n\n// Define IMU I2C address\nconst IMU_I2C_ADDRESS: u8 = 0x68;\n\n// Define IMU registers\nconst IMU_WHO_AM_I: u8 = 0x75;\nconst IMU_ACCEL_XOUT_H: u8 = 0x3B;\nconst IMU_GYRO_XOUT_H: u8 = 0x43;\n\n// Define IMU driver structure\npub struct Imu<I2C> {\n    i2c: I2C,\n}\n\nimpl<I2C, E> Imu<I2C>\nwhere\n    I2C: Write<Error = E> + WriteRead<Error = E>,\n{\n    // Initialize IMU driver\n    pub fn new(i2c: I2C) -> Self {\n        Imu { i2c }\n    }\n\n    // Read WHO_AM_I register to verify IMU connection\n    pub fn who_am_i(&mut self) -> Result<u8, E> {\n        let mut buffer = [0u8; 1];\n        self.i2c\n            .write_read(IMU_I2C_ADDRESS, &[IMU_WHO_AM_I], &mut buffer)?;\n        Ok(buffer[0])\n    }\n\n    // Read accelerometer data\n    pub fn read_accel(&mut self) -> Result<(i16, i16, i16), E> {\n        let mut buffer = [0u8; 6];\n        self.i2c\n            .write_read(IMU_I2C_ADDRESS, &[IMU_ACCEL_XOUT_H], &mut buffer)?;\n\n        let accel_x = ((buffer[0] as i16) << 8) | (buffer[1] as i16);\n        let accel_y = ((buffer[2] as i16) << 8) | (buffer[3] as i16);\n        let accel_z = ((buffer[4] as i16) << 8) | (buffer[5] as i16);\n\n        Ok((accel_x, accel_y, accel_z))\n    }\n\n    // Read gyroscope data\n    pub fn read_gyro(&mut self) -> Result<(i16, i16, i16), E> {\n        let mut buffer = [0u8; 6];\n        self.i2c\n            .write_read(IMU_I2C_ADDRESS, &[IMU_GYRO_XOUT_H], &mut buffer)?;\n\n        let gyro_x = ((buffer[0] as i16) << 8) | (buffer[1] as i16);\n        let gyro_y = ((buffer[2] as i16) << 8) | (buffer[3] as i16);\n        let gyro_z = ((buffer[4] as i16) << 8) | (buffer[5] as i16);\n\n        Ok((gyro_x, gyro_y, gyro_z))\n    }\n}\n\n// Example usage with STM32F4xx HAL\nfn main() {\n    let dp = stm32f4xx_hal::pac::Peripherals::take().unwrap();\n    let rcc = dp.RCC.constrain();\n    let clocks = rcc.cfgr.freeze();\n\n    let gpiob = dp.GPIOB.split();\n    let scl = gpiob.pb6.into_alternate_af4().set_open_drain();\n    let sda = gpiob.pb7.into_alternate_af4().set_open_drain();\n\n    let i2c = I2c::i2c1(dp.I2C1, (scl, sda), 400.khz(), clocks);\n\n    let mut imu = Imu::new(i2c);\n\n    // Verify IMU connection\n    let who_am_i = imu.who_am_i().unwrap();\n    println!(\"WHO_AM_I: {}\", who_am_i);\n\n    // Read accelerometer data\n    let (accel_x, accel_y, accel_z) = imu.read_accel().unwrap();\n    println!(\"Accel: X: {}, Y: {}, Z: {}\", accel_x, accel_y, accel_z);\n\n    // Read gyroscope data\n    let (gyro_x, gyro_y, gyro_z) = imu.read_gyro().unwrap();\n    println!(\"Gyro: X: {}, Y: {}, Z: {}\", gyro_x, gyro_y, gyro_z);\n}\n```",
    "label": 1
  },
  {
    "codes": "function (dimensions, hostModel) {\n\n    dimensions = dimensions || ['x', 'y'];\n\n    var dimensionInfos = {};\n    var dimensionNames = [];\n    var invertedIndicesMap = {};\n\n    for (var i = 0; i < dimensions.length; i++) {\n        // Use the original dimensions[i], where other flag props may exists.\n        var dimensionInfo = dimensions[i];\n\n        if (zrUtil.isString(dimensionInfo)) {\n            dimensionInfo = {name: dimensionInfo};\n        }\n\n        var dimensionName = dimensionInfo.name;\n        dimensionInfo.type = dimensionInfo.type || 'float';\n        if (!dimensionInfo.coordDim) {\n            dimensionInfo.coordDim = dimensionName;\n            dimensionInfo.coordDimIndex = 0;\n        }\n\n        dimensionInfo.otherDims = dimensionInfo.otherDims || {};\n        dimensionNames.push(dimensionName);\n        dimensionInfos[dimensionName] = dimensionInfo;\n\n        dimensionInfo.index = i;\n\n        if (dimensionInfo.createInvertedIndices) {\n            invertedIndicesMap[dimensionName] = [];\n        }\n    }\n\n    /**\n     * @readOnly\n     * @type {Array.<string>}\n     */\n    this.dimensions = dimensionNames;\n\n    /**\n     * Infomation of each data dimension, like data type.\n     * @type {Object}\n     */\n    this._dimensionInfos = dimensionInfos;\n\n    /**\n     * @type {module:echarts/model/Model}\n     */\n    this.hostModel = hostModel;\n\n    /**\n     * @type {module:echarts/model/Model}\n     */\n    this.dataType;\n\n    /**\n     * Indices stores the indices of data subset after filtered.\n     * This data subset will be used in chart.\n     * @type {Array.<number>}\n     * @readOnly\n     */\n    this._indices = null;\n\n    this._count = 0;\n    this._rawCount = 0;\n\n    /**\n     * Data storage\n     * @type {Object.<key, Array.<TypedArray|Array>>}\n     * @private\n     */\n    this._storage = {};\n\n    /**\n     * @type {Array.<string>}\n     */\n    this._nameList = [];\n    /**\n     * @type {Array.<string>}\n     */\n    this._idList = [];\n\n    /**\n     * Models of data option is stored sparse for optimizing memory cost\n     * @type {Array.<module:echarts/model/Model>}\n     * @private\n     */\n    this._optionModels = [];\n\n    /**\n     * Global visual properties after visual coding\n     * @type {Object}\n     * @private\n     */\n    this._visual = {};\n\n    /**\n     * Globel layout properties.\n     * @type {Object}\n     * @private\n     */\n    this._layout = {};\n\n    /**\n     * Item visual properties after visual coding\n     * @type {Array.<Object>}\n     * @private\n     */\n    this._itemVisuals = [];\n\n    /**\n     * Key: visual type, Value: boolean\n     * @type {Object}\n     * @readOnly\n     */\n    this.hasItemVisual = {};\n\n    /**\n     * Item layout properties after layout\n     * @type {Array.<Object>}\n     * @private\n     */\n    this._itemLayouts = [];\n\n    /**\n     * Graphic elemnents\n     * @type {Array.<module:zrender/Element>}\n     * @private\n     */\n    this._graphicEls = [];\n\n    /**\n     * Max size of each chunk.\n     * @type {number}\n     * @private\n     */\n    this._chunkSize = 1e5;\n\n    /**\n     * @type {number}\n     * @private\n     */\n    this._chunkCount = 0;\n\n    /**\n     * @type {Array.<Array|Object>}\n     * @private\n     */\n    this._rawData;\n\n    /**\n     * Raw extent will not be cloned, but only transfered.\n     * It will not be calculated util needed.\n     * key: dim,\n     * value: {end: number, extent: Array.<number>}\n     * @type {Object}\n     * @private\n     */\n    this._rawExtent = {};\n\n    /**\n     * @type {Object}\n     * @private\n     */\n    this._extent = {};\n\n    /**\n     * key: dim\n     * value: extent\n     * @type {Object}\n     * @private\n     */\n    this._approximateExtent = {};\n\n    /**\n     * Cache summary info for fast visit. See \"dimensionHelper\".\n     * @type {Object}\n     * @private\n     */\n    this._dimensionsSummary = summarizeDimensions(this);\n\n    /**\n     * @type {Object.<Array|TypedArray>}\n     * @private\n     */\n    this._invertedIndicesMap = invertedIndicesMap;\n\n    /**\n     * @type {Object}\n     * @private\n     */\n    this._calculationInfo = {};\n}",
    "label": 0
  },
  {
    "codes": "```javascript\n// MongoDB Aggregation Pipeline to Calculate Monthly Sales Trends\n\ndb.sales.aggregate([\n    // Stage 1: Extract year and month from the date field\n    {\n        $project: {\n            year: { $year: \"$date\" },\n            month: { $month: \"$date\" },\n            amount: 1\n        }\n    },\n    // Stage 2: Group by year and month, summing up the sales amount\n    {\n        $group: {\n            _id: { year: \"$year\", month: \"$month\" },\n            totalSales: { $sum: \"$amount\" }\n        }\n    },\n    // Stage 3: Sort by year and month for chronological order\n    {\n        $sort: { \"_id.year\": 1, \"_id.month\": 1 }\n    },\n    // Stage 4: Project the final output format\n    {\n        $project: {\n            _id: 0,\n            year: \"$_id.year\",\n            month: \"$_id.month\",\n            totalSales: 1\n        }\n    }\n]);\n```",
    "label": 1
  },
  {
    "codes": "func (s *EncryptionConfiguration) SetCloudWatchEncryption(v *CloudWatchEncryption) *EncryptionConfiguration {\n\ts.CloudWatchEncryption = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <Servo.h>\n#include <SoftwareSerial.h>\n\n// Define pins for Bluetooth module\nconst int bluetoothTx = 2;\nconst int bluetoothRx = 3;\n\n// Create a software serial port\nSoftwareSerial bluetooth(bluetoothTx, bluetoothRx);\n\n// Create a Servo object\nServo myServo;\n\n// Pin for the servo motor\nconst int servoPin = 9;\n\n// Variables to hold position data from Bluetooth\nchar receivedChar;\n\nvoid setup() {\n  // Attach the servo to the defined pin\n  myServo.attach(servoPin);\n\n  // Initialize serial communication for monitoring\n  Serial.begin(9600);\n\n  // Initialize Bluetooth communication\n  bluetooth.begin(9600);\n\n  // Print a message to indicate setup is done\n  Serial.println(\"Bluetooth controlled Servo ready.\");\n}\n\nvoid loop() {\n  // Check if data is available on Bluetooth\n  if (bluetooth.available()) {\n    // Read the incoming byte\n    receivedChar = bluetooth.read();\n\n    // To monitor received data\n    Serial.print(\"Received: \");\n    Serial.println(receivedChar);\n\n    // Check if the received char is a valid position (0-9)\n    if (receivedChar >= '0' && receivedChar <= '9') {\n      // Map the received char to a servo position from 0 to 180\n      int position = map(receivedChar - '0', 0, 9, 0, 180);\n\n      // Move the servo to the mapped position\n      myServo.write(position);\n      \n      // Feedback for position set\n      Serial.print(\"Servo position set to: \");\n      Serial.println(position);\n    }\n  }\n  // Short delay for stability\n  delay(50);\n}\n```",
    "label": 1
  },
  {
    "codes": "```c\n#include <stdio.h>\n#include <string.h>\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n\n#define AES_256_KEY_LENGTH 32\n#define AES_GCM_IV_LENGTH 12\n#define AES_GCM_TAG_LENGTH 16\n\n// Function to perform AES-GCM encryption\nint aes_gcm_encrypt(unsigned char *plaintext, int plaintext_len,\n                    unsigned char *key, unsigned char *iv,\n                    unsigned char *ciphertext, unsigned char *tag) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int ciphertext_len;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        return -1;\n    }\n\n    // Initialize encryption operation\n    if (EVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL) != 1) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Set IV length\n    if (EVP_EncryptInit_ex(ctx, NULL, NULL, key, iv) != 1) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Provide the message to be encrypted, and receive the ciphertext output\n    if (EVP_EncryptUpdate(ctx, ciphertext, &len, plaintext, plaintext_len) != 1) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    ciphertext_len = len;\n\n    // Finalize the encryption\n    if (EVP_EncryptFinal_ex(ctx, ciphertext + len, &len) != 1) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    ciphertext_len += len;\n\n    // Get the tag\n    if (EVP_GCM_TLS_GET_TAG(ctx, tag) != 1) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Cleanup\n    EVP_CIPHER_CTX_free(ctx);\n\n    return ciphertext_len;\n}\n\n// Function to perform AES-GCM decryption\nint aes_gcm_decrypt(unsigned char *ciphertext, int ciphertext_len,\n                    unsigned char *key, unsigned char *iv,\n                    unsigned char *tag, unsigned char *plaintext) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int plaintext_len;\n    int ret;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        return -1;\n    }\n\n    // Initialize decryption operation\n    if (EVP_DecryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL) != 1) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Set IV length\n    if (EVP_DecryptInit_ex(ctx, NULL, NULL, key, iv) != 1) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Provide the message to be decrypted, and receive the plaintext output\n    if (EVP_DecryptUpdate(ctx, plaintext, &len, ciphertext, ciphertext_len) != 1) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    plaintext_len = len;\n\n    // Set the expected tag value\n    if (EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_TAG, AES_GCM_TAG_LENGTH, tag) != 1) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Finalize the decryption\n    ret = EVP_DecryptFinal_ex(ctx, plaintext + len, &len);\n    // Cleanup\n    EVP_CIPHER_CTX_free(ctx);\n\n    if (ret > 0) {\n        plaintext_len += len; // Successful decryption\n        return plaintext_len;\n    } else {\n        return -1; // Decryption failed\n    }\n}\n\n// Example usage\nint main() {\n    unsigned char key[AES_256_KEY_LENGTH];\n    unsigned char iv[AES_GCM_IV_LENGTH];\n    unsigned char plaintext[] = \"Hello, World!\";\n    unsigned char ciphertext[128];\n    unsigned char tag[AES_GCM_TAG_LENGTH];\n    unsigned char decryptedtext[128];\n\n    // Generate a random key and IV\n    RAND_bytes(key, sizeof(key));\n    RAND_bytes(iv, sizeof(iv));\n\n    // Encrypt\n    int ciphertext_len = aes_gcm_encrypt(plaintext, strlen((char *)plaintext), key, iv, ciphertext, tag);\n    \n    // Decrypt\n    int decryptedtext_len = aes_gcm_decrypt(ciphertext, ciphertext_len, key, iv, tag, decryptedtext);\n    decryptedtext[decryptedtext_len] = '\\0'; // Null-terminate the decrypted text\n\n    // Print results\n    printf(\"Ciphertext Length: %d\\n\", ciphertext_len);\n    printf(\"Decrypted Text: %s\\n\", decryptedtext);\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nfrom machine import Pin, I2C\nfrom time import sleep\n\n# Initialize I2C interface for ESP32\ni2c = I2C(0, scl=Pin(22), sda=Pin(21), freq=100000)\n\n# Sensor I2C address\nsensor_address = 0x48  # Common default for temperature sensors like TMP102\n\ndef read_temperature():\n    # Read two bytes of temperature data\n    raw_data = i2c.readfrom_mem(sensor_address, 0x00, 2)\n    \n    # Convert the data to a temperature value\n    temperature = (raw_data[0] << 8 | raw_data[1]) >> 4\n    \n    # Convert to Celsius if needed\n    if temperature > 0x7FF:\n        temperature -= 4096\n    \n    # Scale to actual temperature value\n    temperature_celsius = temperature * 0.0625\n    return temperature_celsius\n\nwhile True:\n    temp = read_temperature()\n    print(\"Temperature:\", temp, \"C\")\n    sleep(2)\n```",
    "label": 1
  },
  {
    "codes": "func (s *PutDataCatalogEncryptionSettingsInput) SetCatalogId(v string) *PutDataCatalogEncryptionSettingsInput {\n\ts.CatalogId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (c *Glue) ListTriggersPagesWithContext(ctx aws.Context, input *ListTriggersInput, fn func(*ListTriggersOutput, bool) bool, opts ...request.Option) error {\n\tp := request.Pagination{\n\t\tNewRequest: func() (*request.Request, error) {\n\t\t\tvar inCpy *ListTriggersInput\n\t\t\tif input != nil {\n\t\t\t\ttmp := *input\n\t\t\t\tinCpy = &tmp\n\t\t\t}\n\t\t\treq, _ := c.ListTriggersRequest(inCpy)\n\t\t\treq.SetContext(ctx)\n\t\t\treq.ApplyOptions(opts...)\n\t\t\treturn req, nil\n\t\t},\n\t}\n\n\tcont := true\n\tfor p.Next() && cont {\n\t\tcont = fn(p.Page().(*ListTriggersOutput), !p.HasNextPage())\n\t}\n\treturn p.Err()\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"fmt\"\n    \"net\"\n    \"sync\"\n)\n\nfunc handleConnection(conn net.Conn, wg *sync.WaitGroup) {\n    defer wg.Done()\n    defer conn.Close()\n\n    fmt.Println(\"Client connected:\", conn.RemoteAddr())\n    buffer := make([]byte, 1024)\n    \n    for {\n        n, err := conn.Read(buffer)\n        if err != nil {\n            fmt.Println(\"Error reading:\", err)\n            return\n        }\n        fmt.Printf(\"Received: %s\", string(buffer[:n]))\n        \n        _, err = conn.Write(buffer[:n]) // Echo back the received message\n        if err != nil {\n            fmt.Println(\"Error writing:\", err)\n            return\n        }\n    }\n}\n\nfunc main() {\n    listener, err := net.Listen(\"tcp\", \":8080\")\n    if err != nil {\n        fmt.Println(\"Error starting server:\", err)\n        return\n    }\n    defer listener.Close()\n\n    fmt.Println(\"Server is listening on port 8080...\")\n    \n    var wg sync.WaitGroup\n    for {\n        conn, err := listener.Accept()\n        if err != nil {\n            fmt.Println(\"Error accepting connection:\", err)\n            continue\n        }\n\n        wg.Add(1)\n        go handleConnection(conn, &wg) // Handle each client connection in a goroutine\n    }\n\n    wg.Wait() // Wait for all goroutines to finish\n}\n```",
    "label": 1
  },
  {
    "codes": "```verilog\n// PWM Generator Module with adjustable duty cycle and frequency\nmodule pwm_generator #(\n    parameter CLK_FREQ = 50000000, // Clock frequency in Hz\n    parameter PWM_FREQ = 1000      // Desired PWM frequency in Hz\n) (\n    input wire clk,                // Clock input\n    input wire rst,                // Active-high reset\n    input wire [7:0] duty_cycle,   // Duty cycle input (0-255, 255 = 100%)\n    output reg pwm_out             // PWM output signal\n);\n\n    // Calculate the counter limit based on clock and PWM frequency\n    localparam COUNTER_LIMIT = CLK_FREQ / PWM_FREQ;\n    reg [31:0] counter = 0;       // Counter for PWM period\n    reg [31:0] duty_value;        // Duty cycle value in clock cycles\n\n    // Convert duty cycle to clock cycles\n    always @(*) begin\n        duty_value = (duty_cycle * COUNTER_LIMIT) >> 8;\n    end\n\n    // PWM generation logic\n    always @(posedge clk or posedge rst) begin\n        if (rst) begin\n            counter <= 0;\n            pwm_out <= 0;\n        end else begin\n            if (counter < COUNTER_LIMIT) begin\n                counter <= counter + 1;\n                pwm_out <= (counter < duty_value) ? 1 : 0;\n            end else begin\n                counter <= 0;\n                pwm_out <= 0;\n            end\n        end\n    end\n\nendmodule\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"testing\"\n\t\"encoding/json\"\n\t\"github.com/json-iterator/go\"\n)\n\n// Sample JSON data for benchmarking\nvar jsonData = []byte(`{\n\t\"id\": 1,\n\t\"name\": \"John Doe\",\n\t\"email\": \"john@example.com\",\n\t\"age\": 30,\n\t\"address\": {\n\t\t\"street\": \"123 Main St\",\n\t\t\"city\": \"Anytown\",\n\t\t\"state\": \"CA\",\n\t\t\"zip\": \"12345\"\n\t}\n}`)\n\n// Struct to unmarshal JSON data into\ntype Person struct {\n\tID      int `json:\"id\"`\n\tName    string `json:\"name\"`\n\tEmail   string `json:\"email\"`\n\tAge     int `json:\"age\"`\n\tAddress struct {\n\t\tStreet string `json:\"street\"`\n\t\tCity   string `json:\"city\"`\n\t\tState  string `json:\"state\"`\n\t\tZip    string `json:\"zip\"`\n\t} `json:\"address\"`\n}\n\n// Benchmark using the standard library encoding/json\nfunc BenchmarkJSONUnmarshal(b *testing.B) {\n\tvar p Person\n\tfor i := 0; i < b.N; i++ {\n\t\terr := json.Unmarshal(jsonData, &p)\n\t\tif err != nil {\n\t\t\tb.Error(err)\n\t\t}\n\t}\n}\n\n// Benchmark using the json-iterator library\nfunc BenchmarkJSONIteratorUnmarshal(b *testing.B) {\n\tvar p Person\n\tjsonIter := jsoniter.ConfigCompatibleWithStandardLibrary\n\tfor i := 0; i < b.N; i++ {\n\t\terr := jsonIter.Unmarshal(jsonData, &p)\n\t\tif err != nil {\n\t\t\tb.Error(err)\n\t\t}\n\t}\n}\n\n/*\nTo run the benchmark, use the following command from the terminal:\ngo test -bench=.\n*/\n```",
    "label": 1
  },
  {
    "codes": "```ruby\n# Gemfile\ngem 'yaml'\ngem 'erb'\n\n# lib/ssh_config_generator.rb\nrequire 'yaml'\nrequire 'erb'\n\nmodule SshConfigGenerator\n  class Generator\n    def initialize(template_path, output_path)\n      @template_path = template_path\n      @output_path = output_path\n    end\n\n    def generate\n      # Load YAML template\n      config = YAML.load_file(@template_path)\n\n      # Render ERB template\n      template = File.read(File.expand_path('template.erb', __dir__))\n      rendered_config = ERB.new(template).result(binding)\n\n      # Write to output file\n      File.write(@output_path, rendered_config)\n    end\n  end\nend\n\n# template.erb\n<% config.each do |host, settings| %>\nHost <%= host %>\n  <% settings.each do |key, value| %>\n  <%= key %> <%= value %>\n  <% end %>\n<% end %>\n\n# Example usage\n# generator = SshConfigGenerator::Generator.new('config.yml', 'ssh_config')\n# generator.generate\n```\n\n```yaml\n# config.yml\nhost1:\n  HostName: example.com\n  User: user1\n  Port: 22\n\nhost2:\n  HostName: another-example.com\n  User: user2\n  Port: 2222\n```\n\n```bash\n# ssh_config (generated output)\nHost host1\n  HostName example.com\n  User user1\n  Port 22\n\nHost host2\n  HostName another-example.com\n  User user2\n  Port 2222\n```",
    "label": 1
  },
  {
    "codes": "```csharp\n// Custom attribute to validate JWT claims\n[AttributeUsage(AttributeTargets.Method | AttributeTargets.Class, AllowMultiple = true)]\npublic class ValidateJwtClaimAttribute : ActionFilterAttribute\n{\n    private readonly string _claimType;\n    private readonly string _claimValue;\n\n    // Constructor to accept claim type and value\n    public ValidateJwtClaimAttribute(string claimType, string claimValue)\n    {\n        _claimType = claimType;\n        _claimValue = claimValue;\n    }\n\n    // Override OnActionExecuting to validate claims\n    public override void OnActionExecuting(ActionExecutingContext context)\n    {\n        var user = context.HttpContext.User;\n\n        // Check if the user has the specified claim\n        if (!user.HasClaim(_claimType, _claimValue))\n        {\n            context.Result = new ForbidResult(); // Return 403 Forbidden if claim is not valid\n            return;\n        }\n\n        base.OnActionExecuting(context);\n    }\n}\n```\n\n```csharp\n// Example usage in a controller\n[ApiController]\n[Route(\"api/[controller]\")]\npublic class SampleController : ControllerBase\n{\n    [HttpGet(\"secure\")]\n    [ValidateJwtClaim(\"role\", \"admin\")] // Validate that the user has the \"role\" claim with value \"admin\"\n    public IActionResult SecureEndpoint()\n    {\n        return Ok(\"Access granted to secure endpoint.\");\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "def request_slot(client,\n                 service: JID,\n                 filename: str,\n                 size: int,\n                 content_type: str):\n    \"\"\"\n    Request an HTTP upload slot.\n\n    :param client: The client to request the slot with.\n    :type client: :class:`aioxmpp.Client`\n    :param service: Address of the HTTP upload service.\n    :type service: :class:`~aioxmpp.JID`\n    :param filename: Name of the file (without path), may be used by the server\n        to generate the URL.\n    :type filename: :class:`str`\n    :param size: Size of the file in bytes\n    :type size: :class:`int`\n    :param content_type: The MIME type of the file\n    :type content_type: :class:`str`\n    :return: The assigned upload slot.\n    :rtype: :class:`.xso.Slot`\n\n    Sends a :xep:`363` slot request to the XMPP service to obtain HTTP\n    PUT and GET URLs for a file upload.\n\n    The upload slot is returned as a :class:`~.xso.Slot` object.\n    \"\"\"\n\n    payload = Request(filename, size, content_type)\n    return (yield from client.send(IQ(\n        type_=IQType.GET,\n        to=service,\n        payload=payload\n    )))",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Set AWS S3 bucket name\nBUCKET_NAME=\"your-s3-bucket-name\"\n\n# Set directories to backup\nDIRECTORIES_TO_BACKUP=(\n    \"/path/to/directory1\"\n    \"/path/to/directory2\"\n)\n\n# Set AWS CLI profile if not using default\nAWS_PROFILE=\"your-aws-profile\"\n\n# Set current date for timestamping the backup\nCURRENT_DATE=$(date +\"%Y-%m-%d\")\n\n# Loop through each directory and upload it to S3\nfor DIR in \"${DIRECTORIES_TO_BACKUP[@]}\"; do\n    # Check if directory exists\n    if [ -d \"$DIR\" ]; then\n        # Get directory name without leading path\n        DIR_NAME=$(basename \"$DIR\")\n        \n        # Create a tar.gz file of the directory\n        BACKUP_FILE=\"${DIR_NAME}-${CURRENT_DATE}.tar.gz\"\n        tar -czf \"/tmp/${BACKUP_FILE}\" -C \"$DIR\" .\n\n        # Upload the tar.gz file to S3\n        aws s3 cp \"/tmp/${BACKUP_FILE}\" \"s3://${BUCKET_NAME}/${BACKUP_FILE}\" --profile \"$AWS_PROFILE\"\n\n        # Check if upload was successful\n        if [ $? -eq 0 ]; then\n            echo \"Backup of ${DIR} successful\"\n            # Remove the temporary backup file\n            rm \"/tmp/${BACKUP_FILE}\"\n        else\n            echo \"Error: Backup of ${DIR} failed\"\n            exit 1\n        fi\n    else\n        echo \"Error: Directory ${DIR} does not exist\"\n    fi\ndone\n```",
    "label": 1
  },
  {
    "codes": "```python\nfrom fastapi import FastAPI, Depends, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom pydantic import BaseModel\nfrom typing import Optional\nimport jwt\nfrom datetime import datetime, timedelta\n\n# Mock user database\nfake_users_db = {\n    \"user1\": {\n        \"username\": \"user1\",\n        \"password\": \"password1\",\n    }\n}\n\n# JWT settings\nSECRET_KEY = \"your-secret-key\"\nALGORITHM = \"HS256\"\nACCESS_TOKEN_EXPIRE_MINUTES = 30\n\n# FastAPI app\napp = FastAPI()\n\n# OAuth2 scheme\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n# User model\nclass User(BaseModel):\n    username: str\n    password: str\n\n# Token model\nclass Token(BaseModel):\n    access_token: str\n    token_type: str\n\n# Token data model\nclass TokenData(BaseModel):\n    username: Optional[str] = None\n\n# Model input\nclass ModelInput(BaseModel):\n    feature1: float\n    feature2: float\n\n# Model output\nclass ModelOutput(BaseModel):\n    prediction: float\n\n# Function to create JWT token\ndef create_access_token(data: dict, expires_delta: Optional[timedelta] = None):\n    to_encode = data.copy()\n    if expires_delta:\n        expire = datetime.utcnow() + expires_delta\n    else:\n        expire = datetime.utcnow() + timedelta(minutes=15)\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n    return encoded_jwt\n\n# Function to authenticate user\ndef authenticate_user(username: str, password: str):\n    user = fake_users_db.get(username)\n    if not user:\n        return False\n    if user[\"password\"] != password:\n        return False\n    return user\n\n# Dependency to get current user from JWT token\nasync def get_current_user(token: str = Depends(oauth2_scheme)):\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": \"Bearer\"},\n    )\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username: str = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n        token_data = TokenData(username=username)\n    except jwt.PyJWTError:\n        raise credentials_exception\n    user = fake_users_db.get(username)\n    if user is None:\n        raise credentials_exception\n    return user\n\n# Login endpoint to get JWT token\n@app.post(\"/token\", response_model=Token)\nasync def login(form_data: OAuth2PasswordRequestForm = Depends()):\n    user = authenticate_user(form_data.username, form_data.password)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect username or password\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n    access_token = create_access_token(\n        data={\"sub\": user[\"username\"]}, expires_delta=access_token_expires\n    )\n    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n\n# Model prediction endpoint with JWT authentication\n@app.post(\"/predict\", response_model=ModelOutput)\nasync def predict(\n    input_data: ModelInput,\n    current_user: User = Depends(get_current_user)\n):\n    # Mock model prediction logic\n    prediction = input_data.feature1 * 0.5 + input_data.feature2 * 0.5\n    return {\"prediction\": prediction}\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glacier) SetVaultAccessPolicyRequest(input *SetVaultAccessPolicyInput) (req *request.Request, output *SetVaultAccessPolicyOutput) {\n\top := &request.Operation{\n\t\tName:       opSetVaultAccessPolicy,\n\t\tHTTPMethod: \"PUT\",\n\t\tHTTPPath:   \"/{accountId}/vaults/{vaultName}/access-policy\",\n\t}\n\n\tif input == nil {\n\t\tinput = &SetVaultAccessPolicyInput{}\n\t}\n\n\toutput = &SetVaultAccessPolicyOutput{}\n\treq = c.newRequest(op, input, output)\n\treq.Handlers.Unmarshal.Swap(restjson.UnmarshalHandler.Name, protocol.UnmarshalDiscardBodyHandler)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "def findspans(self, type,set=None):\n        \"\"\"Yields span annotation elements of the specified type that include this word.\n\n        Arguments:\n            type: The annotation type, can be passed as using any of the :class:`AnnotationType` member, or by passing the relevant :class:`AbstractSpanAnnotation` or :class:`AbstractAnnotationLayer` class.\n            set (str or None): Constrain by set\n\n        Example::\n\n            for chunk in word.findspans(folia.Chunk):\n                print(\" Chunk class=\", chunk.cls, \" words=\")\n                for word2 in chunk.wrefs(): #print all words in the chunk (of which the word is a part)\n                    print(word2, end=\"\")\n                print()\n\n        Yields:\n            Matching span annotation instances (derived from :class:`AbstractSpanAnnotation`)\n        \"\"\"\n\n        if issubclass(type, AbstractAnnotationLayer):\n            layerclass = type\n        else:\n            layerclass = ANNOTATIONTYPE2LAYERCLASS[type.ANNOTATIONTYPE]\n        e = self\n        while True:\n            if not e.parent: break\n            e = e.parent\n            for layer in e.select(layerclass,set,False):\n                if type is layerclass:\n                    for e2 in layer.select(AbstractSpanAnnotation,set,True, (True, Word, Morpheme)):\n                        if not isinstance(e2, AbstractSpanRole) and self in e2.wrefs():\n                            yield e2\n                else:\n                    for e2 in layer.select(type,set,True, (True, Word, Morpheme)):\n                        if not isinstance(e2, AbstractSpanRole) and self in e2.wrefs():\n                            yield e2",
    "label": 0
  },
  {
    "codes": "```swift\n// Define the puzzle piece node\nlet puzzlePiece = SKSpriteNode(imageNamed: \"puzzlePiece\")\n\n// Define the initial position and scale\nlet initialPosition = CGPoint(x: 100, y: 100)\nlet initialScale = CGSize(width: 0.1, height: 0.1)\n\n// Set the initial properties\npuzzlePiece.position = initialPosition\npuzzlePiece.setScale(0.1)\n\n// Define the actions for the puzzle piece animation\nlet scaleUp = SKAction.scale(to: 1.0, duration: 0.5)\nlet moveToCenter = SKAction.move(to: CGPoint(x: size.width / 2, y: size.height / 2), duration: 1.0)\nlet rotate = SKAction.rotate(byAngle: .pi / 2, duration: 0.5)\nlet fadeIn = SKAction.fadeIn(withDuration: 0.5)\nlet wait = SKAction.wait(forDuration: 0.5)\n\n// Create a sequence of actions\nlet sequence = SKAction.sequence([scaleUp, moveToCenter, rotate, fadeIn, wait])\n\n// Run the sequence on the puzzle piece\npuzzlePiece.run(sequence)\n```",
    "label": 1
  },
  {
    "codes": "def list_plugins(directory=None):\n    \"\"\"Gets a list of the installed themes.\"\"\"\n    repo = require_repo(directory)\n    plugins = get_value(repo, 'plugins')\n    if not plugins or not isinstance(plugins, dict):\n        return None\n    return plugins.keys()",
    "label": 0
  },
  {
    "codes": "function DataView(buffer, byteOffset, byteLength) {\n      if (!(buffer instanceof ArrayBuffer || Class(buffer) === 'ArrayBuffer')) throw TypeError();\n\n      byteOffset = ToUint32(byteOffset);\n      if (byteOffset > buffer.byteLength)\n        throw RangeError('byteOffset out of range');\n\n      if (byteLength === undefined)\n        byteLength = buffer.byteLength - byteOffset;\n      else\n        byteLength = ToUint32(byteLength);\n\n      if ((byteOffset + byteLength) > buffer.byteLength)\n        throw RangeError('byteOffset and length reference an area beyond the end of the buffer');\n\n      Object.defineProperty(this, 'buffer', {value: buffer});\n      Object.defineProperty(this, 'byteLength', {value: byteLength});\n      Object.defineProperty(this, 'byteOffset', {value: byteOffset});\n    }",
    "label": 0
  },
  {
    "codes": "func (c *Glue) BatchDeleteTableVersionWithContext(ctx aws.Context, input *BatchDeleteTableVersionInput, opts ...request.Option) (*BatchDeleteTableVersionOutput, error) {\n\treq, out := c.BatchDeleteTableVersionRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```kotlin\n// Base sealed class for domain-specific errors\nsealed class DomainError {\n    // Represents errors related to authentication\n    sealed class AuthError : DomainError() {\n        object InvalidCredentials : AuthError()\n        object AccountLocked : AuthError()\n        object TokenExpired : AuthError()\n    }\n\n    // Represents errors related to network operations\n    sealed class NetworkError : DomainError() {\n        object NoInternetConnection : NetworkError()\n        object Timeout : NetworkError()\n        object ServerUnavailable : NetworkError()\n    }\n\n    // Represents errors related to data validation\n    sealed class ValidationError : DomainError() {\n        object InvalidEmail : ValidationError()\n        object InvalidPassword : ValidationError()\n        object MissingRequiredField : ValidationError()\n    }\n\n    // Represents errors related to database operations\n    sealed class DatabaseError : DomainError() {\n        object RecordNotFound : DatabaseError()\n        object DuplicateEntry : DatabaseError()\n        object ConstraintViolation : DatabaseError()\n    }\n\n    // Represents custom or unexpected errors\n    data class UnexpectedError(val message: String) : DomainError()\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *GetSecurityConfigurationOutput) SetSecurityConfiguration(v *SecurityConfiguration) *GetSecurityConfigurationOutput {\n\ts.SecurityConfiguration = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```typescript\n// Import necessary Cypress commands\nimport { checkoutPage } from '../pages/checkoutPage';\n\ndescribe('E-commerce Checkout Functionality', () => {\n  beforeEach(() => {\n    // Visit the e-commerce website and navigate to the checkout page\n    cy.visit('/');\n    cy.get('[data-testid=\"cart-icon\"]').click();\n    cy.get('[data-testid=\"checkout-button\"]').click();\n  });\n\n  it('should successfully complete the checkout process', () => {\n    // Fill in the shipping information\n    checkoutPage.fillShippingInfo({\n      name: 'John Doe',\n      address: '123 Main St',\n      city: 'Anytown',\n      zip: '12345',\n      country: 'USA',\n    });\n\n    // Select a payment method\n    checkoutPage.selectPaymentMethod('creditCard');\n\n    // Fill in the payment details\n    checkoutPage.fillPaymentDetails({\n      cardNumber: '4111111111111111',\n      expiryDate: '12/25',\n      cvv: '123',\n    });\n\n    // Place the order\n    checkoutPage.placeOrder();\n\n    // Verify the order confirmation message\n    cy.get('[data-testid=\"order-confirmation\"]').should('contain', 'Thank you for your order!');\n  });\n\n  it('should display an error message for invalid payment details', () => {\n    // Fill in the shipping information\n    checkoutPage.fillShippingInfo({\n      name: 'John Doe',\n      address: '123 Main St',\n      city: 'Anytown',\n      zip: '12345',\n      country: 'USA',\n    });\n\n    // Select a payment method\n    checkoutPage.selectPaymentMethod('creditCard');\n\n    // Fill in invalid payment details\n    checkoutPage.fillPaymentDetails({\n      cardNumber: '4111111111111111',\n      expiryDate: '12/25',\n      cvv: '999', // Invalid CVV\n    });\n\n    // Attempt to place the order\n    checkoutPage.placeOrder();\n\n    // Verify the error message is displayed\n    cy.get('[data-testid=\"payment-error\"]').should('contain', 'Invalid payment details');\n  });\n});\n```\n\n```typescript\n// checkoutPage.ts - Page Object Model for Checkout Page\nexport const checkoutPage = {\n  fillShippingInfo(info: {\n    name: string;\n    address: string;\n    city: string;\n    zip: string;\n    country: string;\n  }) {\n    cy.get('[data-testid=\"shipping-name\"]').type(info.name);\n    cy.get('[data-testid=\"shipping-address\"]').type(info.address);\n    cy.get('[data-testid=\"shipping-city\"]').type(info.city);\n    cy.get('[data-testid=\"shipping-zip\"]').type(info.zip);\n    cy.get('[data-testid=\"shipping-country\"]').select(info.country);\n  },\n\n  selectPaymentMethod(method: string) {\n    cy.get('[data-testid=\"payment-method\"]').select(method);\n  },\n\n  fillPaymentDetails(details: { cardNumber: string; expiryDate: string; cvv: string }) {\n    cy.get('[data-testid=\"card-number\"]').type(details.cardNumber);\n    cy.get('[data-testid=\"expiry-date\"]').type(details.expiryDate);\n    cy.get('[data-testid=\"cvv\"]').type(details.cvv);\n  },\n\n  placeOrder() {\n    cy.get('[data-testid=\"place-order-button\"]').click();\n  },\n};\n```",
    "label": 1
  },
  {
    "codes": "func (s *JsonClassifier) SetCreationTime(v time.Time) *JsonClassifier {\n\ts.CreationTime = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *JobUpdate) SetMaxCapacity(v float64) *JobUpdate {\n\ts.MaxCapacity = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```javascript\n// Utility function to deep clone objects with circular references\nfunction deepClone(obj, hash = new WeakMap()) {\n    // If the object is null or not an object, return it directly\n    if (obj === null || typeof obj !== 'object') {\n        return obj;\n    }\n\n    // If the object is already cloned, return the cloned reference\n    if (hash.has(obj)) {\n        return hash.get(obj);\n    }\n\n    // Handle Date objects\n    if (obj instanceof Date) {\n        return new Date(obj);\n    }\n\n    // Handle RegExp objects\n    if (obj instanceof RegExp) {\n        return new RegExp(obj);\n    }\n\n    // Create a new object or array based on the original\n    const clone = Array.isArray(obj) ? [] : {};\n\n    // Store the cloned object in the hash to handle circular references\n    hash.set(obj, clone);\n\n    // Recursively clone each property\n    for (let key in obj) {\n        if (obj.hasOwnProperty(key)) {\n            clone[key] = deepClone(obj[key], hash);\n        }\n    }\n\n    return clone;\n}\n```",
    "label": 1
  },
  {
    "codes": "def build_dated_queryset(self):\n        \"\"\"\n        Build pages for all years in the queryset.\n        \"\"\"\n        qs = self.get_dated_queryset()\n        months = self.get_date_list(qs)\n        [self.build_month(dt) for dt in months]",
    "label": 0
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\n\n# Generate sample molecular data (atoms with x, y, z coordinates)\ndata = {\n    'Atom': ['C', 'H', 'O', 'N'],\n    'X': [0, 1, 0, -1],\n    'Y': [0, 1, -1, 0],\n    'Z': [0, 1, 1, -1]\n}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Create a 3D scatter plot using Plotly Express\nfig = px.scatter_3d(df, x='X', y='Y', z='Z', text='Atom', color='Atom',\n                    title='3D Molecular Structure Visualization')\n\n# Update marker size and text position for better visualization\nfig.update_traces(marker=dict(size=10), textposition='top center')\n\n# Show the plot\nfig.show()\n```\n\n```python\n# Optional: Add bonds between atoms for a more realistic molecular structure\n# Define bonds as lines between atoms\nbonds = [\n    (0, 1),  # C-H bond\n    (0, 2),  # C-O bond\n    (0, 3)   # C-N bond\n]\n\n# Add lines for bonds\nfor bond in bonds:\n    fig.add_trace(\n        px.line_3d(df.iloc[list(bond)], x='X', y='Y', z='Z').data[0]\n    )\n\n# Show the updated plot with bonds\nfig.show()\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"log\"\n\t\"net\"\n\t\"time\"\n\t\"math/rand\"\n)\n\n// Simulate packet loss by randomly dropping packets\nfunc simulatePacketLoss(conn net.Conn, lossRate float64) {\n\tbuf := make([]byte, 1024)\n\tfor {\n\t\tn, err := conn.Read(buf)\n\t\tif err != nil {\n\t\t\tlog.Println(\"Error reading:\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Randomly drop packets based on loss rate\n\t\tif rand.Float64() < lossRate {\n\t\t\tlog.Println(\"Packet dropped!\")\n\t\t\tcontinue\n\t\t}\n\n\t\t// Forward the packet\n\t\t_, err = conn.Write(buf[:n])\n\t\tif err != nil {\n\t\t\tlog.Println(\"Error writing:\", err)\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc main() {\n\t// Set up a TCP listener\n\tlistener, err := net.Listen(\"tcp\", \":8080\")\n\tif err != nil {\n\t\tlog.Fatal(\"Error starting server:\", err)\n\t}\n\tdefer listener.Close()\n\n\tlog.Println(\"Server started on :8080\")\n\n\t// Accept incoming connections\n\tfor {\n\t\tconn, err := listener.Accept()\n\t\tif err != nil {\n\t\t\tlog.Println(\"Error accepting connection:\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Simulate 10% packet loss\n\t\tgo simulatePacketLoss(conn, 0.1)\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "```go\n// main.go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"time\"\n\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\tclientgoscheme \"k8s.io/client-go/kubernetes/scheme\"\n\tctrl \"sigs.k8s.io/controller-runtime\"\n\t\"sigs.k8s.io/controller-runtime/pkg/client\"\n\t\"sigs.k8s.io/controller-runtime/pkg/log/zap\"\n)\n\n// Define a custom resource type\ntype MyCustomResource struct {\n\tSpec   MyCustomResourceSpec   `json:\"spec\"`\n\tStatus MyCustomResourceStatus `json:\"status\"`\n}\n\ntype MyCustomResourceSpec struct {\n\tMessage string `json:\"message\"`\n}\n\ntype MyCustomResourceStatus struct {\n\tProcessed bool `json:\"processed\"`\n}\n\n// Register the custom resource with the scheme\nvar (\n\tscheme   = runtime.NewScheme()\n\tsetupLog = ctrl.Log.WithName(\"setup\")\n)\n\nfunc init() {\n\t_ = clientgoscheme.AddToScheme(scheme)\n\t// Add custom resource to the scheme\n\t// Note: Replace `mygroup.example.com/v1` with your actual API group and version\n\t// _ = mygroupv1.AddToScheme(scheme)\n}\n\n// Reconcile function for the custom resource\ntype MyCustomResourceReconciler struct {\n\tclient.Client\n\tScheme *runtime.Scheme\n}\n\nfunc (r *MyCustomResourceReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n\tlog := ctrl.Log.WithValues(\"mycustomresource\", req.NamespacedName)\n\n\t// Fetch the custom resource\n\tvar resource MyCustomResource\n\tif err := r.Get(ctx, req.NamespacedName, &resource); err != nil {\n\t\tlog.Error(err, \"unable to fetch MyCustomResource\")\n\t\treturn ctrl.Result{}, client.IgnoreNotFound(err)\n\t}\n\n\t// Process the custom resource\n\tlog.Info(\"Processing custom resource\", \"message\", resource.Spec.Message)\n\tresource.Status.Processed = true\n\n\t// Update the status\n\tif err := r.Status().Update(ctx, &resource); err != nil {\n\t\tlog.Error(err, \"unable to update MyCustomResource status\")\n\t\treturn ctrl.Result{}, err\n\t}\n\n\tlog.Info(\"Successfully processed custom resource\")\n\treturn ctrl.Result{}, nil\n}\n\nfunc main() {\n\tctrl.SetLogger(zap.New())\n\n\tmgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{\n\t\tScheme:             scheme,\n\t\tMetricsBindAddress: \"0\",\n\t})\n\tif err != nil {\n\t\tsetupLog.Error(err, \"unable to start manager\")\n\t\tos.Exit(1)\n\t}\n\n\t// Register the reconciler\n\tif err := (&MyCustomResourceReconciler{\n\t\tClient: mgr.GetClient(),\n\t\tScheme: mgr.GetScheme(),\n\t}).SetupWithManager(mgr); err != nil {\n\t\tsetupLog.Error(err, \"unable to create controller\", \"controller\", \"MyCustomResource\")\n\t\tos.Exit(1)\n\t}\n\n\tsetupLog.Info(\"starting manager\")\n\tif err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {\n\t\tsetupLog.Error(err, \"problem running manager\")\n\t\tos.Exit(1)\n\t}\n}\n```\n\n```go\n// api/v1/mycustomresource_types.go\npackage v1\n\nimport (\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n)\n\n// +kubebuilder:object:root=true\n// +kubebuilder:subresource:status\ntype MyCustomResource struct {\n\tmetav1.TypeMeta   `json:\",inline\"`\n\tmetav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n\tSpec   MyCustomResourceSpec   `json:\"spec,omitempty\"`\n\tStatus MyCustomResourceStatus `json:\"status,omitempty\"`\n}\n\ntype MyCustomResourceSpec struct {\n\tMessage string `json:\"message\"`\n}\n\ntype MyCustomResourceStatus struct {\n\tProcessed bool `json:\"processed\"`\n}\n\n// +kubebuilder:object:root=true\ntype MyCustomResourceList struct {\n\tmetav1.TypeMeta `json:\",inline\"`\n\tmetav1.ListMeta `json:\"metadata,omitempty\"`\n\tItems           []MyCustomResource `json:\"items\"`\n}\n\nfunc init() {\n\tSchemeBuilder.Register(&MyCustomResource{}, &MyCustomResourceList{})\n}\n```\n\n```go\n// Makefile (optional for building and deploying)\n# Build the operator\nbuild:\n\tgo build -o bin/manager main.go\n\n# Run the operator locally\nrun:\n\tgo run main.go\n\n# Deploy the operator to Kubernetes\ndeploy:\n\tkubectl apply -f config/crd/bases/mygroup.example.com_mycustomresources.yaml\n\tkubectl apply -f config/manager/manager.yaml\n```\n\n```yaml\n# config/crd/bases/mygroup.example.com_mycustomresources.yaml\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: mycustomresources.mygroup.example.com\nspec:\n  group: mygroup.example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                message:\n                  type: string\n            status:\n              type: object\n              properties:\n                processed:\n                  type: boolean\n  scope: Namespaced\n  names:\n    plural: mycustomresources\n    singular: mycustomresource\n    kind: MyCustomResource\n    shortNames:\n      - mcr\n```\n\n```yaml\n# config/manager/manager.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mycustomresource-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n        - name: manager\n          image: mycustomresource-operator:latest\n          args:\n            - \"--enable-leader-election\"\n          resources:\n            limits:\n              cpu: \"1\"\n              memory: \"512Mi\"\n            requests:\n              cpu: \"100m\"\n              memory: \"128Mi\"\n```",
    "label": 1
  },
  {
    "codes": "func (c *CloudTrail) UpdateTrailRequest(input *UpdateTrailInput) (req *request.Request, output *UpdateTrailOutput) {\n\top := &request.Operation{\n\t\tName:       opUpdateTrail,\n\t\tHTTPMethod: \"POST\",\n\t\tHTTPPath:   \"/\",\n\t}\n\n\tif input == nil {\n\t\tinput = &UpdateTrailInput{}\n\t}\n\n\toutput = &UpdateTrailOutput{}\n\treq = c.newRequest(op, input, output)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "```swift\nimport Security\n\nfunc saveKeyToKeychain(key: String, value: String, service: String) -> Bool {\n    // Convert the value to Data\n    guard let valueData = value.data(using: .utf8) else {\n        return false\n    }\n    \n    // Create a query dictionary for the Keychain\n    let query: [String: Any] = [\n        kSecClass as String: kSecClassGenericPassword,\n        kSecAttrService as String: service,\n        kSecAttrAccount as String: key,\n        kSecValueData as String: valueData\n    ]\n    \n    // Delete any existing item before adding a new one\n    SecItemDelete(query as CFDictionary)\n    \n    // Add the new item to the Keychain\n    let status = SecItemAdd(query as CFDictionary, nil)\n    \n    // Check if the operation was successful\n    return status == errSecSuccess\n}\n\nfunc retrieveKeyFromKeychain(key: String, service: String) -> String? {\n    // Create a query dictionary for the Keychain\n    let query: [String: Any] = [\n        kSecClass as String: kSecClassGenericPassword,\n        kSecAttrService as String: service,\n        kSecAttrAccount as String: key,\n        kSecMatchLimit as String: kSecMatchLimitOne,\n        kSecReturnData as String: true\n    ]\n    \n    // Retrieve the item from the Keychain\n    var dataTypeRef: AnyObject?\n    let status = SecItemCopyMatching(query as CFDictionary, &dataTypeRef)\n    \n    // Check if the operation was successful and data was retrieved\n    if status == errSecSuccess, let data = dataTypeRef as? Data {\n        return String(data: data, encoding: .utf8)\n    }\n    \n    return nil\n}\n\nfunc deleteKeyFromKeychain(key: String, service: String) -> Bool {\n    // Create a query dictionary for the Keychain\n    let query: [String: Any] = [\n        kSecClass as String: kSecClassGenericPassword,\n        kSecAttrService as String: service,\n        kSecAttrAccount as String: key\n    ]\n    \n    // Delete the item from the Keychain\n    let status = SecItemDelete(query as CFDictionary)\n    \n    // Check if the operation was successful\n    return status == errSecSuccess\n}\n```",
    "label": 1
  },
  {
    "codes": "def allow_upcast(self, to):\n        \"\"\"\n        Return true if the field type may be upcast to the other field type\n        `to`.\n\n        This relation specifies when it is safe to transfer data from this\n        field type to the given other field type `to`.\n\n        This is the case if any of the following holds true:\n\n        * `to` is equal to this type\n        * this type is :attr:`TEXT_SINGLE` and `to` is :attr:`TEXT_PRIVATE`\n        \"\"\"\n\n        if self == to:\n            return True\n        if self == FieldType.TEXT_SINGLE and to == FieldType.TEXT_PRIVATE:\n            return True\n        return False",
    "label": 0
  },
  {
    "codes": "```javascript\n// Import necessary libraries\nconst xss = require('xss');\n\n// Middleware to sanitize user input\nconst sanitizeInput = (req, res, next) => {\n    // Sanitize request body\n    if (req.body) {\n        for (let key in req.body) {\n            if (typeof req.body[key] === 'string') {\n                req.body[key] = xss(req.body[key]);\n            }\n        }\n    }\n\n    // Sanitize request query parameters\n    if (req.query) {\n        for (let key in req.query) {\n            if (typeof req.query[key] === 'string') {\n                req.query[key] = xss(req.query[key]);\n            }\n        }\n    }\n\n    // Sanitize request params\n    if (req.params) {\n        for (let key in req.params) {\n            if (typeof req.params[key] === 'string') {\n                req.params[key] = xss(req.params[key]);\n            }\n        }\n    }\n\n    // Proceed to the next middleware or route handler\n    next();\n};\n\n// Export the middleware\nmodule.exports = sanitizeInput;\n```",
    "label": 1
  },
  {
    "codes": "def _do_east_asian(self):\n        \"\"\"Fetch and update east-asian tables.\"\"\"\n        self._do_retrieve(self.EAW_URL, self.EAW_IN)\n        (version, date, values) = self._parse_east_asian(\n            fname=self.EAW_IN,\n            properties=(u'W', u'F',)\n        )\n        table = self._make_table(values)\n        self._do_write(self.EAW_OUT, 'WIDE_EASTASIAN', version, date, table)",
    "label": 0
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model)\n\n# Save the CoreML model to a file\ncoreml_model.save('your_coreml_model.mlmodel')\n```\n\n```python\n# Alternative approach using TensorFlow 2.x and CoreMLTools\nimport tensorflow as tf\nimport coremltools as ct\n\n# Load the TensorFlow/Keras model\nkeras_model = tf.keras.models.load_model('your_keras_model.h5')\n\n# Convert the model to CoreML format\ncoreml_model = ct.convert(keras_model, source='tensorflow')\n\n# Save the CoreML model\ncoreml_model.save('your_coreml_model.mlmodel')\n```\n\n```python\n# Another approach using a custom input/output description\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Define input and output descriptions\ninput_description = ct.ImageType(shape=(1, 224, 224, 3), bias=[-1,-1,-1], scale=1/127)\noutput_description = ct.TensorType(name='output', shape=(1, 1000))\n\n# Convert the model with custom descriptions\ncoreml_model = ct.convert(keras_model, inputs=[input_description], outputs=[output_description])\n\n# Save the CoreML model\ncoreml_model.save('your_coreml_model.mlmodel')\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport functools\nimport traceback\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.ERROR)\n\ndef log_exceptions(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            # Log the exception with stack trace\n            logging.error(f\"Exception occurred in {func.__name__}: {str(e)}\")\n            logging.error(traceback.format_exc())\n            raise  # Re-raise the exception after logging\n    return wrapper\n```\n\n```python\n# Example usage\n@log_exceptions\ndef risky_function():\n    return 1 / 0\n\nrisky_function()  # This will log the exception with stack trace\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nclass Vector2D {\n    constructor(x, y) {\n        this.x = x; // x-component of the vector\n        this.y = y; // y-component of the vector\n    }\n\n    // Method to add two vectors\n    add(vector) {\n        return new Vector2D(this.x + vector.x, this.y + vector.y);\n    }\n\n    // Method to calculate the magnitude of the vector\n    magnitude() {\n        return Math.sqrt(this.x ** 2 + this.y ** 2);\n    }\n}\n\n// Example usage:\n// const vector1 = new Vector2D(3, 4);\n// const vector2 = new Vector2D(1, 2);\n// const result = vector1.add(vector2); // result is a new Vector2D(4, 6)\n// const mag = vector1.magnitude(); // mag is 5\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.SelectionKey;\nimport java.nio.channels.Selector;\nimport java.nio.channels.ServerSocketChannel;\nimport java.nio.channels.SocketChannel;\nimport java.util.Iterator;\n\npublic class NioServer {\n\n    private Selector selector;\n    private ByteBuffer buffer = ByteBuffer.allocate(256);\n\n    public NioServer(String address, int port) throws IOException {\n        // Open a selector\n        selector = Selector.open();\n\n        // Open a server socket channel\n        ServerSocketChannel serverChannel = ServerSocketChannel.open();\n        // Bind server to the address provided\n        serverChannel.bind(new InetSocketAddress(address, port));\n        // Configure the server channel to be non-blocking\n        serverChannel.configureBlocking(false);\n\n        // Register server channel with selector for accepting connections\n        serverChannel.register(selector, SelectionKey.OP_ACCEPT);\n    }\n\n    private void listen() throws IOException {\n        while (true) {\n            // Select channels that are ready for their registered operations\n            selector.select();\n\n            Iterator<SelectionKey> iterator = selector.selectedKeys().iterator();\n            while (iterator.hasNext()) {\n                SelectionKey key = iterator.next();\n                // Remove the key from selected set\n                iterator.remove();\n\n                if (key.isAcceptable()) {\n                    register(selector, key);\n                }\n\n                if (key.isReadable()) {\n                    // Handle socket channel to read and respond to client\n                    respond(key);\n                }\n            }\n        }\n    }\n\n    private void register(Selector selector, SelectionKey key) throws IOException {\n        // Accept connection and configure it to be non-blocking\n        ServerSocketChannel serverChannel = (ServerSocketChannel) key.channel();\n        SocketChannel clientChannel = serverChannel.accept();\n        clientChannel.configureBlocking(false);\n\n        // Register client channel with selector for reading operations\n        clientChannel.register(selector, SelectionKey.OP_READ);\n    }\n\n    private void respond(SelectionKey key) throws IOException {\n        // Obtain client channel\n        SocketChannel clientChannel = (SocketChannel) key.channel();\n        buffer.clear();\n        \n        // Read data from client and handle it\n        int numRead = clientChannel.read(buffer);\n        if (numRead == -1) {\n            clientChannel.close();\n            return;\n        }\n        \n        // Prepare buffer for writing back to client\n        buffer.flip();\n        clientChannel.write(buffer);\n    }\n\n    public static void main(String[] args) {\n        try {\n            // Create a server instance\n            NioServer server = new NioServer(\"localhost\", 8080);\n            // Start listening for connections\n            server.listen();\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<List<OperationValueInner>> listAsync() {\n        return listWithServiceResponseAsync().map(new Func1<ServiceResponse<List<OperationValueInner>>, List<OperationValueInner>>() {\n            @Override\n            public List<OperationValueInner> call(ServiceResponse<List<OperationValueInner>> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *DataRetrievalRule) SetBytesPerHour(v int64) *DataRetrievalRule {\n\ts.BytesPerHour = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function () {\n            this.coords.w_rs = this.$cache.rs.outerWidth(false);\n\n            if (!this.coords.w_rs) {\n                return;\n            }\n\n            if (this.coords.w_rs !== this.coords.w_rs_old) {\n                this.target = \"base\";\n                this.is_resize = true;\n            }\n\n            if (this.coords.w_rs !== this.coords.w_rs_old || this.force_redraw) {\n                this.setMinMax();\n                this.calc(true);\n                this.drawLabels();\n                if (this.options.grid) {\n                    this.calcGridMargin();\n                    this.calcGridLabels();\n                }\n                this.force_redraw = true;\n                this.coords.w_rs_old = this.coords.w_rs;\n                this.drawShadow();\n            }\n\n            if (!this.coords.w_rs) {\n                return;\n            }\n\n            if (!this.dragging && !this.force_redraw && !this.is_key) {\n                return;\n            }\n\n            if (this.old_from !== this.result.from || this.old_to !== this.result.to || this.force_redraw || this.is_key) {\n\n                this.drawLabels();\n\n                this.$cache.bar[0].style.left = this.coords.p_bar_x + \"%\";\n                this.$cache.bar[0].style.width = this.coords.p_bar_w + \"%\";\n\n                if (this.options.type === \"single\") {\n                    this.$cache.bar[0].style.left = 0;\n                    this.$cache.bar[0].style.width = this.coords.p_bar_w + this.coords.p_bar_x + \"%\";\n\n                    this.$cache.s_single[0].style.left = this.coords.p_single_fake + \"%\";\n\n                    this.$cache.single[0].style.left = this.labels.p_single_left + \"%\";\n                } else {\n                    this.$cache.s_from[0].style.left = this.coords.p_from_fake + \"%\";\n                    this.$cache.s_to[0].style.left = this.coords.p_to_fake + \"%\";\n\n                    if (this.old_from !== this.result.from || this.force_redraw) {\n                        this.$cache.from[0].style.left = this.labels.p_from_left + \"%\";\n                    }\n                    if (this.old_to !== this.result.to || this.force_redraw) {\n                        this.$cache.to[0].style.left = this.labels.p_to_left + \"%\";\n                    }\n\n                    this.$cache.single[0].style.left = this.labels.p_single_left + \"%\";\n                }\n\n                this.writeToInput();\n\n                if ((this.old_from !== this.result.from || this.old_to !== this.result.to) && !this.is_start) {\n                    this.$cache.input.trigger(\"change\");\n                    this.$cache.input.trigger(\"input\");\n                }\n\n                this.old_from = this.result.from;\n                this.old_to = this.result.to;\n\n                // callbacks call\n                if (!this.is_resize && !this.is_update && !this.is_start && !this.is_finish) {\n                    this.callOnChange();\n                }\n                if (this.is_key || this.is_click) {\n                    this.is_key = false;\n                    this.is_click = false;\n                    this.callOnFinish();\n                }\n\n                this.is_update = false;\n                this.is_resize = false;\n                this.is_finish = false;\n            }\n\n            this.is_start = false;\n            this.is_key = false;\n            this.is_click = false;\n            this.force_redraw = false;\n        }",
    "label": 0
  },
  {
    "codes": "function set(key, value) {\n  let config = getGlobalConfig();\n  if (key && typeof key === 'string' && typeof value !== 'undefined') {\n    config = _.set(config, key, value);\n  } else if (_.isObject(key)) {\n    config = _.merge(config, key);\n  } else if (typeof value !== 'undefined') {\n    config = _.merge(config, value);\n  }\n  // update config meta\n  config.meta = config.meta || {};\n  config.meta.updated_at = Math.round(+new Date() / 1000);\n  // write to .serverlessrc file\n  writeFileAtomic.sync(serverlessrcPath, JSON.stringify(config, null, 2));\n  return config;\n}",
    "label": 0
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Management;\nusing System.ServiceProcess;\nusing System.Diagnostics;\n\npublic class UsbMonitorService : ServiceBase\n{\n    private ManagementEventWatcher watcher = null;\n\n    // Initialize the service\n    public UsbMonitorService()\n    {\n        this.ServiceName = \"UsbMonitorService\";\n    }\n\n    // Start the service\n    protected override void OnStart(string[] args)\n    {\n        StartUsbDetection();\n    }\n\n    // Stop the service\n    protected override void OnStop()\n    {\n        StopUsbDetection();\n    }\n\n    // Set up the USB detection\n    private void StartUsbDetection()\n    {\n        WqlEventQuery query = new WqlEventQuery(\n            \"SELECT * FROM Win32_DeviceChangeEvent WHERE EventType = 2 OR EventType = 3\");\n        \n        watcher = new ManagementEventWatcher(query);\n        watcher.EventArrived += new EventArrivedEventHandler(DeviceChangedEvent);\n        watcher.Start();\n    }\n\n    // Clean up resources when stopping USB detection\n    private void StopUsbDetection()\n    {\n        if (watcher != null)\n        {\n            watcher.Stop();\n            watcher.Dispose();\n            watcher = null;\n        }\n    }\n\n    // Handle the device change events\n    private void DeviceChangedEvent(object sender, EventArrivedEventArgs e)\n    {\n        try\n        {\n            int eventType = Convert.ToInt32(e.NewEvent.Properties[\"EventType\"].Value);\n            string message = (eventType == 2) ? \"USB Device Connected\" : \"USB Device Disconnected\";\n\n            // Log to Event Viewer\n            EventLog.WriteEntry(\"UsbMonitorService\", message, EventLogEntryType.Information);\n        }\n        catch (Exception ex)\n        {\n            EventLog.WriteEntry(\"UsbMonitorService\", $\"Exception: {ex.Message}\", EventLogEntryType.Error);\n        }\n    }\n\n    // Main entry point for the service\n    public static void Main()\n    {\n        ServiceBase.Run(new UsbMonitorService());\n    }\n}\n```\n\n- Install the service using `InstallUtil` or create an installer for it.\n- The service will start monitoring when started, using WMI to listen for device changes.\n- Remember to add any necessary configuration to your project (like app.config) if needed.",
    "label": 1
  },
  {
    "codes": "public Observable<JobStreamListResultInner> listByJobAsync(String resourceGroupName, String automationAccountName, UUID jobId) {\n        return listByJobWithServiceResponseAsync(resourceGroupName, automationAccountName, jobId).map(new Func1<ServiceResponse<JobStreamListResultInner>, JobStreamListResultInner>() {\n            @Override\n            public JobStreamListResultInner call(ServiceResponse<JobStreamListResultInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "Here's a Bash script to parse Apache logs and detect spikes in 5xx error responses:\n\n```bash\n#!/bin/bash\n\n# Variables\nLOG_FILE=\"/var/log/apache2/access.log\"\nERROR_THRESHOLD=10 # Define your threshold for error spikes\nTIME_WINDOW=60 # Time window in seconds\n\n# Function to get the current timestamp\nget_timestamp() {\n  date +%s\n}\n\n# Function to parse the log, find 5xx errors and check their frequency\ndetect_error_spikes() {\n  echo \"Detecting 5xx error spikes in Apache logs...\"\n  \n  # Calculate the current timestamp minus the time window\n  current_time=$(get_timestamp)\n  start_time=$((current_time - TIME_WINDOW))\n  \n  # Extract 5xx errors within the time window\n  error_count=$(awk -v start=\"$start_time\" 'BEGIN { count=0 }\n    {\n      match($4, /\\[([^:]+):([^ ]+) ([^\\]]+)\\]/, a);\n      split(a[2], time, \":\");\n      timestamp=mktime(a[1]\" \"time[1]\" \"time[2]\" \"time[3]);\n      if (timestamp >= start && $9 ~ /^5[0-9]{2}$/) {\n        count++;\n      }\n    }\n    END { print count }' \"$LOG_FILE\")\n\n  # Check if the error count exceeds the threshold\n  if (( error_count > ERROR_THRESHOLD )); then\n    echo \"5xx errors spike detected! Count: $error_count in the last $TIME_WINDOW seconds.\"\n  else\n    echo \"No significant 5xx error spike detected.\"\n  fi\n}\n\n# Run the function\ndetect_error_spikes\n```\n\nThis script checks for spikes in 5xx error responses within a specified time window and alerts if the count exceeds a defined threshold. Modify the `LOG_FILE`, `ERROR_THRESHOLD`, and `TIME_WINDOW` variables to suit your needs.",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <Servo.h>\n#include <SoftwareSerial.h>\n\n// Define software serial pins for Bluetooth communication\nconst int bluetoothTx = 2;\nconst int bluetoothRx = 3;\n\n// Create software serial object\nSoftwareSerial bluetooth(bluetoothTx, bluetoothRx);\n\n// Create servo object\nServo myServo;\n\n// Define the servo control pin\nconst int servoPin = 9;\n\nvoid setup() {\n  // Initialize serial communication at 9600 baud rate\n  Serial.begin(9600);\n\n  // Initialize software serial for Bluetooth at 9600 baud rate\n  bluetooth.begin(9600);\n\n  // Attach the servo to the specified pin\n  myServo.attach(servoPin);\n\n  // Initialize servo position to 90 degrees (middle position)\n  myServo.write(90);\n\n  // Debug message indicating setup completion\n  Serial.println(\"Servo Bluetooth Control Initialized\");\n}\n\nvoid loop() {\n  // Check if data is available on the Bluetooth serial connection\n  if (bluetooth.available()) {\n    // Read the data from Bluetooth\n    char command = (char)bluetooth.read();\n\n    // Debug message showing the received command\n    Serial.print(\"Received command: \");\n    Serial.println(command);\n\n    // Control the servo based on the received command\n    switch (command) {\n      case 'L': // Command to move servo left\n        myServo.write(45);\n        Serial.println(\"Servo moved to 45 degrees\");\n        break;\n\n      case 'R': // Command to move servo right\n        myServo.write(135);\n        Serial.println(\"Servo moved to 135 degrees\");\n        break;\n\n      case 'C': // Command to center the servo\n        myServo.write(90);\n        Serial.println(\"Servo centered at 90 degrees\");\n        break;\n\n      default:\n        // Debug message for unrecognized commands\n        Serial.println(\"Unknown command\");\n        break;\n    }\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *CodeGenEdge) SetTarget(v string) *CodeGenEdge {\n\ts.Target = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```yaml\n# chaos-experiment.yaml\napiVersion: litmuschaos.io/v1alpha1\nkind: ChaosEngine\nmetadata:\n  name: pod-delete-chaos\n  namespace: default\nspec:\n  # Define the application under test\n  appinfo:\n    appns: default\n    applabel: \"app=my-app\"\n    appkind: deployment\n\n  # Chaos experiment details\n  chaosServiceAccount: litmus-admin\n  experiments:\n    - name: pod-delete\n      spec:\n        components:\n          # Define the chaos parameters\n          env:\n            - name: TOTAL_CHAOS_DURATION\n              value: \"30\" # Duration in seconds\n\n            - name: CHAOS_INTERVAL\n              value: \"10\" # Interval between pod deletions\n\n            - name: FORCE\n              value: \"false\" # Force delete pods if true\n\n            - name: RANDOMNESS\n              value: \"true\" # Randomly select pods for deletion\n\n# Apply the chaos experiment\n# kubectl apply -f chaos-experiment.yaml\n```\n\n```bash\n# Monitor the chaos experiment\n# kubectl describe chaosengine pod-delete-chaos -n default\n\n# Check the status of the experiment\n# kubectl get chaosresult -n default\n\n# Clean up the experiment\n# kubectl delete -f chaos-experiment.yaml\n```\n\n```yaml\n# Optional: Define a custom chaos experiment for network latency\napiVersion: litmuschaos.io/v1alpha1\nkind: ChaosEngine\nmetadata:\n  name: network-latency-chaos\n  namespace: default\nspec:\n  appinfo:\n    appns: default\n    applabel: \"app=my-app\"\n    appkind: deployment\n\n  chaosServiceAccount: litmus-admin\n  experiments:\n    - name: network-latency\n      spec:\n        components:\n          env:\n            - name: NETWORK_LATENCY\n              value: \"200ms\" # Latency to introduce\n\n            - name: TOTAL_CHAOS_DURATION\n              value: \"60\" # Duration in seconds\n\n            - name: TARGET_CONTAINER\n              value: \"my-container\" # Target container name\n\n# Apply the custom chaos experiment\n# kubectl apply -f network-latency-chaos.yaml\n```\n\n```bash\n# Monitor the custom chaos experiment\n# kubectl describe chaosengine network-latency-chaos -n default\n\n# Clean up the custom experiment\n# kubectl delete -f network-latency-chaos.yaml\n```",
    "label": 1
  },
  {
    "codes": "function isSafeToCreateProjectIn(root, name) {\n  const validFiles = [\n    '.DS_Store',\n    'Thumbs.db',\n    '.git',\n    '.gitignore',\n    '.idea',\n    'README.md',\n    'LICENSE',\n    '.hg',\n    '.hgignore',\n    '.hgcheck',\n    '.npmignore',\n    'mkdocs.yml',\n    'docs',\n    '.travis.yml',\n    '.gitlab-ci.yml',\n    '.gitattributes',\n  ];\n  console.log();\n\n  const conflicts = fs\n    .readdirSync(root)\n    .filter(file => !validFiles.includes(file))\n    // IntelliJ IDEA creates module files before CRA is launched\n    .filter(file => !/\\.iml$/.test(file))\n    // Don't treat log files from previous installation as conflicts\n    .filter(\n      file => !errorLogFilePatterns.some(pattern => file.indexOf(pattern) === 0)\n    );\n\n  if (conflicts.length > 0) {\n    console.log(\n      `The directory ${chalk.green(name)} contains files that could conflict:`\n    );\n    console.log();\n    for (const file of conflicts) {\n      console.log(`  ${file}`);\n    }\n    console.log();\n    console.log(\n      'Either try using a new directory name, or remove the files listed above.'\n    );\n\n    return false;\n  }\n\n  // Remove any remnant files from a previous installation\n  const currentFiles = fs.readdirSync(path.join(root));\n  currentFiles.forEach(file => {\n    errorLogFilePatterns.forEach(errorLogFilePattern => {\n      // This will catch `(npm-debug|yarn-error|yarn-debug).log*` files\n      if (file.indexOf(errorLogFilePattern) === 0) {\n        fs.removeSync(path.join(root, file));\n      }\n    });\n  });\n  return true;\n}",
    "label": 0
  },
  {
    "codes": "def get_permissions(self, virtual_host):\n        \"\"\"Get all Virtual hosts permissions.\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict\n        \"\"\"\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.get(API_VIRTUAL_HOSTS_PERMISSION %\n                                    (\n                                        virtual_host\n                                    ))",
    "label": 0
  },
  {
    "codes": "def run_step(context):\n    \"\"\"Set new context keys from formatting expressions with substitutions.\n\n    Context is a dictionary or dictionary-like.\n    context['contextSetf'] must exist. It's a dictionary.\n    Will iterate context['contextSetf'] and save the values as new keys to the\n    context.\n\n    For example, say input context is:\n        key1: value1\n        key2: value2\n        key3: value3\n        contextSetf:\n            key2: 'aaa_{key1}_zzz'\n            key4: 'bbb_{key3}_yyy'\n\n    This will result in return context:\n        key1: value1\n        key2: aaa_value1_zzz\n        key3: bbb_value3_yyy\n        key4: value3\n    \"\"\"\n    logger.debug(\"started\")\n    context.assert_key_has_value(key='contextSetf', caller=__name__)\n\n    for k, v in context['contextSetf'].items():\n        logger.debug(f\"setting context {k} to value from context {v}\")\n        context[context.get_formatted_iterable(\n            k)] = context.get_formatted_iterable(v)\n\n    logger.info(f\"Set {len(context['contextSetf'])} context items.\")\n\n    logger.debug(\"done\")",
    "label": 0
  },
  {
    "codes": "func (c *Glue) ListCrawlersPagesWithContext(ctx aws.Context, input *ListCrawlersInput, fn func(*ListCrawlersOutput, bool) bool, opts ...request.Option) error {\n\tp := request.Pagination{\n\t\tNewRequest: func() (*request.Request, error) {\n\t\t\tvar inCpy *ListCrawlersInput\n\t\t\tif input != nil {\n\t\t\t\ttmp := *input\n\t\t\t\tinCpy = &tmp\n\t\t\t}\n\t\t\treq, _ := c.ListCrawlersRequest(inCpy)\n\t\t\treq.SetContext(ctx)\n\t\t\treq.ApplyOptions(opts...)\n\t\t\treturn req, nil\n\t\t},\n\t}\n\n\tcont := true\n\tfor p.Next() && cont {\n\t\tcont = fn(p.Page().(*ListCrawlersOutput), !p.HasNextPage())\n\t}\n\treturn p.Err()\n}",
    "label": 0
  },
  {
    "codes": "def _process_keystroke_movement(self, inp, idx, offset):\n        \"\"\"Process keystrokes that adjust index and offset.\"\"\"\n        term = self.term\n        if inp in (u'y', u'k') or inp.code in (term.KEY_UP,):\n            # scroll backward 1 line\n            idx, offset = (idx, offset - self.screen.num_columns)\n        elif inp in (u'e', u'j') or inp.code in (term.KEY_ENTER,\n                                                 term.KEY_DOWN,):\n            # scroll forward 1 line\n            idx, offset = (idx, offset + self.screen.num_columns)\n        elif inp in (u'f', u' ') or inp.code in (term.KEY_PGDOWN,):\n            # scroll forward 1 page\n            idx, offset = (idx + 1, offset)\n        elif inp == u'b' or inp.code in (term.KEY_PGUP,):\n            # scroll backward 1 page\n            idx, offset = (max(0, idx - 1), offset)\n        elif inp.code in (term.KEY_SDOWN,):\n            # scroll forward 10 pages\n            idx, offset = (max(0, idx + 10), offset)\n        elif inp.code in (term.KEY_SUP,):\n            # scroll forward 10 pages\n            idx, offset = (max(0, idx - 10), offset)\n        elif inp.code == term.KEY_HOME:\n            # top\n            idx, offset = (0, 0)\n        elif inp.code == term.KEY_END:\n            # bottom\n            idx, offset = (self.last_page, 0)\n        return idx, offset",
    "label": 0
  },
  {
    "codes": "```csharp\nusing Microsoft.AspNetCore.Mvc.Filters;\nusing Microsoft.AspNetCore.Mvc;\nusing System;\nusing System.Linq;\nusing System.Collections.Generic;\n\n[AttributeUsage(AttributeTargets.Class | AttributeTargets.Method, Inherited = true, AllowMultiple = false)]\npublic class ValidateJwtClaimsAttribute : Attribute, IAuthorizationFilter\n{\n    private readonly string _claimType;\n    private readonly List<string> _validValues;\n\n    public ValidateJwtClaimsAttribute(string claimType, params string[] validValues)\n    {\n        _claimType = claimType;\n        _validValues = validValues.ToList();\n    }\n\n    public void OnAuthorization(AuthorizationFilterContext context)\n    {\n        var userClaims = context.HttpContext.User.Claims;\n\n        if (!userClaims.Any(c => c.Type == _claimType && _validValues.Contains(c.Value)))\n        {\n            context.Result = new ForbidResult();\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.security.KeyStore;\nimport java.security.cert.Certificate;\nimport java.security.cert.CertificateException;\nimport java.security.cert.CertificateFactory;\n\npublic class KeyStoreWrapper {\n\n    private KeyStore keyStore;\n    private final String keyStoreFile;\n    private final char[] password;\n\n    // Constructor initializes the KeyStore\n    public KeyStoreWrapper(String keyStoreFile, char[] password) throws Exception {\n        this.keyStoreFile = keyStoreFile;\n        this.password = password;\n        keyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n        try (FileInputStream fis = new FileInputStream(keyStoreFile)) {\n            keyStore.load(fis, password);\n        } catch (IOException e) {\n            // Initializes a new keystore if file does not exist\n            keyStore.load(null, password);\n        }\n    }\n\n    // Adds a certificate to the KeyStore\n    public void addCertificate(String alias, String certFilePath) throws Exception {\n        try (FileInputStream fis = new FileInputStream(certFilePath)) {\n            CertificateFactory cf = CertificateFactory.getInstance(\"X.509\");\n            Certificate cert = cf.generateCertificate(fis);\n            keyStore.setCertificateEntry(alias, cert);\n            saveKeyStore();\n        }\n    }\n\n    // Removes a certificate from the KeyStore\n    public void removeCertificate(String alias) throws Exception {\n        keyStore.deleteEntry(alias);\n        saveKeyStore();\n    }\n\n    // Retrieves a certificate from the KeyStore\n    public Certificate getCertificate(String alias) throws Exception {\n        return keyStore.getCertificate(alias);\n    }\n\n    // Saves the KeyStore back to the file system\n    private void saveKeyStore() throws Exception {\n        try (FileOutputStream fos = new FileOutputStream(keyStoreFile)) {\n            keyStore.store(fos, password);\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "func (s *GetDatabasesInput) SetMaxResults(v int64) *GetDatabasesInput {\n\ts.MaxResults = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<DatabaseInner>>> listByServerWithServiceResponseAsync(final String resourceGroupName, final String serverName) {\n        return listByServerSinglePageAsync(resourceGroupName, serverName)\n            .concatMap(new Func1<ServiceResponse<Page<DatabaseInner>>, Observable<ServiceResponse<Page<DatabaseInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<DatabaseInner>>> call(ServiceResponse<Page<DatabaseInner>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(listByServerNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```java\n// Import necessary libraries\nimport static org.mockito.Mockito.*;\nimport org.junit.jupiter.api.Test;\nimport org.mockito.InjectMocks;\nimport org.mockito.Mock;\nimport org.mockito.MockitoAnnotations;\nimport javax.sql.DataSource;\nimport java.sql.Connection;\nimport java.sql.SQLException;\n\n// Test class for verifying database transaction rollbacks\npublic class DatabaseRollbackTest {\n\n    // Mock the DataSource and Connection\n    @Mock\n    private DataSource dataSource;\n    @Mock\n    private Connection connection;\n\n    // Inject mocks into the class under test\n    @InjectMocks\n    private DatabaseService databaseService;\n\n    // Initialize mocks before each test\n    public DatabaseRollbackTest() {\n        MockitoAnnotations.openMocks(this);\n    }\n\n    @Test\n    public void testTransactionRollback() throws SQLException {\n        // Arrange: Set up mock behavior\n        when(dataSource.getConnection()).thenReturn(connection);\n        doThrow(new SQLException(\"Simulated SQL error\")).when(connection).commit();\n\n        // Act: Perform the operation that should trigger a rollback\n        try {\n            databaseService.performTransaction();\n        } catch (SQLException e) {\n            // Expected exception\n        }\n\n        // Assert: Verify that rollback was called\n        verify(connection, times(1)).rollback();\n    }\n}\n\n// Class under test\nclass DatabaseService {\n    private DataSource dataSource;\n\n    public DatabaseService(DataSource dataSource) {\n        this.dataSource = dataSource;\n    }\n\n    public void performTransaction() throws SQLException {\n        Connection connection = dataSource.getConnection();\n        try {\n            connection.setAutoCommit(false);\n            // Simulate database operations\n            connection.commit();\n        } catch (SQLException e) {\n            connection.rollback();\n            throw e;\n        } finally {\n            connection.close();\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function nodeCollector_filter(aCallback, aThisObject) {\n    if (!aCallback)\n      throw new Error(arguments.callee.name + \": No callback specified\");\n\n    this.nodes = Array.filter(this.nodes, aCallback, aThisObject);\n\n    return this;\n  }",
    "label": 0
  },
  {
    "codes": "```c\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <stdio.h>\n#include <string.h>\n\n#define AES_GCM_KEY_SIZE 32  // 256-bit key\n#define AES_GCM_IV_SIZE 12   // 96-bit IV\n#define AES_GCM_TAG_SIZE 16  // 128-bit tag\n\n// Function to perform AES-GCM encryption\nint aes_gcm_encrypt(const unsigned char *plaintext, size_t plaintext_len,\n                    const unsigned char *key, const unsigned char *iv,\n                    unsigned char *ciphertext, unsigned char *tag) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int ciphertext_len;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        return -1;\n    }\n\n    // Initialize the encryption operation\n    if (1 != EVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Set the key and IV\n    if (1 != EVP_EncryptInit_ex(ctx, NULL, NULL, key, iv)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Encrypt the plaintext\n    if (1 != EVP_EncryptUpdate(ctx, ciphertext, &len, plaintext, plaintext_len)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    ciphertext_len = len;\n\n    // Finalize the encryption\n    if (1 != EVP_EncryptFinal_ex(ctx, ciphertext + len, &len)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    ciphertext_len += len;\n\n    // Get the tag\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_GET_TAG, AES_GCM_TAG_SIZE, tag)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    return ciphertext_len;\n}\n\n// Function to perform AES-GCM decryption\nint aes_gcm_decrypt(const unsigned char *ciphertext, size_t ciphertext_len,\n                    const unsigned char *key, const unsigned char *iv,\n                    const unsigned char *tag, unsigned char *plaintext) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int plaintext_len;\n    int ret;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        return -1;\n    }\n\n    // Initialize the decryption operation\n    if (1 != EVP_DecryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, NULL, NULL)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Set the key and IV\n    if (1 != EVP_DecryptInit_ex(ctx, NULL, NULL, key, iv)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Decrypt the ciphertext\n    if (1 != EVP_DecryptUpdate(ctx, plaintext, &len, ciphertext, ciphertext_len)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    plaintext_len = len;\n\n    // Set the expected tag value\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_TAG, AES_GCM_TAG_SIZE, (void *)tag)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Finalize the decryption\n    ret = EVP_DecryptFinal_ex(ctx, plaintext + len, &len);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    if (ret > 0) {\n        plaintext_len += len;\n        return plaintext_len;\n    } else {\n        return -1;  // Decryption failed (tag mismatch)\n    }\n}\n\n// Example usage\nint main() {\n    unsigned char key[AES_GCM_KEY_SIZE];\n    unsigned char iv[AES_GCM_IV_SIZE];\n    unsigned char plaintext[] = \"This is a secret message.\";\n    unsigned char ciphertext[128];\n    unsigned char decryptedtext[128];\n    unsigned char tag[AES_GCM_TAG_SIZE];\n    int ciphertext_len, decryptedtext_len;\n\n    // Generate random key and IV\n    if (!RAND_bytes(key, AES_GCM_KEY_SIZE) || !RAND_bytes(iv, AES_GCM_IV_SIZE)) {\n        fprintf(stderr, \"Error generating random key or IV\\n\");\n        return 1;\n    }\n\n    // Encrypt the plaintext\n    ciphertext_len = aes_gcm_encrypt(plaintext, strlen((char *)plaintext), key, iv, ciphertext, tag);\n    if (ciphertext_len == -1) {\n        fprintf(stderr, \"Encryption failed\\n\");\n        return 1;\n    }\n\n    // Decrypt the ciphertext\n    decryptedtext_len = aes_gcm_decrypt(ciphertext, ciphertext_len, key, iv, tag, decryptedtext);\n    if (decryptedtext_len == -1) {\n        fprintf(stderr, \"Decryption failed\\n\");\n        return 1;\n    }\n\n    // Null-terminate the decrypted text\n    decryptedtext[decryptedtext_len] = '\\0';\n\n    // Print the results\n    printf(\"Original: %s\\n\", plaintext);\n    printf(\"Decrypted: %s\\n\", decryptedtext);\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/segmentio/kafka-go\"\n)\n\n// worker processes messages from the input channel and sends results to the output channel\nfunc worker(id int, in <-chan kafka.Message, out chan<- string, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\tfor msg := range in {\n\t\t// Simulate processing\n\t\ttime.Sleep(100 * time.Millisecond)\n\t\tresult := fmt.Sprintf(\"Worker %d processed message: %s\", id, string(msg.Value))\n\t\tout <- result\n\t}\n}\n\n// fanIn collects results from multiple workers into a single channel\nfunc fanIn(out chan<- string, inputs ...<-chan string) {\n\tvar wg sync.WaitGroup\n\twg.Add(len(inputs))\n\n\tfor _, in := range inputs {\n\t\tgo func(ch <-chan string) {\n\t\t\tdefer wg.Done()\n\t\t\tfor result := range ch {\n\t\t\t\tout <- result\n\t\t\t}\n\t\t}(in)\n\t}\n\n\twg.Wait()\n\tclose(out)\n}\n\nfunc main() {\n\t// Kafka reader configuration\n\treader := kafka.NewReader(kafka.ReaderConfig{\n\t\tBrokers:   []string{\"localhost:9092\"},\n\t\tTopic:     \"test-topic\",\n\t\tPartition: 0,\n\t\tMinBytes:  10e3, // 10KB\n\t\tMaxBytes:  10e6, // 10MB\n\t})\n\tdefer reader.Close()\n\n\t// Channels for fan-out and fan-in\n\tmsgChan := make(chan kafka.Message, 10)\n\tresultChan := make(chan string, 10)\n\tworkerChans := make([]chan string, 3)\n\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < 3; i++ {\n\t\tworkerChans[i] = make(chan string, 10)\n\t\twg.Add(1)\n\t\tgo worker(i, msgChan, workerChans[i], &wg)\n\t}\n\n\t// Start fan-in process\n\tgo fanIn(resultChan, workerChans...)\n\n\t// Read messages from Kafka and send to workers\n\tgo func() {\n\t\tfor {\n\t\t\tmsg, err := reader.ReadMessage(context.Background())\n\t\t\tif err != nil {\n\t\t\t\tfmt.Println(\"Error reading message:\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tmsgChan <- msg\n\t\t}\n\t}()\n\n\t// Collect and print results\n\tfor result := range resultChan {\n\t\tfmt.Println(result)\n\t}\n\n\t// Wait for all workers to finish\n\twg.Wait()\n\tclose(msgChan)\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *GetPlanOutput) SetPythonScript(v string) *GetPlanOutput {\n\ts.PythonScript = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.springframework.stereotype.Component;\n\n@Aspect\n@Component\npublic class CircuitBreakerAspect {\n\n    private static final int MAX_FAILURES = 3; // Maximum allowed failures\n    private int failureCount = 0; // Counter for failures\n    private boolean circuitOpen = false; // Circuit breaker state\n\n    @Around(\"@annotation(CircuitBreaker)\") // Apply to methods annotated with @CircuitBreaker\n    public Object handleCircuitBreaker(ProceedingJoinPoint joinPoint) throws Throwable {\n        if (circuitOpen) {\n            throw new RuntimeException(\"Circuit is open. Service unavailable.\");\n        }\n\n        try {\n            Object result = joinPoint.proceed(); // Execute the method\n            failureCount = 0; // Reset failure count on success\n            return result;\n        } catch (Exception e) {\n            failureCount++; // Increment failure count on exception\n            if (failureCount >= MAX_FAILURES) {\n                circuitOpen = true; // Open the circuit if max failures reached\n            }\n            throw e; // Re-throw the exception\n        }\n    }\n}\n```\n\n```java\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\n@Target(ElementType.METHOD) // Apply to methods\n@Retention(RetentionPolicy.RUNTIME) // Available at runtime\npublic @interface CircuitBreaker {\n    // Marker annotation for methods to be wrapped with circuit breaker\n}\n```\n\n```java\nimport org.springframework.stereotype.Service;\n\n@Service\npublic class ExampleService {\n\n    @CircuitBreaker // Apply circuit breaker to this method\n    public String performOperation() {\n        // Simulate a service call that might fail\n        if (Math.random() > 0.5) {\n            throw new RuntimeException(\"Service failure\");\n        }\n        return \"Operation successful\";\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "def send(self, stanza, *, timeout=None, cb=None):\n        \"\"\"\n        Send a stanza.\n\n        :param stanza: Stanza to send\n        :type stanza: :class:`~.IQ`, :class:`~.Presence` or :class:`~.Message`\n        :param timeout: Maximum time in seconds to wait for an IQ response, or\n                        :data:`None` to disable the timeout.\n        :type timeout: :class:`~numbers.Real` or :data:`None`\n        :param cb: Optional callback which is called synchronously when the\n            reply is received (IQ requests only!)\n        :raise OSError: if the underlying XML stream fails and stream\n            management is not disabled.\n        :raise aioxmpp.stream.DestructionRequested:\n           if the stream is closed while sending the stanza or waiting for a\n           response.\n        :raise aioxmpp.errors.XMPPError: if an error IQ response is received\n        :raise aioxmpp.errors.ErroneousStanza: if the IQ response could not be\n            parsed\n        :raise ValueError: if `cb` is given and `stanza` is not an IQ request.\n        :return: IQ response :attr:`~.IQ.payload` or :data:`None`\n\n        Send the stanza and wait for it to be sent. If the stanza is an IQ\n        request, the response is awaited and the :attr:`~.IQ.payload` of the\n        response is returned.\n\n        If the stream is currently not ready, this method blocks until the\n        stream is ready to send payload stanzas. Note that this may be before\n        initial presence has been sent. To synchronise with that type of\n        events, use the appropriate signals.\n\n        The `timeout` as well as any of the exception cases referring to a\n        \"response\" do not apply for IQ response stanzas, message stanzas or\n        presence stanzas sent with this method, as this method only waits for\n        a reply if an IQ *request* stanza is being sent.\n\n        If `stanza` is an IQ request and the response is not received within\n        `timeout` seconds, :class:`TimeoutError` (not\n        :class:`asyncio.TimeoutError`!) is raised.\n\n        If `cb` is given, `stanza` must be an IQ request (otherwise,\n        :class:`ValueError` is raised before the stanza is sent). It must be a\n        callable returning an awaitable. It receives the response stanza as\n        first and only argument. The returned awaitable is awaited by\n        :meth:`send` and the result is returned instead of the original\n        payload. `cb` is called synchronously from the stream handling loop\n        when the response is received, so it can benefit from the strong\n        ordering guarantees given by XMPP XML Streams.\n\n        The `cb` may also return :data:`None`, in which case :meth:`send` will\n        simply return the IQ payload as if `cb` was not given. Since the return\n        value of coroutine functions is awaitable, it is valid and supported to\n        pass a coroutine function as `cb`.\n\n        .. warning::\n\n            Remember that it is an implementation detail of the event loop when\n            a coroutine is scheduled after it awaited an awaitable; this\n            implies that if the caller of :meth:`send` is merely awaiting the\n            :meth:`send` coroutine, the strong ordering guarantees of XMPP XML\n            Streams are lost.\n\n            To regain those, use the `cb` argument.\n\n        .. note::\n\n            For the sake of readability, unless you really need the strong\n            ordering guarantees, avoid the use of the `cb` argument. Avoid\n            using a coroutine function unless you really need to.\n\n        .. versionchanged:: 0.10\n\n            * This method now waits until the stream is ready to send stanza\u00b8\n              payloads.\n            * This method was moved from\n              :meth:`aioxmpp.stream.StanzaStream.send`.\n\n        .. versionchanged:: 0.9\n\n            The `cb` argument was added.\n\n        .. versionadded:: 0.8\n        \"\"\"\n        if not self.running:\n            raise ConnectionError(\"client is not running\")\n\n        if not self.established:\n            self.logger.debug(\"send(%s): stream not established, waiting\",\n                              stanza)\n            # wait for the stream to be established\n            stopped_fut = self.on_stopped.future()\n            failure_fut = self.on_failure.future()\n            established_fut = asyncio.ensure_future(\n                self.established_event.wait()\n            )\n            done, pending = yield from asyncio.wait(\n                [\n                    established_fut,\n                    failure_fut,\n                    stopped_fut,\n                ],\n                return_when=asyncio.FIRST_COMPLETED,\n            )\n            if not established_fut.done():\n                established_fut.cancel()\n            if failure_fut.done():\n                if not stopped_fut.done():\n                    stopped_fut.cancel()\n                failure_fut.exception()\n                raise ConnectionError(\"client failed to connect\")\n            if stopped_fut.done():\n                raise ConnectionError(\"client shut down by user request\")\n\n            self.logger.debug(\"send(%s): stream established, sending\")\n\n        return (yield from self.stream._send_immediately(stanza,\n                                                         timeout=timeout,\n                                                         cb=cb))",
    "label": 0
  },
  {
    "codes": "```java\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Pointcut;\nimport org.springframework.stereotype.Component;\n\n@Component\n@Aspect\npublic class CircuitBreakerAspect {\n\n    // Define a pointcut to match methods that need circuit breaker functionality\n    @Pointcut(\"execution(* com.example.service.*.*(..))\")\n    public void circuitBreakerMethods() {}\n\n    @Around(\"circuitBreakerMethods()\")\n    public Object wrapWithCircuitBreaker(ProceedingJoinPoint joinPoint) throws Throwable {\n        CircuitBreaker circuitBreaker = CircuitBreakerRegistry\n                .ofDefaults()\n                .circuitBreaker(joinPoint.getSignature().toShortString());\n\n        // Execute the method with circuit breaker\n        return circuitBreaker.executeSupplier(() -> {\n            try {\n                // Proceed with the method execution\n                return joinPoint.proceed();\n            } catch (Throwable throwable) {\n                throw new RuntimeException(\"Method execution failed\", throwable);\n            }\n        });\n    }\n}\n```\n\n```xml\n<!-- Add necessary dependencies for Resilience4j and Spring AOP in your pom.xml -->\n<dependency>\n    <groupId>io.github.resilience4j</groupId>\n    <artifactId>resilience4j-spring-boot2</artifactId>\n    <version>1.7.0</version>\n</dependency>\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-aop</artifactId>\n</dependency>\n```\n\n```yaml\n# Optional: Configure default circuit breaker settings in application.yml\nresilience4j.circuitbreaker:\n  configs:\n    default:\n      registerHealthIndicator: true\n      slidingWindowSize: 100\n      failureRateThreshold: 50\n```\n\n```java\nimport io.github.resilience4j.circuitbreaker.CircuitBreaker;\nimport io.github.resilience4j.circuitbreaker.CircuitBreakerRegistry;\n```\n\n```java\n// Note: Make sure to configure your Spring application to enable AspectJ auto proxying\n@SpringBootApplication\n@EnableAspectJAutoProxy\npublic class Application {}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<KeyBundle>> restoreKeyWithServiceResponseAsync(String vaultBaseUrl, byte[] keyBundleBackup) {\n        if (vaultBaseUrl == null) {\n            throw new IllegalArgumentException(\"Parameter vaultBaseUrl is required and cannot be null.\");\n        }\n        if (this.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.apiVersion() is required and cannot be null.\");\n        }\n        if (keyBundleBackup == null) {\n            throw new IllegalArgumentException(\"Parameter keyBundleBackup is required and cannot be null.\");\n        }\n        KeyRestoreParameters parameters = new KeyRestoreParameters();\n        parameters.withKeyBundleBackup(keyBundleBackup);\n        String parameterizedHost = Joiner.on(\", \").join(\"{vaultBaseUrl}\", vaultBaseUrl);\n        return service.restoreKey(this.apiVersion(), this.acceptLanguage(), parameters, parameterizedHost, this.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<KeyBundle>>>() {\n                @Override\n                public Observable<ServiceResponse<KeyBundle>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<KeyBundle> clientResponse = restoreKeyDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def page_data(self, idx, offset):\n        \"\"\"\n        Return character data for page of given index and offset.\n\n        :param idx: page index.\n        :type idx: int\n        :param offset: scrolling region offset of current page.\n        :type offset: int\n        :returns: list of tuples in form of ``(ucs, name)``\n        :rtype: list[(unicode, unicode)]\n        \"\"\"\n        size = self.screen.page_size\n\n        while offset < 0 and idx:\n            offset += size\n            idx -= 1\n        offset = max(0, offset)\n\n        while offset >= size:\n            offset -= size\n            idx += 1\n\n        if idx == self.last_page:\n            offset = 0\n        idx = min(max(0, idx), self.last_page)\n\n        start = (idx * self.screen.page_size) + offset\n        end = start + self.screen.page_size\n        return (idx, offset), self._page_data[start:end]",
    "label": 0
  },
  {
    "codes": "def _parse_ssl_options(self, ssl_kwargs):\n        \"\"\"Parse TLS Options.\n\n        :param ssl_kwargs:\n        :rtype: dict\n        \"\"\"\n        ssl_options = {}\n        for key in ssl_kwargs:\n            if key not in compatibility.SSL_OPTIONS:\n                LOGGER.warning('invalid option: %s', key)\n                continue\n            if 'ssl_version' in key:\n                value = self._get_ssl_version(ssl_kwargs[key][0])\n            elif 'cert_reqs' in key:\n                value = self._get_ssl_validation(ssl_kwargs[key][0])\n            else:\n                value = ssl_kwargs[key][0]\n            ssl_options[key] = value\n        return ssl_options",
    "label": 0
  },
  {
    "codes": "```dart\n// Import necessary Flutter and Dart packages\nimport 'dart:async';\nimport 'package:flutter/services.dart';\n\n// Define a class for the Flutter plugin\nclass MLAcceleratorPlugin {\n  // Create a MethodChannel to communicate with native code\n  static const MethodChannel _channel = MethodChannel('ml_accelerator_plugin');\n\n  // Method to check if the device has a native ML accelerator\n  static Future<bool> hasMLAccelerator() async {\n    try {\n      final bool result = await _channel.invokeMethod('hasMLAccelerator');\n      return result;\n    } on PlatformException catch (e) {\n      print(\"Failed to check ML accelerator: ${e.message}\");\n      return false;\n    }\n  }\n\n  // Method to perform inference using the native ML accelerator\n  static Future<String> performInference(String modelPath, String inputData) async {\n    try {\n      final String result = await _channel.invokeMethod(\n        'performInference',\n        {'modelPath': modelPath, 'inputData': inputData},\n      );\n      return result;\n    } on PlatformException catch (e) {\n      print(\"Failed to perform inference: ${e.message}\");\n      return \"Error: ${e.message}\";\n    }\n  }\n}\n```\n\n```kotlin\n// Android native code (MainActivity.kt)\npackage com.example.mlacceleratorplugin\n\nimport android.content.Context\nimport android.os.Build\nimport androidx.annotation.NonNull\nimport io.flutter.embedding.android.FlutterActivity\nimport io.flutter.embedding.engine.FlutterEngine\nimport io.flutter.plugin.common.MethodChannel\n\nclass MainActivity: FlutterActivity() {\n    private val CHANNEL = \"ml_accelerator_plugin\"\n\n    override fun configureFlutterEngine(@NonNull flutterEngine: FlutterEngine) {\n        super.configureFlutterEngine(flutterEngine)\n        MethodChannel(flutterEngine.dartExecutor.binaryMessenger, CHANNEL).setMethodCallHandler { call, result ->\n            when (call.method) {\n                \"hasMLAccelerator\" -> {\n                    result.success(hasMLAccelerator())\n                }\n                \"performInference\" -> {\n                    val modelPath = call.argument<String>(\"modelPath\")\n                    val inputData = call.argument<String>(\"inputData\")\n                    result.success(performInference(modelPath, inputData))\n                }\n                else -> {\n                    result.notImplemented()\n                }\n            }\n        }\n    }\n\n    private fun hasMLAccelerator(): Boolean {\n        return Build.VERSION.SDK_INT >= Build.VERSION_CODES.P && packageManager.hasSystemFeature(\"android.hardware.neuralnetworks\")\n    }\n\n    private fun performInference(modelPath: String?, inputData: String?): String {\n        // Implement inference logic using Android's Neural Networks API or other ML libraries\n        return \"Inference result from Android\"\n    }\n}\n```\n\n```swift\n// iOS native code (AppDelegate.swift)\nimport UIKit\nimport Flutter\n\n@UIApplicationMain\n@objc class AppDelegate: FlutterAppDelegate {\n    override func application(\n        _ application: UIApplication,\n        didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?\n    ) -> Bool {\n        let controller : FlutterViewController = window?.rootViewController as! FlutterViewController\n        let mlAcceleratorChannel = FlutterMethodChannel(name: \"ml_accelerator_plugin\",\n                                                      binaryMessenger: controller.binaryMessenger)\n        mlAcceleratorChannel.setMethodCallHandler({\n            (call: FlutterMethodCall, result: @escaping FlutterResult) -> Void in\n            switch call.method {\n            case \"hasMLAccelerator\":\n                result(self.hasMLAccelerator())\n            case \"performInference\":\n                let args = call.arguments as? [String: Any]\n                let modelPath = args?[\"modelPath\"] as? String\n                let inputData = args?[\"inputData\"] as? String\n                result(self.performInference(modelPath: modelPath, inputData: inputData))\n            default:\n                result(FlutterMethodNotImplemented)\n            }\n        })\n\n        GeneratedPluginRegistrant.register(with: self)\n        return super.application(application, didFinishLaunchingWithOptions: launchOptions)\n    }\n\n    private func hasMLAccelerator() -> Bool {\n        // Check if the device has a Neural Engine (e.g., A12 Bionic or later)\n        if #available(iOS 12.0, *) {\n            return true\n        } else {\n            return false\n        }\n    }\n\n    private func performInference(modelPath: String?, inputData: String?) -> String {\n        // Implement inference logic using Core ML or other ML libraries\n        return \"Inference result from iOS\"\n    }\n}\n```\n\n```gradle\n// Android build.gradle (Module: app)\nandroid {\n    ...\n    defaultConfig {\n        ...\n        // Enable ML accelerator support\n        ndk {\n            abiFilters \"armeabi-v7a\", \"arm64-v8a\", \"x86\", \"x86_64\"\n        }\n    }\n    ...\n}\n```\n\n```yaml\n# pubspec.yaml\nflutter:\n  plugin:\n    platforms:\n      android:\n        package: com.example.mlacceleratorplugin\n        pluginClass: MLAcceleratorPlugin\n      ios:\n        pluginClass: MLAcceleratorPlugin\n```",
    "label": 1
  },
  {
    "codes": "function calculateBase(groupItem) {\n    var extent;\n    var baseAxis = groupItem.axis;\n    var seriesModels = groupItem.seriesModels;\n    var seriesCount = seriesModels.length;\n\n    var boxWidthList = groupItem.boxWidthList = [];\n    var boxOffsetList = groupItem.boxOffsetList = [];\n    var boundList = [];\n\n    var bandWidth;\n    if (baseAxis.type === 'category') {\n        bandWidth = baseAxis.getBandWidth();\n    }\n    else {\n        var maxDataCount = 0;\n        each(seriesModels, function (seriesModel) {\n            maxDataCount = Math.max(maxDataCount, seriesModel.getData().count());\n        });\n        extent = baseAxis.getExtent(),\n        Math.abs(extent[1] - extent[0]) / maxDataCount;\n    }\n\n    each(seriesModels, function (seriesModel) {\n        var boxWidthBound = seriesModel.get('boxWidth');\n        if (!zrUtil.isArray(boxWidthBound)) {\n            boxWidthBound = [boxWidthBound, boxWidthBound];\n        }\n        boundList.push([\n            parsePercent(boxWidthBound[0], bandWidth) || 0,\n            parsePercent(boxWidthBound[1], bandWidth) || 0\n        ]);\n    });\n\n    var availableWidth = bandWidth * 0.8 - 2;\n    var boxGap = availableWidth / seriesCount * 0.3;\n    var boxWidth = (availableWidth - boxGap * (seriesCount - 1)) / seriesCount;\n    var base = boxWidth / 2 - availableWidth / 2;\n\n    each(seriesModels, function (seriesModel, idx) {\n        boxOffsetList.push(base);\n        base += boxGap + boxWidth;\n\n        boxWidthList.push(\n            Math.min(Math.max(boxWidth, boundList[idx][0]), boundList[idx][1])\n        );\n    });\n}",
    "label": 0
  },
  {
    "codes": "func (s *BatchDeletePartitionOutput) SetErrors(v []*PartitionError) *BatchDeletePartitionOutput {\n\ts.Errors = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```dart\n// Import necessary packages\nimport 'package:flutter/services.dart';\n\n// Define a method channel to communicate with native code\nconst MethodChannel _channel = MethodChannel('ml_accelerator_plugin');\n\n// Function to check if ML accelerator is available\nFuture<bool> isMLAcceleratorAvailable() async {\n  try {\n    final bool isAvailable = await _channel.invokeMethod('isMLAcceleratorAvailable');\n    return isAvailable;\n  } on PlatformException catch (e) {\n    print(\"Failed to check ML accelerator availability: ${e.message}\");\n    return false;\n  }\n}\n\n// Function to perform inference using the ML accelerator\nFuture<List<double>> performInference(List<double> inputData) async {\n  try {\n    final List<dynamic> result = await _channel.invokeMethod('performInference', inputData);\n    return result.cast<double>();\n  } on PlatformException catch (e) {\n    print(\"Failed to perform inference: ${e.message}\");\n    return [];\n  }\n}\n\n// Native code for Android (Java/Kotlin) and iOS (Swift/Objective-C) would be required\n// to implement the actual functionality of the ML accelerator.\n```\n\n```java\n// Android Native Code (Java)\npackage com.example.mlacceleratorplugin;\n\nimport androidx.annotation.NonNull;\nimport io.flutter.embedding.engine.plugins.FlutterPlugin;\nimport io.flutter.plugin.common.MethodCall;\nimport io.flutter.plugin.common.MethodChannel;\nimport io.flutter.plugin.common.MethodChannel.MethodCallHandler;\nimport io.flutter.plugin.common.MethodChannel.Result;\n\npublic class MLAcceleratorPlugin implements FlutterPlugin, MethodCallHandler {\n  private MethodChannel channel;\n\n  @Override\n  public void onAttachedToEngine(@NonNull FlutterPluginBinding flutterPluginBinding) {\n    channel = new MethodChannel(flutterPluginBinding.getBinaryMessenger(), \"ml_accelerator_plugin\");\n    channel.setMethodCallHandler(this);\n  }\n\n  @Override\n  public void onMethodCall(@NonNull MethodCall call, @NonNull Result result) {\n    if (call.method.equals(\"isMLAcceleratorAvailable\")) {\n      // Check if ML accelerator is available\n      boolean isAvailable = checkMLAcceleratorAvailability();\n      result.success(isAvailable);\n    } else if (call.method.equals(\"performInference\")) {\n      // Perform inference using ML accelerator\n      List<Double> inputData = call.arguments();\n      List<Double> outputData = performInference(inputData);\n      result.success(outputData);\n    } else {\n      result.notImplemented();\n    }\n  }\n\n  private boolean checkMLAcceleratorAvailability() {\n    // Implement logic to check ML accelerator availability\n    return false;\n  }\n\n  private List<Double> performInference(List<Double> inputData) {\n    // Implement logic to perform inference using ML accelerator\n    return new ArrayList<>();\n  }\n\n  @Override\n  public void onDetachedFromEngine(@NonNull FlutterPluginBinding binding) {\n    channel.setMethodCallHandler(null);\n  }\n}\n```\n\n```swift\n// iOS Native Code (Swift)\nimport Flutter\nimport UIKit\n\npublic class SwiftMLAcceleratorPlugin: NSObject, FlutterPlugin {\n  public static func register(with registrar: FlutterPluginRegistrar) {\n    let channel = FlutterMethodChannel(name: \"ml_accelerator_plugin\", binaryMessenger: registrar.messenger())\n    let instance = SwiftMLAcceleratorPlugin()\n    registrar.addMethodCallDelegate(instance, channel: channel)\n  }\n\n  public func handle(_ call: FlutterMethodCall, result: @escaping FlutterResult) {\n    if call.method == \"isMLAcceleratorAvailable\" {\n      // Check if ML accelerator is available\n      let isAvailable = checkMLAcceleratorAvailability()\n      result(isAvailable)\n    } else if call.method == \"performInference\" {\n      // Perform inference using ML accelerator\n      guard let inputData = call.arguments as? [Double] else {\n        result(FlutterError(code: \"INVALID_ARGUMENT\", message: \"Invalid input data\", details: nil))\n        return\n      }\n      let outputData = performInference(inputData: inputData)\n      result(outputData)\n    } else {\n      result(FlutterMethodNotImplemented)\n    }\n  }\n\n  private func checkMLAcceleratorAvailability() -> Bool {\n    // Implement logic to check ML accelerator availability\n    return false\n  }\n\n  private func performInference(inputData: [Double]) -> [Double] {\n    // Implement logic to perform inference using ML accelerator\n    return []\n  }\n}\n```\n\n```yaml\n# pubspec.yaml\nname: ml_accelerator_plugin\ndescription: A Flutter plugin to access device-native ML accelerators.\nversion: 0.0.1\ndependencies:\n  flutter:\n    sdk: flutter\nflutter:\n  plugin:\n    platforms:\n      android:\n        package: com.example.mlacceleratorplugin\n        pluginClass: MLAcceleratorPlugin\n      ios:\n        pluginClass: SwiftMLAcceleratorPlugin\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <stdexcept>\n\nclass DatabaseConnection {\npublic:\n    // Constructor: Establishes a database connection\n    DatabaseConnection(const std::string& connectionString) {\n        std::cout << \"Connecting to database: \" << connectionString << std::endl;\n        // Simulate connection establishment\n        if (connectionString.empty()) {\n            throw std::runtime_error(\"Invalid connection string\");\n        }\n        // Assume connection is successful\n        isConnected = true;\n    }\n\n    // Destructor: Ensures the connection is closed\n    ~DatabaseConnection() {\n        if (isConnected) {\n            std::cout << \"Disconnecting from database\" << std::endl;\n            // Simulate disconnection\n            isConnected = false;\n        }\n    }\n\n    // Execute a query\n    void executeQuery(const std::string& query) {\n        if (!isConnected) {\n            throw std::runtime_error(\"Database not connected\");\n        }\n        std::cout << \"Executing query: \" << query << std::endl;\n        // Simulate query execution\n    }\n\n    // Copy constructor and assignment operator are deleted to prevent copying\n    DatabaseConnection(const DatabaseConnection&) = delete;\n    DatabaseConnection& operator=(const DatabaseConnection&) = delete;\n\nprivate:\n    bool isConnected = false; // Tracks connection status\n};\n\n// Example usage\nint main() {\n    try {\n        DatabaseConnection db(\"valid_connection_string\");\n        db.executeQuery(\"SELECT * FROM users\");\n    } catch (const std::exception& e) {\n        std::cerr << \"Error: \" << e.what() << std::endl;\n    }\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *Trail) SetSnsTopicName(v string) *Trail {\n\ts.SnsTopicName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *TableError) SetTableName(v string) *TableError {\n\ts.TableName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<SharedAccessSignatureAuthorizationRuleInner>>> listKeysSinglePageAsync(final String resourceGroupName, final String resourceName) {\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (resourceName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceName is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        return service.listKeys(this.client.subscriptionId(), resourceGroupName, resourceName, this.client.apiVersion(), this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<Page<SharedAccessSignatureAuthorizationRuleInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<SharedAccessSignatureAuthorizationRuleInner>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<PageImpl<SharedAccessSignatureAuthorizationRuleInner>> result = listKeysDelegate(response);\n                        return Observable.just(new ServiceResponse<Page<SharedAccessSignatureAuthorizationRuleInner>>(result.body(), result.response()));\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport uuid\n\ndef generate_uuid_v4():\n    # Generate a UUID v4 and remove hyphens\n    return str(uuid.uuid4()).replace('-', '')\n```",
    "label": 1
  },
  {
    "codes": "def _set_axis_properties(self, axis):\n        \"\"\"Set AxisProperties and PropertySets\"\"\"\n        if not getattr(axis, 'properties'):\n            axis.properties = AxisProperties()\n            for prop in ['ticks', 'axis', 'major_ticks', 'minor_ticks',\n                         'title', 'labels']:\n                setattr(axis.properties, prop, PropertySet())",
    "label": 0
  },
  {
    "codes": "```python\nimport xml.etree.ElementTree as ET\n\ndef parse_large_xml(file_path):\n    \"\"\"\n    Generator to yield elements from a large XML file one by one.\n    \"\"\"\n    context = ET.iterparse(file_path, events=(\"start\", \"end\"))\n    context = iter(context)\n    \n    event, root = next(context)  # Get the root element\n    \n    for event, elem in context:\n        if event == \"end\" and elem.tag != root.tag:  # Yield non-root elements\n            yield elem\n            root.clear()  # Free memory by clearing processed elements\n```",
    "label": 1
  },
  {
    "codes": "func (s *CrawlerMetrics) SetTablesUpdated(v int64) *CrawlerMetrics {\n\ts.TablesUpdated = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function addonsManager_getSearchFilter(aSpec) {\n    var spec = aSpec || { };\n\n    return this.getElements({\n      type: \"search_filterRadioButtons\",\n      subtype: spec.attribute,\n      value: spec.value\n    });\n  }",
    "label": 0
  },
  {
    "codes": "```typescript\nfunction Retry(retries: number = 3, delay: number = 1000) {\n  return function(target: any, propertyName: string, descriptor: PropertyDescriptor) {\n    const originalMethod = descriptor.value;\n\n    descriptor.value = async function(...args: any[]) {\n      let attempts = 0;\n\n      while (attempts < retries) {\n        try {\n          return await originalMethod.apply(this, args);\n        } catch (error) {\n          attempts++;\n          if (attempts >= retries) {\n            throw error; // If it fails on the last attempt, throw the error\n          }\n          await new Promise(res => setTimeout(res, delay)); // Wait before retrying\n        }\n      }\n    };\n\n    return descriptor;\n  };\n}\n\nclass Example {\n  @Retry(5, 2000) // Retry up to 5 times with a 2 second delay\n  async fetchData() {\n    // Simulate an operation that may fail\n    if (Math.random() < 0.7) {\n      throw new Error('Fetch failed');\n    }\n    return 'Data';\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "function (e) {\n            if (!this.target) {\n                var x;\n                var $handle;\n\n                if (this.options.type === \"single\") {\n                    $handle = this.$cache.single;\n                } else {\n                    $handle = this.$cache.from;\n                }\n\n                x = $handle.offset().left;\n                x += ($handle.width() / 2) - 1;\n\n                this.pointerClick(\"single\", {preventDefault: function () {}, pageX: x});\n            }\n        }",
    "label": 0
  },
  {
    "codes": "def change_node_subscriptions(self, jid, node, subscriptions_to_set):\n        \"\"\"\n        Update the subscriptions at a node.\n\n        :param jid: Address of the PubSub service.\n        :type jid: :class:`aioxmpp.JID`\n        :param node: Name of the node to modify\n        :type node: :class:`str`\n        :param subscriptions_to_set: The subscriptions to set at the node.\n        :type subscriptions_to_set: :class:`~collections.abc.Iterable` of\n            tuples consisting of the JID to (un)subscribe and the subscription\n            level to use.\n        :raises aioxmpp.errors.XMPPError: as returned by the service\n\n        `subscriptions_to_set` must be an iterable of pairs (`jid`,\n        `subscription`), where the `jid` indicates the JID for which the\n        `subscription` is to be set.\n        \"\"\"\n        iq = aioxmpp.stanza.IQ(\n            type_=aioxmpp.structs.IQType.SET,\n            to=jid,\n            payload=pubsub_xso.OwnerRequest(\n                pubsub_xso.OwnerSubscriptions(\n                    node,\n                    subscriptions=[\n                        pubsub_xso.OwnerSubscription(\n                            jid,\n                            subscription\n                        )\n                        for jid, subscription in subscriptions_to_set\n                    ]\n                )\n            )\n        )\n\n        yield from self.client.send(iq)",
    "label": 0
  },
  {
    "codes": "```javascript\nconst xss = require('xss'); // Import xss library\n\n// Middleware to sanitize user input\nconst sanitizeInput = (req, res, next) => {\n    // Sanitize request body\n    if (req.body) {\n        for (let key in req.body) {\n            if (typeof req.body[key] === 'string') {\n                req.body[key] = xss(req.body[key]); // Sanitize each string value\n            }\n        }\n    }\n\n    // Sanitize request query parameters\n    if (req.query) {\n        for (let key in req.query) {\n            if (typeof req.query[key] === 'string') {\n                req.query[key] = xss(req.query[key]); // Sanitize each string value\n            }\n        }\n    }\n\n    // Sanitize request params\n    if (req.params) {\n        for (let key in req.params) {\n            if (typeof req.params[key] === 'string') {\n                req.params[key] = xss(req.params[key]); // Sanitize each string value\n            }\n        }\n    }\n\n    next(); // Proceed to the next middleware/route handler\n};\n\nmodule.exports = sanitizeInput; // Export the middleware\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define source and destination directories\nSRC_DIR=\"/path/to/source\"\nDEST_DIR=\"/path/to/destination\"\n\n# Check if directories exist\nif [ ! -d \"$SRC_DIR\" ]; then\n    echo \"Source directory does not exist.\"\n    exit 1\nfi\n\nif [ ! -d \"$DEST_DIR\" ]; then\n    echo \"Destination directory does not exist.\"\n    exit 1\nfi\n\n# Compare directories using rsync dry run\necho \"Comparing directories...\"\nrsync -avun --delete \"$SRC_DIR/\" \"$DEST_DIR/\"\n\n# Prompt user to confirm sync\nread -p \"Do you want to sync changes? (y/n): \" confirm\n\n# Sync changes if confirmed\nif [ \"$confirm\" == \"y\" ]; then\n    echo \"Syncing directories...\"\n    rsync -avu --delete \"$SRC_DIR/\" \"$DEST_DIR/\"\n    echo \"Sync completed.\"\nelse\n    echo \"Sync aborted.\"\nfi\n```",
    "label": 1
  },
  {
    "codes": "```c\n#include \"FreeRTOS.h\"\n#include \"task.h\"\n\n// Define task priorities\n#define TASK_PRIORITY_ARM_CONTROL    (tskIDLE_PRIORITY + 2)\n#define TASK_PRIORITY_SENSOR_READ    (tskIDLE_PRIORITY + 1)\n#define TASK_PRIORITY_COMMUNICATION  (tskIDLE_PRIORITY + 1)\n\n// Define stack sizes for tasks\n#define STACK_SIZE_ARM_CONTROL       (configMINIMAL_STACK_SIZE + 100)\n#define STACK_SIZE_SENSOR_READ       (configMINIMAL_STACK_SIZE + 80)\n#define STACK_SIZE_COMMUNICATION     (configMINIMAL_STACK_SIZE + 80)\n\n// Task handles\nTaskHandle_t xArmControlTaskHandle = NULL;\nTaskHandle_t xSensorReadTaskHandle = NULL;\nTaskHandle_t xCommunicationTaskHandle = NULL;\n\n// Task function prototypes\nvoid vArmControlTask(void *pvParameters);\nvoid vSensorReadTask(void *pvParameters);\nvoid vCommunicationTask(void *pvParameters);\n\n// Main function to create tasks\nvoid main(void)\n{\n    // Create the Arm Control task\n    xTaskCreate(\n        vArmControlTask,             // Task function\n        \"ArmControl\",                // Task name\n        STACK_SIZE_ARM_CONTROL,      // Stack size\n        NULL,                        // Task parameters\n        TASK_PRIORITY_ARM_CONTROL,   // Task priority\n        &xArmControlTaskHandle       // Task handle\n    );\n\n    // Create the Sensor Read task\n    xTaskCreate(\n        vSensorReadTask,             // Task function\n        \"SensorRead\",                // Task name\n        STACK_SIZE_SENSOR_READ,      // Stack size\n        NULL,                        // Task parameters\n        TASK_PRIORITY_SENSOR_READ,  // Task priority\n        &xSensorReadTaskHandle        // Task handle\n    );\n\n    // Create the Communication task\n    xTaskCreate(\n        vCommunicationTask,          // Task function\n        \"Communication\",             // Task name\n        STACK_SIZE_COMMUNICATION,    // Stack size\n        NULL,                        // Task parameters\n        TASK_PRIORITY_COMMUNICATION, // Task priority\n        &xCommunicationTaskHandle     // Task handle\n    );\n\n    // Start the scheduler\n    vTaskStartScheduler();\n\n    // The program should never reach here\n    for (;;);\n}\n\n// Arm Control Task\nvoid vArmControlTask(void *pvParameters)\n{\n    for (;;)\n    {\n        // Implement robotic arm control logic here\n        // e.g., move arm to a specific position, grip/release objects, etc.\n\n        vTaskDelay(pdMS_TO_TICKS(10)); // Delay for 10ms\n    }\n}\n\n// Sensor Read Task\nvoid vSensorReadTask(void *pvParameters)\n{\n    for (;;)\n    {\n        // Implement sensor reading logic here\n        // e.g., read position sensors, force sensors, etc.\n\n        vTaskDelay(pdMS_TO_TICKS(20)); // Delay for 20ms\n    }\n}\n\n// Communication Task\nvoid vCommunicationTask(void *pvParameters)\n{\n    for (;;)\n    {\n        // Implement communication logic here\n        // e.g., send/receive commands to/from a remote controller\n\n        vTaskDelay(pdMS_TO_TICKS(50)); // Delay for 50ms\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport java.util.regex.*;\n\npublic class MarkdownToPlainText {\n    \n    public static String convertMarkdownToPlainText(String markdown) {\n        if (markdown == null || markdown.isEmpty()) return \"\";\n\n        // Remove code blocks\n        markdown = markdown.replaceAll(\"```[\\\\s\\\\S]*?```\", \"\");\n\n        // Remove inline code\n        markdown = markdown.replaceAll(\"`[^`]*`\", \"\");\n\n        // Remove Markdown headings\n        markdown = markdown.replaceAll(\"^#{1,6}\\\\s*\", \"\");\n\n        // Remove bold and italic markers\n        markdown = markdown.replaceAll(\"(\\\\*\\\\*|__)(.*?)\\\\1\", \"$2\");\n        markdown = markdown.replaceAll(\"(\\\\*|_)(.*?)\\\\1\", \"$2\");\n\n        // Remove links but keep link text\n        markdown = markdown.replaceAll(\"\\\\[([^\\\\]]+)\\\\]\\\\([^\\\\)]+\\\\)\", \"$1\");\n\n        // Remove images but keep alt text\n        markdown = markdown.replaceAll(\"!\\\\[([^\\\\]]*)\\\\]\\\\([^\\\\)]+\\\\)\", \"$1\");\n\n        // Remove blockquotes\n        markdown = markdown.replaceAll(\"^>\\\\s*\", \"\");\n\n        // Remove unordered list markers\n        markdown = markdown.replaceAll(\"^\\\\s*[-+*]\\\\s+\", \"\");\n\n        // Remove ordered list markers\n        markdown = markdown.replaceAll(\"^\\\\s*\\\\d+\\\\.\\\\s+\", \"\");\n\n        // Remove remaining Markdown links, if any\n        markdown = markdown.replaceAll(\"\\\\[([^\\\\]]+)\\\\]\\\\([^\\\\)]+\\\\)\", \"$1\");\n\n        return markdown.trim();\n    }\n\n    public static void main(String[] args) {\n        // Example Markdown text\n        String markdownText = \"# Heading\\nSome *italic* text and **bold** text.\\n\\n\"\n                + \"Here is a [link](http://example.com) and `inline code`.\\n\"\n                + \"> A blockquote.\\n\\n\"\n                + \"1. First item\\n2. Second item\\n\\n\"\n                + \"![alt text](image.jpg)\\n\";\n\n        // Convert to plain text\n        String plainText = convertMarkdownToPlainText(markdownText);\n\n        // Output plain text\n        System.out.println(plainText);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *RemoveTagsFromVaultInput) SetVaultName(v string) *RemoveTagsFromVaultInput {\n\ts.VaultName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *UploadArchiveInput) SetChecksum(v string) *UploadArchiveInput {\n\ts.Checksum = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```dart\nString formatNumberCompact(num number) {\n  if (number < 1000) {\n    return number.toString(); // No formatting needed for numbers less than 1000\n  } else if (number < 1000000) {\n    return '${(number / 1000).toStringAsFixed(1)}K'; // Format as K for thousands\n  } else if (number < 1000000000) {\n    return '${(number / 1000000).toStringAsFixed(1)}M'; // Format as M for millions\n  } else {\n    return '${(number / 1000000000).toStringAsFixed(1)}B'; // Format as B for billions\n  }\n}\n```\n\n```dart\n// Example usage:\nvoid main() {\n  print(formatNumberCompact(123));       // Output: 123\n  print(formatNumberCompact(1234));      // Output: 1.2K\n  print(formatNumberCompact(1234567));   // Output: 1.2M\n  print(formatNumberCompact(1234567890));// Output: 1.2B\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\n// Step 1: Add TensorFlow Lite dependencies to your build.gradle file\ndependencies {\n    implementation 'org.tensorflow:tensorflow-lite:2.8.0'\n    implementation 'org.tensorflow:tensorflow-lite-support:0.2.0'\n}\n\n// Step 2: Load the TensorFlow Lite model\nimport org.tensorflow.lite.Interpreter;\n\nprivate Interpreter tflite;\n\n// Initialize the TensorFlow Lite interpreter\nprivate void loadModelFile() throws IOException {\n    AssetFileDescriptor fileDescriptor = this.getAssets().openFd(\"model.tflite\");\n    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\n    FileChannel fileChannel = inputStream.getChannel();\n    long startOffset = fileDescriptor.getStartOffset();\n    long declaredLength = fileDescriptor.getDeclaredLength();\n    MappedByteBuffer modelFile = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\n    tflite = new Interpreter(modelFile);\n}\n\n// Step 3: Set up camera and frame processing\nprivate void startCamera() {\n    // Use CameraX or Camera2 API to capture frames\n    // Example using CameraX preview:\n    Preview preview = new Preview.Builder().build();\n    CameraSelector cameraSelector = new CameraSelector.Builder()\n            .requireLensFacing(CameraSelector.LENS_FACING_BACK)\n            .build();\n    \n    preview.setSurfaceProvider(previewView.getSurfaceProvider());\n\n    // Bind camera lifecycle\n    Camera camera = cameraProvider.bindToLifecycle(this, cameraSelector, preview, imageAnalysis);\n}\n\n// Step 4: Image analysis and object detection\nprivate ImageAnalysis imageAnalysis = new ImageAnalysis.Builder()\n        .setTargetResolution(new Size(640, 480))\n        .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)\n        .build();\n\nimageAnalysis.setAnalyzer(executor, new ImageAnalysis.Analyzer() {\n    @Override\n    public void analyze(@NonNull ImageProxy image) {\n        if (image.getFormat() == ImageFormat.YUV_420_888) {\n            // Convert ImageProxy to ByteBuffer for model input\n            ByteBuffer buffer = convertImageToByteBuffer(image);\n            // Run inference\n            float[][] result = new float[1][NUM_RESULTS];\n            tflite.run(buffer, result);\n            // Post-process detection results\n            processDetectionResults(result);\n            image.close();\n        }\n    }\n});\n\n// Step 5: Process detection results\nprivate void processDetectionResults(float[][] result) {\n    // Implement logic to interpret the output of the model\n    // Draw bounding boxes on UI or display results\n}\n\n// Helper method: Convert ImageProxy to ByteBuffer\nprivate ByteBuffer convertImageToByteBuffer(ImageProxy image) {\n    // Implement the conversion logic with proper color space transformations\n    // Return a ByteBuffer ready for inference\n}\n\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"crypto/rand\"\n\t\"math/big\"\n\t\"fmt\"\n)\n\n// generatePassword generates a random password with specified length and character sets\nfunc generatePassword(length int, charSets []string) (string, error) {\n\t// Combine all character sets into one string\n\tvar allChars string\n\tfor _, set := range charSets {\n\t\tallChars += set\n\t}\n\n\t// Convert the combined string to a slice of runes for easier indexing\n\tchars := []rune(allChars)\n\tpassword := make([]rune, length)\n\n\t// Generate random indices and build the password\n\tfor i := 0; i < length; i++ {\n\t\trandIndex, err := rand.Int(rand.Reader, big.NewInt(int64(len(chars))))\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tpassword[i] = chars[randIndex.Int64()]\n\t}\n\n\treturn string(password), nil\n}\n\nfunc main() {\n\t// Define character sets\n\tcharSets := []string{\n\t\t\"abcdefghijklmnopqrstuvwxyz\",\n\t\t\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\",\n\t\t\"0123456789\",\n\t\t\"!@#$%^&*()_+\",\n\t}\n\n\t// Generate a password of length 12\n\tpassword, err := generatePassword(12, charSets)\n\tif err != nil {\n\t\tfmt.Println(\"Error generating password:\", err)\n\t\treturn\n\t}\n\n\tfmt.Println(\"Generated Password:\", password)\n}\n```",
    "label": 1
  },
  {
    "codes": "def import_from_json(self, data):\n        \"\"\"\n        Replace the current roster with the :meth:`export_as_json`-compatible\n        dictionary in `data`.\n\n        No events are fired during this activity. After this method completes,\n        the whole roster contents are exchanged with the contents from `data`.\n\n        Also, no data is transferred to the server; this method is intended to\n        be used for roster versioning. See below (in the docs of\n        :class:`Service`).\n        \"\"\"\n        self.version = data.get(\"ver\", None)\n\n        self.items.clear()\n        self.groups.clear()\n        for jid, data in data.get(\"items\", {}).items():\n            jid = structs.JID.fromstr(jid)\n            item = Item(jid)\n            item.update_from_json(data)\n            self.items[jid] = item\n            for group in item.groups:\n                self.groups.setdefault(group, set()).add(item)",
    "label": 0
  },
  {
    "codes": "```kotlin\nfun String.reverseWordsPreserveWhitespace(): String {\n    // Use Regex to match words and whitespace separately\n    val regex = Regex(\"\"\"(\\S+)|(\\s+)\"\"\")\n    return regex.findAll(this)\n        .map { matchResult ->\n            val (word, whitespace) = matchResult.destructured\n            if (word.isNotEmpty()) word.reversed() else whitespace\n        }.joinToString(\"\")\n}\n\n// Example usage:\n// val input = \"Hello   World\"\n// val output = input.reverseWordsPreserveWhitespace()\n// println(output)  // \"olleH   dlroW\"\n```",
    "label": 1
  },
  {
    "codes": "function parseType (props) {\n  const {\n    current\n  } = props\n\n  // \u629b\u51fa\u9519\u8bef\u4fe1\u606f\n  const throwErrorMsg = type => {\n    throw new TypeError(type + ' must be number')\n  }\n\n  if (current) isNumber(current) ? '' : throwErrorMsg('current')\n}",
    "label": 0
  },
  {
    "codes": "def add_consumer_tag(self, tag):\n        \"\"\"Add a Consumer tag.\n\n        :param str tag: Consumer tag.\n        :return:\n        \"\"\"\n        if not is_string(tag):\n            raise AMQPChannelError('consumer tag needs to be a string')\n        if tag not in self._consumer_tags:\n            self._consumer_tags.append(tag)",
    "label": 0
  },
  {
    "codes": "function wb_sheet_idx(wb, sh) {\n\tif(typeof sh == \"number\") {\n\t\tif(sh >= 0 && wb.SheetNames.length > sh) return sh;\n\t\tthrow new Error(\"Cannot find sheet # \" + sh);\n\t} else if(typeof sh == \"string\") {\n\t\tvar idx = wb.SheetNames.indexOf(sh);\n\t\tif(idx > -1) return idx;\n\t\tthrow new Error(\"Cannot find sheet name |\" + sh + \"|\");\n\t} else throw new Error(\"Cannot find sheet |\" + sh + \"|\");\n}",
    "label": 0
  },
  {
    "codes": "function computeLabelSizes(ctx, tickFonts, ticks, caches) {\n\tvar length = ticks.length;\n\tvar widths = [];\n\tvar heights = [];\n\tvar offsets = [];\n\tvar i, j, jlen, label, tickFont, fontString, cache, lineHeight, width, height, nestedLabel, widest, highest;\n\n\tfor (i = 0; i < length; ++i) {\n\t\tlabel = ticks[i].label;\n\t\ttickFont = ticks[i].major ? tickFonts.major : tickFonts.minor;\n\t\tctx.font = fontString = tickFont.string;\n\t\tcache = caches[fontString] = caches[fontString] || {data: {}, gc: []};\n\t\tlineHeight = tickFont.lineHeight;\n\t\twidth = height = 0;\n\t\t// Undefined labels and arrays should not be measured\n\t\tif (!helpers.isNullOrUndef(label) && !helpers.isArray(label)) {\n\t\t\twidth = helpers.measureText(ctx, cache.data, cache.gc, width, label);\n\t\t\theight = lineHeight;\n\t\t} else if (helpers.isArray(label)) {\n\t\t\t// if it is an array let's measure each element\n\t\t\tfor (j = 0, jlen = label.length; j < jlen; ++j) {\n\t\t\t\tnestedLabel = label[j];\n\t\t\t\t// Undefined labels and arrays should not be measured\n\t\t\t\tif (!helpers.isNullOrUndef(nestedLabel) && !helpers.isArray(nestedLabel)) {\n\t\t\t\t\twidth = helpers.measureText(ctx, cache.data, cache.gc, width, nestedLabel);\n\t\t\t\t\theight += lineHeight;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\twidths.push(width);\n\t\theights.push(height);\n\t\toffsets.push(lineHeight / 2);\n\t}\n\tgarbageCollect(caches, length);\n\n\twidest = widths.indexOf(Math.max.apply(null, widths));\n\thighest = heights.indexOf(Math.max.apply(null, heights));\n\n\tfunction valueAt(idx) {\n\t\treturn {\n\t\t\twidth: widths[idx] || 0,\n\t\t\theight: heights[idx] || 0,\n\t\t\toffset: offsets[idx] || 0\n\t\t};\n\t}\n\n\treturn {\n\t\tfirst: valueAt(0),\n\t\tlast: valueAt(length - 1),\n\t\twidest: valueAt(widest),\n\t\thighest: valueAt(highest)\n\t};\n}",
    "label": 0
  },
  {
    "codes": "```rust\n// Import necessary crates\nuse wasm_bindgen::prelude::*;\nuse ort::{Environment, Session, Value};\n\n// Initialize the ONNX runtime environment\n#[wasm_bindgen]\npub fn init_onnx_runtime() -> Result<(), JsValue> {\n    let env = Environment::builder()\n        .with_name(\"web\")\n        .build()\n        .map_err(|e| JsValue::from_str(&e.to_string()))?;\n    Ok(())\n}\n\n// Load an ONNX model and create a session\n#[wasm_bindgen]\npub fn load_model(model_bytes: &[u8]) -> Result<Session, JsValue> {\n    let session = Session::builder()\n        .with_model_from_memory(model_bytes)\n        .map_err(|e| JsValue::from_str(&e.to_string()))?;\n    Ok(session)\n}\n\n// Run inference on the loaded model\n#[wasm_bindgen]\npub fn run_inference(session: &Session, input_data: Vec<f32>, input_shape: Vec<i64>) -> Result<Vec<f32>, JsValue> {\n    // Create input tensor from the provided data and shape\n    let input_tensor = Value::from_array((input_shape.as_slice(), input_data.as_slice()))\n        .map_err(|e| JsValue::from_str(&e.to_string()))?;\n\n    // Run the model\n    let outputs = session.run(vec![input_tensor])\n        .map_err(|e| JsValue::from_str(&e.to_string()))?;\n\n    // Extract the output data\n    let output_data = outputs[0].try_extract::<f32>()\n        .map_err(|e| JsValue::from_str(&e.to_string()))?\n        .to_vec();\n\n    Ok(output_data)\n}\n```\n\n### Comments:\n- `wasm_bindgen`: Used to expose Rust functions to JavaScript.\n- `ort`: ONNX Runtime crate for running ONNX models.\n- `init_onnx_runtime`: Initializes the ONNX runtime environment.\n- `load_model`: Loads an ONNX model from bytes and creates a session.\n- `run_inference`: Runs inference on the model with the provided input data and shape, returning the output.\n\n### Usage:\n- Compile this Rust code to WebAssembly using `wasm-pack`.\n- Use the generated JavaScript bindings in your web application to load and run ONNX models in the browser.",
    "label": 1
  },
  {
    "codes": "```typescript\nfunction Retry(maxRetries: number = 3, delay: number = 1000) {\n    return function (target: any, propertyKey: string, descriptor: PropertyDescriptor) {\n        const originalMethod = descriptor.value;\n\n        descriptor.value = async function (...args: any[]) {\n            let retries = 0;\n\n            while (retries < maxRetries) {\n                try {\n                    const result = await originalMethod.apply(this, args);\n                    return result; // Success, return the result\n                } catch (error) {\n                    retries++;\n                    if (retries >= maxRetries) {\n                        throw error; // Max retries reached, rethrow the error\n                    }\n                    await new Promise(resolve => setTimeout(resolve, delay)); // Wait before retrying\n                }\n            }\n        };\n\n        return descriptor;\n    };\n}\n\n// Example usage:\nclass MyService {\n    @Retry(5, 2000) // Retry 5 times with 2 seconds delay\n    async fetchData() {\n        // Simulate a failing async operation\n        throw new Error(\"Failed to fetch data\");\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function write_ws_xlml_wsopts(ws, opts, idx, wb) {\n\tif(!ws) return \"\";\n\tvar o = [];\n\t/* NOTE: spec technically allows any order, but stick with implied order */\n\n\t/* FitToPage */\n\t/* DoNotDisplayColHeaders */\n\t/* DoNotDisplayRowHeaders */\n\t/* ViewableRange */\n\t/* Selection */\n\t/* GridlineColor */\n\t/* Name */\n\t/* ExcelWorksheetType */\n\t/* IntlMacro */\n\t/* Unsynced */\n\t/* Selected */\n\t/* CodeName */\n\n\tif(ws['!margins']) {\n\t\to.push(\"<PageSetup>\");\n\t\tif(ws['!margins'].header) o.push(writextag(\"Header\", null, {'x:Margin':ws['!margins'].header}));\n\t\tif(ws['!margins'].footer) o.push(writextag(\"Footer\", null, {'x:Margin':ws['!margins'].footer}));\n\t\to.push(writextag(\"PageMargins\", null, {\n\t\t\t'x:Bottom': ws['!margins'].bottom || \"0.75\",\n\t\t\t'x:Left': ws['!margins'].left || \"0.7\",\n\t\t\t'x:Right': ws['!margins'].right || \"0.7\",\n\t\t\t'x:Top': ws['!margins'].top || \"0.75\"\n\t\t}));\n\t\to.push(\"</PageSetup>\");\n\t}\n\n\t/* PageSetup */\n\t/* DisplayPageBreak */\n\t/* TransitionExpressionEvaluation */\n\t/* TransitionFormulaEntry */\n\t/* Print */\n\t/* Zoom */\n\t/* PageLayoutZoom */\n\t/* PageBreakZoom */\n\t/* ShowPageBreakZoom */\n\t/* DefaultRowHeight */\n\t/* DefaultColumnWidth */\n\t/* StandardWidth */\n\n\tif(wb && wb.Workbook && wb.Workbook.Sheets && wb.Workbook.Sheets[idx]) {\n\t\t/* Visible */\n\t\tif(wb.Workbook.Sheets[idx].Hidden) o.push(writextag(\"Visible\", (wb.Workbook.Sheets[idx].Hidden == 1 ? \"SheetHidden\" : \"SheetVeryHidden\"), {}));\n\t\telse {\n\t\t\t/* Selected */\n\t\t\tfor(var i = 0; i < idx; ++i) if(wb.Workbook.Sheets[i] && !wb.Workbook.Sheets[i].Hidden) break;\n\t\t\tif(i == idx) o.push(\"<Selected/>\");\n\t\t}\n\t}\n\n\t/* LeftColumnVisible */\n\n\tif(((((wb||{}).Workbook||{}).Views||[])[0]||{}).RTL) o.push(\"<DisplayRightToLeft/>\");\n\n\t/* GridlineColorIndex */\n\t/* DisplayFormulas */\n\t/* DoNotDisplayGridlines */\n\t/* DoNotDisplayHeadings */\n\t/* DoNotDisplayOutline */\n\t/* ApplyAutomaticOutlineStyles */\n\t/* NoSummaryRowsBelowDetail */\n\t/* NoSummaryColumnsRightDetail */\n\t/* DoNotDisplayZeros */\n\t/* ActiveRow */\n\t/* ActiveColumn */\n\t/* FilterOn */\n\t/* RangeSelection */\n\t/* TopRowVisible */\n\t/* TopRowBottomPane */\n\t/* LeftColumnRightPane */\n\t/* ActivePane */\n\t/* SplitHorizontal */\n\t/* SplitVertical */\n\t/* FreezePanes */\n\t/* FrozenNoSplit */\n\t/* TabColorIndex */\n\t/* Panes */\n\n\t/* NOTE: Password not supported in XLML Format */\n\tif(ws['!protect']) {\n\t\to.push(writetag(\"ProtectContents\", \"True\"));\n\t\tif(ws['!protect'].objects) o.push(writetag(\"ProtectObjects\", \"True\"));\n\t\tif(ws['!protect'].scenarios) o.push(writetag(\"ProtectScenarios\", \"True\"));\n\t\tif(ws['!protect'].selectLockedCells != null && !ws['!protect'].selectLockedCells) o.push(writetag(\"EnableSelection\", \"NoSelection\"));\n\t\telse if(ws['!protect'].selectUnlockedCells != null && !ws['!protect'].selectUnlockedCells) o.push(writetag(\"EnableSelection\", \"UnlockedCells\"));\n\t[\n\t\t[ \"formatCells\", \"AllowFormatCells\" ],\n\t\t[ \"formatColumns\", \"AllowSizeCols\" ],\n\t\t[ \"formatRows\", \"AllowSizeRows\" ],\n\t\t[ \"insertColumns\", \"AllowInsertCols\" ],\n\t\t[ \"insertRows\", \"AllowInsertRows\" ],\n\t\t[ \"insertHyperlinks\", \"AllowInsertHyperlinks\" ],\n\t\t[ \"deleteColumns\", \"AllowDeleteCols\" ],\n\t\t[ \"deleteRows\", \"AllowDeleteRows\" ],\n\t\t[ \"sort\", \"AllowSort\" ],\n\t\t[ \"autoFilter\", \"AllowFilter\" ],\n\t\t[ \"pivotTables\", \"AllowUsePivotTables\" ]\n\t].forEach(function(x) { if(ws['!protect'][x[0]]) o.push(\"<\"+x[1]+\"/>\"); });\n\t}\n\n\tif(o.length == 0) return \"\";\n\treturn writextag(\"WorksheetOptions\", o.join(\"\"), {xmlns:XLMLNS.x});\n}",
    "label": 0
  },
  {
    "codes": "```java\nimport ddf.minim.*;\nimport ddf.minim.analysis.*;\n\nMinim minim;\nAudioInput in;\nFFT fft;\n\nint bands = 512;\n\nvoid setup() {\n  size(1024, 512);\n  minim = new Minim(this);\n  in = minim.getLineIn(Minim.STEREO, 1024);\n  \n  // Initialize FFT with the same sample size as our audio input\n  fft = new FFT(in.bufferSize(), in.sampleRate());\n}\n\nvoid draw() {\n  background(0);\n  \n  // Perform a forward FFT transform on the current audio buffer\n  fft.forward(in.mix);\n  \n  // Calculate the width of each band to be displayed in the spectrum\n  float bandWidth = (float) width / bands;\n  \n  for(int i = 0; i < bands; i++) {\n    // Get amplitude of the current frequency band and map it to the height of the window\n    float amplitude = fft.getBand(i) * 4;\n    \n    // Draw a rectangle representing the amplitude of current frequency band\n    fill(100, 255, 100);\n    rect(i * bandWidth, height, bandWidth, -amplitude);\n  }\n}\n\nvoid stop() {\n  // Close the audio input and Minim library on stopping the sketch\n  in.close();\n  minim.stop();\n  super.stop();\n}\n```\n\n",
    "label": 1
  },
  {
    "codes": "```swift\nimport SwiftUI\nimport HealthKit\n\nstruct HeartRateView: View {\n    @StateObject private var heartRateViewModel = HeartRateViewModel()\n    \n    var body: some View {\n        VStack {\n            Text(\"Live Heart Rate\")\n                .font(.title)\n            Text(\"\\(heartRateViewModel.heartRate) BPM\")\n                .font(.largeTitle)\n        }\n        .onAppear {\n            heartRateViewModel.startHeartRateUpdates()\n        }\n        .onDisappear {\n            heartRateViewModel.stopHeartRateUpdates()\n        }\n    }\n}\n\nclass HeartRateViewModel: ObservableObject {\n    private var healthStore: HKHealthStore?\n    private var heartRateQuery: HKAnchoredObjectQuery?\n    \n    @Published var heartRate: Int = 0\n    \n    init() {\n        healthStore = HKHealthStore()\n        requestAuthorization()\n    }\n    \n    private func requestAuthorization() {\n        guard let heartRateType = HKObjectType.quantityType(forIdentifier: .heartRate) else { return }\n        healthStore?.requestAuthorization(toShare: nil, read: [heartRateType]) { _, _ in }\n    }\n    \n    func startHeartRateUpdates() {\n        guard let heartRateType = HKObjectType.quantityType(forIdentifier: .heartRate) else { return }\n        \n        let query = HKAnchoredObjectQuery(\n            type: heartRateType,\n            predicate: nil,\n            anchor: HKQueryAnchor(fromValue: HKAnchoredObjectQueryNoAnchor),\n            limit: HKObjectQueryNoLimit\n        ) { [weak self] query, samples, _, _, _ in\n            self?.updateHeartRate(samples: samples)\n        }\n        \n        query.updateHandler = { [weak self] query, samples, _, _, _ in\n            self?.updateHeartRate(samples: samples)\n        }\n        \n        healthStore?.execute(query)\n        heartRateQuery = query\n    }\n    \n    private func updateHeartRate(samples: [HKSample]?) {\n        guard let heartRateSamples = samples as? [HKQuantitySample] else { return }\n        \n        DispatchQueue.main.async {\n            if let lastSample = heartRateSamples.last {\n                let heartRateUnit = HKUnit.count().unitDivided(by: HKUnit.minute())\n                self.heartRate = Int(lastSample.quantity.doubleValue(for: heartRateUnit))\n            }\n        }\n    }\n    \n    func stopHeartRateUpdates() {\n        if let query = heartRateQuery {\n            healthStore?.stop(query)\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "func (c *Glue) GetUserDefinedFunctionsWithContext(ctx aws.Context, input *GetUserDefinedFunctionsInput, opts ...request.Option) (*GetUserDefinedFunctionsOutput, error) {\n\treq, out := c.GetUserDefinedFunctionsRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "func (s *JobUpdate) SetLogUri(v string) *JobUpdate {\n\ts.LogUri = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport random\n\n# Define elements\nelements = {\n    \"Fire\": {\"weakness\": \"Water\"},\n    \"Water\": {\"weakness\": \"Earth\"},\n    \"Earth\": {\"weakness\": \"Wind\"},\n    \"Wind\": {\"weakness\": \"Fire\"}\n}\n\n# Define a simple character class\nclass Character:\n    def __init__(self, name, element, health):\n        self.name = name\n        self.element = element\n        self.health = health\n\n    def is_alive(self):\n        return self.health > 0\n\n# Attack function considering elemental weaknesses\ndef attack(attacker, defender):\n    base_damage = random.randint(5, 15)\n    multiplier = 2 if elements[defender.element][\"weakness\"] == attacker.element else 1\n    damage = base_damage * multiplier\n    defender.health -= damage\n    return damage, multiplier\n\n# Initialize characters\nplayer = Character(\"Hero\", \"Fire\", 100)\nenemy = Character(\"Goblin\", \"Wind\", 50)\n\n# Combat loop\nwhile player.is_alive() and enemy.is_alive():\n    # Player's turn\n    print(f\"\\n{player.name}'s turn.\")\n    damage, multiplier = attack(player, enemy)\n    print(f\"{player.name} attacks {enemy.name} for {damage} damage. {'Critical hit!' if multiplier > 1 else ''}\")\n    print(f\"{enemy.name}'s health: {enemy.health}\")\n\n    if not enemy.is_alive():\n        print(f\"{enemy.name} has been defeated!\")\n        break\n\n    # Enemy's turn\n    print(f\"\\n{enemy.name}'s turn.\")\n    damage, multiplier = attack(enemy, player)\n    print(f\"{enemy.name} attacks {player.name} for {damage} damage. {'Critical hit!' if multiplier > 1 else ''}\")\n    print(f\"{player.name}'s health: {player.health}\")\n\n    if not player.is_alive():\n        print(f\"{player.name} has been defeated!\")\n        break\n\nprint(\"\\nCombat ended!\")\n```\n",
    "label": 1
  },
  {
    "codes": "```typescript\n// Import necessary Cypress commands\nimport { checkoutPage } from '../pages/checkoutPage';\n\ndescribe('E-commerce Checkout Functionality', () => {\n  beforeEach(() => {\n    // Visit the e-commerce website and navigate to the checkout page\n    cy.visit('/');\n    cy.get('.cart-icon').click();\n    cy.contains('Proceed to Checkout').click();\n  });\n\n  it('should successfully complete the checkout process', () => {\n    // Fill in the shipping information\n    checkoutPage.fillShippingInfo({\n      firstName: 'John',\n      lastName: 'Doe',\n      address: '123 Main St',\n      city: 'Anytown',\n      zipCode: '12345',\n      country: 'USA'\n    });\n\n    // Select a payment method\n    checkoutPage.selectPaymentMethod('Credit Card');\n\n    // Enter payment details\n    checkoutPage.fillPaymentDetails({\n      cardNumber: '4111111111111111',\n      expirationDate: '12/25',\n      cvv: '123'\n    });\n\n    // Place the order\n    checkoutPage.placeOrder();\n\n    // Verify the order confirmation message\n    cy.contains('Thank you for your order!').should('be.visible');\n  });\n\n  it('should display an error message for invalid payment details', () => {\n    // Fill in the shipping information\n    checkoutPage.fillShippingInfo({\n      firstName: 'Jane',\n      lastName: 'Smith',\n      address: '456 Elm St',\n      city: 'Othertown',\n      zipCode: '67890',\n      country: 'USA'\n    });\n\n    // Select a payment method\n    checkoutPage.selectPaymentMethod('Credit Card');\n\n    // Enter invalid payment details\n    checkoutPage.fillPaymentDetails({\n      cardNumber: '1234567812345678',\n      expirationDate: '01/23',\n      cvv: '999'\n    });\n\n    // Attempt to place the order\n    checkoutPage.placeOrder();\n\n    // Verify the error message for invalid payment details\n    cy.contains('Invalid payment details').should('be.visible');\n  });\n});\n```\n\n```typescript\n// checkoutPage.ts\nexport const checkoutPage = {\n  fillShippingInfo(info: {\n    firstName: string;\n    lastName: string;\n    address: string;\n    city: string;\n    zipCode: string;\n    country: string;\n  }) {\n    cy.get('#firstName').type(info.firstName);\n    cy.get('#lastName').type(info.lastName);\n    cy.get('#address').type(info.address);\n    cy.get('#city').type(info.city);\n    cy.get('#zipCode').type(info.zipCode);\n    cy.get('#country').select(info.country);\n  },\n\n  selectPaymentMethod(method: string) {\n    cy.get('#paymentMethod').select(method);\n  },\n\n  fillPaymentDetails(details: {\n    cardNumber: string;\n    expirationDate: string;\n    cvv: string;\n  }) {\n    cy.get('#cardNumber').type(details.cardNumber);\n    cy.get('#expirationDate').type(details.expirationDate);\n    cy.get('#cvv').type(details.cvv);\n  },\n\n  placeOrder() {\n    cy.get('#placeOrderButton').click();\n  }\n};\n```",
    "label": 1
  },
  {
    "codes": "```java\n// Import necessary libraries\nimport android.Manifest;\nimport android.content.pm.PackageManager;\nimport android.graphics.Bitmap;\nimport android.hardware.camera2.CameraAccessException;\nimport android.hardware.camera2.CameraCaptureSession;\nimport android.hardware.camera2.CameraDevice;\nimport android.hardware.camera2.CameraManager;\nimport android.os.Bundle;\nimport android.util.Size;\nimport android.view.Surface;\nimport android.view.SurfaceHolder;\nimport android.view.SurfaceView;\nimport android.view.View;\nimport androidx.annotation.NonNull;\nimport androidx.appcompat.app.AppCompatActivity;\nimport androidx.core.app.ActivityCompat;\nimport org.tensorflow.lite.DataType;\nimport org.tensorflow.lite.Interpreter;\nimport org.tensorflow.lite.support.common.FileUtil;\nimport org.tensorflow.lite.support.image.ImageProcessor;\nimport org.tensorflow.lite.support.image.TensorImage;\nimport java.io.IOException;\nimport java.nio.MappedByteBuffer;\nimport java.util.Arrays;\nimport java.util.Collections;\n\npublic class MainActivity extends AppCompatActivity {\n\n    private static final int REQUEST_CAMERA_PERMISSION = 1;\n    private CameraDevice cameraDevice;\n    private SurfaceView surfaceView;\n    private Interpreter tflite;\n    \n    // Load TFLite model into Interpreter\n    private void loadModel() {\n        try {\n            MappedByteBuffer tfliteModel = FileUtil.loadMappedFile(this, \"detect.tflite\");\n            tflite = new Interpreter(tfliteModel);\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n    \n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n        \n        surfaceView = findViewById(R.id.surfaceView);\n        loadModel();\n        \n        // Request Camera permission\n        if (ActivityCompat.checkSelfPermission(this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {\n            ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.CAMERA}, REQUEST_CAMERA_PERMISSION);\n        } else {\n            startCamera();\n        }\n    }\n\n    // Start camera preview\n    private void startCamera() {\n        CameraManager cameraManager = (CameraManager) getSystemService(CAMERA_SERVICE);\n        try {\n            String cameraId = cameraManager.getCameraIdList()[0];\n            cameraManager.openCamera(cameraId, new CameraDevice.StateCallback() {\n                @Override\n                public void onOpened(@NonNull CameraDevice camera) {\n                    cameraDevice = camera;\n                    createCameraPreviewSession();\n                }\n\n                @Override\n                public void onDisconnected(@NonNull CameraDevice camera) {\n                    camera.close();\n                }\n                \n                @Override\n                public void onError(@NonNull CameraDevice camera, int error) {\n                    camera.close();\n                    cameraDevice = null;\n                }\n            }, null);\n        } catch (CameraAccessException e) {\n            e.printStackTrace();\n        }\n    }\n\n    // Create a camera preview session\n    private void createCameraPreviewSession() {\n        try {\n            SurfaceHolder holder = surfaceView.getHolder();\n            Surface surface = holder.getSurface();\n            cameraDevice.createCaptureSession(Collections.singletonList(surface),\n                new CameraCaptureSession.StateCallback() {\n                    @Override\n                    public void onConfigured(@NonNull CameraCaptureSession session) {\n                        // Camera session is ready for preview\n                        Surface surface = holder.getSurface();\n                        CameraCaptureSession.CaptureCallback captureCallback = new CameraCaptureSession.CaptureCallback() {\n                            @Override\n                            public void onCaptureCompleted(@NonNull CameraCaptureSession session,\n                                                            @NonNull CaptureRequest request,\n                                                            @NonNull TotalCaptureResult result) {\n                                // Feed camera frame to TFLite model\n                                Bitmap bitmap = surfaceView.getBitmap();\n                                TensorImage inputImage = new TensorImage(DataType.UINT8);\n                                inputImage.load(bitmap);\n                                \n                                // Preprocess and run inference\n                                ImageProcessor imageProcessor = new ImageProcessor.Builder().build();\n                                TensorImage processedImage = imageProcessor.process(inputImage);\n                                Object[] inputArray = { processedImage.getTensorBuffer().getBuffer() };\n                                float[][] output = new float[1][10]; // Adjust based on model output\n                                tflite.run(inputArray, output);\n\n                                // Handle results\n                                handleDetectionResults(output);\n                            }\n                        };\n\n                        session.setRepeatingRequest(\n                                cameraDevice.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW)\n                                        .addTarget(surface)\n                                        .build(),\n                                captureCallback,\n                                null);\n                    }\n\n                    @Override\n                    public void onConfigureFailed(@NonNull CameraCaptureSession session) {\n                    }\n                }, null);\n        } catch (CameraAccessException e) {\n            e.printStackTrace();\n        }\n    }\n\n    // Handle detection results\n    private void handleDetectionResults(float[][] detections) {\n        // Decode and display detection results\n    }\n    \n    @Override\n    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults);\n        if (requestCode == REQUEST_CAMERA_PERMISSION) {\n            // If permission granted, start camera\n            if (grantResults.length > 0 && grantResults[0] == PackageManager.PERMISSION_GRANTED) {\n                startCamera();\n            }\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```javascript\n// Utility function to deep clone objects with circular references\nfunction deepClone(obj, hash = new WeakMap()) {\n    // If the object is null or not an object, return it directly\n    if (obj === null || typeof obj !== 'object') {\n        return obj;\n    }\n\n    // If the object is already cloned, return the cloned reference\n    if (hash.has(obj)) {\n        return hash.get(obj);\n    }\n\n    // Handle Date objects\n    if (obj instanceof Date) {\n        return new Date(obj);\n    }\n\n    // Handle RegExp objects\n    if (obj instanceof RegExp) {\n        return new RegExp(obj);\n    }\n\n    // Create a new object or array based on the original\n    const clone = Array.isArray(obj) ? [] : {};\n\n    // Store the cloned object in the hash to handle circular references\n    hash.set(obj, clone);\n\n    // Recursively clone each property\n    for (let key in obj) {\n        if (obj.hasOwnProperty(key)) {\n            clone[key] = deepClone(obj[key], hash);\n        }\n    }\n\n    return clone;\n}\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define variables\nBACKUP_DIR=\"/path/to/backup\"  # Directory to backup\nS3_BUCKET=\"s3://your-bucket-name\"  # S3 bucket name\nDATE=$(date +%Y-%m-%d_%H-%M-%S)  # Timestamp for backup file\nLOG_FILE=\"/var/log/backup.log\"  # Log file path\n\n# Check if backup directory exists\nif [ ! -d \"$BACKUP_DIR\" ]; then\n  echo \"Backup directory does not exist. Exiting.\" >> \"$LOG_FILE\"\n  exit 1\nfi\n\n# Create a compressed archive of the backup directory\ntar -czf \"/tmp/backup_$DATE.tar.gz\" \"$BACKUP_DIR\" >> \"$LOG_FILE\" 2>&1\n\n# Check if tar command was successful\nif [ $? -ne 0 ]; then\n  echo \"Failed to create backup archive. Exiting.\" >> \"$LOG_FILE\"\n  exit 1\nfi\n\n# Upload the backup archive to S3\naws s3 cp \"/tmp/backup_$DATE.tar.gz\" \"$S3_BUCKET/\" >> \"$LOG_FILE\" 2>&1\n\n# Check if S3 upload was successful\nif [ $? -ne 0 ]; then\n  echo \"Failed to upload backup to S3. Exiting.\" >> \"$LOG_FILE\"\n  exit 1\nfi\n\n# Clean up temporary backup file\nrm -f \"/tmp/backup_$DATE.tar.gz\" >> \"$LOG_FILE\" 2>&1\n\n# Log success message\necho \"Backup completed successfully on $DATE.\" >> \"$LOG_FILE\"\n```",
    "label": 1
  },
  {
    "codes": "def build_expressions(verb):\n    \"\"\"\n    Build expressions for helper verbs\n\n    Parameters\n    ----------\n    verb : verb\n        A verb with a *functions* attribute.\n\n    Returns\n    -------\n    out : tuple\n        (List of Expressions, New columns). The expressions and the\n        new columns in which the results of those expressions will\n        be stored. Even when a result will stored in a column with\n        an existing label, that column is still considered new,\n        i.e An expression ``x='x+1'``, will create a new_column `x`\n        to replace an old column `x`.\n    \"\"\"\n    def partial(func, col, *args, **kwargs):\n        \"\"\"\n        Make a function that acts on a column in a dataframe\n\n        Parameters\n        ----------\n        func : callable\n            Function\n        col : str\n            Column\n        args : tuple\n            Arguments to pass to func\n        kwargs : dict\n            Keyword arguments to func\n\n        Results\n        -------\n        new_func : callable\n            Function that takes a dataframe, and calls the\n            original function on a column in the dataframe.\n        \"\"\"\n        def new_func(gdf):\n            return func(gdf[col], *args, **kwargs)\n\n        return new_func\n\n    def make_statement(func, col):\n        \"\"\"\n        A statement of function called on a column in a dataframe\n\n        Parameters\n        ----------\n        func : str or callable\n            Function to call on a dataframe column\n        col : str\n            Column\n        \"\"\"\n        if isinstance(func, str):\n            expr = '{}({})'.format(func, col)\n        elif callable(func):\n            expr = partial(func, col, *verb.args, **verb.kwargs)\n        else:\n            raise TypeError(\"{} is not a function\".format(func))\n        return expr\n\n    def func_name(func):\n        \"\"\"\n        Return name of a function.\n\n        If the function is `np.sin`, we return `sin`.\n        \"\"\"\n        if isinstance(func, str):\n            return func\n\n        try:\n            return func.__name__\n        except AttributeError:\n            return ''\n\n    # Generate function names. They act as identifiers (postfixed\n    # to the original columns) in the new_column names.\n    if isinstance(verb.functions, (tuple, list)):\n        names = (func_name(func) for func in verb.functions)\n        names_and_functions = zip(names, verb.functions)\n    else:\n        names_and_functions = verb.functions.items()\n\n    # Create statements for the expressions\n    # and postfix identifiers\n    columns = Selector.get(verb)  # columns to act on\n    postfixes = []\n    stmts = []\n    for name, func in names_and_functions:\n        postfixes.append(name)\n        for col in columns:\n            stmts.append(make_statement(func, col))\n\n    if not stmts:\n        stmts = columns\n\n    # Names of the new columns\n    # e.g col1_mean, col2_mean, col1_std, col2_std\n    add_postfix = (isinstance(verb.functions, dict) or\n                   len(verb.functions) > 1)\n    if add_postfix:\n        fmt = '{}_{}'.format\n        new_columns = [fmt(c, p) for p in postfixes for c in columns]\n    else:\n        new_columns = columns\n\n    expressions = [Expression(stmt, col)\n                   for stmt, col in zip(stmts, new_columns)]\n    return expressions, new_columns",
    "label": 0
  },
  {
    "codes": "func (s *StorageDescriptor) SetNumberOfBuckets(v int64) *StorageDescriptor {\n\ts.NumberOfBuckets = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "def run_step(context):\n    \"\"\"Remove specified keys from context.\n\n    Args:\n        Context is a dictionary or dictionary-like.\n        context['contextClear'] must exist. It's a dictionary.\n        Will iterate context['contextClear'] and remove those keys from\n        context.\n\n    For example, say input context is:\n        key1: value1\n        key2: value2\n        key3: value3\n        key4: value4\n        contextClear:\n            - key2\n            - key4\n            - contextClear\n\n    This will result in return context:\n        key1: value1\n        key3: value3\n    \"\"\"\n    logger.debug(\"started\")\n    context.assert_key_has_value(key='contextClear', caller=__name__)\n\n    for k in context['contextClear']:\n        logger.debug(f\"removing {k} from context\")\n        # slightly unorthodox pop returning None means you don't get a KeyError\n        # if key doesn't exist\n        context.pop(k, None)\n        logger.info(f\"removed {k} from context\")\n\n    logger.debug(\"done\")",
    "label": 0
  },
  {
    "codes": "```python\nimport uuid\n\ndef generate_uuid_v4():\n    # Generate a UUID v4 and remove hyphens\n    return str(uuid.uuid4()).replace('-', '')\n```",
    "label": 1
  },
  {
    "codes": "public void updateClientId(String clientId) {\n        if (configForVM != null) {\n            configForVM.withClientId(clientId);\n        } else {\n            configForAppService.withClientId(clientId);\n        }\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<PatternRuleInfo> addPatternAsync(UUID appId, String versionId, PatternRuleCreateObject pattern) {\n        return addPatternWithServiceResponseAsync(appId, versionId, pattern).map(new Func1<ServiceResponse<PatternRuleInfo>, PatternRuleInfo>() {\n            @Override\n            public PatternRuleInfo call(ServiceResponse<PatternRuleInfo> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```python\n#!/usr/bin/env python3\n\nimport sys\nimport yaml\nimport toml\nimport json\n\ndef yaml_to_json(data):\n    return json.dumps(yaml.safe_load(data), indent=4)\n\ndef yaml_to_toml(data):\n    return toml.dumps(yaml.safe_load(data))\n\ndef json_to_yaml(data):\n    return yaml.safe_dump(json.loads(data), default_flow_style=False)\n\ndef json_to_toml(data):\n    return toml.dumps(json.loads(data))\n\ndef toml_to_yaml(data):\n    return yaml.safe_dump(toml.loads(data), default_flow_style=False)\n\ndef toml_to_json(data):\n    return json.dumps(toml.loads(data), indent=4)\n\ndef convert(input_format, output_format, data):\n    if input_format == 'yaml' and output_format == 'json':\n        return yaml_to_json(data)\n    elif input_format == 'yaml' and output_format == 'toml':\n        return yaml_to_toml(data)\n    elif input_format == 'json' and output_format == 'yaml':\n        return json_to_yaml(data)\n    elif input_format == 'json' and output_format == 'toml':\n        return json_to_toml(data)\n    elif input_format == 'toml' and output_format == 'yaml':\n        return toml_to_yaml(data)\n    elif input_format == 'toml' and output_format == 'json':\n        return toml_to_json(data)\n    else:\n        raise ValueError(f\"Unsupported conversion: {input_format} to {output_format}\")\n\ndef main():\n    if len(sys.argv) != 4:\n        print(\"Usage: converter <input_format> <output_format> <file_path>\")\n        sys.exit(1)\n\n    input_format = sys.argv[1].lower()\n    output_format = sys.argv[2].lower()\n    file_path = sys.argv[3]\n\n    with open(file_path, 'r') as file:\n        data = file.read()\n    \n    try:\n        result = convert(input_format, output_format, data)\n        print(result)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n```\n",
    "label": 1
  },
  {
    "codes": "def unsubscribe(self, jid, node=None, *,\n                    subscription_jid=None,\n                    subid=None):\n        \"\"\"\n        Unsubscribe from a node.\n\n        :param jid: Address of the PubSub service.\n        :type jid: :class:`aioxmpp.JID`\n        :param node: Name of the PubSub node to unsubscribe from.\n        :type node: :class:`str`\n        :param subscription_jid: The address to subscribe from the service.\n        :type subscription_jid: :class:`aioxmpp.JID`\n        :param subid: Unique ID of the subscription to remove.\n        :type subid: :class:`str`\n        :raises aioxmpp.errors.XMPPError: as returned by the service\n\n        By default, the unsubscribe request will be for the bare JID of the\n        client. It can be specified explicitly using the `subscription_jid`\n        argument.\n\n        If available, the `subid` should also be specified.\n\n        If an error occurs, the corresponding :class:`~.errors.XMPPError` is\n        raised.\n        \"\"\"\n\n        subscription_jid = subscription_jid or self.client.local_jid.bare()\n\n        iq = aioxmpp.stanza.IQ(to=jid, type_=aioxmpp.structs.IQType.SET)\n        iq.payload = pubsub_xso.Request(\n            pubsub_xso.Unsubscribe(subscription_jid, node=node, subid=subid)\n        )\n\n        yield from self.client.send(iq)",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<ImageList>> createWithServiceResponseAsync(String contentType, BodyModel bodyParameter) {\n        if (this.client.baseUrl() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.baseUrl() is required and cannot be null.\");\n        }\n        if (contentType == null) {\n            throw new IllegalArgumentException(\"Parameter contentType is required and cannot be null.\");\n        }\n        if (bodyParameter == null) {\n            throw new IllegalArgumentException(\"Parameter bodyParameter is required and cannot be null.\");\n        }\n        Validator.validate(bodyParameter);\n        String parameterizedHost = Joiner.on(\", \").join(\"{baseUrl}\", this.client.baseUrl());\n        return service.create(contentType, bodyParameter, this.client.acceptLanguage(), parameterizedHost, this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<ImageList>>>() {\n                @Override\n                public Observable<ServiceResponse<ImageList>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<ImageList> clientResponse = createDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<ExpressRouteCircuitConnectionInner>> createOrUpdateWithServiceResponseAsync(String resourceGroupName, String circuitName, String peeringName, String connectionName, ExpressRouteCircuitConnectionInner expressRouteCircuitConnectionParameters) {\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (circuitName == null) {\n            throw new IllegalArgumentException(\"Parameter circuitName is required and cannot be null.\");\n        }\n        if (peeringName == null) {\n            throw new IllegalArgumentException(\"Parameter peeringName is required and cannot be null.\");\n        }\n        if (connectionName == null) {\n            throw new IllegalArgumentException(\"Parameter connectionName is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (expressRouteCircuitConnectionParameters == null) {\n            throw new IllegalArgumentException(\"Parameter expressRouteCircuitConnectionParameters is required and cannot be null.\");\n        }\n        Validator.validate(expressRouteCircuitConnectionParameters);\n        final String apiVersion = \"2018-08-01\";\n        Observable<Response<ResponseBody>> observable = service.createOrUpdate(resourceGroupName, circuitName, peeringName, connectionName, this.client.subscriptionId(), expressRouteCircuitConnectionParameters, apiVersion, this.client.acceptLanguage(), this.client.userAgent());\n        return client.getAzureClient().getPutOrPatchResultAsync(observable, new TypeToken<ExpressRouteCircuitConnectionInner>() { }.getType());\n    }",
    "label": 0
  },
  {
    "codes": "func (c *Glue) BatchGetPartition(input *BatchGetPartitionInput) (*BatchGetPartitionOutput, error) {\n\treq, out := c.BatchGetPartitionRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "public PagedList<UsageInner> listWebWorkerUsagesNext(final String nextPageLink) {\n        ServiceResponse<Page<UsageInner>> response = listWebWorkerUsagesNextSinglePageAsync(nextPageLink).toBlocking().single();\n        return new PagedList<UsageInner>(response.body()) {\n            @Override\n            public Page<UsageInner> nextPage(String nextPageLink) {\n                return listWebWorkerUsagesNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "func (s *GetJobOutputOutput) SetContentType(v string) *GetJobOutputOutput {\n\ts.ContentType = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public void beginFailover(String resourceGroupName, String serverName, String databaseName, String linkId) {\n        beginFailoverWithServiceResponseAsync(resourceGroupName, serverName, databaseName, linkId).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "def _get_cct(x, y, z):\n        \"\"\"\n        Reference\n        Hernandez-Andres, J., Lee, R. L., & Romero, J. (1999).\n        Calculating correlated color temperatures across the entire gamut of daylight and skylight chromaticities.\n        Applied Optics, 38(27), 5703-5709.\n        \"\"\"\n        x_e = 0.3320\n        y_e = 0.1858\n\n        n = ((x / (x + z + z)) - x_e) / ((y / (x + z + z)) - y_e)\n\n        a_0 = -949.86315\n        a_1 = 6253.80338\n        a_2 = 28.70599\n        a_3 = 0.00004\n\n        t_1 = 0.92159\n        t_2 = 0.20039\n        t_3 = 0.07125\n\n        cct = a_0 + a_1 * numpy.exp(-n / t_1) + a_2 * numpy.exp(-n / t_2) + a_3 * numpy.exp(-n / t_3)\n        return cct",
    "label": 0
  },
  {
    "codes": "```javascript\n// Import D3.js library (ensure it's included in your HTML)\n// <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n\n// Set up SVG dimensions\nconst width = 800;\nconst height = 400;\nconst margin = { top: 20, right: 30, bottom: 30, left: 40 };\n\n// Create SVG element\nconst svg = d3.select(\"body\")\n  .append(\"svg\")\n  .attr(\"width\", width)\n  .attr(\"height\", height);\n\n// Create scales for x and y axes\nconst xScale = d3.scaleTime()\n  .range([margin.left, width - margin.right]);\n\nconst yScale = d3.scaleLinear()\n  .range([height - margin.bottom, margin.top]);\n\n// Create line generator\nconst line = d3.line()\n  .x(d => xScale(d.timestamp))\n  .y(d => yScale(d.price));\n\n// Initialize data array\nlet data = [];\n\n// Function to fetch real-time cryptocurrency price\nasync function fetchPrice() {\n  const response = await fetch('https://api.coingecko.com/api/v3/simple/price?ids=bitcoin&vs_currencies=usd');\n  const result = await response.json();\n  const price = result.bitcoin.usd;\n  const timestamp = new Date();\n\n  // Add new data point\n  data.push({ timestamp, price });\n\n  // Keep only the last 50 data points\n  if (data.length > 50) data.shift();\n\n  // Update scales\n  xScale.domain(d3.extent(data, d => d.timestamp));\n  yScale.domain([d3.min(data, d => d.price) * 0.99, d3.max(data, d => d.price) * 1.01]);\n\n  // Update the line\n  svg.selectAll(\".price-line\")\n    .datum(data)\n    .attr(\"d\", line);\n\n  // Update circles for each data point\n  const circles = svg.selectAll(\".price-circle\")\n    .data(data);\n\n  circles.enter()\n    .append(\"circle\")\n    .attr(\"class\", \"price-circle\")\n    .merge(circles)\n    .attr(\"cx\", d => xScale(d.timestamp))\n    .attr(\"cy\", d => yScale(d.price))\n    .attr(\"r\", 3);\n\n  circles.exit().remove();\n}\n\n// Fetch price every 5 seconds\nsetInterval(fetchPrice, 5000);\n\n// Initial fetch\nfetchPrice();\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<Boolean>> checkExistenceWithServiceResponseAsync(String resourceGroupName) {\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        return service.checkExistence(resourceGroupName, this.client.subscriptionId(), this.client.apiVersion(), this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<Void>, Observable<ServiceResponse<Boolean>>>() {\n                @Override\n                public Observable<ServiceResponse<Boolean>> call(Response<Void> response) {\n                    try {\n                        ServiceResponse<Boolean> clientResponse = checkExistenceDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "public static EntityGetOperation<LocatorInfo> get(String locatorId) {\n        return new DefaultGetOperation<LocatorInfo>(ENTITY_SET, locatorId,\n                LocatorInfo.class);\n    }",
    "label": 0
  },
  {
    "codes": "```c\n#include <stdio.h>\n#include <dirent.h>\n#include <sys/stat.h>\n#include <string.h>\n\n// Function to list files recursively\nvoid listFilesRecursively(const char *basePath) {\n    char path[1000];\n    struct dirent *dp;\n    struct stat statbuf;\n\n    // Open the directory\n    DIR *dir = opendir(basePath);\n    if (!dir) return;\n\n    // Read directory entries\n    while ((dp = readdir(dir)) != NULL) {\n        // Skip \".\" and \"..\"\n        if (strcmp(dp->d_name, \".\") != 0 && strcmp(dp->d_name, \"..\") != 0) {\n            // Construct the full path\n            strcpy(path, basePath);\n            strcat(path, \"/\");\n            strcat(path, dp->d_name);\n\n            // Get file status\n            if (stat(path, &statbuf) continue;\n\n            // Check if it's a directory\n            if (S_ISDIR(statbuf.st_mode)) {\n                // Recursively list files in the directory\n                listFilesRecursively(path);\n            } else {\n                // Print the file path\n                printf(\"%s\\n\", path);\n            }\n        }\n    }\n\n    // Close the directory\n    closedir(dir);\n}\n\nint main() {\n    // Start listing files from the current directory\n    listFilesRecursively(\".\");\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glacier) SetVaultNotifications(input *SetVaultNotificationsInput) (*SetVaultNotificationsOutput, error) {\n\treq, out := c.SetVaultNotificationsRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```rust\n// Add dependencies in Cargo.toml\n// [dependencies]\n// reqwest = { version = \"0.11\", features = [\"json\"] }\n// tokio = { version = \"1\", features = [\"full\"] }\n// serde = { version = \"1\", features = [\"derive\"] }\n// serde_json = \"1.0\"\n\nuse reqwest::Client;\nuse serde::Deserialize;\nuse std::error::Error;\n\n#[derive(Deserialize)]\nstruct TokenResponse {\n    access_token: String,\n    token_type: String,\n    expires_in: Option<u64>,\n}\n\nasync fn get_access_token(client_id: &str, client_secret: &str, token_url: &str) -> Result<String, Box<dyn Error>> {\n    let client = Client::new();\n    \n    // Create the request body\n    let params = [\n        (\"grant_type\", \"client_credentials\"),\n        (\"client_id\", client_id),\n        (\"client_secret\", client_secret),\n    ];\n\n    // Make the request to the token endpoint\n    let resp = client\n        .post(token_url)\n        .form(&params)\n        .send()\n        .await?;\n\n    // Parse the JSON response\n    let token_response: TokenResponse = resp.json().await?;\n    \n    // Return the access token\n    Ok(token_response.access_token)\n}\n\n// Main function to execute the flow\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn Error>> {\n    let client_id = \"your_client_id\";\n    let client_secret = \"your_client_secret\";\n    let token_url = \"https://example.com/oauth/token\";\n\n    // Get the access token\n    match get_access_token(client_id, client_secret, token_url).await {\n        Ok(token) => println!(\"Access Token: {}\", token),\n        Err(e) => eprintln!(\"Error fetching access token: {}\", e),\n    }\n\n    Ok(())\n}\n```",
    "label": 1
  },
  {
    "codes": "def start(self):\n        \"\"\"\n        Send the stream header as described above.\n        \"\"\"\n        attrs = {\n            (None, \"to\"): str(self._to),\n            (None, \"version\"): \".\".join(map(str, self._version))\n        }\n        if self._from:\n            attrs[None, \"from\"] = str(self._from)\n\n        self._writer.startDocument()\n        for prefix, uri in self._nsmap_to_use.items():\n            self._writer.startPrefixMapping(prefix, uri)\n        self._writer.startElementNS(\n            (namespaces.xmlstream, \"stream\"),\n            None,\n            attrs)\n        self._writer.flush()",
    "label": 0
  },
  {
    "codes": "```python\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization\n\n# Sample text data\ntext_data = [\n    \"TensorFlow is an open-source machine learning framework.\",\n    \"Keras provides simple APIs for neural networks.\",\n    \"Natural Language Processing (NLP) is a fascinating field.\"\n]\n\n# Define a TextVectorization layer\nvectorize_layer = TextVectorization(\n    max_tokens=10000,   # Limit on the number of unique tokens\n    output_mode='int',  # Output integers representing the vocabulary indices\n    output_sequence_length=10  # Pad or truncate sequences to 10 tokens\n)\n\n# Train the vectorization layer to adapt the vocabulary based on text data\nvectorize_layer.adapt(text_data)\n\n# Function to preprocess and vectorize text data\ndef preprocess_text(texts):\n    return vectorize_layer(texts)\n\n# Preprocess the text data\nvectorized_text_data = preprocess_text(text_data)\n\n# Print results to verify\nfor i, vectorized_text in enumerate(vectorized_text_data):\n    print(f\"Original Text: {text_data[i]}\")\n    print(f\"Vectorized: {vectorized_text.numpy()}\\n\")\n```\n",
    "label": 1
  },
  {
    "codes": "def temporal_louvain(tnet, resolution=1, intersliceweight=1, n_iter=100, negativeedge='ignore', randomseed=None, consensus_threshold=0.5, temporal_consensus=True, njobs=1):\n    r\"\"\"\n    Louvain clustering for a temporal network.\n\n    Parameters\n    -----------\n    tnet : array, dict, TemporalNetwork\n        Input network\n    resolution : int\n        resolution of Louvain clustering ($\\gamma$)\n    intersliceweight : int\n        interslice weight of multilayer clustering ($\\omega$). Must be positive.\n    n_iter : int\n        Number of iterations to run louvain for\n    randomseed : int\n        Set for reproduceability\n    negativeedge : str\n        If there are negative edges, what should be done with them.\n        Options: 'ignore' (i.e. set to 0). More options to be added.\n    consensus : float (0.5 default)\n        When creating consensus matrix to average over number of iterations, keep values when the consensus is this amount.\n\n    Returns\n    -------\n    communities : array (node,time)\n        node,time array of community assignment\n\n    Notes\n    -------\n\n    References\n    ----------\n    \"\"\"\n\n    tnet = process_input(tnet, ['C', 'G', 'TN'], 'TN')\n    # Divide resolution by the number of timepoints\n    resolution = resolution / tnet.T\n    supranet = create_supraadjacency_matrix(\n        tnet, intersliceweight=intersliceweight)\n    if negativeedge == 'ignore':\n        supranet = supranet[supranet['weight'] > 0]\n    nxsupra = tnet_to_nx(supranet)\n    np.random.seed(randomseed)\n    while True:\n        comtmp = []\n        with ProcessPoolExecutor(max_workers=njobs) as executor:\n            job = {executor.submit(_run_louvain, nxsupra, resolution, tnet.N, tnet.T) for n in range(n_iter)}\n            for j in as_completed(job):\n                comtmp.append(j.result())\n        comtmp = np.stack(comtmp)\n        comtmp = comtmp.transpose()\n        comtmp = np.reshape(comtmp, [tnet.N, tnet.T, n_iter], order='F')\n        if n_iter == 1: \n            break        \n        nxsupra_old = nxsupra\n        nxsupra = make_consensus_matrix(comtmp, consensus_threshold)\n        # If there was no consensus, there are no communities possible, return\n        if nxsupra is None:\n            break\n        if (nx.to_numpy_array(nxsupra, nodelist=np.arange(tnet.N*tnet.T)) == nx.to_numpy_array(nxsupra_old, nodelist=np.arange(tnet.N*tnet.T))).all():\n            break\n    communities = comtmp[:, :, 0]\n    if temporal_consensus == True:\n        communities = make_temporal_consensus(communities)\n    return communities",
    "label": 0
  },
  {
    "codes": "public Observable<Page<JobResponseInner>> listJobsNextAsync(final String nextPageLink) {\n        return listJobsNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<JobResponseInner>>, Page<JobResponseInner>>() {\n                @Override\n                public Page<JobResponseInner> call(ServiceResponse<Page<JobResponseInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.utils import save_image\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\n\n# Hyperparameters\nimage_size = 64\nbatch_size = 128\nlearning_rate = 0.0002\nlatent_vector_size = 100\nnum_epochs = 50\nngf, ndf, nc = 64, 64, 3\n\n# Data transformation and loading\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.CenterCrop(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5] * 3, [0.5] * 3)\n])\n\ndataset = datasets.ImageFolder(root='./data/faces', transform=transform)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Generator Model\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(latent_vector_size, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n# Discriminator Model\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n# Initialize Models\nnetG = Generator().cuda()\nnetD = Discriminator().cuda()\n\n# Loss function and Optimizers\ncriterion = nn.BCELoss()\noptimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n\n# Training\nfor epoch in range(num_epochs):\n    for i, data in enumerate(dataloader, 0):\n        # Update Discriminator\n        netD.zero_grad()\n        real_data = data[0].cuda()\n        batch_size = real_data.size(0)\n        labels = torch.full((batch_size,), 1, dtype=torch.float, device='cuda')\n        output = netD(real_data).view(-1)\n        lossD_real = criterion(output, labels)\n        lossD_real.backward()\n\n        noise = torch.randn(batch_size, latent_vector_size, 1, 1, device='cuda')\n        fake_data = netG(noise)\n        labels.fill_(0)\n        output = netD(fake_data.detach()).view(-1)\n        lossD_fake = criterion(output, labels)\n        lossD_fake.backward()\n        optimizerD.step()\n\n        # Update Generator\n        netG.zero_grad()\n        labels.fill_(1)\n        output = netD(fake_data).view(-1)\n        lossG = criterion(output, labels)\n        lossG.backward()\n        optimizerG.step()\n\n    # Save samples\n    if epoch % 10 == 0:\n        with torch.no_grad():\n            fake_images = netG(fixed_noise).detach().cpu()\n        save_image(fake_images, f'output/fake_images_epoch{epoch}.png', normalize=True)\n```",
    "label": 1
  },
  {
    "codes": "func (s *TableInput) SetName(v string) *TableInput {\n\ts.Name = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *InitiateJobInput) SetJobParameters(v *JobParameters) *InitiateJobInput {\n\ts.JobParameters = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function (axisModel, ecModel, api) {\n\n        var dim = this.dimension;\n\n        var axis = new SingleAxis(\n            dim,\n            axisHelper.createScaleByModel(axisModel),\n            [0, 0],\n            axisModel.get('type'),\n            axisModel.get('position')\n        );\n\n        var isCategory = axis.type === 'category';\n        axis.onBand = isCategory && axisModel.get('boundaryGap');\n        axis.inverse = axisModel.get('inverse');\n        axis.orient = axisModel.get('orient');\n\n        axisModel.axis = axis;\n        axis.model = axisModel;\n        axis.coordinateSystem = this;\n        this._axis = axis;\n    }",
    "label": 0
  },
  {
    "codes": "func (s *InitiateJobOutput) SetJobId(v string) *InitiateJobOutput {\n\ts.JobId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"log\"\n    \"net/http\"\n)\n\n// RecoveryMiddleware recovers from panics in HTTP handlers\nfunc RecoveryMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        defer func() {\n            if err := recover(); err != nil {\n                // Log the panic\n                log.Printf(\"Recovered from panic: %v\", err)\n                \n                // Respond with a 500 Internal Server Error\n                w.WriteHeader(http.StatusInternalServerError)\n                w.Write([]byte(\"Internal Server Error\"))\n            }\n        }()\n        \n        // Call the next handler\n        next.ServeHTTP(w, r)\n    })\n}\n```",
    "label": 1
  },
  {
    "codes": "function(lat, lng) {\n    var point,\n        proj = jvm.WorldMap.maps[this.params.map].projection,\n        centralMeridian = proj.centralMeridian,\n        width = this.width - this.baseTransX * 2 * this.baseScale,\n        height = this.height - this.baseTransY * 2 * this.baseScale,\n        inset,\n        bbox,\n        scaleFactor = this.scale / this.baseScale;\n\n    if (lng < (-180 + centralMeridian)) {\n      lng += 360;\n    }\n\n    point = jvm.Proj[proj.type](lat, lng, centralMeridian);\n\n    inset = this.getInsetForPoint(point.x, point.y);\n    if (inset) {\n      bbox = inset.bbox;\n\n      point.x = (point.x - bbox[0].x) / (bbox[1].x - bbox[0].x) * inset.width * this.scale;\n      point.y = (point.y - bbox[0].y) / (bbox[1].y - bbox[0].y) * inset.height * this.scale;\n\n      return {\n        x: point.x + this.transX*this.scale + inset.left*this.scale,\n        y: point.y + this.transY*this.scale + inset.top*this.scale\n      };\n     } else {\n       return false;\n     }\n  }",
    "label": 0
  },
  {
    "codes": "```kotlin\n// Base class for all domain-specific errors\nsealed class DomainError {\n    // Network-related error, e.g., timeout or unreachable server\n    data class NetworkError(val message: String) : DomainError()\n\n    // Error related to invalid data, such as invalid input or parsing error\n    data class ValidationError(val fieldName: String, val issue: String) : DomainError()\n\n    // Error for authorization failures, like invalid credentials\n    object AuthorizationError : DomainError()\n\n    // Error for unexpected server responses \n    data class ServerError(val code: Int, val message: String) : DomainError()\n\n    // Specific error for resource not being found\n    data class NotFoundError(val resourceId: String) : DomainError()\n\n    // Database-related error considering IO or data corruption issues \n    data class DatabaseError(val details: String) : DomainError()\n\n    // Represents any unknown error not covered by other types\n    data class UnknownError(val exception: Throwable) : DomainError()\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```javascript\n// Utility function to deep clone objects with circular references\nfunction deepClone(obj, cache = new WeakMap()) {\n    // If the object is null or not an object, return it directly\n    if (obj === null || typeof obj !== 'object') {\n        return obj;\n    }\n\n    // If the object is already cloned, return the cached clone\n    if (cache.has(obj)) {\n        return cache.get(obj);\n    }\n\n    // Handle Date objects\n    if (obj instanceof Date) {\n        return new Date(obj);\n    }\n\n    // Handle RegExp objects\n    if (obj instanceof RegExp) {\n        return new RegExp(obj);\n    }\n\n    // Handle Arrays\n    if (Array.isArray(obj)) {\n        const clone = [];\n        cache.set(obj, clone); // Cache the clone before populating it\n        for (let i = 0; i < obj.length; i++) {\n            clone[i] = deepClone(obj[i], cache);\n        }\n        return clone;\n    }\n\n    // Handle Objects\n    const clone = Object.create(Object.getPrototypeOf(obj));\n    cache.set(obj, clone); // Cache the clone before populating it\n    for (const key in obj) {\n        if (obj.hasOwnProperty(key)) {\n            clone[key] = deepClone(obj[key], cache);\n        }\n    }\n\n    return clone;\n}\n```",
    "label": 1
  },
  {
    "codes": "```hcl\nprovider \"aws\" {\n  region = \"us-west-2\"\n}\n\nresource \"aws_iam_role\" \"sagemaker_execution_role\" {\n  name = \"sagemaker_execution_role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Principal = {\n          Service = \"sagemaker.amazonaws.com\"\n        }\n        Action = \"sts:AssumeRole\"\n      },\n    ]\n  })\n}\n\nresource \"aws_iam_role_policy\" \"sagemaker_policy\" {\n  role = aws_iam_role.sagemaker_execution_role.name\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect   = \"Allow\"\n        Action   = \"s3:*\"\n        Resource = \"*\"\n      },\n      {\n        Effect   = \"Allow\"\n        Action   = \"logs:*\"\n        Resource = \"*\"\n      },\n    ]\n  })\n}\n\nresource \"aws_sagemaker_model\" \"ml_model\" {\n  name          = \"my-ml-model\"\n  execution_role_arn = aws_iam_role.sagemaker_execution_role.arn\n  primary_container {\n    image         = \"YOUR_CONTAINER_IMAGE_URI\"\n    model_data_url = \"s3://YOUR_BUCKET/YOUR_MODEL.tar.gz\"\n  }\n}\n\nresource \"aws_sagemaker_endpoint_configuration\" \"config\" {\n  name                     = \"my-endpoint-config\"\n  production_variants {\n    variant_name          = \"AllTraffic\"\n    model_name            = aws_sagemaker_model.ml_model.name\n    initial_instance_count = 1\n    instance_type         = \"ml.m5.large\"\n  }\n}\n\nresource \"aws_sagemaker_endpoint\" \"endpoint\" {\n  endpoint_name         = \"my-endpoint\"\n  endpoint_config_name  = aws_sagemaker_endpoint_configuration.config.name\n}\n```",
    "label": 1
  },
  {
    "codes": "function (valuenum, highlight) {\n            var vals = this.values[valuenum],\n                options = this.options,\n                xaxisOffset = this.xaxisOffset,\n                result = [],\n                range = this.range,\n                stacked = this.stacked,\n                target = this.target,\n                x = valuenum * this.totalBarWidth,\n                canvasHeightEf = this.canvasHeightEf,\n                yoffset = this.yoffset,\n                y, height, color, isNull, yoffsetNeg, i, valcount, val, minPlotted, allMin;\n\n            vals = $.isArray(vals) ? vals : [vals];\n            valcount = vals.length;\n            val = vals[0];\n            isNull = all(null, vals);\n            allMin = all(xaxisOffset, vals, true);\n\n            if (isNull) {\n                if (options.get('nullColor')) {\n                    color = highlight ? options.get('nullColor') : this.calcHighlightColor(options.get('nullColor'), options);\n                    y = (yoffset > 0) ? yoffset - 1 : yoffset;\n                    return target.drawRect(x, y, this.barWidth - 1, 0, color, color);\n                } else {\n                    return undefined;\n                }\n            }\n            yoffsetNeg = yoffset;\n            for (i = 0; i < valcount; i++) {\n                val = vals[i];\n\n                if (stacked && val === xaxisOffset) {\n                    if (!allMin || minPlotted) {\n                        continue;\n                    }\n                    minPlotted = true;\n                }\n\n                if (range > 0) {\n                    height = Math.floor(canvasHeightEf * ((Math.abs(val - xaxisOffset) / range))) + 1;\n                } else {\n                    height = 1;\n                }\n                if (val < xaxisOffset || (val === xaxisOffset && yoffset === 0)) {\n                    y = yoffsetNeg;\n                    yoffsetNeg += height;\n                } else {\n                    y = yoffset - height;\n                    yoffset -= height;\n                }\n                color = this.calcColor(i, val, valuenum);\n                if (highlight) {\n                    color = this.calcHighlightColor(color, options);\n                }\n                result.push(target.drawRect(x, y, this.barWidth - 1, height - 1, color, color));\n            }\n            if (result.length === 1) {\n                return result[0];\n            }\n            return result;\n        }",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define variables\nBACKUP_DIR=\"/path/to/backup\"  # Directory to backup\nS3_BUCKET=\"s3://your-bucket-name\"  # S3 bucket name\nDATE=$(date +%Y-%m-%d)  # Current date for backup naming\nLOG_FILE=\"/var/log/backup.log\"  # Log file path\n\n# Check if backup directory exists\nif [ ! -d \"$BACKUP_DIR\" ]; then\n    echo \"Backup directory does not exist: $BACKUP_DIR\" >> \"$LOG_FILE\"\n    exit 1\nfi\n\n# Create a compressed archive of the backup directory\nBACKUP_FILE=\"/tmp/backup-$DATE.tar.gz\"\ntar -czf \"$BACKUP_FILE\" \"$BACKUP_DIR\" >> \"$LOG_FILE\" 2>&1\n\n# Check if tar command succeeded\nif [ $? -ne 0 ]; then\n    echo \"Failed to create backup archive: $BACKUP_FILE\" >> \"$LOG_FILE\"\n    exit 1\nfi\n\n# Upload the backup file to S3\naws s3 cp \"$BACKUP_FILE\" \"$S3_BUCKET/\" >> \"$LOG_FILE\" 2>&1\n\n# Check if S3 upload succeeded\nif [ $? -ne 0 ]; then\n    echo \"Failed to upload backup to S3: $BACKUP_FILE\" >> \"$LOG_FILE\"\n    exit 1\nfi\n\n# Clean up the temporary backup file\nrm -f \"$BACKUP_FILE\"\n\n# Log success message\necho \"Backup completed successfully: $BACKUP_FILE\" >> \"$LOG_FILE\"\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport psutil\nimport time\nfrom datetime import datetime\n\n# Threshold for CPU usage percentage that will be considered an anomaly\nCPU_USAGE_THRESHOLD = 80\n\n# Time between checks in seconds\nCHECK_INTERVAL = 5\n\n# File to log anomalies\nLOG_FILE = 'cpu_anomalies.log'\n\ndef log_anomaly(cpu_usage):\n    \"\"\"Log anomalies to a file with timestamp\"\"\"\n    with open(LOG_FILE, 'a') as file:\n        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        file.write(f'[{timestamp}] Anomaly detected: CPU usage at {cpu_usage}%\\n')\n\ndef monitor_cpu():\n    \"\"\"Monitor CPU usage and log anomalies\"\"\"\n    while True:\n        # Get the current CPU usage\n        cpu_usage = psutil.cpu_percent(interval=1)\n        \n        # Check if the CPU usage exceeds the threshold\n        if cpu_usage > CPU_USAGE_THRESHOLD:\n            log_anomaly(cpu_usage)\n        \n        # Wait for the next interval\n        time.sleep(CHECK_INTERVAL)\n\nif __name__ == '__main__':\n    # Start monitoring the CPU\n    monitor_cpu()\n```",
    "label": 1
  },
  {
    "codes": "def send(self, xso):\n        \"\"\"\n        Send a single XML stream object.\n\n        :param xso: Object to serialise and send.\n        :type xso: :class:`aioxmpp.xso.XSO`\n        :raises Exception: from any serialisation errors, usually\n                           :class:`ValueError`.\n\n        Serialise the `xso` and send it over the stream. If any serialisation\n        error occurs, no data is sent over the stream and the exception is\n        re-raised; the :meth:`send` method thus provides strong exception\n        safety.\n\n        .. warning::\n\n           The behaviour of :meth:`send` after :meth:`abort` or :meth:`close`\n           and before :meth:`start` is undefined.\n\n        \"\"\"\n        with self._writer.buffer():\n            xso.unparse_to_sax(self._writer)",
    "label": 0
  },
  {
    "codes": "```python\ndef factorial(n):\n    # Using recursion with memoization to optimize repeated calculations\n    memo = {}\n    \n    def helper(x):\n        if x in memo:\n            return memo[x]\n        if x == 0 or x == 1:\n            return 1\n        memo[x] = x * helper(x - 1)\n        return memo[x]\n    \n    return helper(n)\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glacier) PurchaseProvisionedCapacityWithContext(ctx aws.Context, input *PurchaseProvisionedCapacityInput, opts ...request.Option) (*PurchaseProvisionedCapacityOutput, error) {\n\treq, out := c.PurchaseProvisionedCapacityRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "func (s *StartTriggerOutput) SetName(v string) *StartTriggerOutput {\n\ts.Name = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *JobDescription) SetCompleted(v bool) *JobDescription {\n\ts.Completed = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *BatchGetPartitionInput) SetCatalogId(v string) *BatchGetPartitionInput {\n\ts.CatalogId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<List<SiteInner>> beginResumeAsync(final String resourceGroupName, final String name, final ListOperationCallback<SiteInner> serviceCallback) {\n        return AzureServiceFuture.fromPageResponse(\n            beginResumeSinglePageAsync(resourceGroupName, name),\n            new Func1<String, Observable<ServiceResponse<Page<SiteInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<SiteInner>>> call(String nextPageLink) {\n                    return beginResumeNextSinglePageAsync(nextPageLink);\n                }\n            },\n            serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```java\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.springframework.stereotype.Component;\n\n@Aspect\n@Component\npublic class CircuitBreakerAspect {\n\n    private static final int MAX_FAILURES = 3; // Maximum allowed failures\n    private int failureCount = 0; // Counter for failures\n    private boolean circuitOpen = false; // Circuit breaker state\n\n    @Around(\"execution(* com.example.service.*.*(..))\") // Apply to all methods in service package\n    public Object circuitBreaker(ProceedingJoinPoint joinPoint) throws Throwable {\n        if (circuitOpen) {\n            throw new RuntimeException(\"Circuit is open. Service unavailable.\"); // Fail fast if circuit is open\n        }\n\n        try {\n            Object result = joinPoint.proceed(); // Proceed with the method execution\n            failureCount = 0; // Reset failure count on success\n            return result;\n        } catch (Exception e) {\n            failureCount++; // Increment failure count on exception\n            if (failureCount >= MAX_FAILURES) {\n                circuitOpen = true; // Open the circuit if max failures reached\n            }\n            throw e; // Re-throw the exception\n        }\n    }\n\n    // Method to manually reset the circuit breaker (optional)\n    public void resetCircuitBreaker() {\n        circuitOpen = false;\n        failureCount = 0;\n    }\n}\n``` \n\n### Comments:\n- The aspect applies to all methods in the `com.example.service` package.\n- The circuit breaker opens after `MAX_FAILURES` consecutive failures.\n- Once the circuit is open, all subsequent calls fail fast with a `RuntimeException`.\n- The `resetCircuitBreaker` method allows manual resetting of the circuit breaker.\n- This implementation is simple and does not include a timeout or fallback mechanism, which can be added as needed.",
    "label": 1
  },
  {
    "codes": "func (c *Glue) DeleteUserDefinedFunction(input *DeleteUserDefinedFunctionInput) (*DeleteUserDefinedFunctionOutput, error) {\n\treq, out := c.DeleteUserDefinedFunctionRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "def verification_start(self, phone_number, country_code, via='sms',\n                           locale=None, code_length=4):\n        \"\"\"\n        :param string phone_number: stored in your databse or you provided while creating new user.\n        :param string country_code: stored in your databse or you provided while creating new user.\n        :param string via: verification method either sms or call\n        :param string locale: optional default none\n        :param number code_length: optional default 4\n        :return:\n        \"\"\"\n\n        if via != 'sms' and via != 'call':\n            raise AuthyFormatException(\"Invalid Via. Expected 'sms' or 'call'.\")\n\n        options = {\n            'phone_number': phone_number,\n            'country_code': country_code,\n            'via': via\n        }\n\n        if locale:\n            options['locale'] = locale\n\n        try:\n            cl = int(code_length)\n            if cl < 4 or cl > 10:\n                raise ValueError\n            options['code_length'] = cl\n        except ValueError:\n            raise AuthyFormatException(\n                \"Invalid code_length. Expected numeric value from 4-10.\")\n\n        resp = self.post(\"/protected/json/phones/verification/start\", options)\n        return Phone(self, resp)",
    "label": 0
  },
  {
    "codes": "```python\nimport functools\nimport logging\nimport traceback\n\n# Configure logging\nlogging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef log_exceptions(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            # Log the exception with traceback\n            logging.error(f\"Exception occurred in {func.__name__}: {e}\\n{traceback.format_exc()}\")\n            raise  # Re-raise the exception after logging\n    return wrapper\n\n# Example usage\n@log_exceptions\ndef risky_function(x, y):\n    return x / y\n\n# Test the decorator\nrisky_function(10, 0)\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport psutil\nimport time\nimport logging\n\n# Configure logging to write to a file\nlogging.basicConfig(filename='cpu_usage.log', level=logging.INFO, \n                    format='%(asctime)s - %(message)s')\n\n# Define a threshold for CPU usage anomaly (e.g., 80%)\nCPU_THRESHOLD = 80\n\ndef monitor_cpu_usage():\n    while True:\n        # Get current CPU usage percentage\n        cpu_usage = psutil.cpu_percent(interval=1)\n        \n        # Log an anomaly if CPU usage exceeds the threshold\n        if cpu_usage > CPU_THRESHOLD:\n            logging.warning(f\"High CPU usage detected: {cpu_usage}%\")\n        \n        # Sleep for a short duration before checking again\n        time.sleep(5)\n\nif __name__ == \"__main__\":\n    monitor_cpu_usage()\n```",
    "label": 1
  },
  {
    "codes": "def encode(self, obj):\n        \"\"\"Fired for every object.\"\"\"\n        s = super(CustomEncoder, self).encode(obj)\n        # If uncompressed, postprocess for formatting\n        if len(s.splitlines()) > 1:\n            s = self.postprocess(s)\n        return s",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"log\"\n    \"net/http\"\n)\n\n// RecoveryMiddleware recovers from panics in HTTP handlers\nfunc RecoveryMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        defer func() {\n            if err := recover(); err != nil {\n                // Log the panic\n                log.Printf(\"Recovered from panic: %v\", err)\n                \n                // Respond with a 500 Internal Server Error\n                w.WriteHeader(http.StatusInternalServerError)\n                w.Write([]byte(\"Internal Server Error\"))\n            }\n        }()\n        \n        // Call the next handler\n        next.ServeHTTP(w, r)\n    })\n}\n\nfunc main() {\n    // Example handler that might panic\n    panicHandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        panic(\"something went wrong\")\n    })\n\n    // Wrap the handler with the recovery middleware\n    http.Handle(\"/\", RecoveryMiddleware(panicHandler))\n\n    // Start the server\n    log.Println(\"Server started on :8080\")\n    log.Fatal(http.ListenAndServe(\":8080\", nil))\n}\n```",
    "label": 1
  },
  {
    "codes": "def findspan(self, *words):\n        \"\"\"Returns the span element which spans over the specified words or morphemes.\n\n        See also:\n            :meth:`Word.findspans`\n        \"\"\"\n\n        for span in self.select(AbstractSpanAnnotation,None,True):\n            if tuple(span.wrefs()) == words:\n                return span\n        raise NoSuchAnnotation",
    "label": 0
  },
  {
    "codes": "```python\nimport hmac\nimport secrets\n\ndef constant_time_compare(digest_a, digest_b):\n    \"\"\"\n    Securely compare two HMAC digests in constant time to avoid timing attacks.\n    \n    :param digest_a: First HMAC digest (bytes)\n    :param digest_b: Second HMAC digest (bytes)\n    :return: True if digests are equal, False otherwise\n    \"\"\"\n    # Generate a random key for HMAC comparison\n    key = secrets.token_bytes(32)\n    \n    # Compute HMAC for both digests using the same key\n    hmac_a = hmac.new(key, digesta, digestmod='sha256').digest()\n    hmac_b = hmac.new(key, digestb, digestmod='sha256').digest()\n    \n    # Compare the HMACs in constant time\n    return hmac.compare_digest(hmac_a, hmac_b)\n```",
    "label": 1
  },
  {
    "codes": "```kotlin\n// Base sealed class for all domain-specific errors\nsealed class DomainError {\n\n    // Network-related errors\n    sealed class NetworkError : DomainError() {\n        object ConnectionTimedOut : NetworkError() // The network connection timed out\n        object NoInternet : NetworkError() // No internet connection available\n        object ServerUnreachable : NetworkError() // Unable to reach the server\n    }\n\n    // Authentication-related errors\n    sealed class AuthenticationError : DomainError() {\n        object InvalidCredentials : AuthenticationError() // Credentials provided are invalid\n        object TokenExpired : AuthenticationError() // Authentication token has expired\n        object UserNotFound : AuthenticationError() // User does not exist in the system\n    }\n\n    // Validation-related errors\n    sealed class ValidationError : DomainError() {\n        data class InvalidField(val fieldName: String) : ValidationError() // Specific field validation failed\n        object MissingRequiredFields : ValidationError() // Required fields are missing in the input\n    }\n\n    // Database-related errors\n    sealed class DatabaseError : DomainError() {\n        object ConnectionFailed : DatabaseError() // Failed to connect to the database\n        object RecordNotFound : DatabaseError() // No matching record found in the database\n        object QueryFailed : DatabaseError() // The database query execution failed\n    }\n\n    // Generic error\n    data class UnknownError(val message: String) : DomainError() // An unspecified error occurred, contains error message\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model)\n\n# Save the CoreML model to a file\ncoreml_model.save('your_model.mlmodel')\n```\n\n```python\n# Alternative method using TensorFlow 2.x and coremltools\nimport tensorflow as tf\nimport coremltools as ct\n\n# Load the Keras model\nkeras_model = tf.keras.models.load_model('your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model, source='tensorflow')\n\n# Save the CoreML model to a file\ncoreml_model.save('your_model.mlmodel')\n```\n\n```python\n# Another approach using a custom input and output description\nimport coremltools as ct\nfrom keras.models import load_model\n\n# Load the Keras model\nkeras_model = load_model('your_keras_model.h5')\n\n# Define input and output descriptions\ninput_description = ct.ImageType(shape=(1, 224, 224, 3), \noutput_description = ct.ImageType(shape=(1, 1000))\n\n# Convert the Keras model to CoreML format with custom descriptions\ncoreml_model = ct.convert(keras_model, inputs=[input_description], outputs=[output_description])\n\n# Save the CoreML model to a file\ncoreml_model.save('your_model.mlmodel')\n```\n\n```python\n# Using a pre-trained model from Keras applications\nimport coremltools as ct\nfrom keras.applications import MobileNet\n\n# Load a pre-trained MobileNet model\nkeras_model = MobileNet(weights='imagenet')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model)\n\n# Save the CoreML model to a file\ncoreml_model.save('mobilenet.mlmodel')\n```",
    "label": 1
  },
  {
    "codes": "```ruby\nclass Scheduler\n  def initialize\n    @fibers = []\n  end\n\n  def schedule(&block)\n    @fibers << Fiber.new do\n      block.call\n    end\n  end\n\n  def run\n    until @fibers.empty?\n      fiber = @fibers.shift\n      fiber.resume if fiber.alive?\n    end\n  end\nend\n\n# Example Usage\nscheduler = Scheduler.new\n\nscheduler.schedule do\n  puts \"Task 1 started\"\n  Fiber.yield\n  puts \"Task 1 resumed\"\nend\n\nscheduler.schedule do\n  puts \"Task 2 started\"\n  Fiber.yield\n  puts \"Task 2 resumed\"\nend\n\nscheduler.run\n```",
    "label": 1
  },
  {
    "codes": "public QueueDescription createQueue(QueueDescription queueDescription) throws ServiceBusException, InterruptedException {\n        return Utils.completeFuture(this.asyncClient.createQueueAsync(queueDescription));\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport sys\n\ndef greedy_tsp(graph, start):\n    # Initialize variables\n    num_nodes = len(graph)\n    visited = [False] * num_nodes\n    path = [start]\n    visited[start] = True\n    total_cost = 0\n\n    # Start from the initial node\n    current_node = start\n\n    # Loop until all nodes are visited\n    while len(path) < num_nodes:\n        nearest_node = -1\n        min_cost = sys.maxsize\n\n        # Find the nearest unvisited node\n        for neighbor in range(num_nodes):\n            if not visited[neighbor] and graph[current_node][neighbor] < min_cost:\n                min_cost = graph[current_node][neighbor]\n                nearest_node = neighbor\n\n        # Move to the nearest node\n        path.append(nearest_node)\n        visited[nearest_node] = True\n        total_cost += min_cost\n        current_node = nearest_node\n\n    # Return to the starting node to complete the cycle\n    total_cost += graph[current_node][start]\n    path.append(start)\n\n    return path, total_cost\n\n# Example usage:\ngraph = [\n    [0, 10, 15, 20],\n    [10, 0, 35, 25],\n    [15, 35, 0, 30],\n    [20, 25, 30, 0]\n]\nstart_node = 0\npath, cost = greedy_tsp(graph, start_node)\nprint(\"Path:\", path)\nprint(\"Total Cost:\", cost)\n```",
    "label": 1
  },
  {
    "codes": "```javascript\n// Create a new Phaser game instance\nconst config = {\n    type: Phaser.AUTO,\n    width: 800,\n    height: 600,\n    physics: {\n        default: 'arcade',\n        arcade: {\n            debug: false\n        }\n    },\n    scene: {\n        preload: preload,\n        create: create,\n        update: update\n    }\n};\n\nconst game = new Phaser.Game(config);\n\nlet player;\nlet cursors;\nlet particles;\nlet lasers;\nlet nextFire = 0;\nlet fireRate = 100;\n\nfunction preload() {\n    // Load assets\n    this.load.image('ship', 'assets/ship.png');\n    this.load.image('laser', 'assets/laser.png');\n    this.load.image('thrust', 'assets/thrust.png');\n}\n\nfunction create() {\n    // Create player ship\n    player = this.physics.add.sprite(400, 500, 'ship').setOrigin(0.5, 0.5);\n\n    // Create cursor keys for controls\n    cursors = this.input.keyboard.createCursorKeys();\n\n    // Create particle emitter for thrust\n    particles = this.add.particles('thrust');\n    particles.createEmitter({\n        speed: 100,\n        scale: { start: 0.5, end: 0 },\n        blendMode: 'ADD',\n        follow: player,\n        followOffset: { x: 0, y: 50 }\n    });\n\n    // Group for laser shots\n    lasers = this.physics.add.group({\n        defaultKey: 'laser',\n        maxSize: 10\n    });\n\n    // Set bounds for player\n    player.setCollideWorldBounds(true);\n}\n\nfunction update(time, delta) {\n    // Movement controls\n    if (cursors.left.isDown) {\n        player.setVelocityX(-200);\n    } else if (cursors.right.isDown) {\n        player.setVelocityX(200);\n    } else {\n        player.setVelocityX(0);\n    }\n\n    if (cursors.up.isDown) {\n        player.setVelocityY(-200);\n        particles.emitParticleAt(player.x, player.y + 20);\n    } else if (cursors.down.isDown) {\n        player.setVelocityY(200);\n    } else {\n        player.setVelocityY(0);\n    }\n\n    // Fire lasers\n    if (cursors.space.isDown && time > nextFire) {\n        let laser = lasers.get(player.x, player.y - 20);\n        if (laser) {\n            laser.setActive(true).setVisible(true);\n            laser.setVelocityY(-300);\n            nextFire = time + fireRate;\n        }\n    }\n\n    // Recycle lasers\n    lasers.children.iterate(function (laser) {\n        if (laser.active && laser.y < 0) {\n            laser.setActive(false).setVisible(false);\n        }\n    });\n}\n```\n",
    "label": 1
  },
  {
    "codes": "function normalizeLevel(level) {\n  if (level.value <= Logger.Level.ALL.value) {          // ALL is 0.\n    return Logger.Level.ALL;\n\n  } else if (level.value === Logger.Level.OFF.value) {  // OFF is Infinity\n    return Logger.Level.OFF;\n\n  } else if (level.value < Logger.Level.INFO.value) {\n    return Logger.Level.DEBUG;\n\n  } else if (level.value < Logger.Level.WARNING.value) {\n    return Logger.Level.INFO;\n\n  } else if (level.value < Logger.Level.SEVERE.value) {\n    return Logger.Level.WARNING;\n\n  } else {\n    return Logger.Level.SEVERE;\n  }\n}",
    "label": 0
  },
  {
    "codes": "```scala\nimport java.time.Duration\nimport java.util.Collections\nimport org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}\nimport org.apache.kafka.common.serialization.StringDeserializer\n\n// Define Kafka consumer properties\nval props = new java.util.Properties()\nprops.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\")\nprops.put(ConsumerConfig.GROUP_ID_CONFIG, \"scala-consumer-group\")\nprops.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)\nprops.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)\nprops.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\")\n\n// Create Kafka consumer instance\nval consumer = new KafkaConsumer[String, String](props)\n\n// Subscribe to the topic\nval topic = \"real-time-events\"\nconsumer.subscribe(Collections.singletonList(topic))\n\n// Poll for new messages\ntry {\n  while (true) {\n    val records = consumer.poll(Duration.ofMillis(1000))\n    records.forEach { record =>\n      println(s\"Received message: key=${record.key}, value=${record.value}, offset=${record.offset}\")\n    }\n  }\n} finally {\n  consumer.close() // Close the consumer to release resources\n}\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse axum::{\n    body::{self, Body},\n    http::{Request, StatusCode},\n    response::{IntoResponse, Response},\n    routing::get,\n    Router,\n};\nuse tokio::sync::Mutex;\nuse std::sync::Arc;\nuse std::time::{Duration, Instant};\nuse std::collections::HashMap;\n\n#[derive(Default)]\nstruct RateLimiter {\n    requests: Mutex<HashMap<String, (u64, Instant)>>, // IP -> (count, last_request_time)\n    limit: u64,\n    duration: Duration,\n}\n\nimpl RateLimiter {\n    async fn check_rate_limit(&self, ip: String) -> Result<(), StatusCode> {\n        let mut requests = self.requests.lock().await;\n\n        let current_time = Instant::now();\n        let entry = requests.entry(ip.clone()).or_insert((0, current_time));\n\n        let (count, last_request_time) = entry;\n\n        if current_time.duration_since(*last_request_time) > self.duration {\n            *count = 1;\n            *last_request_time = current_time;\n            Ok(())\n        } else if *count < self.limit {\n            *count += 1;\n            Ok(())\n        } else {\n            Err(StatusCode::TOO_MANY_REQUESTS)\n        }\n    }\n}\n\nasync fn handler(rate_limiter: Arc<RateLimiter>, req: Request<Body>) -> Result<impl IntoResponse, StatusCode> {\n    let ip = req.headers().get(\"X-Forwarded-For\")\n        .and_then(|h| h.to_str().ok())\n        .unwrap_or(\"unknown\")\n        .to_string();\n\n    rate_limiter.check_rate_limit(ip).await?;\n    Ok(\"Request processed\")\n}\n\n#[tokio::main]\nasync fn main() {\n    let rate_limiter = Arc::new(RateLimiter {\n        limit: 100, // requests per duration\n        duration: Duration::from_secs(60), // window duration\n        ..Default::default()\n    });\n\n    let app = Router::new()\n        .route(\"/\", get(handler))\n        .layer(axum::AddExtensionLayer::new(rate_limiter));\n\n    axum::Server::bind(&\"0.0.0.0:3000\".parse().unwrap())\n        .serve(app.into_make_service())\n        .await\n        .unwrap();\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.TimeoutException;\n\npublic class AsyncChainWithFallbacks {\n\n    public static void main(String[] args) {\n        ExecutorService executor = Executors.newFixedThreadPool(3);\n\n        CompletableFuture<Void> future = CompletableFuture.supplyAsync(() -> {\n            // Simulate a task that may throw an exception\n            if (Math.random() > 0.5) {\n                throw new RuntimeException(\"Task 1 failed\");\n            }\n            return \"Task 1 result\";\n        }, executor).thenApplyAsync(result -> {\n            // Process the result and return a new value\n            return result + \", Task 2 processed\";\n        }, executor).exceptionally(ex -> {\n            // Fallback for Task 1\n            return \"Fallback result for Task 1\";\n        }).thenApplyAsync(result -> {\n            // Continue processing with the result from either Task 2 or Task 1 fallback\n            return result + \" -> Task 3 processed\";\n        }, executor).exceptionally(ex -> {\n            // Fallback for Task 3\n            return \"Fallback result for Task 3\";\n        }).thenAcceptAsync(finalResult -> {\n            // Final consumer action using the result of Task 3 or its fallback\n            System.out.println(finalResult);\n        }, executor).exceptionally(ex -> {\n            // Final fallback if all else fails\n            System.out.println(\"Final fallback due to: \" + ex.getMessage());\n            return null;\n        });\n\n        // Shutdown the executor to release resources\n        future.join();\n        executor.shutdown();\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function routeChat(call) {\n  call.on('data', function(note) {\n    var key = pointKey(note.getLocation());\n    /* For each note sent, respond with all previous notes that correspond to\n     * the same point */\n    if (route_notes.hasOwnProperty(key)) {\n      _.each(route_notes[key], function(note) {\n        call.write(note);\n      });\n    } else {\n      route_notes[key] = [];\n    }\n    // Then add the new note to the list\n    route_notes[key].push(note);\n  });\n  call.on('end', function() {\n    call.end();\n  });\n}",
    "label": 0
  },
  {
    "codes": "```go\n// main.go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"sync\"\n\t\"time\"\n)\n\n// PredictionRequest represents a single prediction request\ntype PredictionRequest struct {\n\tID    int\n\tInput []float32\n}\n\n// PredictionResponse represents the result of a prediction\ntype PredictionResponse struct {\n\tID     int\n\tOutput []float32\n}\n\n// GPUWorker simulates a GPU worker that processes predictions\nfunc GPUWorker(ctx context.Context, wg *sync.WaitGroup, requests <-chan PredictionRequest, results chan<- PredictionResponse) {\n\tdefer wg.Done()\n\tfor {\n\t\tselect {\n\t\tcase req, ok := <-requests:\n\t\t\tif !ok {\n\t\t\t\treturn // Channel closed, exit the worker\n\t\t\t}\n\t\t\t// Simulate GPU processing\n\t\t\ttime.Sleep(100 * time.Millisecond) // Simulate processing time\n\t\t\toutput := make([]float32, len(req.Input))\n\t\t\tfor i, val := range req.Input {\n\t\t\t\toutput[i] = val * 2 // Example transformation\n\t\t\t}\n\t\t\tresults <- PredictionResponse{ID: req.ID, Output: output}\n\t\tcase <-ctx.Done():\n\t\t\treturn // Context canceled, exit the worker\n\t\t}\n\t}\n}\n\n// BatchProcessor handles batching and dispatching requests to GPU workers\nfunc BatchProcessor(ctx context.Context, numWorkers int, requests []PredictionRequest) []PredictionResponse {\n\tvar wg sync.WaitGroup\n\trequestChan := make(chan PredictionRequest, len(requests))\n\tresultChan := make(chan PredictionResponse, len(requests))\n\n\t// Start GPU workers\n\tfor i := 0; i < numWorkers; i++ {\n\t\twg.Add(1)\n\t\tgo GPUWorker(ctx, &wg, requestChan, resultChan)\n\t}\n\n\t// Send requests to workers\n\tfor _, req := range requests {\n\t\trequestChan <- req\n\t}\n\tclose(requestChan)\n\n\t// Wait for all workers to finish\n\twg.Wait()\n\tclose(resultChan)\n\n\t// Collect results\n\tvar results []PredictionResponse\n\tfor res := range resultChan {\n\t\tresults = append(results, res)\n\t}\n\n\treturn results\n}\n\nfunc main() {\n\t// Example usage\n\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n\tdefer cancel()\n\n\trequests := []PredictionRequest{\n\t\t{ID: 1, Input: []float32{1.0, 2.0, 3.0}},\n\t\t{ID: 2, Input: []float32{4.0, 5.0, 6.0}},\n\t\t{ID: 3, Input: []float32{7.0, 8.0, 9.0}},\n\t}\n\n\tnumWorkers := 3\n\tresults := BatchProcessor(ctx, numWorkers, requests)\n\n\tfor _, res := range results {\n\t\tfmt.Printf(\"Prediction ID: %d, Output: %v\\n\", res.ID, res.Output)\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport RPi.GPIO as GPIO\nimport time\nimport signal\nimport sys\nimport os\nfrom daemon import Daemon\n\n# Define GPIO pin and setup\nGPIO_PIN = 18\nGPIO.setmode(GPIO.BCM)\nGPIO.setup(GPIO_PIN, GPIO.IN, pull_up_down=GPIO.PUD_UP)\n\n# Define the daemon class\nclass GPIOMonitorDaemon(Daemon):\n    def run(self):\n        # Function to handle GPIO interrupt\n        def gpio_callback(channel):\n            timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n            print(f\"GPIO interrupt detected on pin {channel} at {timestamp}\")\n\n        # Add event detection for GPIO pin\n        GPIO.add_event_detect(GPIO_PIN, GPIO.FALLING, callback=gpio_callback, bouncetime=300)\n\n        # Keep the daemon running\n        while True:\n            time.sleep(1)\n\n    def stop(self):\n        # Clean up GPIO on exit\n        GPIO.cleanup()\n        super().stop()\n\n# Main function to handle daemon commands\nif __name__ == \"__main__\":\n    daemon = GPIOMonitorDaemon('/tmp/gpio_monitor_daemon.pid')\n\n    if len(sys.argv) == 2:\n        if 'start' == sys.argv[1]:\n            daemon.start()\n        elif 'stop' == sys.argv[1]:\n            daemon.stop()\n        elif 'restart' == sys.argv[1]:\n            daemon.restart()\n        else:\n            print(\"Unknown command\")\n            sys.exit(2)\n        sys.exit(0)\n    else:\n        print(\"Usage: %s start|stop|restart\" % sys.argv[0])\n        sys.exit(2)\n```",
    "label": 1
  },
  {
    "codes": "```gdscript\nshader_type canvas_item;\n\nuniform sampler2D screen_texture;\nuniform float wave_speed : hint_range(0.0, 10.0) = 2.0;\nuniform float wave_amplitude : hint_range(0.0, 10.0) = 0.1;\nuniform float wave_frequency : hint_range(0.0, 10.0) = 1.0;\nuniform vec2 resolution;\n\nvoid fragment() {\n    // Calculate time-based wave offset\n    float time = TIME * wave_speed;\n    // Generate wave distortion offsets\n    vec2 uv = UV;\n    float wave = sin((uv.y + time) * wave_frequency) * wave_amplitude;\n    uv.x += wave;\n\n    // Sample the screen texture with the distorted UV\n    vec3 color = texture(screen_texture, uv).rgb;\n\n    // Output the final color\n    COLOR = vec4(color, 1.0);\n}\n```",
    "label": 1
  },
  {
    "codes": "function (dataZoomModel) {\n        if (dataZoomModel !== this._dataZoomModel) {\n            return;\n        }\n\n        var targetSeries = this.getTargetSeriesModels();\n        // Culculate data window and data extent, and record them.\n        this._dataExtent = calculateDataExtent(this, this._dimName, targetSeries);\n\n        // this.hasSeriesStacked = false;\n        // each(targetSeries, function (series) {\n            // var data = series.getData();\n            // var dataDim = data.mapDimension(this._dimName);\n            // var stackedDimension = data.getCalculationInfo('stackedDimension');\n            // if (stackedDimension && stackedDimension === dataDim) {\n                // this.hasSeriesStacked = true;\n            // }\n        // }, this);\n\n        // `calculateDataWindow` uses min/maxSpan.\n        setMinMaxSpan(this);\n\n        var dataWindow = this.calculateDataWindow(dataZoomModel.option);\n\n        this._valueWindow = dataWindow.valueWindow;\n        this._percentWindow = dataWindow.percentWindow;\n\n        // Update axis setting then.\n        setAxisModel(this);\n    }",
    "label": 0
  },
  {
    "codes": "def characters(self, chars):\n        \"\"\"\n        Put character data in the currently open element. Special characters\n        (such as ``<``, ``>`` and ``&``) are escaped.\n\n        If `chars` contains any ASCII control character, :class:`ValueError` is\n        raised.\n        \"\"\"\n        self._finish_pending_start_element()\n        if not is_valid_cdata_str(chars):\n            raise ValueError(\"control characters are not allowed in \"\n                             \"well-formed XML\")\n        self._write(xml.sax.saxutils.escape(\n            chars,\n            self._additional_escapes,\n        ).encode(\"utf-8\"))",
    "label": 0
  },
  {
    "codes": "func (s *InventoryRetrievalJobInput) SetLimit(v string) *InventoryRetrievalJobInput {\n\ts.Limit = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nfrom locust import HttpUser, task, between\n\nclass ApiUser(HttpUser):\n    wait_time = between(1, 5)  # Random wait time between tasks\n\n    @task\n    def get_users(self):\n        # GET request to fetch users\n        response = self.client.get(\"/api/users\")\n        assert response.status_code == 200, \"GET /api/users failed\"\n\n    @task(3)  # Higher weight, more frequent execution\n    def create_user(self):\n        # POST request to create a new user\n        payload = {\"name\": \"John Doe\", \"email\": \"john.doe@example.com\"}\n        response = self.client.post(\"/api/users\", json=payload)\n        assert response.status_code == 201, \"POST /api/users failed\"\n\n    @task(2)\n    def update_user(self):\n        # PUT request to update a user\n        payload = {\"name\": \"Jane Doe\", \"email\": \"jane.doe@example.com\"}\n        response = self.client.put(\"/api/users/1\", json=payload)\n        assert response.status_code == 200, \"PUT /api/users/1 failed\"\n\n    @task(1)\n    def delete_user(self):\n        # DELETE request to delete a user\n        response = self.client.delete(\"/api/users/1\")\n        assert response.status_code == 204, \"DELETE /api/users/1 failed\"\n```\n\n### Comments:\n- `wait_time = between(1, 5)`: Simulates random wait times between tasks.\n- `@task`: Defines a task with an optional weight (default is 1).\n- `assert`: Validates the response status code for each request.\n- `self.client`: Used to make HTTP requests (GET, POST, PUT, DELETE).",
    "label": 1
  },
  {
    "codes": "```java\nimport static org.mockito.Mockito.*;\nimport static org.junit.Assert.*;\nimport org.junit.Before;\nimport org.junit.Test;\nimport org.mockito.InjectMocks;\nimport org.mockito.Mock;\nimport org.mockito.MockitoAnnotations;\n\nimport javax.persistence.EntityManager;\nimport javax.persistence.EntityTransaction;\nimport javax.persistence.PersistenceException;\n\npublic class TransactionalServiceTest {\n\n    @Mock\n    private EntityManager entityManager;\n\n    @Mock\n    private EntityTransaction transaction;\n\n    @InjectMocks\n    private TransactionalService transactionalService;\n\n    @Before\n    public void setUp() {\n        MockitoAnnotations.initMocks(this);\n        when(entityManager.getTransaction()).thenReturn(transaction);\n    }\n\n    @Test\n    public void verifyTransactionRollbackOnException() {\n        // Arrange - Simulate exception during transaction\n        doThrow(new PersistenceException(\"Simulated exception\")).when(entityManager).persist(any());\n\n        // Act\n        try {\n            transactionalService.performTransactionalOperation();\n            fail(\"Expected an exception to be thrown\");\n        } catch (PersistenceException e) {\n            // Assert - Verify the rollback was called\n            verify(transaction).begin();\n            verify(transaction).rollback();\n        }\n    }\n\n    // Example service to illustrate transaction handling\n    private class TransactionalService {\n\n        public void performTransactionalOperation() {\n            transaction.begin();\n            try {\n                entityManager.persist(new Object()); // Operation causing exception\n                transaction.commit();\n            } catch (Exception e) {\n                transaction.rollback();\n                throw e;\n            }\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function executeCommand(executor, command) {\n  return toWireValue(command.getParameters()).\n      then(function(parameters) {\n        command.setParameters(parameters);\n        return executor.execute(command);\n      });\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/davidbyttow/govips/v2/vips\"\n\t\"github.com/rwcarlsen/goexif/exif\"\n)\n\nfunc main() {\n\tif len(os.Args) < 2 {\n\t\tlog.Fatal(\"Usage: goexif <image.jpg>\")\n\t}\n\n\t// Initialize the vips library\n\tvips.Startup(nil)\n\tdefer vips.Shutdown()\n\n\t// Open the JPEG file\n\timagePath := os.Args[1]\n\tfile, err := os.Open(imagePath)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer file.Close()\n\n\t// Decode EXIF data\n\tx, err := exif.Decode(file)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Iterate through the EXIF tags and print them\n\tx.Walk(func(name exif.FieldName, tag exif.Exif) error {\n\t\tif name != \"\" {\n\t\t\tval, err := tag.String()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tfmt.Printf(\"%s: %s\\n\", name, val)\n\t\t}\n\t\treturn nil\n\t})\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<Page<SyncAgentLinkedDatabaseInner>>> listLinkedDatabasesSinglePageAsync(final String resourceGroupName, final String serverName, final String syncAgentName) {\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (serverName == null) {\n            throw new IllegalArgumentException(\"Parameter serverName is required and cannot be null.\");\n        }\n        if (syncAgentName == null) {\n            throw new IllegalArgumentException(\"Parameter syncAgentName is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        return service.listLinkedDatabases(resourceGroupName, serverName, syncAgentName, this.client.subscriptionId(), this.client.apiVersion(), this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<Page<SyncAgentLinkedDatabaseInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<SyncAgentLinkedDatabaseInner>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<PageImpl<SyncAgentLinkedDatabaseInner>> result = listLinkedDatabasesDelegate(response);\n                        return Observable.just(new ServiceResponse<Page<SyncAgentLinkedDatabaseInner>>(result.body(), result.response()));\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *LookupAttribute) SetAttributeKey(v string) *LookupAttribute {\n\ts.AttributeKey = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport xml.etree.ElementTree as ET\n\ndef parse_large_xml(file_path):\n    \"\"\"\n    Generator to yield elements from a large XML file one by one.\n    Uses iterparse to handle large files efficiently.\n    \"\"\"\n    context = ET.iterparse(file_path, events=(\"start\", \"end\"))\n    context = iter(context)\n    event, root = next(context)  # Get the root element\n\n    for event, elem in context:\n        if event == \"end\":\n            yield elem\n            root.clear()  # Free up memory by clearing processed elements\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"net\"\n\t\"os\"\n)\n\n// handleConnection manages the client-server interaction\nfunc handleConnection(conn net.Conn) {\n\tdefer conn.Close() // Ensure connection is closed when done\n\treader := bufio.NewReader(conn) // Reader for incoming data\n\n\tfor {\n\t\tmessage, err := reader.ReadString('\\n') // Read client messages\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Client disconnected.\", err)\n\t\t\treturn\n\t\t}\n\t\tfmt.Printf(\"Received: %s\", message) // Print received message\n\n\t\t// Echo message back to client\n\t\tconn.Write([]byte(message))\n\n\t\t// Exit condition (if needed)\n\t\tif message == \"exit\\n\" {\n\t\t\tfmt.Println(\"Connection closed by client request.\")\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc main() {\n\tlistener, err := net.Listen(\"tcp\", \"localhost:8000\") // Start listening on port 8000\n\tif err != nil {\n\t\tfmt.Println(\"Error starting TCP server.\", err)\n\t\tos.Exit(1)\n\t}\n\tdefer listener.Close() // Ensure listener is closed on program exit\n\tfmt.Println(\"Server is listening on localhost:8000\")\n\n\tfor {\n\t\tconn, err := listener.Accept() // Wait for client connections\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error accepting connection.\", err)\n\t\t\tcontinue\n\t\t}\n\t\tfmt.Println(\"Connected to client.\")\n\n\t\t// Handle each connection concurrently\n\t\tgo handleConnection(conn)\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "public void enable(String resourceGroupName, String workflowName) {\n        enableWithServiceResponseAsync(resourceGroupName, workflowName).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "public QueueRuntimeInfo getQueueRuntimeInfo(String path) throws ServiceBusException, InterruptedException {\n        return Utils.completeFuture(this.asyncClient.getQueueRuntimeInfoAsync(path));\n    }",
    "label": 0
  },
  {
    "codes": "```python\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.chrome.service import Service as ChromeService\nfrom selenium.webdriver.chrome.options import Options\nfrom webdriver_manager.chrome import ChromeDriverManager\nimport time\n\n# Setup Chrome options\nchrome_options = Options()\nchrome_options.add_argument(\"--headless\")  # Ensure GUI is off\nchrome_options.add_argument(\"--no-sandbox\")\nchrome_options.add_argument(\"--disable-dev-shm-usage\")\n\n# Initialize WebDriver\ndriver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)\n\ntry:\n    # Open the login page\n    driver.get('https://example.com/login')\n\n    # Find and fill the username field\n    username_field = driver.find_element(By.ID, 'username')\n    username_field.send_keys('testuser')\n\n    # Find and fill the password field\n    password_field = driver.find_element(By.ID, 'password')\n    password_field.send_keys('password')\n\n    # Submit the login form\n    password_field.send_keys(Keys.RETURN)\n\n    # Wait for the response\n    time.sleep(5)  # Seconds to wait\n\n    # Check if login was successful by checking if a logout button or user profile is present\n    # (Assuming that a successful login leads to a page with a 'logout' button)\n    logout_button = driver.find_elements(By.ID, 'logout')\n    assert logout_button, \"Login failed or logout button not found.\"\n\nfinally:\n    # Close the browser\n    driver.quit()\n```",
    "label": 1
  },
  {
    "codes": "def incorrection(self):\n        \"\"\"Is this element part of a correction? If it is, it returns the Correction element (evaluating to True), otherwise it returns None\"\"\"\n        e = self.parent\n\n        while e:\n            if isinstance(e, Correction):\n                return e\n            if isinstance(e, AbstractStructureElement):\n                break\n            e = e.parent\n        return None",
    "label": 0
  },
  {
    "codes": "func (s *BatchGetTriggersInput) SetTriggerNames(v []*string) *BatchGetTriggersInput {\n\ts.TriggerNames = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <stdexcept>\n\nclass DatabaseConnection {\npublic:\n    // Constructor: Establishes the database connection\n    DatabaseConnection(const std::string& connectionString) {\n        std::cout << \"Connecting to database: \" << connectionString << std::endl;\n        // Simulate connection establishment\n        if (connectionString.empty()) {\n            throw std::runtime_error(\"Connection string is empty\");\n        }\n        // Assume connection is successful\n        isConnected = true;\n    }\n\n    // Destructor: Ensures the connection is closed\n    ~DatabaseConnection() {\n        if (isConnected) {\n            std::cout << \"Disconnecting from database\" << std::endl;\n            // Simulate disconnection\n            isConnected = false;\n        }\n    }\n\n    // Execute a query (example method)\n    void executeQuery(const std::string& query) {\n        if (!isConnected) {\n            throw std::runtime_error(\"Not connected to the database\");\n        }\n        std::cout << \"Executing query: \" << query << std::endl;\n        // Simulate query execution\n    }\n\n    // Copy constructor and assignment operator are deleted to prevent copying\n    DatabaseConnection(const DatabaseConnection&) = delete;\n    DatabaseConnection& operator=(const DatabaseConnection&) = delete;\n\nprivate:\n    bool isConnected = false; // Tracks connection status\n};\n\nint main() {\n    try {\n        DatabaseConnection db(\"example_connection_string\");\n        db.executeQuery(\"SELECT * FROM users\");\n    } catch (const std::exception& e) {\n        std::cerr << \"Error: \" << e.what() << std::endl;\n    }\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```kotlin\nfun isCyclic(graph: Map<Int, List<Int>>): Boolean {\n    val visited = mutableSetOf<Int>()\n    val recursionStack = mutableSetOf<Int>()\n\n    fun dfs(node: Int): Boolean {\n        if (node in recursionStack) return true\n        if (node in visited) return false\n        \n        visited.add(node)\n        recursionStack.add(node)\n\n        for (neighbor in graph.getOrDefault(node, emptyList())) {\n            if (dfs(neighbor)) return true\n        }\n        \n        recursionStack.remove(node)\n        return false\n    }\n\n    for (node in graph.keys) {\n        if (dfs(node)) return true\n    }\n    \n    return false\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/segmentio/kafka-go\"\n)\n\n// worker processes messages from the input channel and sends results to the output channel\nfunc worker(id int, in <-chan kafka.Message, out chan<- string, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\tfor msg := range in {\n\t\t// Simulate processing\n\t\tresult := fmt.Sprintf(\"Worker %d processed message: %s\", id, string(msg.Value))\n\t\tout <- result\n\t}\n}\n\n// fanIn collects results from multiple workers into a single channel\nfunc fanIn(out chan<- string, ins ...<-chan string) {\n\tvar wg sync.WaitGroup\n\twg.Add(len(ins))\n\n\tfor _, in := range ins {\n\t\tgo func(ch <-chan string) {\n\t\t\tdefer wg.Done()\n\t\t\tfor result := range ch {\n\t\t\t\tout <- result\n\t\t\t}\n\t\t}(in)\n\t}\n\n\twg.Wait()\n\tclose(out)\n}\n\nfunc main() {\n\t// Kafka reader configuration\n\treader := kafka.NewReader(kafka.ReaderConfig{\n\t\tBrokers:   []string{\"localhost:9092\"},\n\t\tTopic:     \"test-topic\",\n\t\tPartition: 0,\n\t\tMinBytes:  10e3, // 10KB\n\t\tMaxBytes:  10e6, // 10MB\n\t})\n\tdefer reader.Close()\n\n\t// Channels for fan-out/fan-in\n\tmsgChan := make(chan kafka.Message, 10)\n\tresultChan := make(chan string, 10)\n\tworkerChans := make([]chan string, 3)\n\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < 3; i++ {\n\t\tworkerChans[i] = make(chan string, 10)\n\t\twg.Add(1)\n\t\tgo worker(i, msgChan, workerChans[i], &wg)\n\t}\n\n\t// Fan-in results from workers\n\tgo fanIn(resultChan, workerChans...)\n\n\t// Read messages from Kafka and fan-out to workers\n\tgo func() {\n\t\tfor {\n\t\t\tmsg, err := reader.ReadMessage(context.Background())\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatalf(\"Failed to read message: %v\", err)\n\t\t\t}\n\t\t\tmsgChan <- msg\n\t\t}\n\t}()\n\n\t// Print results from the fan-in channel\n\tfor result := range resultChan {\n\t\tfmt.Println(result)\n\t}\n\n\t// Wait for workers to finish\n\twg.Wait()\n}\n```",
    "label": 1
  },
  {
    "codes": "```typescript\nfunction Retry(maxRetries: number = 3, delay: number = 1000) {\n    return function (target: any, propertyKey: string, descriptor: PropertyDescriptor) {\n        const originalMethod = descriptor.value;\n\n        descriptor.value = async function (...args: any[]) {\n            let retries = 0;\n\n            while (retries < maxRetries) {\n                try {\n                    const result = await originalMethod.apply(this, args);\n                    return result; // Success, return the result\n                } catch (error) {\n                    retries++;\n                    if (retries === maxRetries) {\n                        throw error; // Max retries reached, rethrow the error\n                    }\n                    await new Promise(resolve => setTimeout(resolve, delay)); // Wait before retrying\n                }\n            }\n        };\n\n        return descriptor;\n    };\n}\n\n// Example usage:\nclass MyService {\n    @Retry(5, 2000) // Retry 5 times with a 2-second delay\n    async fetchData() {\n        // Simulate a network request that might fail\n        const response = await fetch('https://api.example.com/data');\n        if (!response.ok) {\n            throw new Error('Failed to fetch data');\n        }\n        return response.json();\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```yaml\n---\n# Ansible playbook to deploy a web app across 100+ servers\n\n- name: Deploy Web Application\n  hosts: web_servers  # Group of 100+ servers defined in inventory\n  become: yes         # Run tasks with elevated privileges\n  gather_facts: yes   # Gather facts about the servers\n\n  tasks:\n    - name: Ensure required packages are installed\n      apt:\n        name:\n          - nginx\n          - python3\n          - python3-pip\n        state: present\n      when: ansible_os_family == \"Debian\"  # For Debian-based systems\n\n    - name: Ensure required packages are installed (RedHat)\n      yum:\n        name:\n          - nginx\n          - python3\n          - python3-pip\n        state: present\n      when: ansible_os_family == \"RedHat\"  # For RedHat-based systems\n\n    - name: Clone the web application repository\n      git:\n        repo: \"https://github.com/your-repo/web-app.git\"\n        dest: /var/www/web-app\n        version: main  # Specify the branch or tag\n      notify: Restart Nginx\n\n    - name: Install Python dependencies\n      pip:\n        requirements: /var/www/web-app/requirements.txt\n        executable: pip3\n\n    - name: Configure Nginx\n      template:\n        src: templates/nginx.conf.j2\n        dest: /etc/nginx/sites-available/web-app\n      notify: Restart Nginx\n\n    - name: Enable Nginx configuration\n      file:\n        src: /etc/nginx/sites-available/web-app\n        dest: /etc/nginx/sites-enabled/web-app\n        state: link\n      notify: Restart Nginx\n\n    - name: Ensure Nginx is running and enabled on boot\n      service:\n        name: nginx\n        state: started\n        enabled: yes\n\n  handlers:\n    - name: Restart Nginx\n      service:\n        name: nginx\n        state: restarted\n```",
    "label": 1
  },
  {
    "codes": "function parse_PtgAttrSemi(blob, length, opts) {\n\tvar bitSemi = (blob[blob.l+1] & 0xFF) ? 1 : 0;\n\tblob.l += opts && opts.biff == 2 ? 3 : 4;\n\treturn [bitSemi];\n}",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <stdexcept>\n\nclass DatabaseConnection {\npublic:\n    // Constructor: Establishes the database connection\n    DatabaseConnection(const std::string& connectionString) {\n        std::cout << \"Connecting to database: \" << connectionString << std::endl;\n        // Simulate connection establishment\n        if (connectionString.empty()) {\n            throw std::runtime_error(\"Connection string is empty\");\n        }\n        // Assume connection is successful\n        std::cout << \"Connected to database successfully.\" << std::endl;\n    }\n\n    // Destructor: Closes the database connection\n    ~DatabaseConnection() {\n        std::cout << \"Closing database connection.\" << std::endl;\n        // Simulate connection closure\n    }\n\n    // Execute a query (example method)\n    void executeQuery(const std::string& query) {\n        std::cout << \"Executing query: \" << query << std::endl;\n        // Simulate query execution\n    }\n\n    // Disable copy constructor and assignment operator\n    DatabaseConnection(const DatabaseConnection&) = delete;\n    DatabaseConnection& operator=(const DatabaseConnection&) = delete;\n\n    // Enable move semantics\n    DatabaseConnection(DatabaseConnection&& other) noexcept {\n        std::cout << \"Moving database connection.\" << std::endl;\n        // Transfer ownership of resources\n    }\n\n    DatabaseConnection& operator=(DatabaseConnection&& other) noexcept {\n        std::cout << \"Move-assigning database connection.\" << std::endl;\n        if (this != &other) {\n            // Transfer ownership of resources\n        }\n        return *this;\n    }\n};\n\nint main() {\n    try {\n        DatabaseConnection db(\"example_connection_string\");\n        db.executeQuery(\"SELECT * FROM users\");\n    } catch (const std::exception& e) {\n        std::cerr << \"Error: \" << e.what() << std::endl;\n    }\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```kotlin\nsealed class DomainError {\n    object NotFound : DomainError() // Represents a not found error\n    object Unauthorized : DomainError() // Represents an unauthorized access error\n    data class ValidationError(val message: String) : DomainError() // Represents a validation error with a message\n    data class NetworkError(val code: Int, val message: String) : DomainError() // Represents a network-related error with code and message\n    object UnknownError : DomainError() // Represents an unknown error\n}\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing UnityEngine;\nusing System.Collections.Generic;\n\npublic class MazeGenerator : MonoBehaviour\n{\n    private int width = 10;\n    private int height = 10;\n    private int[,] maze;\n    private System.Random rand = new System.Random();\n    \n    private void Start()\n    {\n        InitializeMaze(); // Initialize the maze with walls\n        GenerateMaze(1, 1); // Start generating the maze from the top-left corner\n        DrawMaze(); // Visualize the maze\n    }\n\n    private void InitializeMaze()\n    {\n        maze = new int[width, height];\n        for (int x = 0; x < width; x++)\n        {\n            for (int y = 0; y < height; y++)\n            {\n                maze[x, y] = 1; // Set all cells as walls\n            }\n        }\n    }\n\n    private void GenerateMaze(int x, int y)\n    {\n        maze[x, y] = 0; // Mark the current cell as a path\n\n        List<Vector2> directions = new List<Vector2>\n        {\n            Vector2.up,\n            Vector2.down,\n            Vector2.left,\n            Vector2.right\n        };\n\n        Shuffle(directions);\n\n        foreach (Vector2 direction in directions)\n        {\n            int newX = x + (int)direction.x * 2;\n            int newY = y + (int)direction.y * 2;\n\n            if (IsInBounds(newX, newY) && maze[newX, newY] == 1)\n            {\n                maze[newX - (int)direction.x, newY - (int)direction.y] = 0;\n                GenerateMaze(newX, newY);\n            }\n        }\n    }\n\n    private void Shuffle(List<Vector2> list)\n    {\n        for (int i = 0; i < list.Count; i++)\n        {\n            int randomIndex = rand.Next(list.Count);\n            Vector2 temp = list[i];\n            list[i] = list[randomIndex];\n            list[randomIndex] = temp;\n        }\n    }\n\n    private bool IsInBounds(int x, int y)\n    {\n        return x > 0 && y > 0 && x < width && y < height;\n    }\n\n    private void DrawMaze()\n    {\n        for (int x = 0; x < width; x++)\n        {\n            for (int y = 0; y < height; y++)\n            {\n                if (maze[x, y] == 1)\n                {\n                    GameObject wall = GameObject.CreatePrimitive(PrimitiveType.Cube);\n                    wall.transform.position = new Vector3(x, 0, y);\n                }\n            }\n        }\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```python\nimport psutil\nimport time\nimport argparse\n\ndef get_network_usage(interval=1):\n    # Initialize a dictionary to store process network usage\n    process_network = {}\n    \n    # Snapshot of network counters before sleep\n    net_before = psutil.net_io_counters(pernic=True)\n    \n    # Sleep for the interval to measure the traffic accurately\n    time.sleep(interval)\n    \n    # Snapshot of network counters after sleep\n    net_after = psutil.net_io_counters(pernic=True)\n\n    # Calculate the difference in bytes sent and received during the interval\n    for net in net_before.keys():\n        bytes_sent = net_after[net].bytes_sent - net_before[net].bytes_sent\n        bytes_recv = net_after[net].bytes_recv - net_before[net].bytes_recv\n        process_network[net] = (bytes_sent, bytes_recv)\n\n    return process_network\n\ndef print_network_usage(interval):\n    # Gather network usage per process and output it\n    net_usage = get_network_usage(interval)\n    for interface, (sent, recv) in net_usage.items():\n        print(f\"Interface {interface} - Sent: {sent} Bytes, Received: {recv} Bytes\")\n\ndef cli():\n    # Setting up argument parser for command line interaction\n    parser = argparse.ArgumentParser(description='Monitor network bandwidth usage per process.')\n    parser.add_argument('-i', '--interval', type=int, default=1, help='Interval in seconds to measure the network usage')\n    \n    args = parser.parse_args()\n    \n    try:\n        # Continuously print network usage data\n        while True:\n            print_network_usage(args.interval)\n    except KeyboardInterrupt:\n        # Handle user interruption gracefully\n        print(\"\\nMonitoring stopped by user.\")\n\nif __name__ == '__main__':\n    # Execute the CLI function if the script is run directly\n    cli()\n```\n",
    "label": 1
  },
  {
    "codes": "```dart\n// flutter_ml_accelerator.dart\n\nimport 'dart:async';\n\nimport 'package:flutter/services.dart';\n\nclass FlutterMLAccelerator {\n  static const MethodChannel _channel = MethodChannel('flutter_ml_accelerator');\n\n  // Initialize the ML Accelerator\n  Future<void> initialize() async {\n    await _channel.invokeMethod('initialize');\n  }\n\n  // Load a model onto the ML Accelerator\n  Future<void> loadModel(String modelPath) async {\n    await _channel.invokeMethod('loadModel', {'modelPath': modelPath});\n  }\n\n  // Run inference using the loaded model and input data\n  Future<dynamic> runInference(Map<String, dynamic> inputData) async {\n    final inferenceResult = await _channel.invokeMethod('runInference', inputData);\n    return inferenceResult;\n  }\n\n  // Dispose resources\n  Future<void> dispose() async {\n    await _channel.invokeMethod('dispose');\n  }\n}\n\n// Example of usage\n/* \nvoid main() async {\n  final mlAccelerator = FlutterMLAccelerator();\n\n  await mlAccelerator.initialize();\n\n  await mlAccelerator.loadModel('assets/models/my_model.tflite');\n\n  final result = await mlAccelerator.runInference({'inputKey': 'inputValue'});\n\n  // Handle the inference result\n\n  await mlAccelerator.dispose();\n}\n*/\n```\n\n```swift\n// ios/Classes/SwiftFlutterMLAcceleratorPlugin.swift\n\nimport Flutter\nimport UIKit\n\npublic class SwiftFlutterMLAcceleratorPlugin: NSObject, FlutterPlugin {\n  public static func register(with registrar: FlutterPluginRegistrar) {\n    let channel = FlutterMethodChannel(name: \"flutter_ml_accelerator\", binaryMessenger: registrar.messenger())\n    let instance = SwiftFlutterMLAcceleratorPlugin()\n    registrar.addMethodCallDelegate(instance, channel: channel)\n  }\n\n  public func handle(_ call: FlutterMethodCall, result: @escaping FlutterResult) {\n    switch call.method {\n    case \"initialize\":\n      // Initialize ML functionalities\n      result(nil)\n    case \"loadModel\":\n      if let args = call.arguments as? [String: Any], let modelPath = args[\"modelPath\"] as? String {\n        // Load the ML model\n        result(nil)\n      }\n    case \"runInference\":\n      if let args = call.arguments as? [String: Any] {\n        // Perform inference using input data\n        result(\"Inference result\")\n      }\n    case \"dispose\":\n      // Dispose resources\n      result(nil)\n    default:\n      result(FlutterMethodNotImplemented)\n    }\n  }\n}\n```\n\n```java\n// android/src/main/java/com/example/flutter_ml_accelerator/FlutterMLAcceleratorPlugin.java\n\npackage com.example.flutter_ml_accelerator;\n\nimport androidx.annotation.NonNull;\nimport io.flutter.embedding.engine.plugins.FlutterPlugin;\nimport io.flutter.plugin.common.MethodCall;\nimport io.flutter.plugin.common.MethodChannel;\nimport io.flutter.plugin.common.MethodChannel.MethodCallHandler;\nimport io.flutter.plugin.common.MethodChannel.Result;\n\npublic class FlutterMLAcceleratorPlugin implements FlutterPlugin, MethodCallHandler {\n  private MethodChannel channel;\n\n  @Override\n  public void onAttachedToEngine(@NonNull FlutterPluginBinding flutterPluginBinding) {\n    channel = new MethodChannel(flutterPluginBinding.getBinaryMessenger(), \"flutter_ml_accelerator\");\n    channel.setMethodCallHandler(this);\n  }\n\n  @Override\n  public void onMethodCall(@NonNull MethodCall call, @NonNull Result result) {\n    switch (call.method) {\n      case \"initialize\":\n        // Initialize ML functionalities\n        result.success(null);\n        break;\n      case \"loadModel\":\n        String modelPath = call.argument(\"modelPath\");\n        if (modelPath != null) {\n          // Load the ML model\n          result.success(null);\n        }\n        break;\n      case \"runInference\":\n        // Perform inference using input data\n        result.success(\"Inference result\");\n        break;\n      case \"dispose\":\n        // Dispose resources\n        result.success(null);\n        break;\n      default:\n        result.notImplemented();\n    }\n  }\n\n  @Override\n  public void onDetachedFromEngine(@NonNull FlutterPluginBinding binding) {\n    channel.setMethodCallHandler(null);\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```verilog\nmodule pwm_generator (\n    input clk,            // Input clock signal\n    input reset,          // Active-high reset signal\n    input [7:0] duty,     // 8-bit duty cycle input (0-255)\n    output reg pwm_out    // PWM output signal\n);\n    \n    reg [7:0] counter;    // 8-bit counter register\n\n    always @(posedge clk or posedge reset) begin\n        if (reset) begin\n            counter <= 8'd0;\n            pwm_out <= 1'b0;\n        end else begin\n            if (counter < duty)\n                pwm_out <= 1'b1;\n            else\n                pwm_out <= 1'b0;\n\n            counter <= counter + 8'd1;\n        end\n    end\n\nendmodule\n```\n",
    "label": 1
  },
  {
    "codes": "func (c *Glacier) RemoveTagsFromVaultRequest(input *RemoveTagsFromVaultInput) (req *request.Request, output *RemoveTagsFromVaultOutput) {\n\top := &request.Operation{\n\t\tName:       opRemoveTagsFromVault,\n\t\tHTTPMethod: \"POST\",\n\t\tHTTPPath:   \"/{accountId}/vaults/{vaultName}/tags?operation=remove\",\n\t}\n\n\tif input == nil {\n\t\tinput = &RemoveTagsFromVaultInput{}\n\t}\n\n\toutput = &RemoveTagsFromVaultOutput{}\n\treq = c.newRequest(op, input, output)\n\treq.Handlers.Unmarshal.Swap(restjson.UnmarshalHandler.Name, protocol.UnmarshalDiscardBodyHandler)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "def _validate_parameters(self):\n        \"\"\"Validate Connection Parameters.\n\n        :return:\n        \"\"\"\n        if not compatibility.is_string(self.parameters['hostname']):\n            raise AMQPInvalidArgument('hostname should be a string')\n        elif not compatibility.is_integer(self.parameters['port']):\n            raise AMQPInvalidArgument('port should be an integer')\n        elif not compatibility.is_string(self.parameters['username']):\n            raise AMQPInvalidArgument('username should be a string')\n        elif not compatibility.is_string(self.parameters['password']):\n            raise AMQPInvalidArgument('password should be a string')\n        elif not compatibility.is_string(self.parameters['virtual_host']):\n            raise AMQPInvalidArgument('virtual_host should be a string')\n        elif not isinstance(self.parameters['timeout'], (int, float)):\n            raise AMQPInvalidArgument('timeout should be an integer or float')\n        elif not compatibility.is_integer(self.parameters['heartbeat']):\n            raise AMQPInvalidArgument('heartbeat should be an integer')",
    "label": 0
  },
  {
    "codes": "```python\nimport sys\n\ndef greedy_tsp(graph, start):\n    # Initialize variables\n    num_nodes = len(graph)\n    visited = [False] * num_nodes\n    path = [start]\n    visited[start] = True\n    total_cost = 0\n\n    # Start from the starting node\n    current_node = start\n\n    # Loop until all nodes are visited\n    while len(path) < num_nodes:\n        # Find the nearest unvisited node\n        nearest_node = -1\n        min_cost = sys.maxsize\n\n        for neighbor in range(num_nodes):\n            if not visited[neighbor] and graph[current_node][neighbor] < min_cost:\n                min_cost = graph[current_node][neighbor]\n                nearest_node = neighbor\n\n        # Move to the nearest node\n        path.append(nearest_node)\n        visited[nearest_node] = True\n        total_cost += min_cost\n        current_node = nearest_node\n\n    # Return to the starting node to complete the cycle\n    total_cost += graph[current_node][start]\n    path.append(start)\n\n    return path, total_cost\n\n# Example usage:\ngraph = [\n    [0, 10, 15, 20],\n    [10, 0, 35, 25],\n    [15, 35, 0, 30],\n    [20, 25, 30, 0]\n]\nstart_node = 0\npath, cost = greedy_tsp(graph, start_node)\nprint(\"Path:\", path)\nprint(\"Total Cost:\", cost)\n```",
    "label": 1
  },
  {
    "codes": "public Observable<IotHubDescriptionInner> getByResourceGroupAsync(String resourceGroupName, String resourceName) {\n        return getByResourceGroupWithServiceResponseAsync(resourceGroupName, resourceName).map(new Func1<ServiceResponse<IotHubDescriptionInner>, IotHubDescriptionInner>() {\n            @Override\n            public IotHubDescriptionInner call(ServiceResponse<IotHubDescriptionInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "def text(self, normalize_spaces=False):\n        \"\"\"Obtain the text (unicode instance)\"\"\"\n        return super(TextContent,self).text(normalize_spaces=normalize_spaces)",
    "label": 0
  },
  {
    "codes": "function ending (count, eleven, singular, other) {\n    if (count === 11) {\n      return eleven;\n    }\n\n    if (count % 10 === 1) {\n      return singular;\n    }\n\n    return other;\n  }",
    "label": 0
  },
  {
    "codes": "def title(self, value=None):\n        \"\"\"Get or set the document's title from/in the metadata\n\n           No arguments: Get the document's title from metadata\n           Argument: Set the document's title in metadata\n        \"\"\"\n        if not (value is None):\n            if (self.metadatatype == \"native\"):\n                self.metadata['title'] = value\n            else:\n                self._title = value\n        if (self.metadatatype == \"native\"):\n            if 'title' in self.metadata:\n                return self.metadata['title']\n            else:\n                return None\n        else:\n            return self._title",
    "label": 0
  },
  {
    "codes": "```java\nimport org.junit.jupiter.api.Test;\nimport static org.junit.jupiter.api.Assertions.*;\n\nimport java.util.Arrays;\n\npublic class SortingAlgorithmEdgeCasesTest {\n\n    @Test\n    public void testEmptyArray() {\n        int[] arr = {};\n        int[] expected = {};\n        Arrays.sort(arr); // Assuming Arrays.sort is the sorting algorithm being tested\n        assertArrayEquals(expected, arr, \"Empty array should remain unchanged\");\n    }\n\n    @Test\n    public void testSingleElementArray() {\n        int[] arr = {42};\n        int[] expected = {42};\n        Arrays.sort(arr);\n        assertArrayEquals(expected, arr, \"Single element array should remain unchanged\");\n    }\n\n    @Test\n    public void testAlreadySortedArray() {\n        int[] arr = {1, 2, 3, 4, 5};\n        int[] expected = {1, 2, 3, 4, 5};\n        Arrays.sort(arr);\n        assertArrayEquals(expected, arr, \"Already sorted array should remain unchanged\");\n    }\n\n    @Test\n    public void testReverseSortedArray() {\n        int[] arr = {5, 4, 3, 2, 1};\n        int[] expected = {1, 2, 3, 4, 5};\n        Arrays.sort(arr);\n        assertArrayEquals(expected, arr, \"Reverse sorted array should be sorted in ascending order\");\n    }\n\n    @Test\n    public void testArrayWithDuplicates() {\n        int[] arr = {3, 1, 2, 3, 1};\n        int[] expected = {1, 1, 2, 3, 3};\n        Arrays.sort(arr);\n        assertArrayEquals(expected, arr, \"Array with duplicates should be sorted correctly\");\n    }\n\n    @Test\n    public void testArrayWithNegativeNumbers() {\n        int[] arr = {-3, -1, -2, 0, 2};\n        int[] expected = {-3, -2, -1, 0, 2};\n        Arrays.sort(arr);\n        assertArrayEquals(expected, arr, \"Array with negative numbers should be sorted correctly\");\n    }\n\n    @Test\n    public void testArrayWithAllIdenticalElements() {\n        int[] arr = {7, 7, 7, 7, 7};\n        int[] expected = {7, 7, 7, 7, 7};\n        Arrays.sort(arr);\n        assertArrayEquals(expected, arr, \"Array with all identical elements should remain unchanged\");\n    }\n\n    @Test\n    public void testLargeArray() {\n        int[] arr = new int[10000];\n        for (int i = 0; i < arr.length; i++) {\n            arr[i] = arr.length - i;\n        }\n        int[] expected = Arrays.copyOf(arr, arr.length);\n        Arrays.sort(expected);\n        Arrays.sort(arr);\n        assertArrayEquals(expected, arr, \"Large array should be sorted correctly\");\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom transformers import pipeline\n\n# Create a text summarization pipeline using a different model\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n# Example text to summarize\ntext = \"\"\"\nThe Hugging Face Transformers library provides an easy-to-use interface for various NLP tasks, \nincluding text summarization. It supports multiple pre-trained models that can be used out-of-the-box \nfor generating summaries of long documents or articles. This pipeline simplifies the process of \nsummarizing text by handling all the necessary preprocessing and postprocessing steps.\n\"\"\"\n\n# Generate the summary\nsummary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n\n# Print the summary\nprint(summary[0]['summary_text'])\n```",
    "label": 1
  },
  {
    "codes": "```javascript\n// Import required modules\nconst jwt = require('jsonwebtoken');\nconst express = require('express');\nconst app = express();\n\n// Middleware to authenticate JWT tokens\nconst authenticateJWT = (req, res, next) => {\n    // Extract the token from the Authorization header\n    const authHeader = req.headers.authorization;\n\n    // Check if the token exists\n    if (authHeader) {\n        // Split the token from the \"Bearer\" prefix\n        const token = authHeader.split(' ')[1];\n\n        // Verify the token using the secret key\n        jwt.verify(token, process.env.JWT_SECRET, (err, user) => {\n            if (err) {\n                // If token is invalid, return 403 Forbidden\n                return res.sendStatus(403);\n            }\n\n            // Attach the user object to the request\n            req.user = user;\n            next();\n        });\n    } else {\n        // If no token is provided, return 401 Unauthorized\n        res.sendStatus(401);\n    }\n};\n\n// Example protected route using the middleware\napp.get('/protected', authenticateJWT, (req, res) => {\n    // Access the authenticated user from the request object\n    res.json({ message: 'Access granted', user: req.user });\n});\n\n// Start the server\napp.listen(3000, () => {\n    console.log('Server is running on port 3000');\n});\n```",
    "label": 1
  },
  {
    "codes": "```ini\n; PlatformIO configuration file for cross-compiling to multiple MCUs\n\n[platformio]\n; Default environment, can be any of the environments listed below\ndefault_envs = uno, nodemcu\n\n; Environment for Arduino Uno\n[env:uno]\nplatform = atmelavr\nboard = uno\nframework = arduino\n; Additional libraries and build flags can be specified here\nlib_deps = \nbuild_flags = \n\n; Environment for ESP8266 NodeMCU\n[env:nodemcu]\nplatform = espressif8266\nboard = nodemcu\nframework = arduino\n; Additional libraries and build flags can be specified here\nlib_deps =\nbuild_flags =\n\n; Additional environments can be added below in a similar manner\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing UnityEngine;\nusing System.Collections.Generic;\n\npublic class MazeGenerator : MonoBehaviour\n{\n    public int width = 10;\n    public int height = 10;\n    public GameObject wallPrefab;\n    public GameObject floorPrefab;\n\n    private int[,] maze;\n\n    void Start()\n    {\n        CreateMaze();\n        DrawMaze();\n    }\n\n    void CreateMaze()\n    {\n        maze = new int[width, height];\n        for (int x = 0; x < width; x++)\n        {\n            for (int y = 0; y < height; y++)\n            {\n                maze[x, y] = 1; // Initialize all walls\n            }\n        }\n\n        // Start creating the maze from a random position\n        GenerateMaze(1, 1);\n    }\n\n    void GenerateMaze(int x, int y)\n    {\n        // Directions for moving in the maze\n        List<Vector2Int> directions = new List<Vector2Int>\n        {\n            new Vector2Int(2, 0), // Right\n            new Vector2Int(-2, 0), // Left\n            new Vector2Int(0, 2), // Up\n            new Vector2Int(0, -2) // Down\n        };\n\n        // Shuffle directions to create randomness\n        for (int i = 0; i < directions.Count; i++)\n        {\n            Vector2Int temp = directions[i];\n            int randomIndex = Random.Range(i, directions.Count);\n            directions[i] = directions[randomIndex];\n            directions[randomIndex] = temp;\n        }\n\n        foreach (var direction in directions)\n        {\n            int newX = x + direction.x;\n            int newY = y + direction.y;\n\n            // Check if the new position is within bounds and walls\n            if (IsInBounds(newX, newY) && maze[newX, newY] == 1)\n            {\n                // Remove the wall between current position and new position\n                maze[x + direction.x / 2, y + direction.y / 2] = 0;\n                maze[newX, newY] = 0;\n\n                // Recursive call to generate the maze from new position\n                GenerateMaze(newX, newY);\n            }\n        }\n    }\n\n    bool IsInBounds(int x, int y)\n    {\n        return x > 0 && x < width - 1 && y > 0 && y < height - 1;\n    }\n\n    void DrawMaze()\n    {\n        for (int x = 0; x < width; x++)\n        {\n            for (int y = 0; y < height; y++)\n            {\n                if (maze[x, y] == 1)\n                {\n                    Instantiate(wallPrefab, new Vector3(x, 0, y), Quaternion.identity);\n                }\n                else\n                {\n                    Instantiate(floorPrefab, new Vector3(x, 0, y), Quaternion.identity);\n                }\n            }\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```ini\n; PlatformIO configuration for cross-compiling to multiple MCUs\n\n[env]\nplatform = ststm32\nframework = stm32cube\n\n; Common settings for all environments\nbuild_flags = \n    -Wall\n    -Wextra\n    -Os\n\n; Environment for STM32F103C8 (Blue Pill)\n[env:bluepill]\nboard = bluepill_f103c8\nupload_protocol = stlink\nbuild_flags = \n    ${env.build_flags}\n    -DSTM32F103x8\n\n; Environment for STM32F407VET6 (Black Pill)\n[env:blackpill]\nboard = blackpill_f407ve\nupload_protocol = stlink\nbuild_flags = \n    ${env.build_flags}\n    -DSTM32F407xx\n\n; Environment for STM32L432KC (Nucleo-32)\n[env:nucleo_l432kc]\nboard = nucleo_l432kc\nupload_protocol = stlink\nbuild_flags = \n    ${env.build_flags}\n    -DSTM32L432xx\n\n; Environment for STM32F746ZG (Nucleo-144)\n[env:nucleo_f746zg]\nboard = nucleo_f746zg\nupload_protocol = stlink\nbuild_flags = \n    ${env.build_flags}\n    -DSTM32F746xx\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport org.junit.jupiter.api.Test;\nimport static org.junit.jupiter.api.Assertions.*;\n\nimport java.util.Arrays;\n\npublic class SortingAlgorithmTest {\n\n    @Test\n    public void testEmptyArray() {\n        int[] arr = {};\n        int[] expected = {};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(expected, arr, \"Empty array should remain unchanged.\");\n    }\n\n    @Test\n    public void testSingleElementArray() {\n        int[] arr = {5};\n        int[] expected = {5};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(expected, arr, \"Single element array should remain unchanged.\");\n    }\n\n    @Test\n    public void testAlreadySortedArray() {\n        int[] arr = {1, 2, 3, 4, 5};\n        int[] expected = {1, 2, 3, 4, 5};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(expected, arr, \"Already sorted array should remain unchanged.\");\n    }\n\n    @Test\n    public void testReverseSortedArray() {\n        int[] arr = {5, 4, 3, 2, 1};\n        int[] expected = {1, 2, 3, 4, 5};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(expected, arr, \"Reverse sorted array should be sorted in ascending order.\");\n    }\n\n    @Test\n    public void testArrayWithDuplicates() {\n        int[] arr = {3, 1, 2, 3, 1};\n        int[] expected = {1, 1, 2, 3, 3};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(expected, arr, \"Array with duplicates should be sorted correctly.\");\n    }\n\n    @Test\n    public void testArrayWithNegativeNumbers() {\n        int[] arr = {-3, -1, -2, 0, 2};\n        int[] expected = {-3, -2, -1, 0, 2};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(expected, arr, \"Array with negative numbers should be sorted correctly.\");\n    }\n\n    @Test\n    public void testArrayWithAllSameElements() {\n        int[] arr = {7, 7, 7, 7};\n        int[] expected = {7, 7, 7, 7};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(expected, arr, \"Array with all same elements should remain unchanged.\");\n    }\n\n    @Test\n    public void testLargeArray() {\n        int[] arr = new int[1000];\n        Arrays.fill(arr, 1);\n        int[] expected = new int[1000];\n        Arrays.fill(expected, 1);\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(expected, arr, \"Large array with same elements should remain unchanged.\");\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\npackage com.example.inventory.controller;\n\nimport com.example.inventory.model.Item;\nimport com.example.inventory.service.InventoryService;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.http.HttpStatus;\nimport org.springframework.http.ResponseEntity;\nimport org.springframework.web.bind.annotation.*;\n\nimport java.util.List;\n\n@RestController\n@RequestMapping(\"/api/inventory\")\npublic class InventoryController {\n\n    @Autowired\n    private InventoryService inventoryService;\n\n    // Get all items\n    @GetMapping(\"/items\")\n    public ResponseEntity<List<Item>> getAllItems() {\n        List<Item> items = inventoryService.getAllItems();\n        return new ResponseEntity<>(items, HttpStatus.OK);\n    }\n\n    // Get a single item by id\n    @GetMapping(\"/items/{id}\")\n    public ResponseEntity<Item> getItemById(@PathVariable Long id) {\n        Item item = inventoryService.getItemById(id);\n        return new ResponseEntity<>(item, HttpStatus.OK);\n    }\n\n    // Add a new item\n    @PostMapping(\"/items\")\n    public ResponseEntity<Item> addItem(@RequestBody Item newItem) {\n        Item item = inventoryService.addItem(newItem);\n        return new ResponseEntity<>(item, HttpStatus.CREATED);\n    }\n\n    // Update an existing item\n    @PutMapping(\"/items/{id}\")\n    public ResponseEntity<Item> updateItem(@PathVariable Long id, @RequestBody Item updatedItem) {\n        Item item = inventoryService.updateItem(id, updatedItem);\n        return new ResponseEntity<>(item, HttpStatus.OK);\n    }\n\n    // Delete an item by id\n    @DeleteMapping(\"/items/{id}\")\n    public ResponseEntity<Void> deleteItem(@PathVariable Long id) {\n        inventoryService.deleteItem(id);\n        return new ResponseEntity<>(HttpStatus.NO_CONTENT);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<VaultInner> createOrUpdateAsync(String resourceGroupName, String vaultName, VaultCreateOrUpdateParameters parameters) {\n        return createOrUpdateWithServiceResponseAsync(resourceGroupName, vaultName, parameters).map(new Func1<ServiceResponse<VaultInner>, VaultInner>() {\n            @Override\n            public VaultInner call(ServiceResponse<VaultInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<Page<DenyAssignmentInner>> listForScopeAsync(final String scope) {\n        return listForScopeWithServiceResponseAsync(scope)\n            .map(new Func1<ServiceResponse<Page<DenyAssignmentInner>>, Page<DenyAssignmentInner>>() {\n                @Override\n                public Page<DenyAssignmentInner> call(ServiceResponse<Page<DenyAssignmentInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```python\nfrom flask import Flask, request, jsonify\nfrom werkzeug.utils import secure_filename\nfrom PIL import Image\nimport os\nimport uuid\nfrom concurrent.futures import ThreadPoolExecutor\n\napp = Flask(__name__)\n\nUPLOAD_FOLDER = 'uploads'\nRESIZED_FOLDER = 'resized'\nVALID_EXTENSIONS = {'png', 'jpg', 'jpeg'}\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['RESIZED_FOLDER'] = RESIZED_FOLDER\n\nexecutor = ThreadPoolExecutor(max_workers=4)  # Allows for asynchronous task execution\n\ndef allowed_file(filename):  # Utility function to check file extension\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in VALID_EXTENSIONS\n\ndef resize_image(filepath, output_size=(100, 100)):\n    try:\n        with Image.open(filepath) as img:\n            img.thumbnail(output_size)\n            resized_filename = f\"{uuid.uuid4().hex}_{os.path.basename(filepath)}\"\n            resized_path = os.path.join(app.config['RESIZED_FOLDER'], resized_filename)\n            img.save(resized_path)\n        return resized_filename, None\n    except Exception as e:\n        return None, str(e)\n\n@app.route('/upload', methods=['POST'])\ndef upload_file():\n    if 'file' not in request.files:\n        return jsonify(message=\"No file part\"), 400\n\n    file = request.files['file']\n\n    if file.filename == '':\n        return jsonify(message=\"No selected file\"), 400\n\n    if file and allowed_file(file.filename):\n        filename = secure_filename(file.filename)\n        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n        file.save(filepath)\n\n        # Submit the image resizing task to the executor\n        future = executor.submit(resize_image, filepath)\n\n        return jsonify(message=\"File uploaded and resizing started\"), 202\n\n    return jsonify(message=\"Invalid file type\"), 400\n\n# Error handling for unexpected exceptions\n@app.errorhandler(500)\ndef internal_server_error(e):\n    return jsonify(message=\"Internal server error\"), 500\n\n# Additional comments on handling and app setup go here\n\nif __name__ == '__main__':\n    os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n    os.makedirs(RESIZED_FOLDER, exist_ok=True)\n    app.run(debug=True)\n\n# This code does not include full lifecycle management of uploaded files and \n# resized images, which should be considered for production.\n```\n\n- **202 Accepted**: Returns this status code when the file is successfully received and the resizing process has started asynchronously.\n- **400 Bad Request**: Returns this status code when no file part is present, no file is selected, or the file type is invalid.\n- **500 Internal Server Error**: Handles any unexpected errors that might occur during processing.",
    "label": 1
  },
  {
    "codes": "func (s *GetDatabaseInput) SetCatalogId(v string) *GetDatabaseInput {\n\ts.CatalogId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\n\t\"github.com/BurntSushi/toml\"\n\t\"gopkg.in/yaml.v3\"\n)\n\n// Define command-line flags\nvar (\n\tinputFile  = flag.String(\"i\", \"\", \"Input file\")\n\toutputFile = flag.String(\"o\", \"\", \"Output file\")\n\tinputFormat  = flag.String(\"if\", \"\", \"Input format (yaml, toml, json)\")\n\toutputFormat = flag.String(\"of\", \"\", \"Output format (yaml, toml, json)\")\n)\n\nfunc main() {\n\tflag.Parse()\n\n\t// Validate input and output formats\n\tif *inputFormat == \"\" || *outputFormat == \"\" {\n\t\tfmt.Println(\"Input and output formats must be specified\")\n\t\tos.Exit(1)\n\t}\n\n\t// Read input file\n\tdata, err := ioutil.ReadFile(*inputFile)\n\tif err != nil {\n\t\tfmt.Printf(\"Error reading input file: %v\\n\", err)\n\t\tos.Exit(1)\n\t}\n\n\t// Parse input data based on input format\n\tvar parsedData interface{}\n\tswitch *inputFormat {\n\tcase \"yaml\":\n\t\tif err := yaml.Unmarshal(data, &parsedData); err != nil {\n\t\t\tfmt.Printf(\"Error parsing YAML: %v\\n\", err)\n\t\t\tos.Exit(1)\n\t\t}\n\tcase \"toml\":\n\t\tif err := toml.Unmarshal(data, &parsedData); err != nil {\n\t\t\tfmt.Printf(\"Error parsing TOML: %v\\n\", err)\n\t\t\tos.Exit(1)\n\t\t}\n\tcase \"json\":\n\t\tif err := json.Unmarshal(data, &parsedData); err != nil {\n\t\t\tfmt.Printf(\"Error parsing JSON: %v\\n\", err)\n\t\t\tos.Exit(1)\n\t\t}\n\tdefault:\n\t\tfmt.Println(\"Unsupported input format\")\n\t\tos.Exit(1)\n\t}\n\n\t// Convert parsed data to output format\n\tvar outputData []byte\n\tswitch *outputFormat {\n\tcase \"yaml\":\n\t\toutputData, err = yaml.Marshal(parsedData)\n\tcase \"toml\":\n\t\toutputData, err = toml.Marshal(parsedData)\n\tcase \"json\":\n\t\toutputData, err = json.MarshalIndent(parsedData, \"\", \"  \")\n\tdefault:\n\t\tfmt.Println(\"Unsupported output format\")\n\t\tos.Exit(1)\n\t}\n\n\tif err != nil {\n\t\tfmt.Printf(\"Error converting data: %v\\n\", err)\n\t\tos.Exit(1)\n\t}\n\n\t// Write output data to file or stdout\n\tif *outputFile != \"\" {\n\t\tif err := ioutil.WriteFile(*outputFile, outputData, 0644); err != nil {\n\t\t\tfmt.Printf(\"Error writing output file: %v\\n\", err)\n\t\t\tos.Exit(1)\n\t\t}\n\t} else {\n\t\tfmt.Println(string(outputData))\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\n// Aspect to implement Circuit Breaker pattern\n@Aspect\npublic class CircuitBreakerAspect {\n\n    private static final int MAX_FAILURES = 3; // Maximum allowed failures\n    private static final long TIMEOUT = 5000; // Timeout in milliseconds\n    private Map<String, Integer> failureCounts = new ConcurrentHashMap<>();\n    private Map<String, Long> lastFailureTimes = new ConcurrentHashMap<>();\n\n    @Around(\"@annotation(CircuitBreaker)\") // Pointcut for methods annotated with @CircuitBreaker\n    public Object circuitBreakerAdvice(ProceedingJoinPoint joinPoint) throws Throwable {\n        String methodName = joinPoint.getSignature().toShortString();\n\n        // Check if the circuit is open (too many failures within timeout)\n        if (isCircuitOpen(methodName)) {\n            throw new CircuitBreakerOpenException(\"Circuit is open for method: \" + methodName);\n        }\n\n        try {\n            // Proceed with the method execution\n            Object result = joinPoint.proceed();\n            // Reset failure count on success\n            failureCounts.remove(methodName);\n            return result;\n        } catch (Exception e) {\n            // Increment failure count and record failure time\n            failureCounts.put(methodName, failureCounts.getOrDefault(methodName, 0) + 1);\n            lastFailureTimes.put(methodName, System.currentTimeMillis());\n            throw e; // Re-throw the exception\n        }\n    }\n\n    // Check if the circuit is open\n    private boolean isCircuitOpen(String methodName) {\n        Integer failures = failureCounts.get(methodName);\n        if (failures != null && failures >= MAX_FAILURES) {\n            Long lastFailureTime = lastFailureTimes.get(methodName);\n            if (lastFailureTime != null && (System.currentTimeMillis() - lastFailureTime) < TIMEOUT) {\n                return true; // Circuit is open\n            } else {\n                // Reset failure count after timeout\n                failureCounts.remove(methodName);\n                lastFailureTimes.remove(methodName);\n            }\n        }\n        return false; // Circuit is closed\n    }\n}\n\n// Custom exception for Circuit Breaker\nclass CircuitBreakerOpenException extends RuntimeException {\n    public CircuitBreakerOpenException(String message) {\n        super(message);\n    }\n}\n\n// Annotation to mark methods for Circuit Breaker\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.METHOD)\npublic @interface CircuitBreaker {\n}\n```",
    "label": 1
  },
  {
    "codes": "```ruby\n# Gemfile\ngem 'yaml'\ngem 'erb'\n\n# lib/ssh_config_generator.rb\nrequire 'yaml'\nrequire 'erb'\n\nmodule SshConfigGenerator\n  class Generator\n    def initialize(template_path, output_path)\n      @template_path = template_path\n      @output_path = output_path\n    end\n\n    def generate\n      # Load YAML template\n      config = YAML.load_file(@template_path)\n\n      # Render ERB template\n      template = File.read(File.expand_path('template.erb', __dir__))\n      rendered = ERB.new(template).result(binding)\n\n      # Write to output file\n      File.write(@output_path, rendered)\n    end\n  end\nend\n\n# template.erb\n<% config.each do |host, settings| %>\nHost <%= host %>\n  <% settings.each do |key, value| %>\n  <%= key %> <%= value %>\n  <% end %>\n<% end %>\n\n# Example usage:\n# generator = SshConfigGenerator::Generator.new('config.yml', 'ssh_config')\n# generator.generate\n```\n\n```yaml\n# config.yml\nhost1:\n  HostName: example.com\n  User: user1\n  Port: 22\n\nhost2:\n  HostName: another.com\n  User: user2\n  Port: 2222\n```\n\n```ssh\n# Generated ssh_config\nHost host1\n  HostName example.com\n  User user1\n  Port 22\n\nHost host2\n  HostName another.com\n  User user2\n  Port 2222\n```\n\n```ruby\n# ssh_config_generator.gemspec\nGem::Specification.new do |s|\n  s.name        = 'ssh_config_generator'\n  s.version     = '0.1.0'\n  s.summary     = 'Generate SSH config files from YAML templates'\n  s.description = 'A Ruby gem to generate SSH config files using YAML templates and ERB.'\n  s.authors     = ['Your Name']\n  s.email       = 'your.email@example.com'\n  s.files       = ['lib/ssh_config_generator.rb']\n  s.homepage    = 'https://github.com/yourusername/ssh_config_generator'\n  s.license     = 'MIT'\nend\n```",
    "label": 1
  },
  {
    "codes": "def _on_io_events(self, fd=None, _events=None):\n        \"\"\"Invoked by Tornado's IOLoop when there are events for the fd\n\n        :param int fd: The file descriptor for the event\n        :param int _events: The events raised\n\n        \"\"\"\n        if fd not in self._connections:\n            LOGGER.warning('Received IO event for non-existing connection')\n            return\n        self._poll_connection(fd)",
    "label": 0
  },
  {
    "codes": "```java\n// Method to batch-insert 10,000 records using Hibernate\npublic void batchInsertRecords(List<MyEntity> entities) {\n    Session session = HibernateUtil.getSessionFactory().openSession();\n    Transaction transaction = null;\n\n    try {\n        transaction = session.beginTransaction();\n\n        // Set batch size for efficient batch processing\n        session.setJdbcBatchSize(50);\n\n        for (int i = 0; i < entities.size(); i++) {\n            session.save(entities.get(i));\n\n            // Flush and clear session periodically to manage memory\n            if (i % 50 == 0) {\n                session.flush();\n                session.clear();\n            }\n        }\n\n        transaction.commit();\n    } catch (Exception e) {\n        if (transaction != null) {\n            transaction.rollback();\n        }\n        e.printStackTrace();\n    } finally {\n        session.close();\n    }\n}\n```\n\n```java\n// HibernateUtil class for session management\npublic class HibernateUtil {\n    private static final SessionFactory sessionFactory = buildSessionFactory();\n\n    private static SessionFactory buildSessionFactory() {\n        try {\n            return new Configuration().configure().buildSessionFactory();\n        } catch (Throwable ex) {\n            System.err.println(\"Initial SessionFactory creation failed.\" + ex);\n            throw new ExceptionInInitializerError(ex);\n        }\n    }\n\n    public static SessionFactory getSessionFactory() {\n        return sessionFactory;\n    }\n}\n```\n\n```java\n// Example entity class\n@Entity\n@Table(name = \"my_entity\")\npublic class MyEntity {\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private Long id;\n\n    private String name;\n\n    // Getters and setters\n}\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<ResourceGroupInner> getAsync(String resourceGroupName, final ServiceCallback<ResourceGroupInner> serviceCallback) {\n        return ServiceFuture.fromResponse(getWithServiceResponseAsync(resourceGroupName), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```c\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <stdio.h>\n#include <string.h>\n\n// Wrapper structure to hold AES-GCM context\ntypedef struct {\n    EVP_CIPHER_CTX *ctx;\n    unsigned char key[32]; // 256-bit key\n    unsigned char iv[12];  // 96-bit IV\n} AES_GCM_CTX;\n\n// Initialize AES-GCM context with key and IV\nint aes_gcm_init(AES_GCM_CTX *gcm_ctx, const unsigned char *key, const unsigned char *iv) {\n    if (!key || !iv) return 0;\n\n    gcm_ctx->ctx = EVP_CIPHER_CTX_new();\n    if (!gcm_ctx->ctx) return 0;\n\n    memcpy(gcm_ctx->key, key, 32);\n    memcpy(gcm_ctx->iv, iv, 12);\n\n    return 1;\n}\n\n// Encrypt plaintext using AES-GCM\nint aes_gcm_encrypt(AES_GCM_CTX *gcm_ctx, const unsigned char *plaintext, size_t plaintext_len,\n                    unsigned char *ciphertext, unsigned char *tag) {\n    int len;\n    int ciphertext_len;\n\n    if (!EVP_EncryptInit_ex(gcm_ctx->ctx, EVP_aes_256_gcm(), NULL, gcm_ctx->key, gcm_ctx->iv))\n        return 0;\n\n    if (!EVP_EncryptUpdate(gcm_ctx->ctx, ciphertext, &len, plaintext, plaintext_len))\n        return 0;\n    ciphertext_len = len;\n\n    if (!EVP_EncryptFinal_ex(gcm_ctx->ctx, ciphertext + len, &len))\n        return 0;\n    ciphertext_len += len;\n\n    if (!EVP_CIPHER_CTX_ctrl(gcm_ctx->ctx, EVP_CTRL_GCM_GET_TAG, 16, tag))\n        return 0;\n\n    return ciphertext_len;\n}\n\n// Decrypt ciphertext using AES-GCM\nint aes_gcm_decrypt(AES_GCM_CTX *gcm_ctx, const unsigned char *ciphertext, size_t ciphertext_len,\n                    const unsigned char *tag, unsigned char *plaintext) {\n    int len;\n    int plaintext_len;\n\n    if (!EVP_DecryptInit_ex(gcm_ctx->ctx, EVP_aes_256_gcm(), NULL, gcm_ctx->key, gcm_ctx->iv))\n        return 0;\n\n    if (!EVP_DecryptUpdate(gcm_ctx->ctx, plaintext, &len, ciphertext, ciphertext_len))\n        return 0;\n    plaintext_len = len;\n\n    if (!EVP_CIPHER_CTX_ctrl(gcm_ctx->ctx, EVP_CTRL_GCM_SET_TAG, 16, (void *)tag))\n        return 0;\n\n    if (EVP_DecryptFinal_ex(gcm_ctx->ctx, plaintext + len, &len) <= 0)\n        return 0;\n    plaintext_len += len;\n\n    return plaintext_len;\n}\n\n// Clean up AES-GCM context\nvoid aes_gcm_cleanup(AES_GCM_CTX *gcm_ctx) {\n    if (gcm_ctx->ctx) {\n        EVP_CIPHER_CTX_free(gcm_ctx->ctx);\n        gcm_ctx->ctx = NULL;\n    }\n}\n\n// Example usage\nint main() {\n    AES_GCM_CTX gcm_ctx;\n    unsigned char key[32], iv[12];\n    unsigned char plaintext[] = \"Hello, AES-GCM!\";\n    unsigned char ciphertext[128];\n    unsigned char decryptedtext[128];\n    unsigned char tag[16];\n\n    // Generate random key and IV\n    RAND_bytes(key, sizeof(key));\n    RAND_bytes(iv, sizeof(iv));\n\n    // Initialize AES-GCM context\n    if (!aes_gcm_init(&gcm_ctx, key, iv)) {\n        fprintf(stderr, \"Failed to initialize AES-GCM context\\n\");\n        return 1;\n    }\n\n    // Encrypt plaintext\n    int ciphertext_len = aes_gcm_encrypt(&gcm_ctx, plaintext, strlen((char *)plaintext), ciphertext, tag);\n    if (ciphertext_len <= 0) {\n        fprintf(stderr, \"Encryption failed\\n\");\n        return 1;\n    }\n\n    // Decrypt ciphertext\n    int decryptedtext_len = aes_gcm_decrypt(&gcm_ctx, ciphertext, ciphertext_len, tag, decryptedtext);\n    if (decryptedtext_len <= 0) {\n        fprintf(stderr, \"Decryption failed\\n\");\n        return 1;\n    }\n\n    // Null-terminate decrypted text\n    decryptedtext[decryptedtext_len] = '\\0';\n\n    printf(\"Decrypted text: %s\\n\", decryptedtext);\n\n    // Clean up\n    aes_gcm_cleanup(&gcm_ctx);\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<List<DenyAssignmentInner>> listForScopeAsync(final String scope, final ListOperationCallback<DenyAssignmentInner> serviceCallback) {\n        return AzureServiceFuture.fromPageResponse(\n            listForScopeSinglePageAsync(scope),\n            new Func1<String, Observable<ServiceResponse<Page<DenyAssignmentInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<DenyAssignmentInner>>> call(String nextPageLink) {\n                    return listForScopeNextSinglePageAsync(nextPageLink);\n                }\n            },\n            serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<List<DenyAssignmentInner>> listForResourceAsync(final String resourceGroupName, final String resourceProviderNamespace, final String parentResourcePath, final String resourceType, final String resourceName, final ListOperationCallback<DenyAssignmentInner> serviceCallback) {\n        return AzureServiceFuture.fromPageResponse(\n            listForResourceSinglePageAsync(resourceGroupName, resourceProviderNamespace, parentResourcePath, resourceType, resourceName),\n            new Func1<String, Observable<ServiceResponse<Page<DenyAssignmentInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<DenyAssignmentInner>>> call(String nextPageLink) {\n                    return listForResourceNextSinglePageAsync(nextPageLink);\n                }\n            },\n            serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"log\"\n    \"net/http\"\n)\n\n// RecoveryMiddleware recovers from panics in HTTP handlers\nfunc RecoveryMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        defer func() {\n            if err := recover(); err != nil {\n                // Log the panic\n                log.Printf(\"Recovered from panic: %v\", err)\n                \n                // Respond with a 500 Internal Server Error\n                w.WriteHeader(http.StatusInternalServerError)\n                w.Write([]byte(\"Internal Server Error\"))\n            }\n        }()\n        \n        // Call the next handler\n        next.ServeHTTP(w, r)\n    })\n}\n\nfunc main() {\n    // Example usage\n    http.Handle(\"/\", RecoveryMiddleware(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        panic(\"something went wrong\")\n    })))\n\n    log.Fatal(http.ListenAndServe(\":8080\", nil))\n}\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glue) StartTrigger(input *StartTriggerInput) (*StartTriggerOutput, error) {\n\treq, out := c.StartTriggerRequest(input)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "def buffer(self):\n        \"\"\"\n        Context manager to temporarily buffer the output.\n\n        :raise RuntimeError: If two :meth:`buffer` context managers are used\n                             nestedly.\n\n        If the context manager is left without exception, the buffered output\n        is sent to the actual sink. Otherwise, it is discarded.\n\n        In addition to the output being buffered, buffer also captures the\n        entire state of the XML generator and restores it to the previous state\n        if the context manager is left with an exception.\n\n        This can be used to fail-safely attempt to serialise a subtree and\n        return to a well-defined state if serialisation fails.\n\n        :meth:`flush` is not called automatically.\n\n        If :meth:`flush` is called while a :meth:`buffer` context manager is\n        active, no actual flushing happens (but unfinished opening tags are\n        closed as usual, see the `short_empty_arguments` parameter).\n        \"\"\"\n        if self._buf_in_use:\n            raise RuntimeError(\"nested use of buffer() is not supported\")\n        self._buf_in_use = True\n        old_write = self._write\n        old_flush = self._flush\n\n        if self._buf is None:\n            self._buf = io.BytesIO()\n        else:\n            try:\n                self._buf.seek(0)\n                self._buf.truncate()\n            except BufferError:\n                # we need a fresh buffer for this, the other is still in use.\n                self._buf = io.BytesIO()\n\n        self._write = self._buf.write\n        self._flush = None\n        try:\n            with self._save_state():\n                yield\n            old_write(self._buf.getbuffer())\n            if old_flush:\n                old_flush()\n        finally:\n            self._buf_in_use = False\n            self._write = old_write\n            self._flush = old_flush",
    "label": 0
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Collections.Concurrent;\nusing System.Threading;\nusing System.Threading.Tasks;\n\nclass Program\n{\n    static void Main(string[] args)\n    {\n        // Create a cancellation token source to signal cancellation\n        var cts = new CancellationTokenSource();\n\n        // Create a collection of items to process\n        var items = new ConcurrentBag<int> { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\n\n        try\n        {\n            // Start a task to cancel the operation after a delay\n            Task.Run(() =>\n            {\n                Thread.Sleep(2000); // Simulate a delay before cancellation\n                cts.Cancel();\n                Console.WriteLine(\"Cancellation requested.\");\n            });\n\n            // Use Parallel.ForEach with cancellation token support\n            Parallel.ForEach(items, new ParallelOptions { CancellationToken = cts.Token }, item =>\n            {\n                // Simulate some work\n                Console.WriteLine($\"Processing item: {item}\");\n                Thread.Sleep(500); // Simulate work delay\n\n                // Check for cancellation request\n                cts.Token.ThrowIfCancellationRequested();\n            });\n        }\n        catch (OperationCanceledException)\n        {\n            Console.WriteLine(\"Operation was canceled.\");\n        }\n        finally\n        {\n            cts.Dispose(); // Clean up the cancellation token source\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Sample text data\ntexts = [\"This is a sample sentence.\", \"Another example for preprocessing.\", \"TensorFlow is great for NLP tasks.\"]\n\n# Initialize Tokenizer\ntokenizer = Tokenizer(num_words=10000)  # Limit vocabulary to top 10,000 words\ntokenizer.fit_on_texts(texts)\n\n# Convert texts to sequences of integers\nsequences = tokenizer.texts_to_sequences(texts)\n\n# Pad sequences to ensure uniform length\nmax_length = 10  # Define maximum sequence length\npadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n\n# Output the preprocessed data\nprint(\"Vocabulary:\", tokenizer.word_index)\nprint(\"Padded Sequences:\\n\", padded_sequences)\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n)\n\n// Matrix represents a 2D matrix\ntype Matrix [][]int\n\n// Multiply two matrices\nfunc multiply(a, b Matrix) Matrix {\n\tn := len(a)\n\tresult := make(Matrix, n)\n\tfor i := range result {\n\t\tresult[i] = make([]int, n)\n\t\tfor j := range result[i] {\n\t\t\tfor k := range a {\n\t\t\t\tresult[i][j] += a[i][k] * b[k][j]\n\t\t\t}\n\t\t}\n\t}\n\treturn result\n}\n\n// Identity matrix of size n\nfunc identity(n int) Matrix {\n\tI := make(Matrix, n)\n\tfor i := range I {\n\t\tI[i] = make([]int, n)\n\t\tI[i][i] = 1\n\t}\n\treturn I\n}\n\n// Matrix exponentiation using fast exponentiation method\nfunc matrixExponentiation(mat Matrix, power int) Matrix {\n\tn := len(mat)\n\tresult := identity(n)\n\n\tfor power > 0 {\n\t\tif power%2 == 1 {\n\t\t\tresult = multiply(result, mat)\n\t\t}\n\t\tmat = multiply(mat, mat)\n\t\tpower /= 2\n\t}\n\treturn result\n}\n\nfunc main() {\n\tmat := Matrix{\n\t\t{1, 1},\n\t\t{1, 0},\n\t}\n\tpower := 5\n\tresult := matrixExponentiation(mat, power)\n\tfmt.Println(result)\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<Page<DeletedSecretItem>> getDeletedSecretsAsync(final String vaultBaseUrl, final Integer maxresults) {\n        return getDeletedSecretsWithServiceResponseAsync(vaultBaseUrl, maxresults)\n            .map(new Func1<ServiceResponse<Page<DeletedSecretItem>>, Page<DeletedSecretItem>>() {\n                @Override\n                public Page<DeletedSecretItem> call(ServiceResponse<Page<DeletedSecretItem>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "function(needle) {\n      if (Array.isArray(needle)) {\n        for (var i = needle.length; i--;) {\n          if (wysihtml5.lang.array(arr).indexOf(needle[i]) !== -1) {\n            return true;\n          }\n        }\n        return false;\n      } else {\n        return wysihtml5.lang.array(arr).indexOf(needle) !== -1;\n      }\n    }",
    "label": 0
  },
  {
    "codes": "```python\nfrom airflow import DAG\nfrom airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator\nfrom airflow.providers.amazon.aws.hooks.redshift import RedshiftHook\nfrom airflow.hooks.base_hook import BaseHook\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom datetime import datetime\n\n# Define the default arguments for the DAG\ndefault_args = {\n    'owner': 'airflow',\n    'start_date': datetime(2023, 1, 1),\n    'retries': 1,\n}\n\n# Define the DAG\ndag = DAG(\n    's3_to_redshift_pipeline',\n    default_args=default_args,\n    description='A simple ETL pipeline from S3 to Redshift',\n    schedule_interval='@daily',\n    catchup=False,\n)\n\n# Define the start task\nstart_task = DummyOperator(\n    task_id='start',\n    dag=dag,\n)\n\n# Load data from S3 to Redshift\nload_to_redshift = S3ToRedshiftOperator(\n    task_id='load_s3_to_redshift',\n    schema='public',\n    table='your_table_name',  # Replace with your Redshift table name\n    s3_bucket='your_s3_bucket',  # Replace with your S3 bucket name\n    s3_key='your_s3_key',  # Path to the data in S3\n    copy_options=['CSV', 'IGNOREHEADER 1'],  # Adjust options as needed\n    redshift_conn_id='your_redshift_connection',  # Connection ID for Redshift\n    aws_conn_id='your_aws_connection',  # Connection ID for AWS\n    dag=dag,\n)\n\n# Define the end task\nend_task = DummyOperator(\n    task_id='end',\n    dag=dag,\n)\n\n# Set task dependencies\nstart_task >> load_to_redshift >> end_task\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"math/rand\"\n\t\"time\"\n)\n\nconst (\n\tlossRate = 0.2 // 20% packet loss\n)\n\nfunc main() {\n\tlistenAddr := \":8080\"\n\tln, err := net.Listen(\"tcp\", listenAddr)\n\tif err != nil {\n\t\tfmt.Println(\"Error setting up listener:\", err)\n\t\treturn\n\t}\n\tdefer ln.Close()\n\n\tfmt.Println(\"Listening for TCP connections on\", listenAddr)\n\n\tfor {\n\t\tconn, err := ln.Accept()\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error accepting connection:\", err)\n\t\t\tcontinue\n\t\t}\n\t\tgo handleConnection(conn)\n\t}\n}\n\nfunc handleConnection(conn net.Conn) {\n\tdefer conn.Close()\n\n\tbuffer := make([]byte, 1024)\n\tfor {\n\t\tn, err := conn.Read(buffer)\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error reading from connection:\", err)\n\t\t\treturn\n\t\t}\n\n\t\tif rand.Float64() < lossRate {\n\t\t\t// Simulate packet loss\n\t\t\tfmt.Println(\"Simulated packet loss, dropping packet\")\n\t\t\tcontinue\n\t\t}\n\n\t\t// Echo back the received data\n\t\t_, err = conn.Write(buffer[:n])\n\t\tif err != nil {\n\t\t\tfmt.Println(\"Error writing to connection:\", err)\n\t\t\treturn\n\t\t}\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\ndef greedy_tsp(distance_matrix):\n    n = len(distance_matrix)  # Number of cities\n    visited = [False] * n  # Track visited cities\n    tour = []  # Store the order of visited cities\n\n    # Start from the first city\n    current_city = 0\n    tour.append(current_city)\n    visited[current_city] = True\n\n    for _ in range(1, n):\n        nearest_city = None\n        min_distance = float('inf')\n\n        for city in range(n):\n            if not visited[city] and distance_matrix[current_city][city] < min_distance:\n                min_distance = distance_matrix[current_city][city]\n                nearest_city = city\n\n        tour.append(nearest_city)\n        visited[nearest_city] = True\n        current_city = nearest_city\n\n    # Return to the starting city to complete the tour\n    tour.append(tour[0])\n\n    return tour\n```\n",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<List<PatternRuleInfo>>> getIntentPatternsWithServiceResponseAsync(UUID appId, String versionId, UUID intentId, GetIntentPatternsOptionalParameter getIntentPatternsOptionalParameter) {\n        if (this.client.endpoint() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.endpoint() is required and cannot be null.\");\n        }\n        if (appId == null) {\n            throw new IllegalArgumentException(\"Parameter appId is required and cannot be null.\");\n        }\n        if (versionId == null) {\n            throw new IllegalArgumentException(\"Parameter versionId is required and cannot be null.\");\n        }\n        if (intentId == null) {\n            throw new IllegalArgumentException(\"Parameter intentId is required and cannot be null.\");\n        }\n        final Integer skip = getIntentPatternsOptionalParameter != null ? getIntentPatternsOptionalParameter.skip() : null;\n        final Integer take = getIntentPatternsOptionalParameter != null ? getIntentPatternsOptionalParameter.take() : null;\n\n        return getIntentPatternsWithServiceResponseAsync(appId, versionId, intentId, skip, take);\n    }",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n)\n\n// Function to multiply two matrices\nfunc multiplyMatrix(A, B [][]int64, mod int64) [][]int64 {\n\tn := len(A)\n\tC := make([][]int64, n)\n\tfor i := range C {\n\t\tC[i] = make([]int64, n)\n\t}\n\tfor i := 0; i < n; i++ {\n\t\tfor j := 0; j < n; j++ {\n\t\t\tC[i][j] = 0\n\t\t\tfor k := 0; k < n; k++ {\n\t\t\t\tC[i][j] = (C[i][j] + A[i][k]*B[k][j]) % mod\n\t\t\t}\n\t\t}\n\t}\n\treturn C\n}\n\n// Function to perform matrix exponentiation\nfunc matrixExponentiation(matrix [][]int64, exp, mod int64) [][]int64 {\n\tn := len(matrix)\n\t// Initialize result as identity matrix\n\tresult := make([][]int64, n)\n\tfor i := range result {\n\t\tresult[i] = make([]int64, n)\n\t\tresult[i][i] = 1\n\t}\n\n\t// Copy of input matrix\n\tbase := make([][]int64, n)\n\tfor i := range base {\n\t\tbase[i] = make([]int64, n)\n\t\tcopy(base[i], matrix[i])\n\t}\n\n\t// Exponentiation by squaring\n\tfor exp > 0 {\n\t\tif exp%2 == 1 {\n\t\t\tresult = multiplyMatrix(result, base, mod)\n\t\t}\n\t\tbase = multiplyMatrix(base, base, mod)\n\t\texp /= 2\n\t}\n\n\treturn result\n}\n\nfunc main() {\n\tmatrix := [][]int64{\n\t\t{1, 1},\n\t\t{1, 0},\n\t}\n\texp := int64(5)\n\tmod := int64(1000000007)\n\n\tresult := matrixExponentiation(matrix, exp, mod)\n\n\tfmt.Println(\"Matrix Exponentiation Result:\")\n\tfor _, row := range result {\n\t\tfmt.Println(row)\n\t}\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```rust\nuse std::collections::HashMap;\nuse std::sync::{Arc, Mutex};\nuse tokio::sync::Semaphore;\nuse warp::Filter;\n\n#[tokio::main]\nasync fn main() {\n    // Shared state between request handlers\n    let state = Arc::new(Mutex::new(State::new()));\n\n    // Request handler\n    let route = warp::any()\n        .and(warp::addr::remote())\n        .and(with_state(state.clone()))\n        .and_then(handle_request);\n\n    // Start the Warp server on port 3030\n    warp::serve(route).run(([127, 0, 0, 1], 3030)).await;\n}\n\ntype IpAddr = std::net::IpAddr;\n\n// Shared application state\n#[derive(Clone)]\nstruct State {\n    limiters: HashMap<IpAddr, Arc<Semaphore>>,\n}\n\nimpl State {\n    // Create a new state with empty limiters\n    fn new() -> Self {\n        Self {\n            limiters: HashMap::new(),\n        }\n    }\n\n    // Get a rate limiter for the given IP, creating one if it doesn't exist\n    fn get_limiter(&mut self, ip: IpAddr) -> Arc<Semaphore> {\n        if !self.limiters.contains_key(&ip) {\n            self.limiters.insert(ip, Arc::new(Semaphore::new(5))); // Limit to 5 concurrent requests\n        }\n        self.limiters.get(&ip).unwrap().clone()\n    }\n}\n\n// Extract state from request context\nfn with_state(\n    state: Arc<Mutex<State>>,\n) -> impl Filter<Extract = (Arc<Mutex<State>>,), Error = std::convert::Infallible> + Clone {\n    warp::any().map(move || state.clone())\n}\n\n// Handle incoming requests\nasync fn handle_request(\n    remote: Option<SocketAddr>,\n    state: Arc<Mutex<State>>,\n) -> Result<impl warp::Reply, warp::Rejection> {\n    if let Some(addr) = remote {\n        let ip = addr.ip();\n        let state = &mut state.lock().unwrap();\n        let limiter = state.get_limiter(ip);\n\n        // Attempt to acquire a permit for the IP address\n        if let Ok(_permit) = limiter.try_acquire() {\n            // Successfully acquired a permit\n            Ok(warp::reply::with_status(\n                \"Request Accepted\",\n                warp::http::StatusCode::OK,\n            ))\n        } else {\n            // Rate limit exceeded\n            Ok(warp::reply::with_status(\n                \"Too Many Requests\",\n                warp::http::StatusCode::TOO_MANY_REQUESTS,\n            ))\n        }\n    } else {\n        // Could not extract IP from request\n        Ok(warp::reply::with_status(\n            \"Unable to determine IP address\",\n            warp::http::StatusCode::BAD_REQUEST,\n        ))\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nfrom flask import Flask, request, jsonify\nimport torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport io\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Load pre-trained model (e.g., ResNet18)\nmodel = models.resnet18(pretrained=True)\nmodel.eval()  # Set model to evaluation mode\n\n# Define image preprocessing\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load ImageNet class labels\nwith open('imagenet_classes.txt') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Define API endpoint for image classification\n@app.route('/predict', methods=['POST'])\ndef predict():\n    # Check if image is provided in the request\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    # Read the image file\n    file = request.files['file']\n    image = Image.open(io.BytesIO(file.read()))\n\n    # Preprocess the image\n    input_tensor = preprocess(image)\n    input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n\n    # Perform inference\n    with torch.no_grad():\n        output = model(input_batch)\n\n    # Get predicted class\n    _, predicted_idx = torch.max(output, 1)\n    predicted_label = labels[predicted_idx.item()]\n\n    # Return the result\n    return jsonify({'predicted_class': predicted_label})\n\n# Run the Flask app\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n```\n\n### Comments:\n1. **Flask Setup**: The Flask app is initialized to handle HTTP requests.\n2. **Model Loading**: A pre-trained ResNet18 model is loaded and set to evaluation mode.\n3. **Image Preprocessing**: Images are resized, cropped, and normalized to match the model's input requirements.\n4. **Class Labels**: ImageNet class labels are loaded from a file.\n5. **Prediction Endpoint**: The `/predict` endpoint accepts an image file, preprocesses it, and returns the predicted class.\n6. **Error Handling**: If no file is provided, an error message is returned.\n7. **Inference**: The model performs inference on the preprocessed image.\n8. **Result Formatting**: The predicted class label is returned in JSON format.\n9. **App Execution**: The app runs on `0.0.0.0` and listens on port `5000`.",
    "label": 1
  },
  {
    "codes": "public TaskInner update(String resourceGroupName, String registryName, String taskName, TaskUpdateParameters taskUpdateParameters) {\n        return updateWithServiceResponseAsync(resourceGroupName, registryName, taskName, taskUpdateParameters).toBlocking().last().body();\n    }",
    "label": 0
  },
  {
    "codes": "func (c *Glacier) GetJobOutputRequest(input *GetJobOutputInput) (req *request.Request, output *GetJobOutputOutput) {\n\top := &request.Operation{\n\t\tName:       opGetJobOutput,\n\t\tHTTPMethod: \"GET\",\n\t\tHTTPPath:   \"/{accountId}/vaults/{vaultName}/jobs/{jobId}/output\",\n\t}\n\n\tif input == nil {\n\t\tinput = &GetJobOutputInput{}\n\t}\n\n\toutput = &GetJobOutputOutput{}\n\treq = c.newRequest(op, input, output)\n\treturn\n}",
    "label": 0
  },
  {
    "codes": "```csharp\nusing UnityEngine;\nusing System.Collections.Generic;\n\npublic class MazeGenerator : MonoBehaviour\n{\n    public int width = 10; // Maze width\n    public int height = 10; // Maze height\n    public GameObject wallPrefab; // Prefab for the walls\n    public GameObject floorPrefab; // Prefab for the floor\n\n    private int[,] maze; // 2D array to represent the maze\n    private List<Vector2Int> visitedCells = new List<Vector2Int>(); // List to track visited cells\n\n    void Start()\n    {\n        GenerateMaze();\n        InstantiateMaze();\n    }\n\n    void GenerateMaze()\n    {\n        maze = new int[width, height]; // Initialize maze grid\n        for (int x = 0; x < width; x++)\n        {\n            for (int y = 0; y < height; y++)\n            {\n                maze[x, y] = 1; // Fill the grid with walls (1 = wall, 0 = path)\n            }\n        }\n\n        // Start generating from a random cell\n        Vector2Int startCell = new Vector2Int(Random.Range(0, width), Random.Range(0, height));\n        visitedCells.Add(startCell);\n        maze[startCell.x, startCell.y] = 0; // Mark the starting cell as a path\n\n        while (visitedCells.Count > 0)\n        {\n            Vector2Int currentCell = visitedCells[visitedCells.Count - 1];\n            List<Vector2Int> neighbors = GetUnvisitedNeighbors(currentCell);\n\n            if (neighbors.Count > 0)\n            {\n                Vector2Int nextCell = neighbors[Random.Range(0, neighbors.Count)];\n                RemoveWallBetween(currentCell, nextCell);\n                visitedCells.Add(nextCell);\n                maze[nextCell.x, nextCell.y] = 0; // Mark the next cell as a path\n            }\n            else\n            {\n                visitedCells.RemoveAt(visitedCells.Count - 1); // Backtrack if no unvisited neighbors\n            }\n        }\n    }\n\n    List<Vector2Int> GetUnvisitedNeighbors(Vector2Int cell)\n    {\n        List<Vector2Int> neighbors = new List<Vector2Int>();\n        Vector2Int[] directions = { Vector2Int.up, Vector2Int.down, Vector2Int.left, Vector2Int.right };\n\n        foreach (var dir in directions)\n        {\n            Vector2Int neighbor = cell + dir * 2; // Check cells two steps away (skipping walls)\n            if (neighbor.x >= 0 && neighbor.x < width && neighbor.y >= 0 && neighbor.y < height && maze[neighbor.x, neighbor.y] == 1)\n            {\n                neighbors.Add(neighbor);\n            }\n        }\n\n        return neighbors;\n    }\n\n    void RemoveWallBetween(Vector2Int cellA, Vector2Int cellB)\n    {\n        Vector2Int wallPosition = (cellA + cellB) / 2; // Calculate the wall position between two cells\n        maze[wallPosition.x, wallPosition.y] = 0; // Remove the wall\n    }\n\n    void InstantiateMaze()\n    {\n        for (int x = 0; x < width; x++)\n        {\n            for (int y = 0; y < height; y++)\n            {\n                Vector3 position = new Vector3(x, 0, y);\n                if (maze[x, y] == 1)\n                {\n                    Instantiate(wallPrefab, position, Quaternion.identity); // Instantiate walls\n                }\n                else\n                {\n                    Instantiate(floorPrefab, position, Quaternion.identity); // Instantiate floors\n                }\n            }\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```julia\n# Import necessary packages\nusing VoronoiDelaunay\nusing Plots\n\n# Function to generate a Voronoi diagram from random points\nfunction generate_voronoi(n::Int; xrange=(0, 100), yrange=(0, 100))\n    # Generate n random points within specified range\n    points = [(rand(xrange...), rand(yrange...)) for _ in 1:n]\n\n    # Create a Voronoi tessellation\n    vor = voronoi(points, verbose=false)\n\n    # Plot the Voronoi diagram\n    plot(vor, linewidth=1, legend=false)  # Plot the Voronoi edges\n    scatter!(vor.vertices, color=:blue, shape=:circle)  # Plot the Voronoi vertices\n    scatter!(points, color=:red, shape=:utriangle)  # Plot the original points\n\n    return vor, points\nend\n\n# Example usage\ngenerate_voronoi(10)  # Generate a Voronoi diagram from 10 random points\n```\n",
    "label": 1
  },
  {
    "codes": "func (c *Glue) CreateUserDefinedFunctionWithContext(ctx aws.Context, input *CreateUserDefinedFunctionInput, opts ...request.Option) (*CreateUserDefinedFunctionOutput, error) {\n\treq, out := c.CreateUserDefinedFunctionRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "public ReplicationLinkInner get(String resourceGroupName, String serverName, String databaseName, String linkId) {\n        return getWithServiceResponseAsync(resourceGroupName, serverName, databaseName, linkId).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "```rust\n// Import necessary crates\nuse embedded_hal::blocking::i2c::{Write, WriteRead};\nuse stm32f4xx_hal::i2c::I2c;\nuse stm32f4xx_hal::prelude::*;\n\n// Define IMU I2C address\nconst IMU_I2C_ADDR: u8 = 0x68;\n\n// Define IMU registers\nconst IMU_WHO_AM_I: u8 = 0x75;\nconst IMU_PWR_MGMT_1: u8 = 0x6B;\nconst IMU_ACCEL_XOUT_H: u8 = 0x3B;\n\n// IMU Driver struct\npub struct Imu<I2C> {\n    i2c: I2C,\n}\n\nimpl<I2C, E> Imu<I2C>\nwhere\n    I2C: Write<Error = E> + WriteRead<Error = E>,\n{\n    // Initialize IMU\n    pub fn new(i2c: I2C) -> Result<Self, E> {\n        let mut imu = Imu { i2c };\n        imu.init()?;\n        Ok(imu)\n    }\n\n    // Initialize IMU settings\n    fn init(&mut self) -> Result<(), E> {\n        // Wake up IMU by writing 0 to PWR_MGMT_1 register\n        self.i2c.write(IMU_I2C_ADDR, &[IMU_PWR_MGMT_1, 0x00])?;\n        Ok(())\n    }\n\n    // Read WHO_AM_I register to verify IMU connection\n    pub fn who_am_i(&mut self) -> Result<u8, E> {\n        let mut data = [0u8; 1];\n        self.i2c.write_read(IMU_I2C_ADDR, &[IMU_WHO_AM_I], &mut data)?;\n        Ok(data[0])\n    }\n\n    // Read accelerometer data\n    pub fn read_accel(&mut self) -> Result<(i16, i16, i16), E> {\n        let mut buffer = [0u8; 6];\n        self.i2c.write_read(IMU_I2C_ADDR, &[IMU_ACCEL_XOUT_H], &mut buffer)?;\n\n        // Combine high and low bytes for each axis\n        let accel_x = ((buffer[0] as i16) << 8) | buffer[1] as i16;\n        let accel_y = ((buffer[2] as i16) << 8) | buffer[3] as i16;\n        let accel_z = ((buffer[4] as i16) << 8) | buffer[5] as i16;\n\n        Ok((accel_x, accel_y, accel_z))\n    }\n}\n\n// Example usage\nfn main() -> ! {\n    let dp = stm32f4xx_hal::pac::Peripherals::take().unwrap();\n    let rcc = dp.RCC.constrain();\n    let clocks = rcc.cfgr.freeze();\n    let gpiob = dp.GPIOB.split();\n\n    // Initialize I2C\n    let scl = gpiob.pb8.into_alternate_af4().set_open_drain();\n    let sda = gpiob.pb9.into_alternate_af4().set_open_drain();\n    let i2c = I2c::i2c1(dp.I2C1, (scl, sda), 400.khz(), clocks);\n\n    // Initialize IMU\n    let mut imu = Imu::new(i2c).unwrap();\n\n    // Verify IMU connection\n    let who_am_i = imu.who_am_i().unwrap();\n    assert_eq!(who_am_i, 0x71); // Expected WHO_AM_I value for the IMU\n\n    // Read accelerometer data\n    let (accel_x, accel_y, accel_z) = imu.read_accel().unwrap();\n    // Use accelerometer data as needed\n\n    loop {}\n}\n```",
    "label": 1
  },
  {
    "codes": "def send(device_id, description, **kwargs):\n    \"\"\"\n    Site: http://parse.com\n    API: https://www.parse.com/docs/push_guide#scheduled/REST\n    Desc: Best app for system administrators\n    \"\"\"\n    headers = {\n        \"X-Parse-Application-Id\": settings.PARSE_APP_ID,\n        \"X-Parse-REST-API-Key\": settings.PARSE_API_KEY,\n        \"User-Agent\": \"DBMail/%s\" % get_version(),\n        \"Content-type\": \"application/json\",\n    }\n\n    data = {\n        \"where\": {\n            \"user_id\": device_id,\n        },\n        \"data\": {\n            \"alert\": description,\n            \"title\": kwargs.pop(\"event\")\n        }\n    }\n\n    _data = kwargs.pop('data', None)\n    if _data is not None:\n        data.update(_data)\n\n    http = HTTPSConnection(kwargs.pop(\"api_url\", \"api.parse.com\"))\n    http.request(\n        \"POST\", \"/1/push\",\n        headers=headers,\n        body=dumps(data))\n    response = http.getresponse()\n\n    if response.status != 200:\n        raise ParseComError(response.reason)\n\n    body = loads(response.read())\n    if body['error']:\n        raise ParseComError(body['error'])\n    return True",
    "label": 0
  },
  {
    "codes": "```javascript\n// MongoDB Aggregation Pipeline to Calculate Monthly Sales Trends\n\ndb.sales.aggregate([\n    // Stage 1: Match documents within a specific date range (optional)\n    {\n        $match: {\n            saleDate: { $gte: ISODate(\"2023-01-01\"), $lte: ISODate(\"2023-12-31\") }\n        }\n    },\n    // Stage 2: Group by year and month, summing up the sales amount\n    {\n        $group: {\n            _id: {\n                year: { $year: \"$saleDate\" },\n                month: { $month: \"$saleDate\" }\n            },\n            totalSales: { $sum: \"$amount\" }\n        }\n    },\n    // Stage 3: Sort by year and month for chronological order\n    {\n        $sort: {\n            \"_id.year\": 1,\n            \"_id.month\": 1\n        }\n    },\n    // Stage 4: Project the results into a readable format\n    {\n        $project: {\n            _id: 0,\n            year: \"$_id.year\",\n            month: \"$_id.month\",\n            totalSales: 1\n        }\n    }\n]);\n```",
    "label": 1
  },
  {
    "codes": "function createExecutor(url) {\n  let agent = new http.Agent({ keepAlive: true });\n  let client = url.then(url => new http.HttpClient(url, agent));\n  let executor = new http.Executor(client);\n  configureExecutor(executor);\n  return executor;\n}",
    "label": 0
  },
  {
    "codes": "```cpp\n// DynamicWeatherSystem.h\n\n#pragma once\n\n#include \"CoreMinimal.h\"\n#include \"GameFramework/Actor.h\"\n#include \"DynamicWeatherSystem.generated.h\"\n\nUENUM(BlueprintType)\nenum class EWeatherType : uint8 {\n    WT_Clear UMETA(DisplayName = \"Clear\"),\n    WT_Cloudy UMETA(DisplayName = \"Cloudy\"),\n    WT_Rain UMETA(DisplayName = \"Rain\"),\n    WT_Snow UMETA(DisplayName = \"Snow\")\n};\n\nUCLASS()\nclass YOURPROJECTNAME_API ADynamicWeatherSystem : public AActor {\n    GENERATED_BODY()\n\npublic:\n    ADynamicWeatherSystem();\n\nprotected:\n    virtual void BeginPlay() override;\n\n    // Weather transition duration in seconds\n    UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"Weather\")\n    float WeatherTransitionDuration;\n\n    UFUNCTION(BlueprintCallable, Category = \"Weather\")\n    void ChangeWeather(EWeatherType NewWeather);\n\nprivate:\n    EWeatherType CurrentWeather;\n\n    void InterpolateWeather(EWeatherType FromWeather, EWeatherType ToWeather, float Alpha);\n    void UpdateWeatherEffects();\n};\n\n// DynamicWeatherSystem.cpp\n\n#include \"DynamicWeatherSystem.h\"\n\nADynamicWeatherSystem::ADynamicWeatherSystem() {\n    PrimaryActorTick.bCanEverTick = true;\n    WeatherTransitionDuration = 5.0f; // Default transition duration\n    CurrentWeather = EWeatherType::WT_Clear; // Default weather\n}\n\nvoid ADynamicWeatherSystem::BeginPlay() {\n    Super::BeginPlay();\n}\n\nvoid ADynamicWeatherSystem::ChangeWeather(EWeatherType NewWeather) {\n    if (CurrentWeather != NewWeather) {\n        // Start the weather transition\n        GetWorld()->GetTimerManager().SetTimerForNextTick([this, NewWeather]() {\n            InterpolateWeather(CurrentWeather, NewWeather, 0);\n        });\n\n        CurrentWeather = NewWeather;\n    }\n}\n\nvoid ADynamicWeatherSystem::InterpolateWeather(EWeatherType FromWeather, EWeatherType ToWeather, float Alpha) {\n    // Perform interpolation logic here based on Alpha (0 to 1)\n    // Update the visual and audio aspects of the weather accordingly\n    UpdateWeatherEffects();\n\n    // Use a timer to decrease Alpha over the transition duration\n    Alpha += GetWorld()->GetDeltaSeconds() / WeatherTransitionDuration;\n    if (Alpha < 1.0f) {\n        GetWorld()->GetTimerManager().SetTimerForNextTick([this, FromWeather, ToWeather, Alpha]() {\n            InterpolateWeather(FromWeather, ToWeather, Alpha);\n        });\n    }\n}\n\nvoid ADynamicWeatherSystem::UpdateWeatherEffects() {\n    // Update the sky, particle systems, audio, etc. based on CurrentWeather\n    // (E.g., swap materials, enable/disable particle systems)\n}\n```",
    "label": 1
  },
  {
    "codes": "def get(self, virtual_host):\n        \"\"\"Get Virtual Host details.\n\n        :param str virtual_host: Virtual host name\n\n        :raises ApiError: Raises if the remote server encountered an error.\n        :raises ApiConnectionError: Raises if there was a connectivity issue.\n\n        :rtype: dict\n        \"\"\"\n        virtual_host = quote(virtual_host, '')\n        return self.http_client.get(API_VIRTUAL_HOST % virtual_host)",
    "label": 0
  },
  {
    "codes": "func (s *CrawlerMetrics) SetTimeLeftSeconds(v float64) *CrawlerMetrics {\n\ts.TimeLeftSeconds = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"log\"\n    \"net/http\"\n)\n\n// RecoveryMiddleware recovers from panics in HTTP handlers\nfunc RecoveryMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        defer func() {\n            if err := recover(); err != nil {\n                // Log the panic\n                log.Printf(\"Recovered from panic: %v\", err)\n                \n                // Respond with a 500 Internal Server Error\n                w.WriteHeader(http.StatusInternalServerError)\n                w.Write([]byte(\"Internal Server Error\"))\n            }\n        }()\n        \n        // Call the next handler\n        next.ServeHTTP(w, r)\n    })\n}\n\nfunc main() {\n    // Example usage\n    http.Handle(\"/\", RecoveryMiddleware(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        panic(\"something went wrong\")\n    })))\n\n    log.Fatal(http.ListenAndServe(\":8080\", nil))\n}\n```",
    "label": 1
  },
  {
    "codes": "function error(status, msg) {\n  var err = new Error(msg);\n  err.status = status;\n  return err;\n}",
    "label": 0
  },
  {
    "codes": "function excludeNodeDevDependencies(servicePath) {\n  const exAndIn = {\n    include: [],\n    exclude: [],\n  };\n\n  // the files where we'll write the dependencies into\n  const tmpDir = os.tmpdir();\n  const randHash = crypto.randomBytes(8).toString('hex');\n  const nodeDevDepFile = path.join(tmpDir, `node-dependencies-${randHash}-dev`);\n  const nodeProdDepFile = path.join(tmpDir, `node-dependencies-${randHash}-prod`);\n\n  try {\n    const packageJsonFilePaths = globby.sync([\n      '**/package.json',\n      // TODO add glob for node_modules filtering\n    ], {\n      cwd: servicePath,\n      dot: true,\n      silent: true,\n      follow: true,\n      nosort: true,\n    });\n\n    // filter out non node_modules file paths\n    const packageJsonPaths = _.filter(packageJsonFilePaths, (filePath) => {\n      const isNodeModulesDir = !!filePath.match(/node_modules/);\n      return !isNodeModulesDir;\n    });\n\n    if (_.isEmpty(packageJsonPaths)) {\n      return BbPromise.resolve(exAndIn);\n    }\n\n    // NOTE: using mapSeries here for a sequential computation (w/o race conditions)\n    return BbPromise.mapSeries(packageJsonPaths, (packageJsonPath) => {\n      // the path where the package.json file lives\n      const fullPath = path.join(servicePath, packageJsonPath);\n      const dirWithPackageJson = fullPath.replace(path.join(path.sep, 'package.json'), '');\n\n      // we added a catch which resolves so that npm commands with an exit code of 1\n      // (e.g. if the package.json is invalid) won't crash the dev dependency exclusion process\n      return BbPromise.map(['dev', 'prod'], (env) => {\n        const depFile = env === 'dev' ? nodeDevDepFile : nodeProdDepFile;\n        return childProcess.execAsync(\n          `npm ls --${env}=true --parseable=true --long=false --silent >> ${depFile}`,\n          { cwd: dirWithPackageJson }\n        ).catch(() => BbPromise.resolve());\n      });\n    })\n    // NOTE: using mapSeries here for a sequential computation (w/o race conditions)\n    .then(() => BbPromise.mapSeries(['dev', 'prod'], (env) => {\n      const depFile = env === 'dev' ? nodeDevDepFile : nodeProdDepFile;\n      return fs.readFileAsync(depFile)\n        .then((fileContent) => _.compact(\n          (_.uniq(_.split(fileContent.toString('utf8'), '\\n'))),\n          elem => elem.length > 0\n        )).catch(() => BbPromise.resolve());\n    }))\n    .then((devAndProDependencies) => {\n      const devDependencies = devAndProDependencies[0];\n      const prodDependencies = devAndProDependencies[1];\n\n      // NOTE: the order for _.difference is important\n      const dependencies = _.difference(devDependencies, prodDependencies);\n      const nodeModulesRegex = new RegExp(`${path.join('node_modules', path.sep)}.*`, 'g');\n\n      if (!_.isEmpty(dependencies)) {\n        return BbPromise\n          .map(dependencies, (item) => item.replace(path.join(servicePath, path.sep), ''))\n          .filter((item) => item.length > 0 && item.match(nodeModulesRegex))\n          .reduce((globs, item) => {\n            const packagePath = path.join(servicePath, item, 'package.json');\n            return fs.readFileAsync(packagePath, 'utf-8').then((packageJsonFile) => {\n              const lastIndex = item.lastIndexOf(path.sep) + 1;\n              const moduleName = item.substr(lastIndex);\n              const modulePath = item.substr(0, lastIndex);\n\n              const packageJson = JSON.parse(packageJsonFile);\n              const bin = packageJson.bin;\n\n              const baseGlobs = [path.join(item, '**')];\n\n              // NOTE: pkg.bin can be object, string, or undefined\n              if (typeof bin === 'object') {\n                _.each(_.keys(bin), (executable) => {\n                  baseGlobs.push(path.join(modulePath, '.bin', executable));\n                });\n              // only 1 executable with same name as lib\n              } else if (typeof bin === 'string') {\n                baseGlobs.push(path.join(modulePath, '.bin', moduleName));\n              }\n\n              return globs.concat(baseGlobs);\n            });\n          }, [])\n          .then((globs) => {\n            exAndIn.exclude = exAndIn.exclude.concat(globs);\n            return exAndIn;\n          });\n      }\n\n      return exAndIn;\n    })\n    .then(() => {\n      // cleanup\n      fs.unlinkSync(nodeDevDepFile);\n      fs.unlinkSync(nodeProdDepFile);\n      return exAndIn;\n    })\n    .catch(() => exAndIn);\n  } catch (e) {\n    // fail silently\n  }\n}",
    "label": 0
  },
  {
    "codes": "```java\npublic class DecimalToBinaryConverter {\n\n    // Method to convert decimal number to binary representation\n    public static String decimalToBinary(int decimal) {\n        if (decimal == 0) {\n            return \"0\";\n        }\n        \n        // Using StringBuilder to construct the binary string\n        StringBuilder binary = new StringBuilder();\n        \n        int number = decimal;\n        \n        // Loop to calculate binary equivalent\n        while (number > 0) {\n            int remainder = number % 2; // Get the remainder (0 or 1)\n            binary.insert(0, remainder); // Prepend remainder to the binary string\n            number = number / 2; // Divide the number by 2\n        }\n        \n        return binary.toString();\n    }\n    \n    public static void main(String[] args) {\n        int decimalNumber = 19;\n        System.out.println(\"Binary of \" + decimalNumber + \" is: \" + decimalToBinary(decimalNumber));\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```c\n#include \"FreeRTOS.h\"\n#include \"task.h\"\n\n// Task handlers\nTaskHandle_t xControlTaskHandle = NULL;\nTaskHandle_t xSensorTaskHandle = NULL;\nTaskHandle_t xCommunicationTaskHandle = NULL;\n\n// Priorities\n#define CONTROL_TASK_PRIORITY      (tskIDLE_PRIORITY + 3)\n#define SENSOR_TASK_PRIORITY       (tskIDLE_PRIORITY + 2)\n#define COMMUNICATION_TASK_PRIORITY (tskIDLE_PRIORITY + 1)\n\n// Stack sizes\n#define CONTROL_TASK_STACK_SIZE    (configMINIMAL_STACK_SIZE * 2)\n#define SENSOR_TASK_STACK_SIZE     (configMINIMAL_STACK_SIZE * 2)\n#define COMMUNICATION_TASK_STACK_SIZE (configMINIMAL_STACK_SIZE)\n\n// Function prototypes for tasks\nvoid vControlTask(void *pvParameters);\nvoid vSensorTask(void *pvParameters);\nvoid vCommunicationTask(void *pvParameters);\n\nint main(void)\n{\n    // Create Control Task for the robotic arm\n    xTaskCreate(vControlTask, \"Control Task\", CONTROL_TASK_STACK_SIZE, NULL, CONTROL_TASK_PRIORITY, &xControlTaskHandle);\n    \n    // Create Sensor Task\n    xTaskCreate(vSensorTask, \"Sensor Task\", SENSOR_TASK_STACK_SIZE, NULL, SENSOR_TASK_PRIORITY, &xSensorTaskHandle);\n    \n    // Create Communication Task\n    xTaskCreate(vCommunicationTask, \"Communication Task\", COMMUNICATION_TASK_STACK_SIZE, NULL, COMMUNICATION_TASK_PRIORITY, &xCommunicationTaskHandle);\n    \n    // Start the scheduler\n    vTaskStartScheduler();\n    \n    // Should never reach here\n    for(;;);\n}\n\n// Control Task to handle arm movements\nvoid vControlTask(void *pvParameters)\n{\n    for(;;)\n    {\n        // Insert control logic here\n        // Example: Adjust motor speeds, execute movement algorithms\n        vTaskDelay(pdMS_TO_TICKS(100)); // Simulation delay\n    }\n}\n\n// Sensor Task to handle sensor data readings\nvoid vSensorTask(void *pvParameters)\n{\n    for(;;)\n    {\n        // Insert sensor reading logic here\n        // Example: Update sensor values, process sensor data\n        vTaskDelay(pdMS_TO_TICKS(50)); // Simulation delay\n    }\n}\n\n// Communication Task to handle external communications\nvoid vCommunicationTask(void *pvParameters)\n{\n    for(;;)\n    {\n        // Insert communication logic here\n        // Example: Send/Receive data with external interfaces\n        vTaskDelay(pdMS_TO_TICKS(200)); // Simulation delay\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "def check_pianoroll(arr):\n    \"\"\"\n    Return True if the array is a standard piano-roll matrix. Otherwise,\n    return False. Raise TypeError if the input object is not a numpy array.\n\n    \"\"\"\n    if not isinstance(arr, np.ndarray):\n        raise TypeError(\"`arr` must be of np.ndarray type\")\n    if not (np.issubdtype(arr.dtype, np.bool_)\n            or np.issubdtype(arr.dtype, np.number)):\n        return False\n    if arr.ndim != 2:\n        return False\n    if arr.shape[1] != 128:\n        return False\n    return True",
    "label": 0
  },
  {
    "codes": "```java\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.security.KeyStore;\nimport java.security.cert.Certificate;\nimport java.security.cert.CertificateException;\nimport java.security.cert.CertificateFactory;\nimport java.security.cert.X509Certificate;\n\npublic class KeyStoreWrapper {\n\n    private KeyStore keyStore;\n    private String keyStoreFile;\n    private char[] password;\n\n    public KeyStoreWrapper(String keyStoreFile, char[] password) throws Exception {\n        this.keyStoreFile = keyStoreFile;\n        this.password = password;\n        this.keyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n        loadKeyStore();\n    }\n    \n    // Load the KeyStore from file\n    private void loadKeyStore() throws Exception {\n        try (FileInputStream fis = new FileInputStream(keyStoreFile)) {\n            keyStore.load(fis, password);\n        } catch (IOException e) {\n            keyStore.load(null, password);  // Create empty keyStore if file not found\n        }\n    }\n\n    // Save the KeyStore to file\n    public void saveKeyStore() throws Exception {\n        try (FileOutputStream fos = new FileOutputStream(keyStoreFile)) {\n            keyStore.store(fos, password);\n        }\n    }\n\n    // Add a certificate to the KeyStore\n    public void addCertificate(String alias, X509Certificate certificate) throws Exception {\n        keyStore.setCertificateEntry(alias, certificate);\n        saveKeyStore();\n    }\n\n    // Retrieve a certificate from the KeyStore\n    public X509Certificate getCertificate(String alias) throws Exception {\n        Certificate cert = keyStore.getCertificate(alias);\n        if (cert != null && cert instanceof X509Certificate) {\n            return (X509Certificate) cert;\n        }\n        return null;\n    }\n\n    // Remove a certificate from the KeyStore\n    public void removeCertificate(String alias) throws Exception {\n        keyStore.deleteEntry(alias);\n        saveKeyStore();\n    }\n\n    // Load an X.509 certificate from a file\n    public static X509Certificate loadCertificate(String filePath) throws CertificateException, IOException {\n        try (FileInputStream fis = new FileInputStream(filePath)) {\n            CertificateFactory certFactory = CertificateFactory.getInstance(\"X.509\");\n            return (X509Certificate) certFactory.generateCertificate(fis);\n        }\n    }\n\n    // Example usage\n    public static void main(String[] args) throws Exception {\n        char[] password = \"changeit\".toCharArray();\n        KeyStoreWrapper keyStoreWrapper = new KeyStoreWrapper(\"keystore.jks\", password);\n        \n        // Load certificate from file\n        X509Certificate cert = KeyStoreWrapper.loadCertificate(\"mycert.crt\");\n        \n        // Add the certificate to the KeyStore\n        keyStoreWrapper.addCertificate(\"mycert\", cert);\n        \n        // Retrieve the certificate from the KeyStore\n        X509Certificate retrievedCert = keyStoreWrapper.getCertificate(\"mycert\");\n        System.out.println(\"Certificate: \" + retrievedCert);\n        \n        // Remove the certificate from the KeyStore\n        keyStoreWrapper.removeCertificate(\"mycert\");\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function (dimName, axisIndex, dataZoomModel, ecModel) {\n\n    /**\n     * @private\n     * @type {string}\n     */\n    this._dimName = dimName;\n\n    /**\n     * @private\n     */\n    this._axisIndex = axisIndex;\n\n    /**\n     * @private\n     * @type {Array.<number>}\n     */\n    this._valueWindow;\n\n    /**\n     * @private\n     * @type {Array.<number>}\n     */\n    this._percentWindow;\n\n    /**\n     * @private\n     * @type {Array.<number>}\n     */\n    this._dataExtent;\n\n    /**\n     * {minSpan, maxSpan, minValueSpan, maxValueSpan}\n     * @private\n     * @type {Object}\n     */\n    this._minMaxSpan;\n\n    /**\n     * @readOnly\n     * @type {module: echarts/model/Global}\n     */\n    this.ecModel = ecModel;\n\n    /**\n     * @private\n     * @type {module: echarts/component/dataZoom/DataZoomModel}\n     */\n    this._dataZoomModel = dataZoomModel;\n\n    // /**\n    //  * @readOnly\n    //  * @private\n    //  */\n    // this.hasSeriesStacked;\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<PolicySetDefinitionInner>>> listByManagementGroupNextWithServiceResponseAsync(final String nextPageLink) {\n        return listByManagementGroupNextSinglePageAsync(nextPageLink)\n            .concatMap(new Func1<ServiceResponse<Page<PolicySetDefinitionInner>>, Observable<ServiceResponse<Page<PolicySetDefinitionInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<PolicySetDefinitionInner>>> call(ServiceResponse<Page<PolicySetDefinitionInner>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(listByManagementGroupNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport random\n\n# Generate random data\ndata = random.sample(range(1, 101), 50)\n\n# Initialize figure and axis\nfig, ax = plt.subplots()\nax.set_title(\"Bubble Sort Visualization\")\n\n# Create a bar plot\nbars = ax.bar(range(len(data)), data, color='skyblue')\n\n# Function to update the plot for each frame\ndef update(frame):\n    # Perform one step of the sorting algorithm\n    for i in range(len(data) - 1):\n        if data[i] > data[i + 1]:\n            data[i], data[i + 1] = data[i + 1], data[i]\n    \n    # Update the heights of the bars\n    for bar, height in zip(bars, data):\n        bar.set_height(height)\n    \n    return bars\n\n# Create the animation\nani = animation.FuncAnimation(fig, update, frames=range(len(data)), repeat=False, blit=True)\n\n# Display the animation\nplt.show()\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.security.KeyStore;\nimport java.security.cert.Certificate;\nimport java.security.cert.CertificateFactory;\nimport java.util.Enumeration;\n\npublic class KeyStoreWrapper {\n\n    private KeyStore keyStore;\n    private String keyStoreType;\n    private char[] password;\n\n    // Constructor initializes the KeyStore with the specified type and password\n    public KeyStoreWrapper(String keyStoreType, char[] password) throws Exception {\n        this.keyStoreType = keyStoreType;\n        this.password = password;\n        this.keyStore = KeyStore.getInstance(keyStoreType);\n        keyStore.load(null, password);\n    }\n\n    // Loads a KeyStore from a given file path\n    public void loadFromFile(String filePath) throws Exception {\n        try (FileInputStream fis = new FileInputStream(filePath)) {\n            keyStore.load(fis, password);\n        }\n    }\n\n    // Saves the current KeyStore to a specified file path\n    public void saveToFile(String filePath) throws Exception {\n        try (FileOutputStream fos = new FileOutputStream(filePath)) {\n            keyStore.store(fos, password);\n        }\n    }\n\n    // Adds a certificate to the KeyStore with a given alias\n    public void addCertificate(String alias, String certPath) throws Exception {\n        try (FileInputStream fis = new FileInputStream(certPath)) {\n            CertificateFactory factory = CertificateFactory.getInstance(\"X.509\");\n            Certificate certificate = factory.generateCertificate(fis);\n            keyStore.setCertificateEntry(alias, certificate);\n        }\n    }\n\n    // Retrieves a certificate from the KeyStore by its alias\n    public Certificate getCertificate(String alias) throws Exception {\n        return keyStore.getCertificate(alias);\n    }\n\n    // Deletes a certificate from the KeyStore using its alias\n    public void deleteCertificate(String alias) throws Exception {\n        keyStore.deleteEntry(alias);\n    }\n\n    // Lists all certificate aliases stored in the KeyStore\n    public Enumeration<String> listAliases() throws Exception {\n        return keyStore.aliases();\n    }\n\n    public static void main(String[] args) {\n        // Example usage: manage certificates using KeyStoreWrapper class\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\npublic class LongestPalindromicSubstring {\n\n    // Main method to find the longest palindromic substring\n    public String longestPalindrome(String s) {\n        if (s == null || s.length() < 1) return \"\";\n\n        int start = 0; // starting index of the longest palindrome found\n        int end = 0; // ending index of the longest palindrome found\n\n        for (int i = 0; i < s.length(); i++) {\n            // Check for the longest palindrome centered around i (odd length)\n            int len1 = expandFromCenter(s, i, i);\n            // Check for the longest palindrome centered between i and i+1 (even length)\n            int len2 = expandFromCenter(s, i, i + 1);\n\n            // Get the maximum length from the two cases\n            int len = Math.max(len1, len2);\n            if (len > end - start) {\n                // Update start and end if a longer palindrome is found\n                start = i - (len - 1) / 2;\n                end = i + len / 2;\n            }\n        }\n\n        // Return the longest palindromic substring\n        return s.substring(start, end + 1);\n    }\n\n    // Helper method to expand around the center and calculate the length of the palindrome\n    private int expandFromCenter(String s, int left, int right) {\n        // Expand the window while it forms a palindrome\n        while (left >= 0 && right < s.length() && s.charAt(left) == s.charAt(right)) {\n            left--;  // move left pointer to the left\n            right++; // move right pointer to the right\n        }\n        // Return the length of the palindrome\n        return right - left - 1;\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nclass Vector2D {\n    constructor(x, y) {\n        this.x = x;\n        this.y = y;\n    }\n\n    add(vector) {\n        return new Vector2D(this.x + vector.x, this.y + vector.y);\n    }\n\n    magnitude() {\n        return Math.sqrt(this.x ** 2 + this.y ** 2);\n    }\n}\n\n// Example usage:\n// const vector1 = new Vector2D(3, 4);\n// const vector2 = new Vector2D(1, 2);\n// const result = vector1.add(vector2);\n// console.log(`Magnitude: ${result.magnitude()}`);\n```",
    "label": 1
  },
  {
    "codes": "```java\n// Aspect to implement Circuit Breaker pattern\n@Aspect\n@Component\npublic class CircuitBreakerAspect {\n\n    private static final int MAX_FAILURES = 3;\n    private static final long TIMEOUT = 5000; // 5 seconds\n    private final Map<String, CircuitBreaker> circuitBreakers = new ConcurrentHashMap<>();\n\n    @Around(\"@annotation(CircuitBreaker)\")\n    public Object circuitBreaker(ProceedingJoinPoint joinPoint) throws Throwable {\n        String methodName = joinPoint.getSignature().toShortString();\n        CircuitBreaker circuitBreaker = circuitBreakers.computeIfAbsent(methodName, k -> new CircuitBreaker(MAX_FAILURES, TIMEOUT));\n\n        if (circuitBreaker.isOpen()) {\n            throw new CircuitBreakerOpenException(\"Circuit breaker is open for method: \" + methodName);\n        }\n\n        try {\n            Object result = joinPoint.proceed();\n            circuitBreaker.recordSuccess();\n            return result;\n        } catch (Exception e) {\n            circuitBreaker.recordFailure();\n            throw e;\n        }\n    }\n\n    private static class CircuitBreaker {\n        private final int maxFailures;\n        private final long timeout;\n        private int failureCount = 0;\n        private long lastFailureTime = 0;\n\n        public CircuitBreaker(int maxFailures, long timeout) {\n            this.maxFailures = maxFailures;\n            this.timeout = timeout;\n        }\n\n        public synchronized boolean isOpen() {\n            if (failureCount >= maxFailures && (System.currentTimeMillis() - lastFailureTime) < timeout) {\n                return true;\n            }\n            return false;\n        }\n\n        public synchronized void recordSuccess() {\n            failureCount = 0;\n        }\n\n        public synchronized void recordFailure() {\n            failureCount++;\n            lastFailureTime = System.currentTimeMillis();\n        }\n    }\n\n    public static class CircuitBreakerOpenException extends RuntimeException {\n        public CircuitBreakerOpenException(String message) {\n            super(message);\n        }\n    }\n}\n\n// Custom annotation to mark methods for Circuit Breaker\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.METHOD)\npublic @interface CircuiterBreaker {\n}\n```\n\n```java\n// Example usage of the Circuit Breaker aspect\n@Service\npublic class MyService {\n\n    @CircuiterBreaker\n    public String performOperation() {\n        // Simulate a potentially failing operation\n        if (Math.random() > 0.5) {\n            throw new RuntimeException(\"Operation failed\");\n        }\n        return \"Operation succeeded\";\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```yaml\n# Custom Resource Definition (CRD) for QueueDepthScaler\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: queuedepthscalers.autoscaling.example.com\nspec:\n  group: autoscaling.example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                queueName:\n                  type: string\n                targetQueueDepth:\n                  type: integer\n                minReplicas:\n                  type: integer\n                maxReplicas:\n                  type: integer\n  scope: Namespaced\n  names:\n    plural: queuedepthscalers\n    singular: queuedepthscaler\n    kind: QueueDepthScaler\n    shortNames:\n      - qds\n---\n# QueueDepthScaler instance\napiVersion: autoscaling.example.com/v1\nkind: QueueDepthScaler\nmetadata:\n  name: pytorch-queue-scaler\n  namespace: default\nspec:\n  queueName: \"pytorch-inference-queue\"\n  targetQueueDepth: 100\n  minReplicas: 1\n  maxReplicas: 10\n---\n# Deployment for PyTorch Inference\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pytorch-inference\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pytorch-inference\n  template:\n    metadata:\n      labels:\n        app: pytorch-inference\n    spec:\n      containers:\n      - name: pytorch-inference\n        image: pytorch-inference:latest\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: \"2Gi\"\n          requests:\n            cpu: \"0.5\"\n            memory: \"1Gi\"\n---\n# Horizontal Pod Autoscaler (HPA) with custom metrics\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: pytorch-inference-hpa\n  namespace: default\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: pytorch-inference\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: External\n    external:\n      metric:\n        name: queue_depth\n        selector:\n          matchLabels:\n            queue: \"pytorch-inference-queue\"\n      target:\n        type: AverageValue\n        averageValue: 100\n---\n# Service to expose PyTorch Inference\napiVersion: v1\nkind: Service\nmetadata:\n  name: pytorch-inference-service\n  namespace: default\nspec:\n  selector:\n    app: pytorch-inference\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n---\n# Prometheus ServiceMonitor for queue depth metrics\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: pytorch-queue-monitor\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: pytorch-inference\n  endpoints:\n  - port: metrics\n    interval: 30s\n  namespaceSelector:\n    matchNames:\n    - default\n```",
    "label": 1
  },
  {
    "codes": "public TopicDescription getTopic(String path) throws ServiceBusException, InterruptedException {\n        return Utils.completeFuture(this.asyncClient.getTopicAsync(path));\n    }",
    "label": 0
  },
  {
    "codes": "func (s *ListMultipartUploadsOutput) SetUploadsList(v []*UploadListElement) *ListMultipartUploadsOutput {\n\ts.UploadsList = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "def set_bookmarks(self, bookmarks):\n        \"\"\"\n        Store the sequence of bookmarks `bookmarks`.\n\n        Causes signals to be fired to reflect the changes.\n\n        .. note:: This should normally not be used. It does not\n                  mitigate the race condition between clients\n                  concurrently modifying the bookmarks and may lead to\n                  data loss. Use :meth:`add_bookmark`,\n                  :meth:`discard_bookmark` and :meth:`update_bookmark`\n                  instead. This method still has use-cases (modifying\n                  the bookmarklist at large, e.g. by syncing the\n                  remote store with local data).\n        \"\"\"\n        with (yield from self._lock):\n            yield from self._set_bookmarks(bookmarks)\n            self._diff_emit_update(bookmarks)",
    "label": 0
  },
  {
    "codes": "public RoleDefinitionInner createOrUpdate(String scope, String roleDefinitionId, RoleDefinitionProperties properties) {\n        return createOrUpdateWithServiceResponseAsync(scope, roleDefinitionId, properties).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<ManagedInstanceEncryptionProtectorInner> createOrUpdateAsync(String resourceGroupName, String managedInstanceName, ManagedInstanceEncryptionProtectorInner parameters) {\n        return createOrUpdateWithServiceResponseAsync(resourceGroupName, managedInstanceName, parameters).map(new Func1<ServiceResponse<ManagedInstanceEncryptionProtectorInner>, ManagedInstanceEncryptionProtectorInner>() {\n            @Override\n            public ManagedInstanceEncryptionProtectorInner call(ServiceResponse<ManagedInstanceEncryptionProtectorInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "function realloc(b, sz) {\n\tvar L = b.length, M = 2*L > sz ? 2*L : sz + 5, i = 0;\n\tif(L >= sz) return b;\n\tif(has_buf) {\n\t\tvar o = new_unsafe_buf(M);\n\t\t// $FlowIgnore\n\t\tif(b.copy) b.copy(o);\n\t\telse for(; i < b.length; ++i) o[i] = b[i];\n\t\treturn o;\n\t} else if(use_typed_arrays) {\n\t\tvar a = new Uint8Array(M);\n\t\tif(a.set) a.set(b);\n\t\telse for(; i < b.length; ++i) a[i] = b[i];\n\t\treturn a;\n\t}\n\tb.length = M;\n\treturn b;\n}",
    "label": 0
  },
  {
    "codes": "func (s *ListProvisionedCapacityInput) SetAccountId(v string) *ListProvisionedCapacityInput {\n\ts.AccountId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *PhysicalConnectionRequirements) SetSubnetId(v string) *PhysicalConnectionRequirements {\n\ts.SubnetId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Source and destination directories\nSOURCE_DIR=\"/path/to/source/\"\nDEST_DIR=\"/path/to/destination/\"\n\n# Rsync options:\n# -a: Archive mode (preserves permissions, timestamps, symbolic links, etc.)\n# -v: Verbose output\n# -z: Compress file data during the transfer\n# -u: Skip files that are newer on the receiver\n# --delete: Delete files in destination that are not in source\nRSYNC_OPTIONS=\"-avzu --delete\"\n\n# Perform the rsync operation\nrsync $RSYNC_OPTIONS \"$SOURCE_DIR\" \"$DEST_DIR\"\n\n# End of script\n```\n",
    "label": 1
  },
  {
    "codes": "```python\n# Definition for singly-linked list node\nclass ListNode:\n    def __init__(self, val=0, next=None):\n        self.val = val\n        self.next = next\n\n# Function to reverse the linked list\ndef reverse_linked_list(head):\n    prev = None\n    current = head\n    \n    # Traverse the list and reverse the links\n    while current:\n        next_node = current.next  # Store the next node\n        current.next = prev      # Reverse the current node's pointer\n        prev = current           # Move prev to current node\n        current = next_node      # Move current to next node\n    \n    # prev will be the new head of the reversed list\n    return prev\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport functools\nimport logging\nimport traceback\n\n# Configure logging\nlogging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef log_exceptions(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            # Log the exception with traceback\n            logging.error(f\"Exception occurred in {func.__name__}: {str(e)}\")\n            logging.error(traceback.format_exc())\n            raise  # Re-raise the exception after logging\n    return wrapper\n\n# Example usage\n@log_exceptions\ndef risky_function():\n    return 1 / 0\n\nrisky_function()\n```",
    "label": 1
  },
  {
    "codes": "def qualified_note_rate(pianoroll, threshold=2):\n    \"\"\"Return the ratio of the number of the qualified notes (notes longer than\n    `threshold` (in time step)) to the total number of notes in a pianoroll.\"\"\"\n    _validate_pianoroll(pianoroll)\n    if np.issubdtype(pianoroll.dtype, np.bool_):\n        pianoroll = pianoroll.astype(np.uint8)\n    padded = np.pad(pianoroll, ((1, 1), (0, 0)), 'constant')\n    diff = np.diff(padded, axis=0).reshape(-1)\n    onsets = (diff > 0).nonzero()[0]\n    offsets = (diff < 0).nonzero()[0]\n    n_qualified_notes = np.count_nonzero(offsets - onsets >= threshold)\n    return n_qualified_notes / len(onsets)",
    "label": 0
  },
  {
    "codes": "func (s *CreateDevEndpointOutput) SetNumberOfNodes(v int64) *CreateDevEndpointOutput {\n\ts.NumberOfNodes = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public List<SubscriptionDescription> getSubscriptions(String topicName) throws ServiceBusException, InterruptedException {\n        return Utils.completeFuture(this.asyncClient.getSubscriptionsAsync(topicName));\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<Void> regenerateAccessKeyAsync(String resourceGroupName, String workflowName, KeyType keyType) {\n        return regenerateAccessKeyWithServiceResponseAsync(resourceGroupName, workflowName, keyType).map(new Func1<ServiceResponse<Void>, Void>() {\n            @Override\n            public Void call(ServiceResponse<Void> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "public PagedList<DeletedSasDefinitionItem> getDeletedSasDefinitions(final String vaultBaseUrl, final String storageAccountName, final Integer maxresults) {\n        ServiceResponse<Page<DeletedSasDefinitionItem>> response = getDeletedSasDefinitionsSinglePageAsync(vaultBaseUrl, storageAccountName, maxresults).toBlocking().single();\n        return new PagedList<DeletedSasDefinitionItem>(response.body()) {\n            @Override\n            public Page<DeletedSasDefinitionItem> nextPage(String nextPageLink) {\n                return getDeletedSasDefinitionsNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "def export_as_json(self):\n        \"\"\"\n        Export the whole roster as currently stored on the client side into a\n        JSON-compatible dictionary and return that dictionary.\n        \"\"\"\n        return {\n            \"items\": {\n                str(jid): item.export_as_json()\n                for jid, item in self.items.items()\n            },\n            \"ver\": self.version\n        }",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/segmentio/kafka-go\"\n)\n\n// Worker function to process Kafka messages\nfunc worker(id int, messages <-chan kafka.Message, results chan<- string, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\tfor msg := range messages {\n\t\t// Simulate processing time\n\t\ttime.Sleep(100 * time.Millisecond)\n\t\tresult := fmt.Sprintf(\"Worker %d processed message: %s\", id, string(msg.Value))\n\t\tresults <- result\n\t}\n}\n\nfunc main() {\n\t// Kafka reader configuration\n\treader := kafka.NewReader(kafka.ReaderConfig{\n\t\tBrokers:   []string{\"localhost:9092\"},\n\t\tTopic:     \"test-topic\",\n\t\tPartition: 0,\n\t\tMinBytes:  10e3, // 10KB\n\t\tMaxBytes:  10e6, // 10MB\n\t})\n\tdefer reader.Close()\n\n\t// Channels for fan-out and fan-in\n\tmessages := make(chan kafka.Message, 100)\n\tresults := make(chan string, 100)\n\n\t// WaitGroup to wait for all workers to finish\n\tvar wg sync.WaitGroup\n\n\t// Start worker pool\n\tnumWorkers := 5\n\tfor i := 1; i <= numWorkers; i++ {\n\t\twg.Add(1)\n\t\tgo worker(i, messages, results, &wg)\n\t}\n\n\t// Goroutine to read messages from Kafka and fan-out to workers\n\tgo func() {\n\t\tfor {\n\t\t\tmsg, err := reader.ReadMessage(context.Background())\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatalf(\"Failed to read message: %v\", err)\n\t\t\t}\n\t\t\tmessages <- msg\n\t\t}\n\t}()\n\n\t// Goroutine to fan-in results from workers\n\tgo func() {\n\t\tfor result := range results {\n\t\t\tfmt.Println(result)\n\t\t}\n\t}()\n\n\t// Wait for all workers to finish\n\twg.Wait()\n\tclose(messages)\n\tclose(results)\n}\n```",
    "label": 1
  },
  {
    "codes": "func (c *Glue) CreateDevEndpointWithContext(ctx aws.Context, input *CreateDevEndpointInput, opts ...request.Option) (*CreateDevEndpointOutput, error) {\n\treq, out := c.CreateDevEndpointRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```python\nfrom flask import Flask, request, jsonify\nfrom tensorflow.keras.models import load_model\nimport numpy as np\nfrom PIL import Image\nimport io\n\napp = Flask(__name__)\n\n# Load the pre-trained model\nmodel = load_model('your_model.h5')\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    # Check if an image file is in the request\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n    \n    file = request.files['file']\n\n    # Read and preprocess the image file\n    try:\n        image = Image.open(file)\n        image = image.resize((224, 224))  # Adjust size as per your model's requirement\n        img_array = np.array(image) / 255.0  # Normalize the image\n        img_array = np.expand_dims(img_array, axis=0)  # Add a batch dimension\n        \n        # Run the model prediction\n        prediction = model.predict(img_array)\n        predicted_class = np.argmax(prediction, axis=1).tolist()\n        \n        return jsonify({'predicted_class': predicted_class}), 200\n    \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n```\n\n",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<WorkflowInner> createOrUpdateAsync(String resourceGroupName, String workflowName, WorkflowInner workflow, final ServiceCallback<WorkflowInner> serviceCallback) {\n        return ServiceFuture.fromResponse(createOrUpdateWithServiceResponseAsync(resourceGroupName, workflowName, workflow), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<CertificateIssuerItem>>> getCertificateIssuersNextSinglePageAsync(final String nextPageLink) {\n        if (nextPageLink == null) {\n            throw new IllegalArgumentException(\"Parameter nextPageLink is required and cannot be null.\");\n        }\n        String nextUrl = String.format(\"%s\", nextPageLink);\n        return service.getCertificateIssuersNext(nextUrl, this.acceptLanguage(), this.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<Page<CertificateIssuerItem>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<CertificateIssuerItem>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<PageImpl<CertificateIssuerItem>> result = getCertificateIssuersNextDelegate(response);\n                        return Observable.just(new ServiceResponse<Page<CertificateIssuerItem>>(result.body(), result.response()));\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *Crawler) SetLastUpdated(v time.Time) *Crawler {\n\ts.LastUpdated = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public Observable<Page<UsageInner>> listWebWorkerUsagesAsync(final String resourceGroupName, final String name, final String workerPoolName) {\n        return listWebWorkerUsagesWithServiceResponseAsync(resourceGroupName, name, workerPoolName)\n            .map(new Func1<ServiceResponse<Page<UsageInner>>, Page<UsageInner>>() {\n                @Override\n                public Page<UsageInner> call(ServiceResponse<Page<UsageInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<DeletedSasDefinitionBundle> getDeletedSasDefinitionAsync(String vaultBaseUrl, String storageAccountName, String sasDefinitionName, final ServiceCallback<DeletedSasDefinitionBundle> serviceCallback) {\n        return ServiceFuture.fromResponse(getDeletedSasDefinitionWithServiceResponseAsync(vaultBaseUrl, storageAccountName, sasDefinitionName), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "func (c *Glacier) ListVaultsWithContext(ctx aws.Context, input *ListVaultsInput, opts ...request.Option) (*ListVaultsOutput, error) {\n\treq, out := c.ListVaultsRequest(input)\n\treq.SetContext(ctx)\n\treq.ApplyOptions(opts...)\n\treturn out, req.Send()\n}",
    "label": 0
  },
  {
    "codes": "```javascript\n[\n  // Match sales documents within a specific period\n  {\n    $match: {\n      saleDate: {\n        $gte: ISODate(\"2023-01-01\"),\n        $lte: ISODate(\"2023-12-31\")\n      }\n    }\n  },\n  // Group sales by month and sum the total sales amount\n  {\n    $group: {\n      _id: {\n        year: { $year: \"$saleDate\" },\n        month: { $month: \"$saleDate\" }\n      },\n      totalSales: { $sum: \"$amount\" }\n    }\n  },\n  // Sort the results by year and month in ascending order\n  {\n    $sort: {\n      \"_id.year\": 1,\n      \"_id.month\": 1\n    }\n  },\n  // Project fields to format the output\n  {\n    $project: {\n      month: {\n        $concat: [\n          { $toString: \"$_id.year\" },\n          \"-\",\n          { $cond: { if: { $lt: [\"$_id.month\", 10] }, then: \"0\", else: \"\" } },\n          { $toString: \"$_id.month\" }\n        ]\n      },\n      totalSales: 1\n    }\n  }\n]\n```\n",
    "label": 1
  },
  {
    "codes": "def repeated_query(qname, rdtype,\n                   nattempts=None,\n                   resolver=None,\n                   require_ad=False,\n                   executor=None):\n    \"\"\"\n    Repeatedly fire a DNS query until either the number of allowed attempts\n    (`nattempts`) is excedeed or a non-error result is returned (NXDOMAIN is\n    a non-error result).\n\n    If `nattempts` is :data:`None`, it is set to 3 if `resolver` is\n    :data:`None` and to 2 otherwise. This way, no query is made without a\n    possible change to a local parameter. (When using the thread-local\n    resolver, it will be re-configured after the first failed query and after\n    the second failed query, TCP is used. With a fixed resolver, TCP is used\n    after the first failed query.)\n\n    `qname` must be the (IDNA encoded, as :class:`bytes`) name to query,\n    `rdtype` the record type to query for. If `resolver` is not :data:`None`,\n    it must be a DNSPython :class:`dns.resolver.Resolver` instance; if it is\n    :data:`None`, the resolver obtained from :func:`get_resolver` is used.\n\n    If `require_ad` is :data:`True`, the peer resolver is asked to do DNSSEC\n    validation and if the AD flag is missing in the response,\n    :class:`ValueError` is raised. If `require_ad` is :data:`False`, the\n    resolver is asked to do DNSSEC validation nevertheless, but missing\n    validation (in constrast to failed validation) is not an error.\n\n    .. note::\n\n       This function modifies the flags of the `resolver` instance, no matter\n       if it uses the thread-local resolver instance or the resolver passed as\n       an argument.\n\n    If the first query fails and `resolver` is :data:`None` and the\n    thread-local resolver has not been overridden with :func:`set_resolver`,\n    :func:`reconfigure_resolver` is called and the query is re-attempted\n    immediately.\n\n    If the next query after reconfiguration of the resolver (if the\n    preconditions for resolver reconfigurations are not met, this applies to\n    the first failing query), :func:`repeated_query` switches to TCP.\n\n    If no result is received before the number of allowed attempts is exceeded,\n    :class:`TimeoutError` is raised.\n\n    Return the result set or :data:`None` if the domain does not exist.\n\n    This is a coroutine; the query is executed in an `executor` using the\n    :meth:`asyncio.BaseEventLoop.run_in_executor` of the current event loop. By\n    default, the default executor provided by the event loop is used, but it\n    can be overridden using the `executor` argument.\n\n    If the used resolver raises :class:`dns.resolver.NoNameservers`\n    (semantically, that no nameserver was able to answer the request), this\n    function suspects that DNSSEC validation failed, as responding with\n    SERVFAIL is what unbound does. To test that case, a simple check is made:\n    the query is repeated, but with a flag set which indicates that we would\n    like to do the validation ourselves. If that query succeeds, we assume that\n    the error is in fact due to DNSSEC validation failure and raise\n    :class:`ValidationError`. Otherwise, the answer is discarded and the\n    :class:`~dns.resolver.NoNameservers` exception is treated as normal\n    timeout. If the exception re-occurs in the second query, it is re-raised,\n    as it indicates a serious configuration problem.\n    \"\"\"\n    global _state\n\n    loop = asyncio.get_event_loop()\n\n    # tlr = thread-local resolver\n    use_tlr = False\n    if resolver is None:\n        resolver = get_resolver()\n        use_tlr = not _state.overridden_resolver\n\n    if nattempts is None:\n        if use_tlr:\n            nattempts = 3\n        else:\n            nattempts = 2\n\n    if nattempts <= 0:\n        raise ValueError(\"query cannot succeed with non-positive amount \"\n                         \"of attempts\")\n\n    qname = qname.decode(\"ascii\")\n\n    def handle_timeout():\n        nonlocal use_tlr, resolver, use_tcp\n        if use_tlr and i == 0:\n            reconfigure_resolver()\n            resolver = get_resolver()\n        else:\n            use_tcp = True\n\n    use_tcp = False\n    for i in range(nattempts):\n        resolver.set_flags(dns.flags.RD | dns.flags.AD)\n        try:\n            answer = yield from loop.run_in_executor(\n                executor,\n                functools.partial(\n                    resolver.query,\n                    qname,\n                    rdtype,\n                    tcp=use_tcp\n                )\n            )\n\n            if require_ad and not (answer.response.flags & dns.flags.AD):\n                raise ValueError(\"DNSSEC validation not available\")\n        except (TimeoutError, dns.resolver.Timeout):\n            handle_timeout()\n            continue\n        except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN):\n            return None\n        except (dns.resolver.NoNameservers):\n            # make sure we have the correct config\n            if use_tlr and i == 0:\n                reconfigure_resolver()\n                resolver = get_resolver()\n                continue\n            resolver.set_flags(dns.flags.RD | dns.flags.AD | dns.flags.CD)\n            try:\n                yield from loop.run_in_executor(\n                    executor,\n                    functools.partial(\n                        resolver.query,\n                        qname,\n                        rdtype,\n                        tcp=use_tcp,\n                        raise_on_no_answer=False\n                    ))\n            except (dns.resolver.Timeout, TimeoutError):\n                handle_timeout()\n                continue\n            except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):\n                pass\n            raise ValidationError(\n                \"nameserver error, most likely DNSSEC validation failed\",\n            )\n        break\n    else:\n        raise TimeoutError()\n\n    return answer",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<AgreementTermsInner> getAgreementAsync(String publisherId, String offerId, String planId, final ServiceCallback<AgreementTermsInner> serviceCallback) {\n        return ServiceFuture.fromResponse(getAgreementWithServiceResponseAsync(publisherId, offerId, planId), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "def get_error_name(error):\n    \"\"\"Return canonical error name as string.\n\n    For builtin errors like ValueError or Exception, will return the bare\n    name, like ValueError or Exception.\n\n    For all other exceptions, will return modulename.errorname, such as\n    arbpackage.mod.myerror\n\n    Args:\n        error: Exception object.\n\n    Returns:\n        str. Canonical error name.\n\n    \"\"\"\n    error_type = type(error)\n    if error_type.__module__ in ['__main__', 'builtins']:\n        return error_type.__name__\n    else:\n        return f'{error_type.__module__}.{error_type.__name__}'",
    "label": 0
  },
  {
    "codes": "```python\ndef factorial(n):\n    \"\"\"Calculate the factorial of a non-negative integer n using iteration.\"\"\"\n    if n < 0:\n        raise ValueError(\"Factorial is not defined for negative numbers.\")\n    result = 1\n    for i in range(2, n + 1):\n        result *= i\n    return result\n```",
    "label": 1
  },
  {
    "codes": "function findPages(\n  options = {},\n  directory = path.resolve(__dirname, '../../../../pages'),\n  pages = [],\n) {\n  fs.readdirSync(directory).forEach(item => {\n    const itemPath = path.resolve(directory, item);\n    const pathname = itemPath\n      .replace(new RegExp(`\\\\${path.sep}`, 'g'), '/')\n      .replace(/^.*\\/pages/, '')\n      .replace('.js', '')\n      .replace(/^\\/index$/, '/') // Replace `index` by `/`.\n      .replace(/\\/index$/, '');\n\n    if (pathname.indexOf('.eslintrc') !== -1) {\n      return;\n    }\n\n    if (\n      options.front &&\n      pathname.indexOf('/components') === -1 &&\n      pathname.indexOf('/api') === -1\n    ) {\n      return;\n    }\n\n    if (fs.statSync(itemPath).isDirectory()) {\n      const children = [];\n      pages.push({\n        pathname,\n        children,\n      });\n      findPages(options, itemPath, children);\n      return;\n    }\n\n    if (!jsRegex.test(item) || blackList.includes(pathname)) {\n      return;\n    }\n\n    pages.push({\n      pathname,\n    });\n  });\n\n  // sort by pathnames without '-' so that e.g. card comes before card-action\n  pages.sort((a, b) => {\n    const pathnameA = a.pathname.replace(/-/g, '');\n    const pathnameB = b.pathname.replace(/-/g, '');\n    if (pathnameA < pathnameB) return -1;\n    if (pathnameA > pathnameB) return 1;\n    return 0;\n  });\n\n  return pages;\n}",
    "label": 0
  },
  {
    "codes": "```elixir\ndefmodule DistributedStateManager do\n  use GenServer\n\n  # Client API\n\n  def start_link(initial_state \\\\ %{}) do\n    GenServer.start_link(__MODULE__, initial_state, name: __MODULE__)\n  end\n\n  def set(key, value) do\n    GenServer.call(__MODULE__, {:set, key, value})\n  end\n\n  def get(key) do\n    GenServer.call(__MODULE__, {:get, key})\n  end\n\n  def delete(key) do\n    GenServer.call(__MODULE__, {:delete, key})\n  end\n\n  # Server Callbacks\n\n  @impl true\n  def init(initial_state) do\n    {:ok, initial_state}\n  end\n\n  @impl true\n  def handle_call({:set, key, value}, _from, state) do\n    new_state = Map.put(state, key, value)\n    {:reply, :ok, new_state}\n  end\n\n  @impl true\n  def handle_call({:get, key}, _from, state) do\n    value = Map.get(state, key, :not_found)\n    {:reply, value, state}\n  end\n\n  @impl true\n  def handle_call({:delete, key}, _from, state) do\n    new_state = Map.delete(state, key)\n    {:reply, :ok, new_state}\n  end\nend\n```",
    "label": 1
  },
  {
    "codes": "```csharp\n// Extension method to truncate strings with ellipsis\npublic static class StringExtensions\n{\n    public static string TruncateWithEllipsis(this string input, int maxLength)\n    {\n        // Check if input is null or shorter than maxLength\n        if (string.IsNullOrEmpty(input) || input.Length <= maxLength)\n        {\n            return input;\n        }\n\n        // Truncate the string and append ellipsis\n        return input.Substring(0, maxLength - 3) + \"...\";\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```dart\nString timeAgo(DateTime dateTime) {\n  final Duration difference = DateTime.now().difference(dateTime);\n\n  if (difference.inDays > 0) {\n    return '${difference.inDays} day${difference.inDays == 1 ? '' : 's'} ago';\n  } else if (difference.inHours > 0) {\n    return '${difference.inHours} hour${difference.inHours == 1 ? '' : 's'} ago';\n  } else if (difference.inMinutes > 0) {\n    return '${difference.inMinutes} minute${difference.inMinutes == 1 ? '' : 's'} ago';\n  } else if (difference.inSeconds > 0) {\n    return '${difference.inSeconds} second${difference.inSeconds == 1 ? '' : 's'} ago';\n  } else {\n    return 'just now';\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Define variables\nBACKUP_DIR=\"/path/to/backup\"  # Directory to backup\nS3_BUCKET=\"s3://your-bucket-name\"  # S3 bucket name\nDATE=$(date +%Y-%m-%d_%H-%M-%S)  # Current date and time\nLOG_FILE=\"/var/log/backup_$DATE.log\"  # Log file\n\n# Create a compressed archive of the backup directory\ntar -czf \"$BACKUP_DIR/backup_$DATE.tar.gz\" -C \"$BACKUP_DIR\" .\n\n# Upload the archive to S3\naws s3 cp \"$BACKUP_DIR/backup_$DATE.tar.gz\" \"$S3_BUCKET/backup_$DATE.tar.gz\" >> \"$LOG_FILE\" 2>&1\n\n# Check if the upload was successful\nif [ $? -eq 0 ]; then\n    echo \"Backup successful. Archive uploaded to S3.\" >> \"$LOG_FILE\"\n    # Optionally, delete the local archive after successful upload\n    rm \"$BACKUP_DIR/backup_$DATE.tar.gz\"\nelse\n    echo \"Backup failed. Check the log file for details.\" >> \"$LOG_FILE\"\nfi\n\n# Clean up old backups (optional)\n# Delete backups older than 30 days\nfind \"$BACKUP_DIR\" -type f -name \"backup_*.tar.gz\" -mtime +30 -exec rm {} \\;\n\n# End of script\n```",
    "label": 1
  },
  {
    "codes": "public PagedList<DenyAssignmentInner> listForResource(final String resourceGroupName, final String resourceProviderNamespace, final String parentResourcePath, final String resourceType, final String resourceName) {\n        ServiceResponse<Page<DenyAssignmentInner>> response = listForResourceSinglePageAsync(resourceGroupName, resourceProviderNamespace, parentResourcePath, resourceType, resourceName).toBlocking().single();\n        return new PagedList<DenyAssignmentInner>(response.body()) {\n            @Override\n            public Page<DenyAssignmentInner> nextPage(String nextPageLink) {\n                return listForResourceNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.clustering import KMeans\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"CustomerClustering\") \\\n    .getOrCreate()\n\n# Load customer data\ndata_path = \"hdfs:///path/to/customer_data.csv\"  # Change accordingly\ncustomer_data = spark.read.csv(data_path, header=True, inferSchema=True)\n\n# Define the features for clustering\nfeature_columns = ['feature1', 'feature2', 'feature3']  # Replace with actual feature names\nvector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\nvector_data = vector_assembler.transform(customer_data)\n\n# Train KMeans model\nkmeans = KMeans().setK(5).setSeed(1)  # Set desired number of clusters `K`\nmodel = kmeans.fit(vector_data)\n\n# Make predictions\npredictions = model.transform(vector_data)\n\n# Show clustering results\npredictions.select(\"features\", \"prediction\").show()\n\n# Evaluate clustering\nwssse = model.computeCost(vector_data)\nprint(f\"Within Set Sum of Squared Errors = {wssse}\")\n\n# Stop Spark session\nspark.stop()\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nconst xss = require('xss');\n\n// Middleware function to sanitize user input\nfunction sanitizeInput(req, res, next) {\n    // Use the xss library to sanitize query, body, and params\n    const sanitize = (value) => {\n        if (typeof value === 'string') {\n            return xss(value);\n        } else if (typeof value === 'object' && value !== null) {\n            for (let key in value) {\n                value[key] = sanitize(value[key]);\n            }\n        }\n        return value;\n    };\n\n    // Sanitize query parameters\n    req.query = sanitize(req.query);\n    \n    // Sanitize request body\n    req.body = sanitize(req.body);\n\n    // Sanitize URL parameters\n    req.params = sanitize(req.params);\n\n    // Continue to the next middleware\n    next();\n}\n\nmodule.exports = sanitizeInput;\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport gym\nimport numpy as np\nfrom collections import defaultdict\n\n# Initialize the environment\nenv = gym.make('FrozenLake-v1')\n\n# Hyperparameters\nalpha = 0.1  # Learning rate\ngamma = 0.99  # Discount factor\nepsilon = 1.0  # Exploration rate\nepsilon_decay = 0.995\nepsilon_min = 0.01\nnum_episodes = 10000\n\n# Initialize Q-table with zeros\nQ = defaultdict(lambda: np.zeros(env.action_space.n))\n\n# Function to choose an action using epsilon-greedy policy\ndef choose_action(state):\n    if np.random.rand() < epsilon:\n        return env.action_space.sample()  # Explore: random action\n    else:\n        return np.argmax(Q[state])  # Exploit: best known action\n\n# Training the agent\nfor episode in range(num_episodes):\n    state = env.reset()\n    done = False\n    \n    while not done:\n        action = choose_action(state)\n        next_state, reward, done, info = env.step(action)\n        \n        # Update Q-value using the Bellman equation\n        best_next_action = np.argmax(Q[next_state])\n        td_target = reward + gamma * Q[next_state][best_next_action]\n        td_error = td_target - Q[state][action]\n        Q[state][action] += alpha * td_error\n        \n        state = next_state\n    \n    # Decay epsilon to reduce exploration over time\n    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n\n# Test the trained agent\nstate = env.reset()\ndone = False\nwhile not done:\n    action = np.argmax(Q[state])\n    state, reward, done, info = env.step(action)\n    env.render()\n\nenv.close()\n```",
    "label": 1
  },
  {
    "codes": "def first(script, value=None, default=None, vars={}, url=None, opener=default_opener, library_paths=[]):\n    \"\"\"\n    Transform object by jq script, returning the first result.\n    Return default if result is empty.\n    \"\"\"\n    return compile(script, vars, library_paths).first(_get_value(value, url, opener), default)",
    "label": 0
  },
  {
    "codes": "func (s *DevEndpoint) SetPublicKey(v string) *DevEndpoint {\n\ts.PublicKey = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```kotlin\n// Import necessary libraries\nimport android.hardware.Sensor\nimport android.hardware.SensorEvent\nimport android.hardware.SensorEventListener\nimport android.hardware.SensorManager\nimport android.os.Bundle\nimport androidx.appcompat.app.AppCompatActivity\nimport com.github.mikephil.charting.charts.LineChart\nimport com.github.mikephil.charting.data.Entry\nimport com.github.mikephil.charting.data.LineData\nimport com.github.mikephil.charting.data.LineDataSet\nimport kotlin.math.roundToInt\n\n// MainActivity\nclass MainActivity : AppCompatActivity(), SensorEventListener {\n\n    private lateinit var sensorManager: SensorManager\n    private var accelerometer: Sensor? = null\n    private lateinit var lineChart: LineChart\n    private val dataPointsX = ArrayList<Entry>()\n    private val dataPointsY = ArrayList<Entry>()\n    private val dataPointsZ = ArrayList<Entry>()\n    private var timestamp = 0f\n\n    override fun onCreate(savedInstanceState: Bundle?) {\n        super.onCreate(savedInstanceState)\n        setContentView(R.layout.activity_main)\n\n        // Initialize SensorManager and sensor\n        sensorManager = getSystemService(SENSOR_SERVICE) as SensorManager\n        accelerometer = sensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER)\n        \n        // Initialize the LineChart\n        lineChart = findViewById(R.id.lineChart)\n        lineChart.setTouchEnabled(true)\n        lineChart.setPinchZoom(true)\n        lineChart.description.isEnabled = false\n\n        // Updating chart periodically\n        lineChart.postDelayed({ updateChart() }, 100)\n    }\n\n    override fun onResume() {\n        super.onResume()\n        // Register sensor listener\n        accelerometer?.also { acc ->\n            sensorManager.registerListener(this, acc, SensorManager.SENSOR_DELAY_UI)\n        }\n    }\n\n    override fun onPause() {\n        super.onPause()\n        // Unregister sensor listener\n        sensorManager.unregisterListener(this)\n    }\n\n    override fun onSensorChanged(event: SensorEvent?) {\n        if (event != null && event.sensor.type == Sensor.TYPE_ACCELEROMETER) {\n            // Update data points for X, Y, Z axis\n            timestamp += 0.1f\n            dataPointsX.add(Entry(timestamp, event.values[0]))\n            dataPointsY.add(Entry(timestamp, event.values[1]))\n            dataPointsZ.add(Entry(timestamp, event.values[2]))\n\n            // Only keep latest 50 points in the chart\n            if (dataPointsX.size > 50) {\n                dataPointsX.removeAt(0)\n                dataPointsY.removeAt(0)\n                dataPointsZ.removeAt(0)\n            }\n        }\n    }\n\n    override fun onAccuracyChanged(sensor: Sensor?, accuracy: Int) {\n        // No action needed here\n    }\n\n    private fun updateChart() {\n        // Create LineDataSets\n        val dataSetX = LineDataSet(dataPointsX, \"X-Axis\")\n        dataSetX.setColor(android.graphics.Color.RED)\n        val dataSetY = LineDataSet(dataPointsY, \"Y-Axis\")\n        dataSetY.setColor(android.graphics.Color.GREEN)\n        val dataSetZ = LineDataSet(dataPointsZ, \"Z-Axis\")\n        dataSetZ.setColor(android.graphics.Color.BLUE)\n\n        // Add LineDataSets to LineData\n        val lineData = LineData(dataSetX, dataSetY, dataSetZ)\n\n        // Update the chart\n        lineChart.data = lineData\n        lineChart.notifyDataSetChanged()\n        lineChart.invalidate()\n\n        // Schedule next update\n        lineChart.postDelayed({ updateChart() }, 100)\n    }\n}\n```\n\n```xml\n<!-- Layout file: res/layout/activity_main.xml -->\n<LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:orientation=\"vertical\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n    <com.github.mikephil.charting.charts.LineChart\n        android:id=\"@+id/lineChart\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\" />\n\n</LinearLayout>\n```\n\n```gradle\n// Add this to your build.gradle (Module: app) dependencies\ndependencies {\n    implementation 'com.github.PhilJay:MPAndroidChart:v3.1.0'\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport org.junit.jupiter.api.Test;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.DisplayName;\nimport static org.junit.jupiter.api.Assertions.*;\n\nimport java.util.Arrays;\n\nclass SortingAlgorithmTest {\n\n    private SortingAlgorithm sorter;\n\n    @BeforeEach\n    void setUp() {\n        sorter = new SortingAlgorithm(); // Assume SortingAlgorithm is the class with the sorting method\n    }\n\n    @Test\n    @DisplayName(\"Test empty array\")\n    void testEmptyArray() {\n        int[] input = {};\n        int[] expected = {};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Empty array should remain unchanged\");\n    }\n\n    @Test\n    @DisplayName(\"Test single element array\")\n    void testSingleElementArray() {\n        int[] input = {5};\n        int[] expected = {5};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Single element array should remain unchanged\");\n    }\n\n    @Test\n    @DisplayName(\"Test already sorted array\")\n    void testAlreadySortedArray() {\n        int[] input = {1, 2, 3, 4, 5};\n        int[] expected = {1, 2, 3, 4, 5};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Already sorted array should remain unchanged\");\n    }\n\n    @Test\n    @DisplayName(\"Test reverse sorted array\")\n    void testReverseSortedArray() {\n        int[] input = {5, 4, 3, 2, 1};\n        int[] expected = {1, 2, 3, 4, 5};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Reverse sorted array should be sorted in ascending order\");\n    }\n\n    @Test\n    @DisplayName(\"Test array with duplicate elements\")\n    void testArrayWithDuplicates() {\n        int[] input = {3, 1, 2, 3, 1};\n        int[] expected = {1, 1, 2, 3, 3};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Array with duplicates should be sorted correctly\");\n    }\n\n    @Test\n    @DisplayName(\"Test array with negative numbers\")\n    void testArrayWithNegativeNumbers() {\n        int[] input = {-3, -1, -2, 0, 2};\n        int[] expected = {-3, -2, -1, 0, 2};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Array with negative numbers should be sorted correctly\");\n    }\n\n    @Test\n    @DisplayName(\"Test array with all elements the same\")\n    void testArrayWithAllElementsSame() {\n        int[] input = {7, 7, 7, 7};\n        int[] expected = {7, 7, 7, 7};\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Array with all elements the same should remain unchanged\");\n    }\n\n    @Test\n    @DisplayName(\"Test large array\")\n    void testLargeArray() {\n        int[] input = new int[1000];\n        for (int i = 0; i < input.length; i++) {\n            input[i] = input.length - i;\n        }\n        int[] expected = Arrays.copyOf(input, input.length);\n        Arrays.sort(expected);\n        sorter.sort(input);\n        assertArrayEquals(expected, input, \"Large array should be sorted correctly\");\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function parse_MergeCells(blob, length)/*:Array<Range>*/ {\n\tvar merges/*:Array<Range>*/ = [];\n\tvar cmcs = blob.read_shift(2);\n\twhile (cmcs--) merges.push(parse_Ref8U(blob,length));\n\treturn merges;\n}",
    "label": 0
  },
  {
    "codes": "public EventHubConsumerGroupInfoInner createEventHubConsumerGroup(String resourceGroupName, String resourceName, String eventHubEndpointName, String name) {\n        return createEventHubConsumerGroupWithServiceResponseAsync(resourceGroupName, resourceName, eventHubEndpointName, name).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "def send_message(self, msg):\n        \"\"\"\n        Send a message to the MUC.\n\n        :param msg: The message to send.\n        :type msg: :class:`aioxmpp.Message`\n        :return: The stanza token of the message.\n        :rtype: :class:`~aioxmpp.stream.StanzaToken`\n\n        There is no need to set the address attributes or the type of the\n        message correctly; those will be overridden by this method to conform\n        to the requirements of a message to the MUC. Other attributes are left\n        untouched (except that :meth:`~.StanzaBase.autoset_id` is called) and\n        can be used as desired for the message.\n\n        .. seealso::\n\n            :meth:`.AbstractConversation.send_message` for the full interface\n            specification.\n        \"\"\"\n        msg.type_ = aioxmpp.MessageType.GROUPCHAT\n        msg.to = self._mucjid\n        # see https://mail.jabber.org/pipermail/standards/2017-January/032048.html  # NOQA\n        # for a full discussion on the rationale for this.\n        # TL;DR: we want to help entities to discover that a message is related\n        # to a MUC.\n        msg.xep0045_muc_user = muc_xso.UserExt()\n        result = self.service.client.enqueue(msg)\n        return result",
    "label": 0
  },
  {
    "codes": "def get_tags(self, tag, quiet=1):\n        \"\"\"\n        Returns which tag alternatives can be identified in the BIDS derivatives structure.\n        \"\"\"\n        if not self.pipeline:\n            print('Please set pipeline first.')\n            self.get_pipeline_alternatives(quiet)\n        else:\n            if tag == 'sub':\n                datapath = self.BIDS_dir + '/derivatives/' + self.pipeline + '/'\n                tag_alternatives = [\n                    f.split('sub-')[1] for f in os.listdir(datapath) if os.path.isdir(datapath + f) and 'sub-' in f]\n            elif tag == 'ses':\n                tag_alternatives = []\n                for sub in self.bids_tags['sub']:\n                    tag_alternatives += [f.split('ses-')[1] for f in os.listdir(\n                        self.BIDS_dir + '/derivatives/' + self.pipeline + '/' + 'sub-' + sub) if 'ses' in f]\n                tag_alternatives = set(tag_alternatives)\n            else:\n                files = self.get_selected_files(quiet=1)\n                tag_alternatives = []\n                for f in files:\n                    f = f.split('.')[0]\n                    f = f.split('/')[-1]\n                    tag_alternatives += [t.split('-')[1]\n                                         for t in f.split('_') if t.split('-')[0] == tag]\n                tag_alternatives = set(tag_alternatives)\n            if quiet == 0:\n                print(tag + ' alternatives: ' + ', '.join(tag_alternatives))\n            return list(tag_alternatives)",
    "label": 0
  },
  {
    "codes": "func (s *GetJobOutputOutput) SetContentRange(v string) *GetJobOutputOutput {\n\ts.ContentRange = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *CreateScriptOutput) SetPythonScript(v string) *CreateScriptOutput {\n\ts.PythonScript = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/rwcarlsen/goexif/exif\"\n\t\"github.com/rwcarlsen/goexif/mknote\"\n)\n\nfunc main() {\n\t// Open the JPEG file\n\tfile, err := os.Open(\"image.jpg\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer file.Close()\n\n\t// Register the MakerNote parser for Nikon cameras\n\texif.RegisterParsers(mknote.All...)\n\n\t// Decode the EXIF data\n\tx, err := exif.Decode(file)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Extract and print specific EXIF tags\n\tdateTime, err := x.DateTime()\n\tif err != nil {\n\t\tlog.Println(\"DateTime not found\")\n\t} else {\n\t\tfmt.Printf(\"DateTime: %s\\n\", dateTime)\n\t}\n\n\tlat, long, err := x.LatLong()\n\tif err != nil {\n\t\tlog.Println(\"LatLong not found\")\n\t} else {\n\t\tfmt.Printf(\"Latitude: %f, Longitude: %f\\n\", lat, long)\n\t}\n\n\tcameraModel, err := x.Get(exif.Model)\n\tif err != nil {\n\t\tlog.Println(\"Camera Model not found\")\n\t} else {\n\t\tfmt.Printf(\"Camera Model: %s\\n\", cameraModel.String())\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse wasm_bindgen::prelude::*;\nuse wasm_bindgen::JsCast;\nuse onnxruntime::environment::Environment;\nuse onnxruntime::ndarray::Array;\nuse onnxruntime::session::SessionBuilder;\n\n// Include this line to allow conversion from WebAssembly's JavaScript runtime.\n#[wasm_bindgen]\nextern \"C\" {\n    // Function to log messages to the JavaScript console.\n    #[wasm_bindgen(js_namespace = console)]\n    fn log(s: &str);\n}\n\n// Define the function to be accessible from JavaScript.\n#[wasm_bindgen]\npub async fn run_onnx_model(input_js_array: js_sys::Float32Array) -> Result<js_sys::Float32Array, JsValue> {\n    // Initialize the ONNX Runtime environment.\n    let environment = Environment::builder().build().unwrap();\n    let session = SessionBuilder::new(&environment).unwrap()\n        .with_model_from_file(\"path_to_your_onnx_model.onnx\").unwrap();\n\n    // Convert input JavaScript array into Rust's ndarray.\n    let input_data: Array<f32, _> = Array::from_shape_vec((1, input_js_array.length() as usize), input_js_array.to_vec()).unwrap();\n\n    log(\"Running the ONNX Model...\");\n\n    // Run the model with input data and get the output.\n    let output_tensor = session.run(vec![input_data]).unwrap();\n\n    // Convert output from Rust's ndarray to JavaScript's Float32Array.\n    let output_js_array = js_sys::Float32Array::from(output_tensor[0].as_slice().unwrap());\n\n    // Return the result as a JavaScript-usable object.\n    Ok(output_js_array)\n}\n```",
    "label": 1
  },
  {
    "codes": "function onError (err) {\n  console.error(chalk.red('\u2717 error:'), chalk.red(err.message))\n  notifier.notify({\n    title: 'Build failed:',\n    message: err.message\n  })\n  this.emit('end')\n\n  // When running without watch, process.exit(1) on error\n  if (!watchifyEnabled) {\n    process.exit(1)\n  }\n}",
    "label": 0
  },
  {
    "codes": "function(method, restoreScrollPosition) {\n      var body                  = this.doc.body,\n          oldScrollTop          = restoreScrollPosition && body.scrollTop,\n          oldScrollLeft         = restoreScrollPosition && body.scrollLeft,\n          className             = \"_wysihtml5-temp-placeholder\",\n          placeholderHtml       = '<span class=\"' + className + '\">' + wysihtml5.INVISIBLE_SPACE + '</span>',\n          range                 = this.getRange(true),\n          caretPlaceholder,\n          newCaretPlaceholder,\n          nextSibling, prevSibling,\n          node, node2, range2,\n          newRange;\n\n      // Nothing selected, execute and say goodbye\n      if (!range) {\n        method(body, body);\n        return;\n      }\n\n      if (!range.collapsed) {\n        range2 = range.cloneRange();\n        node2 = range2.createContextualFragment(placeholderHtml);\n        range2.collapse(false);\n        range2.insertNode(node2);\n        range2.detach();\n      }\n\n      node = range.createContextualFragment(placeholderHtml);\n      range.insertNode(node);\n\n      if (node2) {\n        caretPlaceholder = this.contain.querySelectorAll(\".\" + className);\n        range.setStartBefore(caretPlaceholder[0]);\n        range.setEndAfter(caretPlaceholder[caretPlaceholder.length -1]);\n      }\n      this.setSelection(range);\n\n      // Make sure that a potential error doesn't cause our placeholder element to be left as a placeholder\n      try {\n        method(range.startContainer, range.endContainer);\n      } catch(e) {\n        setTimeout(function() { throw e; }, 0);\n      }\n      caretPlaceholder = this.contain.querySelectorAll(\".\" + className);\n      if (caretPlaceholder && caretPlaceholder.length) {\n        newRange = rangy.createRange(this.doc);\n        nextSibling = caretPlaceholder[0].nextSibling;\n        if (caretPlaceholder.length > 1) {\n          prevSibling = caretPlaceholder[caretPlaceholder.length -1].previousSibling;\n        }\n        if (prevSibling && nextSibling) {\n          newRange.setStartBefore(nextSibling);\n          newRange.setEndAfter(prevSibling);\n        } else {\n          newCaretPlaceholder = this.doc.createTextNode(wysihtml5.INVISIBLE_SPACE);\n          dom.insert(newCaretPlaceholder).after(caretPlaceholder[0]);\n          newRange.setStartBefore(newCaretPlaceholder);\n          newRange.setEndAfter(newCaretPlaceholder);\n        }\n        this.setSelection(newRange);\n        for (var i = caretPlaceholder.length; i--;) {\n         caretPlaceholder[i].parentNode.removeChild(caretPlaceholder[i]);\n        }\n\n      } else {\n        // fallback for when all hell breaks loose\n        this.contain.focus();\n      }\n\n      if (restoreScrollPosition) {\n        body.scrollTop  = oldScrollTop;\n        body.scrollLeft = oldScrollLeft;\n      }\n\n      // Remove it again, just to make sure that the placeholder is definitely out of the dom tree\n      try {\n        caretPlaceholder.parentNode.removeChild(caretPlaceholder);\n      } catch(e2) {}\n    }",
    "label": 0
  },
  {
    "codes": "function (originRegionArr, mapName, nameMap) {\n        // Not use the original\n        var regionsArr = (originRegionArr || []).slice();\n\n        var dataNameMap = zrUtil.createHashMap();\n        for (var i = 0; i < regionsArr.length; i++) {\n            dataNameMap.set(regionsArr[i].name, regionsArr[i]);\n        }\n\n        var source = geoSourceManager.load(mapName, nameMap);\n        zrUtil.each(source.regions, function (region) {\n            var name = region.name;\n            !dataNameMap.get(name) && regionsArr.push({name: name});\n        });\n\n        return regionsArr;\n    }",
    "label": 0
  },
  {
    "codes": "def previous_page(self, max_=None):\n        \"\"\"\n        Return a query set which requests the page before this response.\n\n        :param max_: Maximum number of items to return.\n        :type max_: :class:`int` or :data:`None`\n        :rtype: :class:`ResultSetMetadata`\n        :return: A new request set up to request the previous page.\n\n        Must be called on a result set which has :attr:`first` set.\n        \"\"\"\n\n        result = type(self)()\n        result.before = Before(self.first.value)\n        result.max_ = max_\n        return result",
    "label": 0
  },
  {
    "codes": "```html\n<template>\n  <div>\n    <line-chart :chart-data=\"formattedChartData\" />\n  </div>\n</template>\n\n<script>\nimport { Line } from 'vue-chartjs';\nimport { Chart as ChartJS, Title, Tooltip, Legend, LineElement, CategoryScale, LinearScale } from 'chart.js';\n\nChartJS.register(Title, Tooltip, Legend, LineElement, CategoryScale, LinearScale);\n\nexport default {\n  components: {\n    LineChart: {\n      extends: Line,\n      props: ['chartData'],\n      data() {\n        return {\n          options: {\n            responsive: true,\n            maintainAspectRatio: false,\n            plugins: {\n              legend: {\n                display: true,\n                position: 'bottom',\n              },\n            },\n            scales: {\n              x: {\n                display: true,\n                title: {\n                  display: true,\n                  text: 'Date',\n                },\n              },\n              y: {\n                display: true,\n                title: {\n                  display: true,\n                  text: 'Stock Price',\n                },\n              },\n            },\n          },\n        };\n      },\n      mounted() {\n        this.renderChart(this.chartData, this.options);\n      },\n    },\n  },\n  data() {\n    return {\n      rawData: [\n        /* Example Data: { date: '2023-01-01', price: 100 }, ... */\n      ],\n    };\n  },\n  computed: {\n    formattedChartData() {\n      return {\n        labels: this.rawData.map(data => data.date),\n        datasets: [\n          {\n            label: 'Stock Price',\n            backgroundColor: '#42A5F5',\n            borderColor: '#42A5F5',\n            data: this.rawData.map(data => data.price),\n            fill: false,\n          },\n        ],\n      };\n    },\n  },\n};\n</script>\n\n<style scoped>\ndiv {\n  height: 400px;\n}\n</style>\n```",
    "label": 1
  },
  {
    "codes": "def main(opts):\n    \"\"\"Program entry point.\"\"\"\n    term = Terminal()\n    style = Style()\n\n    # if the terminal supports colors, use a Style instance with some\n    # standout colors (magenta, cyan).\n    if term.number_of_colors:\n        style = Style(attr_major=term.magenta,\n                      attr_minor=term.bright_cyan,\n                      alignment=opts['--alignment'])\n    style.name_len = term.width - 15\n\n    screen = Screen(term, style, wide=opts['--wide'])\n    pager = Pager(term, screen, opts['character_factory'])\n\n    with term.location(), term.cbreak(), \\\n            term.fullscreen(), term.hidden_cursor():\n        pager.run(writer=echo, reader=term.inkey)\n    return 0",
    "label": 0
  },
  {
    "codes": "```yaml\n---\n- name: Deploy web app across 100+ servers\n  hosts: webservers\n  become: yes\n\n  tasks:\n    - name: Ensure the web server is installed\n      ansible.builtin.yum:\n        name: httpd\n        state: present\n      register: httpd_install\n\n    - name: Start and enable the web server\n      ansible.builtin.service:\n        name: httpd\n        state: started\n        enabled: yes\n      when: httpd_install.changed\n\n    - name: Copy web application files to the server\n      ansible.builtin.copy:\n        src: /path/to/app/\n        dest: /var/www/html/\n        owner: apache\n        group: apache\n        mode: '0644'\n      notify:\n        - Restart web server\n\n    - name: Deploy configuration for load balancing\n      ansible.builtin.template:\n        src: /path/to/your/load_balancer.conf.j2\n        dest: /etc/httpd/conf.d/load_balancer.conf\n        owner: root\n        group: root\n        mode: '0644'\n      notify:\n        - Restart web server\n\n  handlers:\n    - name: Restart web server\n      ansible.builtin.service:\n        name: httpd\n        state: restarted\n```\n",
    "label": 1
  },
  {
    "codes": "```ini\n; PlatformIO configuration file for cross-compiling to multiple MCUs\n\n[env]\nplatform = ststm32\nframework = stm32cube\n\n; Common settings for all environments\nbuild_flags = \n    -Wall\n    -Wextra\n    -Os\n\n; Environment for STM32F103C8 (Blue Pill)\n[env:bluepill]\nboard = bluepill_f103c8\nupload_protocol = stlink\nbuild_flags = \n    ${env.build_flags}\n    -DMCU_STM32F103C8\n\n; Environment for STM32F407VGT6 (Discovery Board)\n[env:discovery_f407]\nboard = disco_f407vg\nupload_protocol = stlink\nbuild_flags = \n    ${env.build_flags}\n    -DMCU_STM32F407VGT6\n\n; Environment for STM32F746ZG (Nucleo Board)\n[env:nucleo_f746zg]\nboard = nucleo_f746zg\nupload_protocol = stlink\nbuild_flags = \n    ${env.build_flags}\n    -DMCU_STM32F746ZG\n\n; Environment for STM32G071RB (Nucleo Board)\n[env:nucleo_g071rb]\nboard = nucleo_g071rb\nupload_protocol = stlink\nbuild_flags = \n    ${env.build_flags}\n    -DMCU_STM32G071RB\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport xml.etree.ElementTree as ET\n\ndef parse_large_xml(file_path):\n    \"\"\"\n    Generator to yield elements from a large XML file one by one.\n    \"\"\"\n    context = ET.iterparse(file_path, events=(\"start\", \"end\"))\n    context = iter(context)\n    event, root = next(context)  # Get the root element\n\n    for event, elem in context:\n        if event == \"end\":\n            yield elem\n            root.clear()  # Free up memory by clearing processed elements\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport functools\nimport traceback\nimport logging\n\n# Configure the logger\nlogging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef exception_logger(func):\n    \"\"\"Decorator to log any exceptions with traceback.\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            # Capture the traceback and log the exception\n            tb_str = traceback.format_exc()\n            logging.error(f\"Exception occurred in {func.__name__}: {e}\\n{tb_str}\")\n            # Optionally, re-raise the exception after logging\n            raise\n    return wrapper\n\n# Example usage\n@exception_logger\ndef risky_division(x, y):\n    return x / y\n\n# This will log an exception with traceback\n# risky_division(10, 0)\n```",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<CertificateOperation> deleteCertificateOperationAsync(String vaultBaseUrl, String certificateName, final ServiceCallback<CertificateOperation> serviceCallback) {\n        return ServiceFuture.fromResponse(deleteCertificateOperationWithServiceResponseAsync(vaultBaseUrl, certificateName), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "def try_utf8_decode(value):\n    \"\"\"Try to decode an object.\n\n    :param value:\n    :return:\n    \"\"\"\n    if not value or not is_string(value):\n        return value\n    elif PYTHON3 and not isinstance(value, bytes):\n        return value\n    elif not PYTHON3 and not isinstance(value, unicode):\n        return value\n\n    try:\n        return value.decode('utf-8')\n    except UnicodeDecodeError:\n        pass\n\n    return value",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n    \"fmt\"\n    \"log\"\n    \"os\"\n\n    \"github.com/rwcarlsen/goexif/exif\"\n)\n\nfunc main() {\n    // Open JPEG file\n    file, err := os.Open(\"yourfile.jpg\") // Change \"yourfile.jpg\" to your target JPEG file\n    if err != nil {\n        log.Fatalf(\"failed to open file: %v\", err)\n    }\n    defer file.Close()\n\n    // Extract EXIF data\n    x, err := exif.Decode(file)\n    if err != nil {\n        log.Fatalf(\"failed to decode exif data: %v\", err)\n    }\n\n    // Get specific EXIF tags (example: Date and Camera Model)\n    date, err := x.DateTime()\n    if err != nil {\n        fmt.Println(\"Date not found\")\n    } else {\n        fmt.Println(\"Date and Time:\", date)\n    }\n\n    cameraModel, err := x.Get(exif.Model)\n    if err != nil {\n        fmt.Println(\"Camera model not found\")\n    } else {\n        model, _ := cameraModel.StringVal()\n        fmt.Println(\"Camera Model:\", model)\n    }\n\n    // Iterate over all EXIF tags (optional)\n    x.Walk(exif.WalkFunc(func(name exif.FieldName, tag *tiff.Tag) error {\n        fmt.Printf(\"%v: %v\\n\", name, tag)\n        return nil\n    }))\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\n# Import necessary libraries\nimport coremltools as ct\nfrom tensorflow.keras.models import load_model\n\n# Load the pre-trained Keras model\nkeras_model_path = 'path_to_your_keras_model.h5'\nkeras_model = load_model(keras_model_path)\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(\n    keras_model,\n    inputs=[ct.TensorType(shape=keras_model.input_shape)]\n)\n\n# Save the converted CoreML model\ncoreml_model_path = 'path_to_save_your_coreml_model.mlmodel'\ncoreml_model.save(coreml_model_path)\n```",
    "label": 1
  },
  {
    "codes": "def trim_trailing_silence(self):\n        \"\"\"Trim the trailing silence of the pianoroll.\"\"\"\n        length = self.get_active_length()\n        self.pianoroll = self.pianoroll[:length]",
    "label": 0
  },
  {
    "codes": "func (s *ListJobsInput) SetStatuscode(v string) *ListJobsInput {\n\ts.Statuscode = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "func (s *GetConnectionsInput) SetFilter(v *GetConnectionsFilter) *GetConnectionsInput {\n\ts.Filter = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nfrom fastapi import FastAPI, Request, HTTPException, status, Depends\nfrom fastapi.security.api_key import APIKeyHeader, APIKey\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\nfrom slowapi.extension import Limiter as ExtensionLimiter\nfrom starlette.status import HTTP_429_TOO_MANY_REQUESTS\n\napp = FastAPI()\nlimiter = Limiter(key_func=get_remote_address)\napp.state.limiter = limiter\napp.add_exception_handler(HTTP_429_TOO_MANY_REQUESTS, _rate_limit_exceeded_handler)\n\nAPI_KEY_NAME = \"X-API-Key\"\napi_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)\n\nVALID_API_KEYS = {\"my_secret_key\": \"User_1\"}\n\n@app.get(\"/endpoint\")\n@limiter.limit(\"5/minute\")  # Rate limit definition\nasync def protected_endpoint(\n    request: Request,\n    api_key: str = Depends(api_key_header)\n):\n    # API key authentication\n    if api_key not in VALID_API_KEYS:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid API Key\"\n        )\n    return {\"message\": \"Success\"}\n\n# Note: Typically, more response codes would be handled, such as:\n# HTTP_200_OK for a successful request\n# HTTP_401_UNAUTHORIZED for failed authentication\n# HTTP_429_TOO_MANY_REQUESTS when rate limits are exceeded\n```\n\n- **200 OK**: On successful request and valid API Key authentication.\n- **401 Unauthorized**: If the API Key is invalid or missing.\n- **429 Too Many Requests**: When the rate limit (`5/minute`) is exceeded.",
    "label": 1
  },
  {
    "codes": "def check_x509_hostname(x509, hostname):\n    \"\"\"\n    Check whether the given :class:`OpenSSL.crypto.X509` certificate `x509`\n    matches the given `hostname`.\n\n    Return :data:`True` if the name matches and :data:`False` otherwise. This\n    uses :func:`ssl.match_hostname` and :func:`extract_python_dict_from_x509`.\n    \"\"\"\n\n    cert_structure = extract_python_dict_from_x509(x509)\n    try:\n        ssl.match_hostname(cert_structure, hostname)\n    except ssl.CertificateError:\n        return False\n    return True",
    "label": 0
  },
  {
    "codes": "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define a custom CNN model\nclass CustomCNN(nn.Module):\n    def __init__(self):\n        super(CustomCNN, self).__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        \n        # Max pooling layers\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 10)  # 10 classes for CIFAR-10\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        # Apply convolutions, ReLU, and pooling\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        \n        # Flatten the tensor\n        x = x.view(-1, 128 * 4 * 4)\n        \n        # Apply fully connected layers with dropout\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        \n        return x\n\n# Instantiate the model\nmodel = CustomCNN()\n\n# Print the model architecture\nprint(model)\n```",
    "label": 1
  },
  {
    "codes": "```javascript\ndb.sales.aggregate([\n  {\n    // Convert 'saleDate' to a format that includes only the year and month\n    $addFields: {\n      yearMonth: { $dateToString: { format: \"%Y-%m\", date: \"$saleDate\" } }\n    }\n  },\n  {\n    // Group by the year and month, summing up the total sales amount for each period\n    $group: {\n      _id: \"$yearMonth\",\n      totalSales: { $sum: \"$amount\" }\n    }\n  },\n  {\n    // Sort the monthly sales totals in ascending order of time\n    $sort: {\n      _id: 1\n    }\n  },\n  {\n    // Rename the fields for easier readability in output\n    $project: {\n      month: \"$_id\",\n      totalSales: 1,\n      _id: 0\n    }\n  }\n]);\n```",
    "label": 1
  },
  {
    "codes": "func (s *NotificationProperty) Validate() error {\n\tinvalidParams := request.ErrInvalidParams{Context: \"NotificationProperty\"}\n\tif s.NotifyDelayAfter != nil && *s.NotifyDelayAfter < 1 {\n\t\tinvalidParams.Add(request.NewErrParamMinValue(\"NotifyDelayAfter\", 1))\n\t}\n\n\tif invalidParams.Len() > 0 {\n\t\treturn invalidParams\n\t}\n\treturn nil\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<BackupShortTermRetentionPolicyInner>> createOrUpdateWithServiceResponseAsync(String resourceGroupName, String serverName, String databaseName) {\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (serverName == null) {\n            throw new IllegalArgumentException(\"Parameter serverName is required and cannot be null.\");\n        }\n        if (databaseName == null) {\n            throw new IllegalArgumentException(\"Parameter databaseName is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        final String policyName = \"default\";\n        final Integer retentionDays = null;\n        BackupShortTermRetentionPolicyInner parameters = new BackupShortTermRetentionPolicyInner();\n        parameters.withRetentionDays(null);\n        Observable<Response<ResponseBody>> observable = service.createOrUpdate(resourceGroupName, serverName, databaseName, policyName, this.client.subscriptionId(), this.client.apiVersion(), this.client.acceptLanguage(), parameters, this.client.userAgent());\n        return client.getAzureClient().getPutOrPatchResultAsync(observable, new TypeToken<BackupShortTermRetentionPolicyInner>() { }.getType());\n    }",
    "label": 0
  },
  {
    "codes": "```typescript\nfunction shuffleArray<T>(array: T[]): T[] {\n    // Loop through the array from the end to the beginning\n    for (let i = array.length - 1; i > 0; i--) {\n        // Generate a random index between 0 and i (inclusive)\n        const j = Math.floor(Math.random() * (i + 1));\n        \n        // Swap the elements at indices i and j\n        [array[i], array[j]] = [array[j], array[i]];\n    }\n    return array;\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\n\n# Generate random data\ndata = np.random.randint(1, 100, 50)\n\n# Create a figure and axis\nfig, ax = plt.subplots()\nax.set_title(\"Bubble Sort Visualization\")\n\n# Initialize the bar plot\nbars = ax.bar(range(len(data)), data, color='blue')\n\n# Function to update the plot for each frame\ndef update(frame):\n    # Perform one step of the sorting algorithm\n    for i in range(len(data) - frame - 1):\n        if data[i] > data[i + 1]:\n            data[i], data[i + 1] = data[i + 1], data[i]\n    \n    # Update the heights of the bars\n    for bar, height in zip(bars, data):\n        bar.set_height(height)\n    \n    return bars\n\n# Create the animation\nani = animation.FuncAnimation(fig, update, frames=len(data), repeat=False, blit=True)\n\n# Display the animation\nplt.show()\n```",
    "label": 1
  },
  {
    "codes": "```R\n# Load necessary libraries\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(plotly)\n\n# Load COVID-19 vaccination data\n# Assuming the data is in a CSV file named 'vaccination_data.csv'\nvaccination_data <- read.csv(\"vaccination_data.csv\")\n\n# Define UI for the application\nui <- fluidPage(\n  titlePanel(\"COVID-19 Vaccination Rates by Country\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"country\", \"Select Country:\", \n                  choices = unique(vaccination_data$country), \n                  selected = \"United States\"),\n      sliderInput(\"date_range\", \"Select Date Range:\",\n                  min = as.Date(min(vaccination_data$date)), \n                  max = as.Date(max(vaccination_data$date)), \n                  value = c(as.Date(min(vaccination_data$date)), \n                            as.Date(max(vaccination_data$date))))\n    ),\n    \n    mainPanel(\n      plotlyOutput(\"vaccination_plot\"),\n      dataTableOutput(\"vaccination_table\")\n    )\n  )\n)\n\n# Define server logic\nserver <- function(input, output) {\n  \n  # Filter data based on user input\n  filtered_data <- reactive({\n    vaccination_data %>%\n      filter(country == input$country,\n             date >= input$date_range[1] & date <= input$date_range[2])\n  })\n  \n  # Render the vaccination plot\n  output$vaccination_plot <- renderPlotly({\n    ggplotly(\n      ggplot(filtered_data(), aes(x = date, y = people_vaccinated_per_hundred)) +\n        geom_line(color = \"blue\") +\n        labs(title = paste(\"Vaccination Rate in\", input$country),\n             x = \"Date\", y = \"People Vaccinated per Hundred\") +\n        theme_minimal()\n    )\n  })\n  \n  # Render the vaccination table\n  output$vaccination_table <- renderDataTable({\n    filtered_data() %>%\n      select(date, people_vaccinated_per_hundred, total_vaccinations)\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n```\n\n### Comments:\n- **Data Loading**: The data is assumed to be in a CSV file named `vaccination_data.csv`. Ensure the file is in the working directory or provide the correct path.\n- **UI Components**: The UI includes a dropdown to select a country and a date range slider to filter the data.\n- **Reactive Filtering**: The data is filtered based on the selected country and date range.\n- **Plotly Output**: The vaccination rate is plotted using `ggplot2` and rendered interactively with `plotly`.\n- **Data Table**: A table is displayed showing the vaccination data for the selected country and date range.\n- **Customization**: The plot and table can be further customized based on specific requirements.",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <stdexcept>\n\nclass DatabaseConnection {\npublic:\n    // Constructor: Establishes the database connection\n    DatabaseConnection(const std::string& connectionString) {\n        std::cout << \"Connecting to database: \" << connectionString << std::endl;\n        // Simulate connection establishment\n        if (connectionString.empty()) {\n            throw std::runtime_error(\"Connection string is empty\");\n        }\n        // Assume connection is successful\n        isConnected = true;\n    }\n\n    // Destructor: Ensures the connection is closed\n    ~DatabaseConnection() {\n        if (isConnected) {\n            std::cout << \"Disconnecting from database\" << std::endl;\n            // Simulate disconnection\n            isConnected = false;\n        }\n    }\n\n    // Execute a query (example method)\n    void executeQuery(const std::string& query) {\n        if (!isConnected) {\n            throw std::runtime_error(\"Database is not connected\");\n        }\n        std::cout << \"Executing query: \" << query << std::endl;\n        // Simulate query execution\n    }\n\n    // Copy constructor and assignment operator are deleted to prevent copying\n    DatabaseConnection(const DatabaseConnection&) = delete;\n    DatabaseConnection& operator=(const DatabaseConnection&) = delete;\n\nprivate:\n    bool isConnected = false; // Tracks connection status\n};\n\nint main() {\n    try {\n        DatabaseConnection db(\"example_connection_string\");\n        db.executeQuery(\"SELECT * FROM users\");\n    } catch (const std::exception& e) {\n        std::cerr << \"Error: \" << e.what() << std::endl;\n    }\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "def hasannotation(self,Class,set=None):\n        \"\"\"Returns an integer indicating whether such as annotation exists, and if so, how many.\n\n        See :meth:`AllowTokenAnnotation.annotations`` for a description of the parameters.\"\"\"\n        return sum( 1 for _ in self.select(Class,set,True,default_ignore_annotations))",
    "label": 0
  },
  {
    "codes": "```java\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Pointcut;\nimport org.springframework.stereotype.Component;\n\nimport com.netflix.hystrix.HystrixCommand;\nimport com.netflix.hystrix.HystrixCommandGroupKey;\n\n@Aspect\n@Component\npublic class CircuitBreakerAspect {\n\n    @Pointcut(\"@annotation(CircuitBreaker)\")\n    public void circuitBreakerMethods() {}\n\n    @Around(\"circuitBreakerMethods()\")\n    public Object wrapWithCircuitBreaker(ProceedingJoinPoint joinPoint) throws Throwable {\n        HystrixCommand<Object> command = new HystrixCommand<Object>(\n            HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(joinPoint.getTarget().getClass().getSimpleName()))) {\n\n            @Override\n            protected Object run() throws Exception {\n                try {\n                    return joinPoint.proceed();\n                } catch (Throwable throwable) {\n                    throw new Exception(throwable);\n                }\n            }\n\n            @Override\n            protected Object getFallback() {\n                return \"Fallback result\";\n            }\n        };\n\n        return command.execute();\n    }\n}\n```\n\n```java\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\n\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.METHOD)\npublic @interface CircuitBreaker {\n    // Annotation for marking methods\n}\n```\n\n```xml\n<!-- Add Hystrix dependency to pom.xml -->\n<dependency>\n    <groupId>com.netflix.hystrix</groupId>\n    <artifactId>hystrix-core</artifactId>\n    <version>1.5.18</version>\n</dependency>\n```\n\n```yaml\n# Application configuration in application.yml\nhystrix:\n  command:\n    default:\n      execution:\n        isolation:\n          thread:\n            timeoutInMilliseconds: 1000 # Timeout configuration\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"net/http\"\n)\n\n// RecoveryMiddleware is a middleware that recovers from panics in HTTP handlers.\nfunc RecoveryMiddleware(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tdefer func() {\n\t\t\tif rec := recover(); rec != nil {\n\t\t\t\thttp.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError)\n\t\t\t}\n\t\t}()\n\t\tnext.ServeHTTP(w, r)\n\t})\n}\n```",
    "label": 1
  },
  {
    "codes": "```kotlin\nsealed class DomainError {\n\n    // Represents a network-related error\n    object NetworkError : DomainError()\n\n    // Represents an error where data was not found\n    object NotFoundError : DomainError()\n\n    // Represents a validation error\n    data class ValidationError(val message: String) : DomainError()\n\n    // Represents an authentication error\n    object AuthenticationError : DomainError()\n\n    // Represents a server error\n    object ServerError : DomainError()\n\n    // Generic case for other errors\n    data class UnknownError(val message: String) : DomainError()\n}\n```",
    "label": 1
  },
  {
    "codes": "async function findWindowsPortRange() {\n  // First, check if we're running on XP.  If this initial command fails,\n  // we just fallback on the default IANA range.\n  let stdout = await execute('cmd.exe /c ver');\n  if (/Windows XP/.test(stdout)) {\n    // TODO: Try to read these values from the registry.\n    return {min: 1025, max: 5000};\n  } else {\n    stdout = await execute('netsh int ipv4 show dynamicport tcp');\n    /* > netsh int ipv4 show dynamicport tcp\n      Protocol tcp Dynamic Port Range\n      ---------------------------------\n      Start Port : 49152\n      Number of Ports : 16384\n     */\n    let range = stdout.split(/\\n/)\n        .filter((line) => /.*:\\s*\\d+/.test(line))\n        .map((line) => Number(line.split(/:\\s*/)[1]));\n    return {\n      min: range[0],\n      max: range[0] + range[1]\n    };\n  }\n}",
    "label": 0
  },
  {
    "codes": "```javascript\nconst jwt = require('jsonwebtoken');\n\n// Middleware to authenticate using JWT\nfunction authenticateJWT(req, res, next) {\n    const authHeader = req.headers.authorization;\n\n    if (authHeader) {\n        // Extract the token from the authorization header\n        const token = authHeader.split(' ')[1];\n        \n        // Verify the token using jwt.verify\n        jwt.verify(token, process.env.ACCESS_TOKEN_SECRET, (err, user) => {\n            if (err) {\n                // Send 403 if token verification fails\n                return res.sendStatus(403);\n            }\n            // Store user data in request object\n            req.user = user;\n            // Proceed to the next middleware/route\n            next();\n        });\n    } else {\n        // Send 401 if no token is provided\n        res.sendStatus(401);\n    }\n}\n\nmodule.exports = authenticateJWT;\n```\n",
    "label": 1
  },
  {
    "codes": "def delocate_wheel(in_wheel,\n                   out_wheel = None,\n                   lib_sdir = '.dylibs',\n                   lib_filt_func = None,\n                   copy_filt_func = filter_system_libs,\n                   require_archs = None,\n                   check_verbose = False,\n                  ):\n    \"\"\" Update wheel by copying required libraries to `lib_sdir` in wheel\n\n    Create `lib_sdir` in wheel tree only if we are copying one or more\n    libraries.\n\n    If `out_wheel` is None (the default), overwrite the wheel `in_wheel`\n    in-place.\n\n    Parameters\n    ----------\n    in_wheel : str\n        Filename of wheel to process\n    out_wheel : None or str\n        Filename of processed wheel to write.  If None, overwrite `in_wheel`\n    lib_sdir : str, optional\n        Subdirectory name in wheel package directory (or directories) to store\n        needed libraries.\n    lib_filt_func : None or str or callable, optional\n        If None, inspect all files for dependencies on dynamic libraries. If\n        callable, accepts filename as argument, returns True if we should\n        inspect the file, False otherwise. If str == \"dylibs-only\" then inspect\n        only files with known dynamic library extensions (``.dylib``, ``.so``).\n    copy_filt_func : None or callable, optional\n        If callable, called on each library name detected as a dependency; copy\n        where ``copy_filt_func(libname)`` is True, don't copy otherwise.\n        Default is callable rejecting only libraries beginning with\n        ``/usr/lib`` or ``/System``.  None means copy all libraries. This will\n        usually end up copying large parts of the system run-time.\n    require_archs : None or str or sequence, optional\n        If None, do no checks of architectures in libraries.  If sequence,\n        sequence of architectures (output from ``lipo -info``) that every\n        library in the wheels should have (e.g. ``['x86_64, 'i386']``). An\n        empty sequence results in checks that depended libraries have the same\n        archs as depending libraries.  If string, either \"intel\" (corresponds\n        to sequence ``['x86_64, 'i386']``) or name of required architecture\n        (e.g \"i386\" or \"x86_64\").\n    check_verbose : bool, optional\n        If True, print warning messages about missing required architectures\n\n    Returns\n    -------\n    copied_libs : dict\n        dict containing the (key, value) pairs of (``copied_lib_path``,\n        ``dependings_dict``), where ``copied_lib_path`` is a library real path\n        that was copied into `lib_sdir` of the wheel packages, and\n        ``dependings_dict`` is a dictionary with key, value pairs where the key\n        is a path in the wheel depending on ``copied_lib_path``, and the value\n        is the ``install_name`` of ``copied_lib_path`` in the depending\n        library. The filenames in the keys are relative to the wheel root path.\n    \"\"\"\n    if lib_filt_func == \"dylibs-only\":\n        lib_filt_func = _dylibs_only\n    in_wheel = abspath(in_wheel)\n    if out_wheel is None:\n        out_wheel = in_wheel\n    else:\n        out_wheel = abspath(out_wheel)\n    in_place = in_wheel == out_wheel\n    with TemporaryDirectory() as tmpdir:\n        all_copied = {}\n        wheel_dir = realpath(pjoin(tmpdir, 'wheel'))\n        zip2dir(in_wheel, wheel_dir)\n        for package_path in find_package_dirs(wheel_dir):\n            lib_path = pjoin(package_path, lib_sdir)\n            lib_path_exists = exists(lib_path)\n            copied_libs = delocate_path(package_path, lib_path,\n                                        lib_filt_func, copy_filt_func)\n            if copied_libs and lib_path_exists:\n                raise DelocationError(\n                    '{0} already exists in wheel but need to copy '\n                    '{1}'.format(lib_path, '; '.join(copied_libs)))\n            if len(os.listdir(lib_path)) == 0:\n                shutil.rmtree(lib_path)\n            # Check architectures\n            if not require_archs is None:\n                stop_fast = not check_verbose\n                bads = check_archs(copied_libs, require_archs, stop_fast)\n                if len(bads) != 0:\n                    if check_verbose:\n                        print(bads_report(bads, pjoin(tmpdir, 'wheel')))\n                    raise DelocationError(\n                        \"Some missing architectures in wheel\")\n            # Change install ids to be unique within Python space\n            install_id_root = (DLC_PREFIX +\n                               relpath(package_path, wheel_dir) +\n                               '/')\n            for lib in copied_libs:\n                lib_base = basename(lib)\n                copied_path = pjoin(lib_path, lib_base)\n                set_install_id(copied_path, install_id_root + lib_base)\n                validate_signature(copied_path)\n            _merge_lib_dict(all_copied, copied_libs)\n        if len(all_copied):\n            rewrite_record(wheel_dir)\n        if len(all_copied) or not in_place:\n            dir2zip(wheel_dir, out_wheel)\n    return stripped_lib_dict(all_copied, wheel_dir + os.path.sep)",
    "label": 0
  },
  {
    "codes": "```python\nimport psutil\nimport time\nimport argparse\n\ndef get_network_usage(interval=1):\n    \"\"\"Get network usage per process.\"\"\"\n    processes = {}\n    for proc in psutil.process_iter(['pid', 'name']):\n        try:\n            connections = proc.connections()\n            if connections:\n                processes[proc.pid] = {\n                    'name': proc.info['name'],\n                    'sent': 0,\n                    'recv': 0\n                }\n        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n            continue\n\n    # Get initial network I/O counters\n    io_counters = psutil.net_io_counters(pernic=True)\n    time.sleep(interval)\n\n    # Get updated network I/O counters\n    new_io_counters = psutil.net_io_counters(pernic=True)\n\n    # Calculate bandwidth usage\n    for pid, proc_info in processes.items():\n        try:\n            proc = psutil.Process(pid)\n            io = proc.io_counters()\n            new_io = psutil.Process(pid).io_counters()\n\n            sent = new_io.bytes_sent - io.bytes_sent\n            recv = new_io.bytes_recv - io.bytes_recv\n\n            processes[pid]['sent'] = sent\n            processes[pid]['recv'] = recv\n        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n            continue\n\n    return processes\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Monitor network bandwidth usage per process.\")\n    parser.add_argument('-i', '--interval', type=int, default=1, help=\"Interval in seconds to check bandwidth usage.\")\n    args = parser.parse_args()\n\n    while True:\n        processes = get_network_usage(args.interval)\n        print(\"\\nPID\\tName\\t\\tSent (bytes)\\tRecv (bytes)\")\n        print(\"=\"*40)\n        for pid, info in processes.items():\n            print(f\"{pid}\\t{info['name']}\\t{info['sent']}\\t\\t{info['recv']}\")\n        time.sleep(args.interval)\n\nif __name__ == \"__main__\":\n    main()\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"fmt\"\n)\n\n// Matrix type representing a 2D slice\ntype Matrix [][]int\n\n// Matrix multiplication\nfunc multiply(A, B Matrix) Matrix {\n\tn := len(A)\n\tresult := make(Matrix, n)\n\tfor i := range result {\n\t\tresult[i] = make([]int, n)\n\t\tfor j := range result[i] {\n\t\t\tfor k := range A[i] {\n\t\t\t\tresult[i][j] += A[i][k] * B[k][j]\n\t\t\t}\n\t\t}\n\t}\n\treturn result\n}\n\n// Matrix exponentiation using exponentiation by squaring\nfunc matrixExponentiation(mat Matrix, exp int) Matrix {\n\tn := len(mat)\n\t// Initialize the result as an identity matrix\n\tresult := make(Matrix, n)\n\tfor i := range result {\n\t\tresult[i] = make([]int, n)\n\t\tresult[i][i] = 1 // Identity matrix\n\t}\n\t\n\tfor exp > 0 {\n\t\tif exp%2 == 1 {\n\t\t\tresult = multiply(result, mat)\n\t\t}\n\t\tmat = multiply(mat, mat)\n\t\texp /= 2\n\t}\n\treturn result\n}\n\nfunc main() {\n\t// Example matrix\n\tmatrix := Matrix{\n\t\t{1, 1},\n\t\t{1, 0},\n\t}\n\texp := 5 // Exponent\n\n\tresult := matrixExponentiation(matrix, exp)\n\n\t// Print the result matrix\n\tfor _, row := range result {\n\t\tfmt.Println(row)\n\t}\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *InitiateVaultLockOutput) SetLockId(v string) *InitiateVaultLockOutput {\n\ts.LockId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Apache log file location\nLOG_FILE=\"/var/log/apache2/access.log\"\n# Time window in minutes to check for spikes\nTIME_WINDOW=5\n# Error threshold to detect a spike\nERROR_THRESHOLD=10\n\n# Get the current time in seconds since epoch\nCURRENT_TIME=$(date +%s)\n\n# Calculate the start time of the window to monitor\nSTART_TIME=$(($CURRENT_TIME - $TIME_WINDOW * 60))\n\n# Parse the log file, filtering based on the time window,\n# and count the occurrences of 5xx errors\nERROR_COUNT=$(awk -v start_time=\"$START_TIME\" '\n{\n    # Extract the timestamp and convert it to epoch time\n    # Assuming the log format has this structure [21/Oct/2023:16:05:49 +0000]\n    match($0, /\\[([0-9]{2})\\/([a-zA-Z]{3})\\/([0-9]{4}):([0-9]{2}):([0-9]{2}):([0-9]{2})/, time)\n    # Create a formatted timestamp string for \"date\" to convert\n    datetime = time[3] \" \" time[2] \" \" time[1] \" \" time[4] \":\" time[5] \":\" time[6]\n    # Convert to epoch time\n    cmd = \"date -d\\\"\" datetime \"\\\" +%s\"\n    cmd | getline log_time\n    close(cmd)\n\n    # Check if the log entry is within the time window\n    if (log_time >= start_time) {\n        # Extract the status code (assuming it is the 9th field in common log format)\n        status_code = $9\n        # Check if it's a 5xx error\n        if (status_code ~ /^5[0-9][0-9]$/) {\n            count++\n        }\n    }\n}\n\nEND {\n    print count\n}' \"$LOG_FILE\")\n\n# Check if the error count exceeds the threshold\nif [ \"$ERROR_COUNT\" -gt \"$ERROR_THRESHOLD\" ]; then\n    echo \"Spike detected: $ERROR_COUNT 5xx errors in the last $TIME_WINDOW minutes.\"\nelse\n    echo \"No spike: $ERROR_COUNT 5xx errors in the last $TIME_WINDOW minutes.\"\nfi\n```\n\nThis script checks Apache logs for 5xx errors in a specified time window and reports spikes based on a threshold. Adjust `LOG_FILE`, `TIME_WINDOW`, and `ERROR_THRESHOLD` as needed.",
    "label": 1
  },
  {
    "codes": "```rust\nuse oauth2::{Client, TokenResponse, ClientId, ClientSecret, AuthUrl, TokenUrl, Scope};\nuse oauth2::basic::BasicClient;\nuse oauth2::reqwest::async_http_client;\nuse url::Url;\n\n#[tokio::main]\nasync fn main() {\n    // Define OAuth2 endpoints\n    let auth_url = AuthUrl::new(\"https://auth.example.com/oauth/authorize\".to_string())\n        .expect(\"Invalid authorization endpoint URL\");\n    let token_url = TokenUrl::new(\"https://auth.example.com/oauth/token\".to_string())\n        .expect(\"Invalid token endpoint URL\");\n\n    // Create OAuth2 client\n    let client = BasicClient::new(\n        ClientId::new(\"client_id\".to_string()),\n        Some(ClientSecret::new(\"client_secret\".to_string())),\n        auth_url,\n        Some(token_url),\n    );\n\n    // Request token using client credentials flow\n    let token_result = client\n        .exchange_client_credentials()\n        .add_scope(Scope::new(\"read\".to_string()))\n        .request_async(async_http_client)\n        .await;\n\n    // Handle token response\n    match token_result {\n        Ok(token) => {\n            println!(\"Access Token: {}\", token.access_token().secret());\n            println!(\"Expires in: {:?}\", token.expires_in());\n        }\n        Err(e) => {\n            eprintln!(\"Error fetching token: {:?}\", e);\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<ServiceResponse<Page<VaultInner>>> listBySubscriptionWithServiceResponseAsync(final Integer top) {\n        return listBySubscriptionSinglePageAsync(top)\n            .concatMap(new Func1<ServiceResponse<Page<VaultInner>>, Observable<ServiceResponse<Page<VaultInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<VaultInner>>> call(ServiceResponse<Page<VaultInner>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(listBySubscriptionNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def start(self):\n        \"\"\"\n        Initiate the session by starting to execute the command with the peer.\n\n        :return: The :attr:`~.xso.Command.first_payload` of the response\n\n        This sends an empty command IQ request with the\n        :attr:`~.ActionType.EXECUTE` action.\n\n        The :attr:`status`, :attr:`response` and related attributes get updated\n        with the newly received values.\n        \"\"\"\n\n        if self._response is not None:\n            raise RuntimeError(\"command execution already started\")\n\n        request = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=self._peer_jid,\n            payload=adhoc_xso.Command(self._command_name),\n        )\n\n        self._response = yield from self._stream.send_iq_and_wait_for_reply(\n            request,\n        )\n\n        return self._response.first_payload",
    "label": 0
  },
  {
    "codes": "```java\nimport org.apache.pdfbox.multipdf.PDFMergerUtility;\nimport org.apache.pdfbox.pdmodel.PDDocument;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.List;\n\npublic class PDFMerger {\n\n    /**\n     * Merges a list of PDF files into a single PDF document.\n     *\n     * @param inputFiles List of files to be merged.\n     * @param outputFile The file where the merged PDF will be saved.\n     * @throws IOException If an error occurs during file operations.\n     */\n    public static void mergePDFs(List<File> inputFiles, File outputFile) throws IOException {\n        // Create PDFMergerUtility instance\n        PDFMergerUtility pdfMerger = new PDFMergerUtility();\n        \n        // Add input PDF files to the merger utility\n        for (File inputFile : inputFiles) {\n            pdfMerger.addSource(inputFile);\n        }\n        \n        // Set the destination file for the merged document\n        pdfMerger.setDestinationFileName(outputFile.getAbsolutePath());\n        \n        // Merge the documents\n        pdfMerger.mergeDocuments(null);\n    }\n\n    // Example usage\n    public static void main(String[] args) {\n        // List of input PDF files to be merged\n        List<File> inputFiles = List.of(new File(\"document1.pdf\"), new File(\"document2.pdf\"));\n        \n        // Output file for the merged document\n        File outputFile = new File(\"mergedDocument.pdf\");\n        \n        try {\n            // Call mergePDFs utility method\n            mergePDFs(inputFiles, outputFile);\n            System.out.println(\"PDF documents merged successfully!\");\n        } catch (IOException e) {\n            e.printStackTrace();\n            System.err.println(\"An error occurred while merging PDF documents.\");\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "public Observable<StorageBundle> setStorageAccountAsync(String vaultBaseUrl, String storageAccountName, String resourceId, String activeKeyName, boolean autoRegenerateKey) {\n        return setStorageAccountWithServiceResponseAsync(vaultBaseUrl, storageAccountName, resourceId, activeKeyName, autoRegenerateKey).map(new Func1<ServiceResponse<StorageBundle>, StorageBundle>() {\n            @Override\n            public StorageBundle call(ServiceResponse<StorageBundle> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```powershell\n# Define the directory containing the files you want to rename\n$directoryPath = \"C:\\Path\\To\\Directory\"\n\n# Define the regular expression pattern to match files\n$regexPattern = '^oldname(\\d+)\\.txt$' # Example pattern\n\n# Define the replacement pattern; use $1, $2, ... for captured groups from the regex\n$replacementPattern = 'newname$1.txt'\n\n# Get all files in the directory\nGet-ChildItem -Path $directoryPath | ForEach-Object {\n    # Check if the file name matches the regex pattern\n    if ($_ -match $regexPattern) {\n        # Construct the new file name using the replacement pattern\n        $newFileName = [regex]::Replace($_.Name, $regexPattern, $replacementPattern)\n\n        # Construct the full path for the old and new file names\n        $oldFilePath = Join-Path -Path $directoryPath -ChildPath $_.Name\n        $newFilePath = Join-Path -Path $directoryPath -ChildPath $newFileName\n\n        # Rename the file\n        Rename-Item -Path $oldFilePath -NewName $newFileName\n\n        # Output the old and new file names for verification\n        Write-Host \"Renamed '$($_.Name)' to '$newFileName'\"\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport uuid\n\ndef generate_uuid_v4():\n    # Generate a UUID v4 and remove hyphens\n    return str(uuid.uuid4()).replace('-', '')\n```",
    "label": 1
  },
  {
    "codes": "```rust\nuse std::collections::HashMap;\n\n// Function to calculate the nth Fibonacci number using memoization\nfn fibonacci(n: u32, memo: &mut HashMap<u32, u64>) -> u64 {\n    // Check if the result is already in the memoization map\n    if let Some(&result) = memo.get(&n) {\n        return result;\n    }\n\n    // Base cases\n    if n == 0 {\n        return 0;\n    }\n    if n == 1 {\n        return 1;\n    }\n\n    // Recursive case with memoization\n    let result = fibonacci(n - 1, memo) + fibonacci(n - 2, memo);\n    memo.insert(n, result); // Store the result in the memoization map\n    result\n}\n\n// Example usage\nfn main() {\n    let mut memo = HashMap::new();\n    let n = 10;\n    println!(\"Fibonacci({}) = {}\", n, fibonacci(n, &mut memo));\n}\n```",
    "label": 1
  },
  {
    "codes": "def XYZ_to_Lab(cobj, *args, **kwargs):\n    \"\"\"\n    Converts XYZ to Lab.\n    \"\"\"\n    illum = cobj.get_illuminant_xyz()\n    temp_x = cobj.xyz_x / illum[\"X\"]\n    temp_y = cobj.xyz_y / illum[\"Y\"]\n    temp_z = cobj.xyz_z / illum[\"Z\"]\n\n    if temp_x > color_constants.CIE_E:\n        temp_x = math.pow(temp_x, (1.0 / 3.0))\n    else:\n        temp_x = (7.787 * temp_x) + (16.0 / 116.0)\n\n    if temp_y > color_constants.CIE_E:\n        temp_y = math.pow(temp_y, (1.0 / 3.0))\n    else:\n        temp_y = (7.787 * temp_y) + (16.0 / 116.0)\n\n    if temp_z > color_constants.CIE_E:\n        temp_z = math.pow(temp_z, (1.0 / 3.0))\n    else:\n        temp_z = (7.787 * temp_z) + (16.0 / 116.0)\n\n    lab_l = (116.0 * temp_y) - 16.0\n    lab_a = 500.0 * (temp_x - temp_y)\n    lab_b = 200.0 * (temp_y - temp_z)\n    return LabColor(\n        lab_l, lab_a, lab_b, observer=cobj.observer, illuminant=cobj.illuminant)",
    "label": 0
  },
  {
    "codes": "function isEmptyValue(val, axisType) {\n    return axisType === 'category'\n        ? val == null\n        : (val == null || isNaN(val)); // axisType === 'value'\n}",
    "label": 0
  },
  {
    "codes": "func (s *CompleteMultipartUploadInput) SetAccountId(v string) *CompleteMultipartUploadInput {\n\ts.AccountId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```java\n// MainActivity.java\npackage com.example.objectdetection;\n\nimport android.Manifest;\nimport android.content.pm.PackageManager;\nimport android.graphics.Bitmap;\nimport android.graphics.BitmapFactory;\nimport android.os.Bundle;\nimport android.util.Log;\nimport android.widget.ImageView;\nimport android.widget.TextView;\n\nimport androidx.annotation.NonNull;\nimport androidx.appcompat.app.AppCompatActivity;\nimport androidx.core.app.ActivityCompat;\nimport androidx.core.content.ContextCompat;\n\nimport org.tensorflow.lite.Interpreter;\nimport org.tensorflow.lite.support.common.FileUtil;\nimport org.tensorflow.lite.support.image.TensorImage;\nimport org.tensorflow.lite.support.label.Category;\nimport org.tensorflow.lite.support.model.Model;\n\nimport java.io.IOException;\nimport java.nio.MappedByteBuffer;\nimport java.util.List;\n\npublic class MainActivity extends AppCompatActivity {\n\n    private static final int CAMERA_PERMISSION_CODE = 100;\n    private Interpreter interpreter;\n    private ImageView imageView;\n    private TextView resultTextView;\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        imageView = findViewById(R.id.imageView);\n        resultTextView = findViewById(R.id.resultTextView);\n\n        // Check and request camera permission\n        if (ContextCompat.checkSelfPermission(this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {\n            ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.CAMERA}, CAMERA_PERMISSION_CODE);\n        } else {\n            initializeModel();\n        }\n    }\n\n    private void initializeModel() {\n        try {\n            // Load the TensorFlow Lite model\n            MappedByteBuffer modelBuffer = FileUtil.loadMappedFile(this, \"model.tflite\");\n            interpreter = new Interpreter(modelBuffer);\n\n            // Load and process the image\n            Bitmap bitmap = BitmapFactory.decodeResource(getResources(), R.drawable.test_image);\n            TensorImage tensorImage = new TensorImage();\n            tensorImage.load(bitmap);\n\n            // Run inference\n            float[][] output = new float[1][10]; // Adjust output size based on your model\n            interpreter.run(tensorImage.getBuffer(), output);\n\n            // Display results\n            List<Category> results = TensorImage.fromTensorBuffer(output[0]);\n            StringBuilder resultText = new StringBuilder();\n            for (Category category : results) {\n                resultText.append(category.getLabel()).append(\": \").append(category.getScore()).append(\"\\n\");\n            }\n            resultTextView.setText(resultText.toString());\n\n        } catch (IOException e) {\n            Log.e(\"MainActivity\", \"Error loading model\", e);\n        }\n    }\n\n    @Override\n    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults);\n        if (requestCode == CAMERA_PERMISSION_CODE && grantResults.length > 0 && grantResults[0] == PackageManager.PERMISSION_GRANTED) {\n            initializeModel();\n        } else {\n            Log.e(\"MainActivity\", \"Camera permission denied\");\n        }\n    }\n\n    @Override\n    protected void onDestroy() {\n        super.onDestroy();\n        if (interpreter != null) {\n            interpreter.close();\n        }\n    }\n}\n```\n\n```xml\n<!-- activity_main.xml -->\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n    <ImageView\n        android:id=\"@+id/imageView\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:layout_centerHorizontal=\"true\"\n        android:src=\"@drawable/test_image\" />\n\n    <TextView\n        android:id=\"@+id/resultTextView\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:layout_below=\"@id/imageView\"\n        android:layout_marginTop=\"16dp\"\n        android:text=\"Results will be shown here\" />\n</RelativeLayout>\n```\n\n```xml\n<!-- AndroidManifest.xml -->\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.example.objectdetection\">\n\n    <uses-permission android:name=\"android.permission.CAMERA\" />\n\n    <application\n        android:allowBackup=\"true\"\n        android:icon=\"@mipmap/ic_launcher\"\n        android:label=\"@string/app_name\"\n        android:roundIcon=\"@mipmap/ic_launcher_round\"\n        android:supportsRtl=\"true\"\n        android:theme=\"@style/Theme.ObjectDetection\">\n        <activity android:name=\".MainActivity\">\n            <intent-filter>\n                <action android:name=\"android.intent.action.MAIN\" />\n                <category android:name=\"android.intent.category.LAUNCHER\" />\n            </intent-filter>\n        </activity>\n    </application>\n</manifest>\n```\n\n```gradle\n// build.gradle (Module: app)\ndependencies {\n    implementation 'org.tensorflow:tensorflow-lite:2.8.0'\n    implementation 'org.tensorflow:tensorflow-lite-support:0.3.1'\n}\n```\n\n```gradle\n// build.gradle (Project: ObjectDetection)\nallprojects {\n    repositories {\n        google()\n        mavenCentral()\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport random\n\n# Define Character class\nclass Character:\n    def __init__(self, name, health, element, strength):\n        self.name = name\n        self.health = health\n        self.element = element\n        self.strength = strength\n\n# Function to calculate elemental weakness multiplier\ndef calculate_weakness(defender_element, attack_element):\n    weaknesses = {\n        'fire': 'water',\n        'water': 'electric',\n        'electric': 'earth',\n        'earth': 'fire'\n    }\n    \n    return 2 if weaknesses[defender_element] == attack_element else 1\n\n# Implement the combat system\ndef attack(attacker, defender):\n    weakness_multiplier = calculate_weakness(defender.element, attacker.element)\n    damage = attacker.strength * weakness_multiplier\n    defender.health -= damage\n    return f\"{attacker.name} attacks {defender.name} for {damage} damage!\"\n\n# Run a simple combat scenario\ndef main():\n    player = Character(name='Hero', health=100, element='fire', strength=10)\n    enemy = Character(name='Villain', health=50, element='earth', strength=8)\n    \n    print(f\"{player.name} (Element: {player.element}, Health: {player.health})\")\n    print(f\"VS\")\n    print(f\"{enemy.name} (Element: {enemy.element}, Health: {enemy.health})\\n\")\n    \n    while player.health > 0 and enemy.health > 0:\n        print(attack(player, enemy))\n        if enemy.health <= 0:\n            print(f\"{enemy.name} has been defeated!\")\n            break\n        print(attack(enemy, player))\n        if player.health <= 0:\n            print(f\"{player.name} has been defeated!\")\n            break\n        print(f\"\\n{player.name} Health: {player.health}, {enemy.name} Health: {enemy.health}\\n\")\n\nif __name__ == \"__main__\":\n    main()\n```",
    "label": 1
  },
  {
    "codes": "func (s *DevEndpoint) SetPublicAddress(v string) *DevEndpoint {\n\ts.PublicAddress = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```cpp\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <fstream>\n#include <vector>\n#include <iostream>\n\n// Function to encrypt data using AES-256-CBC\nbool encryptFile(const std::string& inputFile, const std::string& outputFile, const std::vector<unsigned char>& key) {\n    // Generate a random IV (Initialization Vector)\n    unsigned char iv[EVP_MAX_IV_LENGTH];\n    if (!RAND_bytes(iv, EVP_MAX_IV_LENGTH)) {\n        std::cerr << \"Error generating IV.\" << std::endl;\n        return false;\n    }\n\n    // Open input and output files\n    std::ifstream inFile(inputFile, std::ios::binary);\n    std::ofstream outFile(outputFile, std::ios::binary);\n    if (!inFile || !outFile) {\n        std::cerr << \"Error opening files.\" << std::endl;\n        return false;\n    }\n\n    // Write the IV to the output file\n    outFile.write(reinterpret_cast<char*>(iv), EVP_MAX_IV_LENGTH);\n\n    // Initialize the encryption context\n    EVP_CIPHER_CTX* ctx = EVP_CIPHER_CTX_new();\n    if (!ctx) {\n        std::cerr << \"Error creating encryption context.\" << std::endl;\n        return false;\n    }\n\n    // Initialize encryption operation\n    if (1 != EVP_EncryptInit_ex(ctx, EVP_aes_256_cbc(), nullptr, key.data(), iv)) {\n        std::cerr << \"Error initializing encryption.\" << std::endl;\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n\n    // Buffer for reading and encrypting data\n    std::vector<unsigned char> inBuf(1024);\n    std::vector<unsigned char> outBuf(1024 + EVP_MAX_BLOCK_LENGTH);\n    int outLen;\n\n    // Encrypt the file data\n    while (inFile) {\n        inFile.read(reinterpret_cast<char*>(inBuf.data()), inBuf.size());\n        int inLen = inFile.gcount();\n\n        if (1 != EVP_EncryptUpdate(ctx, outBuf.data(), &outLen, inBuf.data(), inLen)) {\n            std::cerr << \"Error encrypting data.\" << std::endl;\n            EVP_CIPHER_CTX_free(ctx);\n            return false;\n        }\n\n        outFile.write(reinterpret_cast<char*>(outBuf.data()), outLen);\n    }\n\n    // Finalize encryption\n    if (1 != EVP_EncryptFinal_ex(ctx, outBuf.data(), &outLen)) {\n        std::cerr << \"Error finalizing encryption.\" << std::endl;\n        EVP_CIPHER_CTX_free(ctx);\n        return false;\n    }\n\n    outFile.write(reinterpret_cast<char*>(outBuf.data()), outLen);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n    return true;\n}\n\nint main() {\n    // Example key (32 bytes for AES-256)\n    std::vector<unsigned char> key = {\n        0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,\n        0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,\n        0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17,\n        0x18, 0x19, 0x1A, 0x1B, 0x1C, 0x1D, 0x1E, 0x1F\n    };\n\n    // Encrypt the file\n    if (encryptFile(\"input.txt\", \"output.enc\", key)) {\n        std::cout << \"File encrypted successfully.\" << std::endl;\n    } else {\n        std::cerr << \"File encryption failed.\" << std::endl;\n    }\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```lua\n-- Destructible Terrain Script for Roblox\n\n-- Function to create destructible part\nlocal function createDestructiblePart(position, size)\n    local part = Instance.new(\"Part\")\n    part.Size = size\n    part.Position = position\n    part.Anchored = false\n    part.Material = Enum.Material.Grass\n    part.BrickColor = BrickColor.new(\"Earth green\")\n    part.Parent = workspace\n    part.Name = \"DestructibleTerrain\"\n\n    local surfaceGui = Instance.new(\"SurfaceGui\")\n    surfaceGui.Parent = part\n    surfaceGui.Face = Enum.NormalId.Top\n\n    local frame = Instance.new(\"Frame\")\n    frame.Size = UDim2.new(1, 0, 1, 0)\n    frame.BackgroundTransparency = 0.7\n    frame.BackgroundColor3 = Color3.new(0, 1, 0)\n    frame.Parent = surfaceGui\n\n    return part\nend\n\n-- Event to handle destruction\nlocal function onDestruction(part)\n    -- Particle effect on destruction\n    local destructionEffect = Instance.new(\"ParticleEmitter\")\n    destructionEffect.Texture = \"rbxassetid://24221849\"\n    destructionEffect.Lifetime = NumberRange.new(0.5, 1)\n    destructionEffect.Rate = 100\n    destructionEffect.Parent = part\n\n    -- Remove part after effect is played\n    wait(0.5)\n    part:Destroy()\nend\n\n-- Function to monitor collisions with the terrain\nlocal function monitorDestruction()\n    for _, part in ipairs(workspace:GetChildren()) do\n        if part.Name == \"DestructibleTerrain\" then\n            part.Touched:Connect(function(otherPart)\n                if otherPart:IsA(\"Projectile\") or otherPart:IsA(\"Tool\") then\n                    onDestruction(part)\n                end\n            end)\n        end\n    end\nend\n\n-- Main function to generate destructible terrain\nlocal function generateDestructibleTerrain()\n    for x = -50, 50, 10 do\n        for z = -50, 50, 10 do\n            createDestructiblePart(Vector3.new(x, 5, z), Vector3.new(10, 10, 10))\n        end\n    end\n    monitorDestruction()\nend\n\n-- Start the terrain generation\ngenerateDestructibleTerrain()\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport os\nimport tempfile\nfrom contextlib import contextmanager\n\n@contextmanager\ndef atomic_write(file_path, mode='w', **kwargs):\n    # Create a temporary file with .tmp suffix\n    tmp_file_path = f\"{file_path}.tmp\"\n    try:\n        # Open the temporary file\n        with open(tmp_file_path, mode, **kwargs) as tmp_file:\n            yield tmp_file  # Yield the file object to the caller\n        # If no exceptions, rename the temporary file to the target file\n        os.replace(tmp_file_path, file_path)\n    except Exception:\n        # If an exception occurs, remove the temporary file\n        if os.path.exists(tmp_file_path):\n            os.remove(tmp_file_path)\n        raise  # Re-raise the exception\n```",
    "label": 1
  },
  {
    "codes": "public Observable<Page<ClusterInner>> listAsync() {\n        return listWithServiceResponseAsync().map(new Func1<ServiceResponse<List<ClusterInner>>, Page<ClusterInner>>() {\n            @Override\n            public Page<ClusterInner> call(ServiceResponse<List<ClusterInner>> response) {\n                PageImpl<ClusterInner> page = new PageImpl<>();\n                page.setItems(response.body());\n                return page;\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<Page<PolicySetDefinitionInner>> listByManagementGroupNextAsync(final String nextPageLink) {\n        return listByManagementGroupNextWithServiceResponseAsync(nextPageLink)\n            .map(new Func1<ServiceResponse<Page<PolicySetDefinitionInner>>, Page<PolicySetDefinitionInner>>() {\n                @Override\n                public Page<PolicySetDefinitionInner> call(ServiceResponse<Page<PolicySetDefinitionInner>> response) {\n                    return response.body();\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```rust\n// Cargo.toml\n[dependencies]\nzip = { version = \"0.6\", features = [\"aes-crypto\"] }\n```\n\n```rust\n// src/lib.rs\nuse std::fs::File;\nuse std::io::{Read, Write};\nuse zip::ZipArchive;\nuse zip::write::SimpleFileOptions;\nuse zip::result::ZipResult;\n\n/// Reads a password-protected ZIP file and extracts its contents\npub fn read_zip_with_password(zip_path: &str, password: &str) -> ZipResult<()> {\n    let file = File::open(zip_path)?;\n    let mut archive = ZipArchive::new(file)?;\n\n    for i in 0..archive.len() {\n        let mut file = archive.by_index_decrypt(i, password.as_bytes())??;\n        let mut contents = String::new();\n        file.read_to_string(&mut contents)?;\n        println!(\"File {}: {}\", i, contents);\n    }\n\n    Ok(())\n}\n\n/// Writes files into a password-protected ZIP archive\npub fn write_zip_with_password(zip_path: &str, files: Vec<(&str, &str)>, password: &str) -> ZipResult<()> {\n    let file = File::create(zip_path)?;\n    let mut zip = zip::ZipWriter::new(file);\n\n    let options = SimpleFileOptions::default()\n        .compression_method(zip::CompressionMethod::Deflated)\n        .with_deprecated_encryption(password.as_bytes());\n\n    for (name, content) in files {\n        zip.start_file(name, options)?;\n        zip.write_all(content.as_bytes())?;\n    }\n\n    zip.finish()?;\n    Ok(())\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_zip_read_write() {\n        let zip_path = \"test.zip\";\n        let password = \"secret\";\n        let files = vec![(\"file1.txt\", \"Hello, World!\"), (\"file2.txt\", \"Rust is awesome!\")];\n\n        // Write files to a password-protected ZIP\n        write_zip_with_password(zip_path, files.clone(), password).unwrap();\n\n        // Read files from the password-protected ZIP\n        read_zip_with_password(zip_path, password).unwrap();\n\n        // Clean up\n        std::fs::remove_file(zip_path).unwrap();\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```lua\n-- Define the terrain part\nlocal terrainPart = Instance.new(\"Part\")\nterrainPart.Size = Vector3.new(50, 5, 50)\nterrainPart.Position = Vector3.new(0, 2.5, 0)\nterrainPart.Anchored = true\nterrainPart.Material = Enum.Material.Grass\nterrainPart.Parent = workspace\n\n-- Function to handle destruction\nlocal function destroyTerrain(hit)\n    if hit == terrainPart then\n        local explosion = Instance.new(\"Explosion\")\n        explosion.Position = hit.Position\n        explosion.BlastRadius = 10\n        explosion.BlastPressure = 100000\n        explosion.Parent = workspace\n        terrainPart:Destroy()\n    end\nend\n\n-- Connect the function to the Touched event\nterrainPart.Touched:Connect(destroyTerrain)\n```\n\n```lua\n-- Alternative method using a grid of smaller parts\nlocal gridSize = 10\nlocal partSize = 5\n\nfor x = 1, gridSize do\n    for z = 1, gridSize do\n        local part = Instance.new(\"Part\")\n        part.Size = Vector3.new(partSize, 5, partSize)\n        part.Position = Vector3.new(x * partSize - partSize / 2, 2.5, z * partSize - partSize / 2)\n        part.Anchored = true\n        part.Material = Enum.Material.Grass\n        part.Parent = workspace\n\n        part.Touched:Connect(function(hit)\n            if hit and hit.Parent then\n                part:Destroy()\n            end\n        end)\n    end\nend\n```\n\n```lua\n-- Another alternative using a custom destruction effect\nlocal terrainPart = Instance.new(\"Part\")\nterrainPart.Size = Vector3.new(50, 5, 50)\nterrainPart.Position = Vector3.new(0, 2.5, 0)\nterrainPart.Anchored = true\nterrainPart.Material = Enum.Material.Grass\nterrainPart.Parent = workspace\n\nlocal function destroyTerrain(hit)\n    if hit == terrainPart then\n        local debris = Instance.new(\"Part\")\n        debris.Size = Vector3.new(5, 5, 5)\n        debris.Position = hit.Position\n        debris.Velocity = Vector3.new(math.random(-50, 50), math.random(50, 100), math.random(-50, 50))\n        debris.Material = Enum.Material.Grass\n        debris.Parent = workspace\n        terrainPart:Destroy()\n    end\nend\n\nterrainPart.Touched:Connect(destroyTerrain)\n```",
    "label": 1
  },
  {
    "codes": "def setimdi(self, node): #OBSOLETE\n        \"\"\"OBSOLETE\"\"\"\n        ns = {'imdi': 'http://www.mpi.nl/IMDI/Schema/IMDI'}\n        self.metadatatype = MetaDataType.IMDI\n        if LXE:\n            self.metadata = ElementTree.tostring(node, xml_declaration=False, pretty_print=True, encoding='utf-8')\n        else:\n            self.metadata = ElementTree.tostring(node, encoding='utf-8')\n        n = node.xpath('imdi:Session/imdi:Title', namespaces=ns)\n        if n and n[0].text: self._title = n[0].text\n        n = node.xpath('imdi:Session/imdi:Date', namespaces=ns)\n        if n and n[0].text: self._date = n[0].text\n        n = node.xpath('//imdi:Source/imdi:Access/imdi:Publisher', namespaces=ns)\n        if n and n[0].text: self._publisher = n[0].text\n        n = node.xpath('//imdi:Source/imdi:Access/imdi:Availability', namespaces=ns)\n        if n and n[0].text: self._license = n[0].text\n        n = node.xpath('//imdi:Languages/imdi:Language/imdi:ID', namespaces=ns)\n        if n and n[0].text: self._language = n[0].text",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<UsageInner>>> listWebWorkerUsagesWithServiceResponseAsync(final String resourceGroupName, final String name, final String workerPoolName) {\n        return listWebWorkerUsagesSinglePageAsync(resourceGroupName, name, workerPoolName)\n            .concatMap(new Func1<ServiceResponse<Page<UsageInner>>, Observable<ServiceResponse<Page<UsageInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<UsageInner>>> call(ServiceResponse<Page<UsageInner>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(listWebWorkerUsagesNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def _set_all_axis_color(self, axis, color):\n        \"\"\"Set axis ticks, title, labels to given color\"\"\"\n        for prop in ['ticks', 'axis', 'major_ticks', 'minor_ticks', 'title',\n                     'labels']:\n            prop_set = getattr(axis.properties, prop)\n            if color and prop in ['title', 'labels']:\n                prop_set.fill = ValueRef(value=color)\n            elif color and prop in ['axis', 'major_ticks', 'minor_ticks',\n                                    'ticks']:\n                prop_set.stroke = ValueRef(value=color)",
    "label": 0
  },
  {
    "codes": "def assert_key_has_value(self, key, caller):\n        \"\"\"Assert that context contains key which also has a value.\n\n        Args:\n            key: validate this key exists in context AND has a value that isn't\n                 None.\n            caller: string. calling function name - this used to construct\n                    error messages\n\n        Raises:\n            KeyNotInContextError: Key doesn't exist\n            KeyInContextHasNoValueError: context[key] is None\n            AssertionError: if key is None\n\n        \"\"\"\n        assert key, (\"key parameter must be specified.\")\n        self.assert_key_exists(key, caller)\n\n        if self[key] is None:\n            raise KeyInContextHasNoValueError(\n                f\"context['{key}'] must have a value for {caller}.\")",
    "label": 0
  },
  {
    "codes": "def node_definitions(id_fetcher, type_resolver=None, id_resolver=None):\n    '''\n    Given a function to map from an ID to an underlying object, and a function\n    to map from an underlying object to the concrete GraphQLObjectType it\n    corresponds to, constructs a `Node` interface that objects can implement,\n    and a field config for a `node` root field.\n\n    If the type_resolver is omitted, object resolution on the interface will be\n    handled with the `isTypeOf` method on object types, as with any GraphQL\n    interface without a provided `resolveType` method.\n    '''\n    node_interface = GraphQLInterfaceType(\n        'Node',\n        description='An object with an ID',\n        fields=lambda: OrderedDict((\n            ('id', GraphQLField(\n                GraphQLNonNull(GraphQLID),\n                description='The id of the object.',\n                resolver=id_resolver,\n            )),\n        )),\n        resolve_type=type_resolver\n    )\n    node_field = GraphQLField(\n        node_interface,\n        description='Fetches an object given its ID',\n        args=OrderedDict((\n            ('id', GraphQLArgument(\n                GraphQLNonNull(GraphQLID),\n                description='The ID of an object'\n            )),\n        )),\n        resolver=lambda obj, args, *_: id_fetcher(args.get('id'), *_)\n    )\n    return node_interface, node_field",
    "label": 0
  },
  {
    "codes": "func (c *Glue) GetCrawlerMetricsPagesWithContext(ctx aws.Context, input *GetCrawlerMetricsInput, fn func(*GetCrawlerMetricsOutput, bool) bool, opts ...request.Option) error {\n\tp := request.Pagination{\n\t\tNewRequest: func() (*request.Request, error) {\n\t\t\tvar inCpy *GetCrawlerMetricsInput\n\t\t\tif input != nil {\n\t\t\t\ttmp := *input\n\t\t\t\tinCpy = &tmp\n\t\t\t}\n\t\t\treq, _ := c.GetCrawlerMetricsRequest(inCpy)\n\t\t\treq.SetContext(ctx)\n\t\t\treq.ApplyOptions(opts...)\n\t\t\treturn req, nil\n\t\t},\n\t}\n\n\tcont := true\n\tfor p.Next() && cont {\n\t\tcont = fn(p.Page().(*GetCrawlerMetricsOutput), !p.HasNextPage())\n\t}\n\treturn p.Err()\n}",
    "label": 0
  },
  {
    "codes": "```python\nfrom locust import HttpUser, task, between\n\nclass ApiUser(HttpUser):\n    wait_time = between(1, 3)  # Wait time between requests\n\n    @task\n    def get_items(self):\n        response = self.client.get(\"/api/items\")\n        if response.status_code != 200:\n            print(f\"Error: {response.status_code}\")\n\n    @task\n    def create_item(self):\n        response = self.client.post(\"/api/items\", json={\"name\": \"NewItem\"})\n        if response.status_code != 201:\n            print(f\"Error: {response.status_code}\")\n\n    @task\n    def update_item(self):\n        response = self.client.put(\"/api/items/1\", json={\"name\": \"UpdatedItem\"})\n        if response.status_code != 200:\n            print(f\"Error: {response.status_code}\")\n\n    @task\n    def delete_item(self):\n        response = self.client.delete(\"/api/items/1\")\n        if response.status_code != 204:\n            print(f\"Error: {response.status_code}\")\n```",
    "label": 1
  },
  {
    "codes": "def make_time(h=0, m=0, s=0, ms=0, frames=None, fps=None):\n    \"\"\"\n    Convert time to milliseconds.\n\n    See :func:`pysubs2.time.times_to_ms()`. When both frames and fps are specified,\n    :func:`pysubs2.time.frames_to_ms()` is called instead.\n\n    Raises:\n        ValueError: Invalid fps, or one of frames/fps is missing.\n\n    Example:\n        >>> make_time(s=1.5)\n        1500\n        >>> make_time(frames=50, fps=25)\n        2000\n\n    \"\"\"\n    if frames is None and fps is None:\n        return times_to_ms(h, m, s, ms)\n    elif frames is not None and fps is not None:\n        return frames_to_ms(frames, fps)\n    else:\n        raise ValueError(\"Both fps and frames must be specified\")",
    "label": 0
  },
  {
    "codes": "```lua\n-- Destructible Terrain Script for Roblox\n\n-- Create a part to represent the terrain\nlocal terrain = Instance.new(\"Part\")\nterrain.Size = Vector3.new(50, 5, 50)\nterrain.Position = Vector3.new(0, 2.5, 0)\nterrain.Anchored = true\nterrain.CanCollide = true\nterrain.Parent = workspace\n\n-- Function to handle terrain destruction\nlocal function destroyTerrain(hit)\n    -- Check if the hit object is a tool or projectile\n    if hit:IsA(\"Tool\") or hit:IsA(\"BasePart\") then\n        -- Create a hole in the terrain\n        local hole = Instance.new(\"Part\")\n        hole.Size = Vector3.new(5, 5, 5)\n        hole.Position = hit.Position\n        hole.Anchored = true\n        hole.CanCollide = false\n        hole.Transparency = 1\n        hole.Parent = workspace\n\n        -- Remove the terrain part where the hole is\n        terrain:Destroy()\n    end\nend\n\n-- Connect the function to the terrain's Touched event\nterrain.Touched:Connect(destroyTerrain)\n```\n\nThis script creates a simple destructible terrain in Roblox. When a tool or projectile touches the terrain, it creates a hole by destroying the terrain part at the point of contact.",
    "label": 1
  },
  {
    "codes": "function getDateTimeTranslations(localeData) {\n  const dayNames = localeData.main(`dates/calendars/gregorian/days`);\n  const monthNames = localeData.main(`dates/calendars/gregorian/months`);\n  const erasNames = localeData.main(`dates/calendars/gregorian/eras`);\n  const dayPeriods = getDayPeriodsAmPm(localeData);\n\n  const dayPeriodsFormat = removeDuplicates([\n    objectValues(dayPeriods.format.narrow),\n    objectValues(dayPeriods.format.abbreviated),\n    objectValues(dayPeriods.format.wide)\n  ]);\n\n  const dayPeriodsStandalone = removeDuplicates([\n    objectValues(dayPeriods['stand-alone'].narrow),\n    objectValues(dayPeriods['stand-alone'].abbreviated),\n    objectValues(dayPeriods['stand-alone'].wide)\n  ]);\n\n  const daysFormat = removeDuplicates([\n    objectValues(dayNames.format.narrow),\n    objectValues(dayNames.format.abbreviated),\n    objectValues(dayNames.format.wide),\n    objectValues(dayNames.format.short)\n  ]);\n\n  const daysStandalone = removeDuplicates([\n    objectValues(dayNames['stand-alone'].narrow),\n    objectValues(dayNames['stand-alone'].abbreviated),\n    objectValues(dayNames['stand-alone'].wide),\n    objectValues(dayNames['stand-alone'].short)\n  ]);\n\n  const monthsFormat = removeDuplicates([\n    objectValues(monthNames.format.narrow),\n    objectValues(monthNames.format.abbreviated),\n    objectValues(monthNames.format.wide)\n  ]);\n\n  const monthsStandalone = removeDuplicates([\n    objectValues(monthNames['stand-alone'].narrow),\n    objectValues(monthNames['stand-alone'].abbreviated),\n    objectValues(monthNames['stand-alone'].wide)\n  ]);\n\n  const eras = removeDuplicates([\n    [erasNames.eraNarrow['0'], erasNames.eraNarrow['1']],\n    [erasNames.eraAbbr['0'], erasNames.eraAbbr['1']],\n    [erasNames.eraNames['0'], erasNames.eraNames['1']]\n  ]);\n\n  const dateTimeTranslations = [\n    ...removeDuplicates([dayPeriodsFormat, dayPeriodsStandalone]),\n    ...removeDuplicates([daysFormat, daysStandalone]),\n    ...removeDuplicates([monthsFormat, monthsStandalone]),\n    eras\n  ];\n\n  return dateTimeTranslations;\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<Void> failoverAllowDataLossAsync(String resourceGroupName, String serverName, String databaseName, String linkId, final ServiceCallback<Void> serviceCallback) {\n        return ServiceFuture.fromResponse(failoverAllowDataLossWithServiceResponseAsync(resourceGroupName, serverName, databaseName, linkId), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "function processVuexGetters (instance) {\n  const getters =\n    instance.$options.vuex &&\n    instance.$options.vuex.getters\n  if (getters) {\n    return Object.keys(getters).map(key => {\n      return {\n        type: 'vuex getters',\n        key,\n        value: instance[key]\n      }\n    })\n  } else {\n    return []\n  }\n}",
    "label": 0
  },
  {
    "codes": "function generateBaseCurrencies(localeData, addDigits) {\n  const currenciesData = localeData.main('numbers/currencies');\n  const fractions = new cldrJs('en').get(`supplemental/currencyData/fractions`);\n  const currencies = {};\n  Object.keys(currenciesData).forEach(key => {\n    let symbolsArray = [];\n    const symbol = currenciesData[key].symbol;\n    const symbolNarrow = currenciesData[key]['symbol-alt-narrow'];\n    if (symbol && symbol !== key) {\n      symbolsArray.push(symbol);\n    }\n    if (symbolNarrow && symbolNarrow !== symbol) {\n      if (symbolsArray.length > 0) {\n        symbolsArray.push(symbolNarrow);\n      } else {\n        symbolsArray = [undefined, symbolNarrow];\n      }\n    }\n    if (addDigits && fractions[key] && fractions[key]['_digits']) {\n      const digits = parseInt(fractions[key]['_digits'], 10);\n      if (symbolsArray.length === 2) {\n        symbolsArray.push(digits);\n      } else if (symbolsArray.length === 1) {\n        symbolsArray = [...symbolsArray, undefined, digits];\n      } else {\n        symbolsArray = [undefined, undefined, digits];\n      }\n    }\n    if (symbolsArray.length > 0) {\n      currencies[key] = symbolsArray;\n    }\n  });\n  return currencies;\n}",
    "label": 0
  },
  {
    "codes": "function( elem, classNames ) {\n\t\t\tjQuery.each((classNames || \"\").split(/\\s+/), function(i, className){\n\t\t\t\tif ( elem.nodeType == 1 && !jQuery.className.has( elem.className, className ) )\n\t\t\t\t\telem.className += (elem.className ? \" \" : \"\") + className;\n\t\t\t});\n\t\t}",
    "label": 0
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Collections.Concurrent;\nusing System.Threading;\nusing System.Threading.Tasks;\n\npublic class Program\n{\n    public static void Main()\n    {\n        // Create a cancellation token source\n        var cts = new CancellationTokenSource();\n        \n        // Create a collection to process\n        var items = new ConcurrentBag<int> { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\n\n        try\n        {\n            // Start a task to cancel after 2 seconds\n            Task.Run(() =>\n            {\n                Thread.Sleep(2000); // Simulate a delay\n                cts.Cancel(); // Cancel the operation\n            });\n\n            // Parallel.ForEach with cancellation token support\n            Parallel.ForEach(items, new ParallelOptions { CancellationToken = cts.Token }, item =>\n            {\n                // Simulate work\n                Thread.Sleep(500); // Simulate processing time\n                Console.WriteLine($\"Processed item: {item}\");\n            });\n        }\n        catch (OperationCanceledException)\n        {\n            Console.WriteLine(\"Operation was canceled.\");\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *CrawlerMetrics) SetMedianRuntimeSeconds(v float64) *CrawlerMetrics {\n\ts.MedianRuntimeSeconds = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<JobResponseInner> getJobAsync(String resourceGroupName, String resourceName, String jobId, final ServiceCallback<JobResponseInner> serviceCallback) {\n        return ServiceFuture.fromResponse(getJobWithServiceResponseAsync(resourceGroupName, resourceName, jobId), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport hmac\n\ndef compare_hmac_digests(digest_a, digest_b):\n    # Use hmac.compare_digest to securely compare digests\n    return hmac.compare_digest(digest_a, digest_b)\n```",
    "label": 1
  },
  {
    "codes": "```scala\nimport java.time.Duration\nimport java.util.Collections\nimport org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}\nimport org.apache.kafka.common.serialization.StringDeserializer\n\n// Define Kafka consumer properties\nval props = new java.util.Properties()\nprops.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\")\nprops.put(ConsumerConfig.GROUP_ID_CONFIG, \"scala-consumer-group\")\nprops.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)\nprops.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)\nprops.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\")\n\n// Create Kafka consumer instance\nval consumer = new KafkaConsumer[String, String](props)\n\n// Subscribe to the topic\nval topic = \"real-time-events\"\nconsumer.subscribe(Collections.singletonList(topic))\n\n// Poll for new messages\nwhile (true) {\n  val records = consumer.poll(Duration.ofMillis(1000))\n  records.forEach { record =>\n    // Process each record\n    println(s\"Received message: key=${record.key()}, value=${record.value()}, offset=${record.offset()}\")\n  }\n}\n\n// Close the consumer (this won't be reached in the infinite loop)\nconsumer.close()\n```",
    "label": 1
  },
  {
    "codes": "```dart\nString formatNumberCompact(num number) {\n  if (number < 1000) {\n    return number.toString();\n  } else if (number < 1000000) {\n    return '${(number / 1000).toStringAsFixed(1)}K';\n  } else if (number < 1000000000) {\n    return '${(number / 1000000).toStringAsFixed(1)}M';\n  } else if (number < 1000000000000) {\n    return '${(number / 1000000000).toStringAsFixed(1)}B';\n  } else {\n    return '${(number / 1000000000000).toStringAsFixed(1)}T';\n  }\n}\n```",
    "label": 1
  },
  {
    "codes": "```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Real-time Crypto Prices</title>\n    <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n    <style>\n        .line {\n            fill: none;\n            stroke: steelblue;\n            stroke-width: 2px;\n        }\n    </style>\n</head>\n<body>\n    <svg width=\"800\" height=\"500\"></svg>\n    <script>\n        const svg = d3.select('svg'),\n              margin = {top: 20, right: 20, bottom: 30, left: 50},\n              width = +svg.attr('width') - margin.left - margin.right,\n              height = +svg.attr('height') - margin.top - margin.bottom,\n              g = svg.append('g').attr('transform', `translate(${margin.left},${margin.top})`);\n\n        const x = d3.scaleTime().rangeRound([0, width]);\n        const y = d3.scaleLinear().rangeRound([height, 0]);\n\n        const line = d3.line()\n            .x(d => x(d.time))\n            .y(d => y(d.price));\n\n        g.append('g')\n            .attr('class', 'axis axis--x')\n            .attr('transform', `translate(0,${height})`);\n\n        g.append('g')\n            .attr('class', 'axis axis--y')\n            .append('text')\n            .attr('fill', '#000')\n            .attr('transform', 'rotate(-90)')\n            .attr('y', 6)\n            .attr('dy', '0.71em')\n            .attr('text-anchor', 'end')\n            .text('Price (USD)');\n\n        g.append('path')\n            .datum([])\n            .attr('class', 'line');\n\n        function update(data) {\n            x.domain(d3.extent(data, d => d.time));\n            y.domain(d3.extent(data, d => d.price));\n\n            g.select('.line')\n                .datum(data)\n                .attr('d', line);\n\n            g.select('.axis--x')\n                .call(d3.axisBottom(x));\n\n            g.select('.axis--y')\n                .call(d3.axisLeft(y));\n        }\n\n        let data = [];\n        fetchCryptoPrices();\n\n        function fetchCryptoPrices() {\n            // Use any public API that supports real-time crypto prices\n            fetch('https://api.coindesk.com/v1/bpi/currentprice/BTC.json')\n                .then(response => response.json())\n                .then(json => {\n                    const time = new Date();\n                    const price = json.bpi.USD.rate_float;\n                    data.push({time, price});\n                    if (data.length > 100) data.shift();\n                    update(data);\n                })\n                .catch(error => console.error('Error fetching crypto prices:', error));\n\n            setTimeout(fetchCryptoPrices, 5000); // Fetch data every 5 seconds\n        }\n    </script>\n</body>\n</html>\n```\n",
    "label": 1
  },
  {
    "codes": "```javascript\nimport React, { useState, useEffect, useRef } from 'react';\n\nfunction Chat() {\n  const [messages, setMessages] = useState([]);\n  const [newMessage, setNewMessage] = useState('');\n  const messageEndRef = useRef(null);\n\n  useEffect(() => {\n    // Simulate receiving messages\n    const interval = setInterval(() => {\n      setMessages(prevMessages => [\n        ...prevMessages,\n        { user: 'Friend', text: 'New message!' }\n      ]);\n    }, 5000);\n\n    return () => clearInterval(interval);\n  }, []);\n\n  useEffect(() => {\n    // Scroll to the latest message\n    messageEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n  }, [messages]);\n\n  const sendMessage = () => {\n    if(newMessage.trim()) {\n      setMessages([...messages, { user: 'You', text: newMessage }]);\n      setNewMessage('');\n    }\n  };\n\n  return (\n    <div style={{ width: '300px', border: '1px solid #ccc', padding: '10px' }}>\n      <div style={{ height: '200px', overflowY: 'scroll', borderBottom: '1px solid #ccc' }}>\n        {messages.map((msg, index) => (\n          <div key={index}><strong>{msg.user}:</strong> {msg.text}</div>\n        ))}\n        <div ref={messageEndRef} />\n      </div>\n      <div style={{ marginTop: '10px' }}>\n        <input\n          type=\"text\"\n          value={newMessage}\n          onChange={e => setNewMessage(e.target.value)}\n          onKeyPress={(e) => e.key === 'Enter' && sendMessage()}\n          style={{ width: '70%', padding: '5px' }}\n        />\n        <button onClick={sendMessage} style={{ width: '25%', padding: '5px' }}>Send</button>\n      </div>\n    </div>\n  );\n}\n\nexport default Chat;\n```\n\n",
    "label": 1
  },
  {
    "codes": "```yaml\n---\n# Ansible playbook to deploy a web app across 100+ servers\n\n- name: Deploy web application across multiple servers\n  hosts: all\n  become: yes\n  vars:\n    app_version: \"1.0.0\"\n    app_source: \"https://example.com/webapp-{{ app_version }}.tar.gz\"\n    deploy_dir: \"/var/www/webapp\"\n    app_port: 8080\n\n  tasks:\n    - name: Ensure deploy directory exists\n      file:\n        path: \"{{ deploy_dir }}\"\n        state: directory\n        owner: www-data\n        group: www-data\n        mode: '0755'\n\n    - name: Download the web application\n      get_url:\n        url: \"{{ app_source }}\"\n        dest: \"/tmp/webapp-{{ app_version }}.tar.gz\"\n        checksum: \"sha256:abc123...\"\n\n    - name: Extract the web application\n      unarchive:\n        src: \"/tmp/webapp-{{ app_version }}.tar.gz\"\n        dest: \"{{ deploy_dir }}\"\n        remote_src: yes\n\n    - name: Install required dependencies\n      apt:\n        name:\n          - python3\n          - python3-pip\n          - nginx\n        state: present\n\n    - name: Install Python dependencies\n      pip:\n        requirements: \"{{ deploy_dir }}/requirements.txt\"\n\n    - name: Configure Nginx for the web app\n      template:\n        src: templates/nginx.conf.j2\n        dest: /etc/nginx/sites-available/webapp\n      notify: Restart Nginx\n\n    - name: Enable Nginx configuration\n      file:\n        src: /etc/nginx/sites-available/webapp\n        dest: /etc/nginx/sites-enabled/webapp\n        state: link\n\n    - name: Ensure Nginx is running\n      service:\n        name: nginx\n        state: started\n        enabled: yes\n\n    - name: Start the web application\n      command: \"{{ deploy_dir }}/start.sh\"\n      args:\n        chdir: \"{{ deploy_dir }}\"\n      become_user: www-data\n\n  handlers:\n    - name: Restart Nginx\n      service:\n        name: nginx\n        state: restarted\n```",
    "label": 1
  },
  {
    "codes": "```swift\nimport SwiftUI\nimport HealthKit\n\nstruct HeartRateView: View {\n    @State private var heartRate: Double = 0\n    private let healthStore = HKHealthStore()\n    \n    var body: some View {\n        VStack {\n            Text(\"Heart Rate\")\n                .font(.largeTitle)\n                .padding()\n            \n            Text(\"\\(Int(heartRate)) BPM\")\n                .font(.system(size: 50, weight: .bold, design: .rounded))\n                .foregroundColor(.red)\n                .padding()\n        }\n        .onAppear {\n            requestHealthKitAuthorization()\n            startObservingHeartRate()\n        }\n    }\n    \n    private func requestHealthKitAuthorization() {\n        let heartRateType = HKObjectType.quantityType(forIdentifier: .heartRate)!\n        healthStore.requestAuthorization(toShare: nil, read: [heartRateType]) { success, error in\n            if !success {\n                print(\"HealthKit authorization failed: \\(String(describing: error))\")\n            }\n        }\n    }\n    \n    private func startObservingHeartRate() {\n        let heartRateType = HKObjectType.quantityType(forIdentifier: .heartRate)!\n        let query = HKObserverQuery(sampleType: heartRateType, predicate: nil) { query, completionHandler, error in\n            if let error = error {\n                print(\"Heart rate observer query failed: \\(error)\")\n                return\n            }\n            fetchLatestHeartRate()\n        }\n        healthStore.execute(query)\n    }\n    \n    private func fetchLatestHeartRate() {\n        let heartRateType = HKQuantityType.quantityType(forIdentifier: .heartRate)!\n        let sortDescriptor = NSSortDescriptor(key: HKSampleSortIdentifierEndDate, ascending: false)\n        let query = HKSampleQuery(sampleType: heartRateType, predicate: nil, limit: 1, sortDescriptors: [sortDescriptor]) { query, results, error in\n            guard let sample = results?.first as? HKQuantitySample else {\n                print(\"No heart rate data available\")\n                return\n            }\n            DispatchQueue.main.async {\n                self.heartRate = sample.quantity.doubleValue(for: HKUnit(from: \"count/min\"))\n            }\n        }\n        healthStore.execute(query)\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *CreateTrailInput) SetName(v string) *CreateTrailInput {\n\ts.Name = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<SkuInfoInner>>> listMultiRolePoolSkusWithServiceResponseAsync(final String resourceGroupName, final String name) {\n        return listMultiRolePoolSkusSinglePageAsync(resourceGroupName, name)\n            .concatMap(new Func1<ServiceResponse<Page<SkuInfoInner>>, Observable<ServiceResponse<Page<SkuInfoInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<SkuInfoInner>>> call(ServiceResponse<Page<SkuInfoInner>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(listMultiRolePoolSkusNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```scala\nimport java.time.Duration\nimport java.util.Collections\nimport org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}\nimport org.apache.kafka.common.serialization.StringDeserializer\n\n// Define Kafka consumer properties\nval props = new java.util.Properties()\nprops.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\")\nprops.put(ConsumerConfig.GROUP_ID_CONFIG, \"scala-consumer-group\")\nprops.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)\nprops.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[StringDeserializer].getName)\nprops.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\")\n\n// Create Kafka consumer instance\nval consumer = new KafkaConsumer[String, String](props)\n\n// Subscribe to the topic\nval topic = \"real-time-events\"\nconsumer.subscribe(Collections.singletonList(topic))\n\n// Poll for new messages\nwhile (true) {\n  val records = consumer.poll(Duration.ofMillis(1000))\n  records.forEach { record =>\n    // Process each record\n    println(s\"Received message: key=${record.key}, value=${record.value}, offset=${record.offset}\")\n  }\n}\n\n// Close the consumer (this won't be reached in the infinite loop)\nconsumer.close()\n```",
    "label": 1
  },
  {
    "codes": "function useAmoPreviewUrls() {\n  var prefSrv = prefs.preferences;\n\n  for each (var preference in AMO_PREFERENCES) {\n    var pref = prefSrv.getPref(preference.name, \"\");\n    prefSrv.setPref(preference.name,\n                    pref.replace(preference.old, preference.new));\n  }\n}",
    "label": 0
  },
  {
    "codes": "```python\nfrom flask import Flask, request, jsonify\nfrom werkzeug.utils import secure_filename\nfrom PIL import Image\nimport os\nimport uuid\nfrom concurrent.futures import ThreadPoolExecutor\n\napp = Flask(__name__)\nexecutor = ThreadPoolExecutor(2)  # Thread pool for async tasks\n\n# Configuration\nUPLOAD_FOLDER = 'uploads'\nALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\n# Ensure upload folder exists\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\n\ndef allowed_file(filename):\n    \"\"\"Check if the file extension is allowed.\"\"\"\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef resize_image(image_path, output_path, size=(128, 128)):\n    \"\"\"Resize the image to the specified size.\"\"\"\n    with Image.open(image_path) as img:\n        img.thumbnail(size)\n        img.save(output_path)\n\n@app.route('/upload', methods=['POST'])\ndef upload_image():\n    \"\"\"Endpoint to upload and resize images asynchronously.\"\"\"\n    if 'file' not in request.files:\n        return jsonify({\"error\": \"No file part\"}), 400\n\n    file = request.files['file']\n\n    if file.filename == '':\n        return jsonify({\"error\": \"No selected file\"}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({\"error\": \"File type not allowed\"}), 400\n\n    # Save the uploaded file\n    filename = secure_filename(file.filename)\n    unique_id = str(uuid.uuid4())\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], f\"{unique_id}_{filename}\")\n    file.save(file_path)\n\n    # Define the resized image path\n    resized_filename = f\"resized_{unique_id}_{filename}\"\n    resized_path = os.path.join(app.config['UPLOAD_FOLDER'], resized_filename)\n\n    # Resize the image asynchronously\n    executor.submit(resize_image, file_path, resized_path)\n\n    return jsonify({\"message\": \"File uploaded and will be resized asynchronously\", \"file_id\": unique_id}), 202\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```",
    "label": 1
  },
  {
    "codes": "def make_temporal_consensus(com_membership):\n    r\"\"\"\n    Matches community labels accross time-points\n\n    Jaccard matching is in a greedy fashiong. Matching the largest community at t with the community at t-1.\n\n    Parameters\n    ----------\n\n    com_membership : array\n        Shape should be node, time.\n\n    Returns\n    -------\n\n    D : array\n        temporal consensus matrix using Jaccard distance\n\n    \"\"\"\n\n    com_membership = np.array(com_membership)\n    # make first indicies be between 0 and 1.\n    com_membership[:, 0] = clean_community_indexes(com_membership[:, 0])\n    # loop over all timepoints, get jacccard distance in greedy manner for largest community to time period before\n    for t in range(1, com_membership.shape[1]):\n        ct, counts_t = np.unique(com_membership[:, t], return_counts=True)\n        ct = ct[np.argsort(counts_t)[::-1]]\n        c1back = np.unique(com_membership[:, t-1])\n        new_index = np.zeros(com_membership.shape[0])\n        for n in ct:\n            if len(c1back) > 0:\n                d = np.ones(int(c1back.max())+1)\n                for m in c1back:\n                    v1 = np.zeros(com_membership.shape[0])\n                    v2 = np.zeros(com_membership.shape[0])\n                    v1[com_membership[:, t] == n] = 1\n                    v2[com_membership[:, t-1] == m] = 1\n                    d[int(m)] = jaccard(v1, v2)\n                bestval = np.argmin(d)\n            else:\n                bestval = new_index.max() + 1\n            new_index[com_membership[:, t] == n] = bestval\n            c1back = np.array(np.delete(c1back, np.where(c1back == bestval)))\n        com_membership[:, t] = new_index\n    return com_membership",
    "label": 0
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"testing\"\n\n\t\"github.com/json-iterator/go\"\n)\n\nvar jsonData = []byte(`{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}`)\n\ntype Person struct {\n\tName string `json:\"name\"`\n\tAge  int    `json:\"age\"`\n\tCity string `json:\"city\"`\n}\n\nfunc BenchmarkEncodingJsonUnmarshal(b *testing.B) {\n\tvar person Person\n\tfor i := 0; i < b.N; i++ {\n\t\t// Decoding JSON using the standard library\n\t\tif err := json.Unmarshal(jsonData, &person); err != nil {\n\t\t\tb.Fatalf(\"Error decoding JSON: %v\", err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkJsonIteratorUnmarshal(b *testing.B) {\n\tvar person Person\n\tfor i := 0; i < b.N; i++ {\n\t\t// Decoding JSON using the json-iterator library\n\t\tif err := jsoniter.Unmarshal(jsonData, &person); err != nil {\n\t\t\tb.Fatalf(\"Error decoding JSON: %v\", err)\n\t\t}\n\t}\n}\n```\n\nTo run the benchmark, execute the following command in the terminal inside your Go project directory:\n\n```sh\ngo test -bench=.\n```",
    "label": 1
  },
  {
    "codes": "public PagedList<DeletedCertificateItem> getDeletedCertificatesNext(final String nextPageLink) {\n        ServiceResponse<Page<DeletedCertificateItem>> response = getDeletedCertificatesNextSinglePageAsync(nextPageLink).toBlocking().single();\n        return new PagedList<DeletedCertificateItem>(response.body()) {\n            @Override\n            public Page<DeletedCertificateItem> nextPage(String nextPageLink) {\n                return getDeletedCertificatesNextSinglePageAsync(nextPageLink).toBlocking().single().body();\n            }\n        };\n    }",
    "label": 0
  },
  {
    "codes": "```java\npublic void batchInsertUsers(SessionFactory sessionFactory, List<User> users) { \n    int batchSize = 50;\n    Transaction transaction = null;\n    try (Session session = sessionFactory.openSession()) {\n        transaction = session.beginTransaction();\n        for (int i = 0; i < users.size(); i++) {\n            session.save(users.get(i));  // Save each user entity\n            if (i > 0 && i % batchSize == 0) { \n                session.flush();  // Flush a batch of inserts\n                session.clear();  // Clear the context\n            }\n        }\n        transaction.commit();  // Commit transaction\n    } catch (RuntimeException e) { \n        if (transaction != null) transaction.rollback();  // Rollback in case of an exception\n        throw e;\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```kotlin\n// Extension function to reverse words in a string while preserving whitespace\nfun String.reverseWords(): String {\n    // Split the string into words and whitespace using regex\n    val parts = this.split(\"(?<=\\\\s)|(?=\\\\s)\".toRegex())\n    \n    // Reverse each word and keep whitespace as is\n    return parts.joinToString(\"\") { part ->\n        if (part.isNotBlank()) part.reversed() else part\n    }\n}\n\n// Example usage:\n// val input = \"Hello  world  Kotlin\"\n// println(input.reverseWords()) // Output: \"olleH  dlrow  niltoK\"\n```",
    "label": 1
  },
  {
    "codes": "func (s *TableInput) SetRetention(v int64) *TableInput {\n\ts.Retention = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "def seek(self, amount, reference=\"relative\", precision=\"default-precise\"):\n        \"\"\"Mapped mpv seek command, see man mpv(1).\"\"\"\n        self.command('seek', amount, reference, precision)",
    "label": 0
  },
  {
    "codes": "```python\nfrom flask import Flask, request, jsonify\nfrom werkzeug.utils import secure_filename\nfrom PIL import Image\nimport os\nimport uuid\nfrom concurrent.futures import ThreadPoolExecutor\n\napp = Flask(__name__)\nexecutor = ThreadPoolExecutor(2)  # Adjust the number of threads as needed\n\n# Configuration\nUPLOAD_FOLDER = 'uploads/'\nALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\n# Helper function to check allowed file extensions\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n# Asynchronous image resizing function\ndef resize_image_async(file_path, output_path, size):\n    with Image.open(file_path) as img:\n        img.thumbnail(size)\n        img.save(output_path)\n\n# Endpoint to upload and resize images\n@app.route('/upload', methods=['POST'])\ndef upload_image():\n    if 'file' not in request.files:\n        return jsonify({\"error\": \"No file part\"}), 400\n\n    file = request.files['file']\n\n    if file.filename == '':\n        return jsonify({\"error\": \"No selected file\"}), 400\n\n    if file and allowed_file(file.filename):\n        filename = secure_filename(file.filename)\n        unique_id = str(uuid.uuid4())\n        file_path = os.path.join(app.config['UPLOAD_FOLDER'], unique_id + '_' + filename)\n        file.save(file_path)\n\n        # Define the output path for the resized image\n        output_path = os.path.join(app.config['UPLOAD_FOLDER'], unique_id + '_resized_' + filename)\n\n        # Get the desired size from the request (default to 128x128)\n        size = request.form.get('size', (128, 128))\n        if isinstance(size, str):\n            size = tuple(map(int, size.split(',')))\n\n        # Submit the resizing task to the thread pool\n        executor.submit(resize_image_async, file_path, output_path, size)\n\n        return jsonify({\"message\": \"Image uploaded and resizing started\", \"file_id\": unique_id}), 202\n\n    return jsonify({\"error\": \"File type not allowed\"}), 400\n\nif __name__ == '__main__':\n    if not os.path.exists(UPLOAD_FOLDER):\n        os.makedirs(UPLOAD_FOLDER)\n    app.run(debug=True)\n```",
    "label": 1
  },
  {
    "codes": "function () {\n            if (!this.options) {\n                return;\n            }\n\n            var values_num = this.options.values.length;\n            var p_values = this.options.p_values;\n            var text_single;\n            var text_from;\n            var text_to;\n            var from_pretty;\n            var to_pretty;\n\n            if (this.options.hide_from_to) {\n                return;\n            }\n\n            if (this.options.type === \"single\") {\n\n                if (values_num) {\n                    text_single = this.decorate(p_values[this.result.from]);\n                    this.$cache.single.html(text_single);\n                } else {\n                    from_pretty = this._prettify(this.result.from);\n\n                    text_single = this.decorate(from_pretty, this.result.from);\n                    this.$cache.single.html(text_single);\n                }\n\n                this.calcLabels();\n\n                if (this.labels.p_single_left < this.labels.p_min + 1) {\n                    this.$cache.min[0].style.visibility = \"hidden\";\n                } else {\n                    this.$cache.min[0].style.visibility = \"visible\";\n                }\n\n                if (this.labels.p_single_left + this.labels.p_single_fake > 100 - this.labels.p_max - 1) {\n                    this.$cache.max[0].style.visibility = \"hidden\";\n                } else {\n                    this.$cache.max[0].style.visibility = \"visible\";\n                }\n\n            } else {\n\n                if (values_num) {\n\n                    if (this.options.decorate_both) {\n                        text_single = this.decorate(p_values[this.result.from]);\n                        text_single += this.options.values_separator;\n                        text_single += this.decorate(p_values[this.result.to]);\n                    } else {\n                        text_single = this.decorate(p_values[this.result.from] + this.options.values_separator + p_values[this.result.to]);\n                    }\n                    text_from = this.decorate(p_values[this.result.from]);\n                    text_to = this.decorate(p_values[this.result.to]);\n\n                    this.$cache.single.html(text_single);\n                    this.$cache.from.html(text_from);\n                    this.$cache.to.html(text_to);\n\n                } else {\n                    from_pretty = this._prettify(this.result.from);\n                    to_pretty = this._prettify(this.result.to);\n\n                    if (this.options.decorate_both) {\n                        text_single = this.decorate(from_pretty, this.result.from);\n                        text_single += this.options.values_separator;\n                        text_single += this.decorate(to_pretty, this.result.to);\n                    } else {\n                        text_single = this.decorate(from_pretty + this.options.values_separator + to_pretty, this.result.to);\n                    }\n                    text_from = this.decorate(from_pretty, this.result.from);\n                    text_to = this.decorate(to_pretty, this.result.to);\n\n                    this.$cache.single.html(text_single);\n                    this.$cache.from.html(text_from);\n                    this.$cache.to.html(text_to);\n\n                }\n\n                this.calcLabels();\n\n                var min = Math.min(this.labels.p_single_left, this.labels.p_from_left),\n                    single_left = this.labels.p_single_left + this.labels.p_single_fake,\n                    to_left = this.labels.p_to_left + this.labels.p_to_fake,\n                    max = Math.max(single_left, to_left);\n\n                if (this.labels.p_from_left + this.labels.p_from_fake >= this.labels.p_to_left) {\n                    this.$cache.from[0].style.visibility = \"hidden\";\n                    this.$cache.to[0].style.visibility = \"hidden\";\n                    this.$cache.single[0].style.visibility = \"visible\";\n\n                    if (this.result.from === this.result.to) {\n                        if (this.target === \"from\") {\n                            this.$cache.from[0].style.visibility = \"visible\";\n                        } else if (this.target === \"to\") {\n                            this.$cache.to[0].style.visibility = \"visible\";\n                        } else if (!this.target) {\n                            this.$cache.from[0].style.visibility = \"visible\";\n                        }\n                        this.$cache.single[0].style.visibility = \"hidden\";\n                        max = to_left;\n                    } else {\n                        this.$cache.from[0].style.visibility = \"hidden\";\n                        this.$cache.to[0].style.visibility = \"hidden\";\n                        this.$cache.single[0].style.visibility = \"visible\";\n                        max = Math.max(single_left, to_left);\n                    }\n                } else {\n                    this.$cache.from[0].style.visibility = \"visible\";\n                    this.$cache.to[0].style.visibility = \"visible\";\n                    this.$cache.single[0].style.visibility = \"hidden\";\n                }\n\n                if (min < this.labels.p_min + 1) {\n                    this.$cache.min[0].style.visibility = \"hidden\";\n                } else {\n                    this.$cache.min[0].style.visibility = \"visible\";\n                }\n\n                if (max > 100 - this.labels.p_max - 1) {\n                    this.$cache.max[0].style.visibility = \"hidden\";\n                } else {\n                    this.$cache.max[0].style.visibility = \"visible\";\n                }\n\n            }\n        }",
    "label": 0
  },
  {
    "codes": "```powershell\n# Define the directory containing files you want to rename\n$directoryPath = \"C:\\Path\\To\\Your\\Files\"\n\n# Define the regex pattern to match files that need to be renamed\n$pattern = \"^oldname(\\d+)\\.txt$\"\n\n# Define the replacement pattern to rename the files\n$replacement = \"newname$1.txt\"\n\n# Get all files in the directory\nGet-ChildItem -Path $directoryPath -File | ForEach-Object {\n    $fileName = $_.Name\n    \n    # If the file name matches the regex pattern\n    if ($fileName -match $pattern) {\n        # Construct the new file name using the replacement pattern\n        $newFileName = [Regex]::Replace($fileName, $pattern, $replacement)\n        \n        # Define the full path for the new file name\n        $newFilePath = Join-Path -Path $directoryPath -ChildPath $newFileName\n        \n        # Rename the file\n        Rename-Item -Path $_.FullName -NewName $newFilePath\n\n        # Output the renaming action for verification\n        Write-Output \"Renamed '$fileName' to '$newFileName'\"\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "def substitute_environ(self):\n        \"\"\"\n        Substitute environment variables into settings.\n        \"\"\"\n        for attr_name in dir(self):\n            if attr_name.startswith('_') or attr_name.upper() != attr_name:\n                continue\n\n            orig_value = getattr(self, attr_name)\n            is_required = isinstance(orig_value, Required)\n            orig_type = orig_value.v_type if is_required else type(orig_value)\n            env_var_name = self._ENV_PREFIX + attr_name\n            env_var = os.getenv(env_var_name, None)\n            if env_var is not None:\n                if issubclass(orig_type, bool):\n                    env_var = env_var.upper() in ('1', 'TRUE')\n                elif issubclass(orig_type, int):\n                    env_var = int(env_var)\n                elif issubclass(orig_type, Path):\n                    env_var = Path(env_var)\n                elif issubclass(orig_type, bytes):\n                    env_var = env_var.encode()\n                # could do floats here and lists etc via json\n                setattr(self, attr_name, env_var)\n            elif is_required and attr_name not in self._custom_settings:\n                raise RuntimeError('The required environment variable \"{0}\" is currently not set, '\n                                   'you\\'ll need to run `source activate.settings.sh` '\n                                   'or you can set that single environment variable with '\n                                   '`export {0}=\"<value>\"`'.format(env_var_name))",
    "label": 0
  },
  {
    "codes": "```java\nimport javafx.application.Application;\nimport javafx.scene.Scene;\nimport javafx.scene.canvas.Canvas;\nimport javafx.scene.canvas.GraphicsContext;\nimport javafx.scene.input.MouseEvent;\nimport javafx.scene.layout.Pane;\nimport javafx.scene.paint.Color;\nimport javafx.stage.Stage;\n\npublic class FractalApp extends Application {\n\n    private static final int WIDTH = 800;\n    private static final int HEIGHT = 800;\n\n    @Override\n    public void start(Stage primaryStage) {\n        Pane root = new Pane();\n        Canvas canvas = new Canvas(WIDTH, HEIGHT);\n        GraphicsContext gc = canvas.getGraphicsContext2D();\n        \n        // Draw initial fractal\n        drawFractal(gc, WIDTH/2, HEIGHT/2, 200);\n\n        // Add mouse event handler to redraw fractal on click\n        canvas.addEventHandler(MouseEvent.MOUSE_CLICKED, event -> {\n            gc.clearRect(0, 0, WIDTH, HEIGHT);\n            drawFractal(gc, event.getX(), event.getY(), 200);\n        });\n\n        root.getChildren().add(canvas);\n        Scene scene = new Scene(root, WIDTH, HEIGHT);\n\n        primaryStage.setTitle(\"Interactive Fractal Patterns\");\n        primaryStage.setScene(scene);\n        primaryStage.show();\n    }\n\n    // Method to draw a simple fractal pattern centered at (x, y)\n    private void drawFractal(GraphicsContext gc, double x, double y, double size) {\n        gc.setStroke(Color.BLACK);\n        gc.setLineWidth(2);\n        drawRecursivePattern(gc, x, y, size, 5);\n    }\n\n    // Recursive method for drawing a fractal pattern\n    private void drawRecursivePattern(GraphicsContext gc, double x, double y, double size, int depth) {\n        if (depth == 0) return;\n\n        // Draw a triangle\n        gc.strokeLine(x, y - size / 2, x - size / 2, y + size / 2);\n        gc.strokeLine(x, y - size / 2, x + size / 2, y + size / 2);\n        gc.strokeLine(x - size / 2, y + size / 2, x + size / 2, y + size / 2);\n\n        // Recursive calls for smaller triangles\n        drawRecursivePattern(gc, x, y - size / 2, size / 2, depth - 1);\n        drawRecursivePattern(gc, x - size / 2, y + size / 2, size / 2, depth - 1);\n        drawRecursivePattern(gc, x + size / 2, y + size / 2, size / 2, depth - 1);\n    }\n\n    public static void main(String[] args) {\n        launch(args);\n    }\n}\n```\n\nThe code creates a JavaFX application that displays an interactive fractal pattern. The fractal is redrawn centered on the mouse click position on the canvas. The `drawRecursivePattern` method is responsible for creating the fractal pattern recursively.",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<AppServiceEnvironmentResourceInner> beginCreateOrUpdateAsync(String resourceGroupName, String name, AppServiceEnvironmentResourceInner hostingEnvironmentEnvelope, final ServiceCallback<AppServiceEnvironmentResourceInner> serviceCallback) {\n        return ServiceFuture.fromResponse(beginCreateOrUpdateWithServiceResponseAsync(resourceGroupName, name, hostingEnvironmentEnvelope), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "def calc_density(self, density_standard=None):\n        \"\"\"\n        Calculates the density of the SpectralColor. By default, Status T\n        density is used, and the correct density distribution (Red, Green,\n        or Blue) is chosen by comparing the Red, Green, and Blue components of\n        the spectral sample (the values being red in via \"filters\").\n        \"\"\"\n        if density_standard is not None:\n            return density.ansi_density(self, density_standard)\n        else:\n            return density.auto_density(self)",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<EventHubConsumerGroupInfoInner>>> listEventHubConsumerGroupsWithServiceResponseAsync(final String resourceGroupName, final String resourceName, final String eventHubEndpointName) {\n        return listEventHubConsumerGroupsSinglePageAsync(resourceGroupName, resourceName, eventHubEndpointName)\n            .concatMap(new Func1<ServiceResponse<Page<EventHubConsumerGroupInfoInner>>, Observable<ServiceResponse<Page<EventHubConsumerGroupInfoInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<EventHubConsumerGroupInfoInner>>> call(ServiceResponse<Page<EventHubConsumerGroupInfoInner>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(listEventHubConsumerGroupsNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<SecretBundle> setSecretAsync(String vaultBaseUrl, String secretName, String value, Map<String, String> tags, String contentType, SecretAttributes secretAttributes) {\n        return setSecretWithServiceResponseAsync(vaultBaseUrl, secretName, value, tags, contentType, secretAttributes).map(new Func1<ServiceResponse<SecretBundle>, SecretBundle>() {\n            @Override\n            public SecretBundle call(ServiceResponse<SecretBundle> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<SyncAgentKeyPropertiesInner>> generateKeyWithServiceResponseAsync(String resourceGroupName, String serverName, String syncAgentName) {\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (serverName == null) {\n            throw new IllegalArgumentException(\"Parameter serverName is required and cannot be null.\");\n        }\n        if (syncAgentName == null) {\n            throw new IllegalArgumentException(\"Parameter syncAgentName is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        return service.generateKey(resourceGroupName, serverName, syncAgentName, this.client.subscriptionId(), this.client.apiVersion(), this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<SyncAgentKeyPropertiesInner>>>() {\n                @Override\n                public Observable<ServiceResponse<SyncAgentKeyPropertiesInner>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<SyncAgentKeyPropertiesInner> clientResponse = generateKeyDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```swift\nimport UIKit\nimport PlaygroundSupport\n\n// Create a UIView to animate\nlet myView = UIView(frame: CGRect(x: 100, y: 100, width: 100, height: 100))\nmyView.backgroundColor = .blue\n\n// Setting up the playground live view\nPlaygroundPage.current.liveView = myView\n\n// Function to add rotation animation\nfunc addRotationAnimation(to view: UIView, duration: CFTimeInterval) {\n    // Create a CABasicAnimation for the 'transform.rotation.z' key path\n    let rotationAnimation = CABasicAnimation(keyPath: \"transform.rotation.z\")\n    \n    // Set the rotation angle to 360 degrees in radians\n    rotationAnimation.toValue = NSNumber(value: Double.pi * 2)\n    \n    // Set the duration of the animation\n    rotationAnimation.duration = duration\n    \n    // Repeat the animation forever\n    rotationAnimation.repeatCount = .infinity\n    \n    // Add the animation to the view's layer\n    view.layer.add(rotationAnimation, forKey: \"rotationAnimation\")\n}\n\n// Call the function with the view and a duration\naddRotationAnimation(to: myView, duration: 2.0)\n```",
    "label": 1
  },
  {
    "codes": "function () {\n        var range = this._model.get('range');\n\n        var rg = range;\n\n        if (zrUtil.isArray(rg) && rg.length === 1) {\n            rg = rg[0];\n        }\n\n        if (/^\\d{4}$/.test(rg)) {\n            range = [rg + '-01-01', rg + '-12-31'];\n        }\n\n        if (/^\\d{4}[\\/|-]\\d{1,2}$/.test(rg)) {\n\n            var start = this.getDateInfo(rg);\n            var firstDay = start.date;\n            firstDay.setMonth(firstDay.getMonth() + 1);\n\n            var end = this.getNextNDay(firstDay, -1);\n            range = [start.formatedDate, end.formatedDate];\n        }\n\n        if (/^\\d{4}[\\/|-]\\d{1,2}[\\/|-]\\d{1,2}$/.test(rg)) {\n            range = [rg, rg];\n        }\n\n        var tmp = this._getRangeInfo(range);\n\n        if (tmp.start.time > tmp.end.time) {\n            range.reverse();\n        }\n\n        return range;\n    }",
    "label": 0
  },
  {
    "codes": "```kotlin\nimport kotlinx.coroutines.*\nimport java.io.IOException\nimport kotlin.math.pow\n\n// Retry with exponential backoff\nsuspend fun <T> retryWithExponentialBackoff(\n    retries: Int = 3, // Maximum number of retries\n    initialDelay: Long = 100, // Initial delay in milliseconds\n    maxDelay: Long = 5000, // Maximum delay in milliseconds\n    block: suspend () -> T // HTTP request or any suspendable block\n): T {\n    var currentDelay = initialDelay\n    repeat(retries) { attempt ->\n        try {\n            return block() // Execute the HTTP request\n        } catch (e: IOException) { // Catch network-related exceptions\n            if (attempt == retries - 1) throw e // Throw exception if max retries reached\n        }\n        delay(currentDelay) // Wait before retrying\n        currentDelay = (currentDelay * 2).coerceAtMost(maxDelay) // Double the delay, capped at maxDelay\n    }\n    throw IllegalStateException(\"Unreachable code\") // Should never reach here\n}\n\n// Example usage\nsuspend fun makeHttpRequest(): String {\n    // Simulate an HTTP request\n    if (Math.random() < 0.5) throw IOException(\"Network error\")\n    return \"Success\"\n}\n\nfun main() = runBlocking {\n    try {\n        val result = retryWithExponentialBackoff { makeHttpRequest() }\n        println(result) // Print \"Success\" if request succeeds\n    } catch (e: IOException) {\n        println(\"Request failed after retries: ${e.message}\")\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```julia\n# Import necessary packages\nusing VoronoiDelaunay\nusing Plots\n\n# Generate random points\nn = 50  # Number of points\npoints = rand(2, n)  # 2D random points\n\n# Compute Voronoi diagram\ntess = VoronoiTessellation(points)\n\n# Plot the Voronoi diagram\nplot(tess, points, legend=false, aspect_ratio=:equal, title=\"Voronoi Diagram\")\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport psutil\nimport time\nimport argparse\nfrom collections import defaultdict\n\ndef get_network_usage_per_process(interval=1):\n    # Get initial data\n    old_data = psutil.net_io_counters(pernic=True)\n    \n    time.sleep(interval)\n    \n    # Get updated data\n    new_data = psutil.net_io_counters(pernic=True)\n    \n    process_bandwidth_usage = defaultdict(lambda: {'sent': 0, 'recv': 0})\n    \n    # Calculate bandwidth usage per process\n    for conn in psutil.net_connections(kind='inet'):\n        try:\n            process = psutil.Process(conn.pid)\n            pname = process.name()\n            old_sent_recv = old_data[conn.laddr.ip].bytes_sent, old_data[conn.laddr.ip].bytes_recv\n            new_sent_recv = new_data[conn.laddr.ip].bytes_sent, new_data[conn.laddr.ip].bytes_recv\n            \n            if old_sent_recv and new_sent_recv:\n                process_bandwidth_usage[pname]['sent'] += new_sent_recv[0] - old_sent_recv[0]\n                process_bandwidth_usage[pname]['recv'] += new_sent_recv[1] - old_sent_recv[1]\n        \n        except (psutil.NoSuchProcess, psutil.AccessDenied, KeyError):\n            pass\n\n    return process_bandwidth_usage\n\ndef main():\n    parser = argparse.ArgumentParser(description='Monitor network bandwidth usage per process.')\n    parser.add_argument('--interval', type=int, default=1, help='Interval in seconds to update the usage data')\n    args = parser.parse_args()\n    \n    try:\n        while True:\n            usage = get_network_usage_per_process(interval=args.interval)\n            print('Process Bandwidth Usage:')\n            for process, bandwidth in usage.items():\n                print(f'{process}: Sent = {bandwidth[\"sent\"]} bytes, Received = {bandwidth[\"recv\"]} bytes')\n            time.sleep(args.interval)\n    except KeyboardInterrupt:\n        print(\"\\nTerminating monitoring...\")\n\nif __name__ == \"__main__\":\n    main()\n```\n",
    "label": 1
  },
  {
    "codes": "public KeyBundle createKey(String vaultBaseUrl, String keyName, JsonWebKeyType kty) {\n        return createKeyWithServiceResponseAsync(vaultBaseUrl, keyName, kty).toBlocking().single().body();\n    }",
    "label": 0
  },
  {
    "codes": "```javascript\nconst xss = require('xss'); // Import the xss library\n\n// Middleware to sanitize user input against XSS attacks\nconst sanitizeInput = (req, res, next) => {\n    // Sanitize request body\n    if (req.body) {\n        for (let key in req.body) {\n            if (typeof req.body[key] === 'string') {\n                req.body[key] = xss(req.body[key]); // Sanitize each string value\n            }\n        }\n    }\n\n    // Sanitize request query parameters\n    if (req.query) {\n        for (let key in req.query) {\n            if (typeof req.query[key] === 'string') {\n                req.query[key] = xss(req.query[key]); // Sanitize each string value\n            }\n        }\n    }\n\n    // Sanitize request params\n    if (req.params) {\n        for (let key in req.params) {\n            if (typeof req.params[key] === 'string') {\n                req.params[key] = xss(req.params[key]); // Sanitize each string value\n            }\n        }\n    }\n\n    next(); // Proceed to the next middleware or route handler\n};\n\nmodule.exports = sanitizeInput; // Export the middleware\n```",
    "label": 1
  },
  {
    "codes": "```c\n#include <openssl/evp.h>\n#include <openssl/rand.h>\n#include <stdio.h>\n#include <string.h>\n\n// Encrypt plaintext using AES-GCM\nint aes_gcm_encrypt(const unsigned char *plaintext, int plaintext_len,\n                    const unsigned char *key, const unsigned char *iv,\n                    unsigned char *ciphertext, unsigned char *tag) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int ciphertext_len;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        return -1;\n    }\n\n    // Initialize the encryption operation\n    if (1 != EVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, key, iv)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Encrypt the plaintext\n    if (1 != EVP_EncryptUpdate(ctx, ciphertext, &len, plaintext, plaintext_len)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    ciphertext_len = len;\n\n    // Finalize the encryption\n    if (1 != EVP_EncryptFinal_ex(ctx, ciphertext + len, &len)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    ciphertext_len += len;\n\n    // Get the authentication tag\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_GET_TAG, 16, tag)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    return ciphertext_len;\n}\n\n// Decrypt ciphertext using AES-GCM\nint aes_gcm_decrypt(const unsigned char *ciphertext, int ciphertext_len,\n                    const unsigned char *key, const unsigned char *iv,\n                    const unsigned char *tag, unsigned char *plaintext) {\n    EVP_CIPHER_CTX *ctx;\n    int len;\n    int plaintext_len;\n    int ret;\n\n    // Create and initialize the context\n    if (!(ctx = EVP_CIPHER_CTX_new())) {\n        return -1;\n    }\n\n    // Initialize the decryption operation\n    if (1 != EVP_DecryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, key, iv)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Decrypt the ciphertext\n    if (1 != EVP_DecryptUpdate(ctx, plaintext, &len, ciphertext, ciphertext_len)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n    plaintext_len = len;\n\n    // Set the expected tag value\n    if (1 != EVP_CIPHER_CTX_ctrl(ctx, EVP_CTRL_GCM_SET_TAG, 16, (void *)tag)) {\n        EVP_CIPHER_CTX_free(ctx);\n        return -1;\n    }\n\n    // Finalize the decryption\n    ret = EVP_DecryptFinal_ex(ctx, plaintext + len, &len);\n\n    // Clean up\n    EVP_CIPHER_CTX_free(ctx);\n\n    if (ret > 0) {\n        plaintext_len += len;\n        return plaintext_len;\n    } else {\n        // Authentication failed\n        return -1;\n    }\n}\n\n// Example usage\nint main() {\n    unsigned char key[32]; // 256-bit key\n    unsigned char iv[12];  // 96-bit IV\n    unsigned char plaintext[] = \"Hello, AES-GCM!\";\n    unsigned char ciphertext[128];\n    unsigned char decryptedtext[128];\n    unsigned char tag[16];\n\n    // Generate random key and IV\n    if (!RAND_bytes(key, sizeof(key)) || !RAND_bytes(iv, sizeof(iv))) {\n        fprintf(stderr, \"Error generating random bytes\\n\");\n        return 1;\n    }\n\n    // Encrypt\n    int ciphertext_len = aes_gcm_encrypt(plaintext, strlen((char *)plaintext), key, iv, ciphertext, tag);\n    if (ciphertext_len == -1) {\n        fprintf(stderr, \"Encryption failed\\n\");\n        return 1;\n    }\n\n    // Decrypt\n    int decryptedtext_len = aes_gcm_decrypt(ciphertext, ciphertext_len, key, iv, tag, decryptedtext);\n    if (decryptedtext_len == -1) {\n        fprintf(stderr, \"Decryption failed\\n\");\n        return 1;\n    }\n\n    // Null-terminate the decrypted text\n    decryptedtext[decryptedtext_len] = '\\0';\n\n    // Print results\n    printf(\"Original: %s\\n\", plaintext);\n    printf(\"Decrypted: %s\\n\", decryptedtext);\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```go\n// Import necessary packages\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"github.com/BurntSushi/toml\"\n\t\"gopkg.in/yaml.v2\"\n)\n\n// Function to convert YAML to JSON\nfunc yamlToJSON(yamlData []byte) ([]byte, error) {\n\tvar jsonData map[string]interface{}\n\terr := yaml.Unmarshal(yamlData, &jsonData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tjsonOutput, err := json.MarshalIndent(jsonData, \"\", \"  \")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn jsonOutput, nil\n}\n\n// Function to convert YAML to TOML\nfunc yamlToTOML(yamlData []byte) ([]byte, error) {\n\tvar tomlData map[string]interface{}\n\terr := yaml.Unmarshal(yamlData, &tomlData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttomlOutput, err := toml.Marshal(tomlData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn tomlOutput, nil\n}\n\n// Function to convert JSON to YAML\nfunc jsonToYAML(jsonData []byte) ([]byte, error) {\n\tvar yamlData map[string]interface{}\n\terr := json.Unmarshal(jsonData, &yamlData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tyamlOutput, err := yaml.Marshal(yamlData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn yamlOutput, nil\n}\n\n// Function to convert JSON to TOML\nfunc jsonToTOML(jsonData []byte) ([]byte, error) {\n\tvar tomlData map[string]interface{}\n\terr := json.Unmarshal(jsonData, &tomlData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttomlOutput, err := toml.Marshal(tomlData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn tomlOutput, nil\n}\n\n// Function to convert TOML to JSON\nfunc tomlToJSON(tomlData []byte) ([]byte, error) {\n\tvar jsonData map[string]interface{}\n\terr := toml.Unmarshal(tomlData, &jsonData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tjsonOutput, err := json.MarshalIndent(jsonData, \"\", \"  \")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn jsonOutput, nil\n}\n\n// Function to convert TOML to YAML\nfunc tomlToYAML(tomlData []byte) ([]byte, error) {\n\tvar yamlData map[string]interface{}\n\terr := toml.Unmarshal(tomlData, &yamlData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tyamlOutput, err := yaml.Marshal(yamlData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn yamlOutput, nil\n}\n\n// Main function to handle CLI arguments\nfunc main() {\n\tif len(os.Args) < 4 {\n\t\tlog.Fatalf(\"Usage: %s <input-file> <output-format> <output-file>\\n\", os.Args[0])\n\t}\n\n\tinputFile := os.Args[1]\n\toutputFormat := os.Args[2]\n\toutputFile := os.Args[3]\n\n\tinputData, err := ioutil.ReadFile(inputFile)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error reading input file: %v\\n\", err)\n\t}\n\n\tvar outputData []byte\n\n\tswitch filepath.Ext(inputFile) {\n\tcase \".yaml\", \".yml\":\n\t\tswitch outputFormat {\n\t\tcase \"json\":\n\t\t\toutputData, err = yamlToJSON(inputData)\n\t\tcase \"toml\":\n\t\t\toutputData, err = yamlToTOML(inputData)\n\t\tdefault:\n\t\t\tlog.Fatalf(\"Unsupported output format: %s\\n\", outputFormat)\n\t\t}\n\tcase \".json\":\n\t\tswitch outputFormat {\n\t\tcase \"yaml\":\n\t\t\toutputData, err = jsonToYAML(inputData)\n\t\tcase \"toml\":\n\t\t\toutputData, err = jsonToTOML(inputData)\n\t\tdefault:\n\t\t\tlog.Fatalf(\"Unsupported output format: %s\\n\", outputFormat)\n\t\t}\n\tcase \".toml\":\n\t\tswitch outputFormat {\n\t\tcase \"json\":\n\t\t\toutputData, err = tomlToJSON(inputData)\n\t\tcase \"yaml\":\n\t\t\toutputData, err = tomlToYAML(inputData)\n\t\tdefault:\n\t\t\tlog.Fatalf(\"Unsupported output format: %s\\n\", outputFormat)\n\t\t}\n\tdefault:\n\t\tlog.Fatalf(\"Unsupported input file format\\n\")\n\t}\n\n\tif err != nil {\n\t\tlog.Fatalf(\"Conversion error: %v\\n\", err)\n\t}\n\n\terr = ioutil.WriteFile(outputFile, outputData, 0644)\n\tif err != nil {\n\t\tlog.Fatalf(\"Error writing output file: %v\\n\", err)\n\t}\n\n\tfmt.Printf(\"Converted %s to %s successfully.\\n\", inputFile, outputFile)\n}\n```\n",
    "label": 1
  },
  {
    "codes": "```powershell\n# Define the directory containing the files\n$directory = \"C:\\Path\\To\\Your\\Files\"\n\n# Define the regex pattern to match and the replacement string\n$regexPattern = \"OldPattern\"\n$replacement = \"NewPattern\"\n\n# Get all files in the directory\nGet-ChildItem -Path $directory | ForEach-Object {\n    # Check if the file name matches the regex pattern\n    if ($_.Name -match $regexPattern) {\n        # Create the new file name by replacing the pattern\n        $newName = $_.Name -replace $regexPattern, $replacement\n        \n        # Rename the file\n        Rename-Item -Path $_.FullName -NewName $newName\n        \n        # Output the change for logging\n        Write-Output \"Renamed '$($_.Name)' to '$newName'\"\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *DeleteTableInput) SetDatabaseName(v string) *DeleteTableInput {\n\ts.DatabaseName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```rust\nuse wasm_bindgen::prelude::*;\nuse wasm_bindgen::JsCast;\nuse onnxruntime::{environment::Environment, session::Session, GraphOptimizationLevel};\nuse web_sys::{window, console};\nuse std::sync::Once;\n\nstatic INIT: Once = Once::new();\n\n#[wasm_bindgen(start)]\npub fn start() {\n    INIT.call_once(|| {\n        console_error_panic_hook::set_once();\n        console::log_1(&JsValue::from_str(\"WASM Module Initialized\"));\n    });\n}\n\n#[wasm_bindgen]\npub fn run_onnx_model(model_bytes: &[u8], input_data: Vec<f32>) -> Vec<f32> {\n    // Create the ONNX Runtime environment\n    let env = Environment::builder()\n        .with_name(\"onnx-environment\")\n        .build()\n        .unwrap();\n\n    // Create a session with the model data\n    let session = Session::from_bytes(env, &model_bytes, GraphOptimizationLevel::All)\n        .expect(\"Failed to create session from model bytes\");\n\n    // Get input and output shapes\n    let input = session.inputs.get(0).expect(\"Failed to get model input\");\n\n    // Define the shape for input data\n    let input_shape = input.dimensions.iter().map(|d| {\n        match d {\n            Some(dim) => *dim as usize,\n            None => 1,\n        }\n    }).collect::<Vec<_>>();\n\n    // Run the model\n    let input_tensor_values: Vec<Vec<f32>> = vec![input_data];\n    let outputs: Vec<Vec<f32>> = session\n        .run(vec![(\"input_tensor\".to_string(), input_tensor_values)])\n        .expect(\"Failed to run session\");\n\n    // Return the first output which is assumed to be our result\n    outputs.into_iter().next().unwrap()\n}\n\n#[wasm_bindgen]\npub fn alert(message: &str) {\n    let window = window().expect(\"should have a window in this context\");\n    window.alert_with_message(message).expect(\"alert should succeed\");\n}\n```",
    "label": 1
  },
  {
    "codes": "```python\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\n\n# Sample data\n# df = pd.read_csv('your_data.csv')\n\n# Specify the features and target\n# X = df.drop('target_column', axis=1)\n# y = df['target_column']\n\n# Splitting the dataset\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define numerical features (modify according to your dataset)\nnumerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# Create a Column Transformer to handle missing data and scaling\npreprocessing = ColumnTransformer(\n    transformers=[\n        ('num', Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\n            ('scaler', StandardScaler())                 # Feature scaling\n        ]), numerical_features)\n    ]\n)\n\n# Create the full pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessing)\n    # Add additional steps as needed (e.g., model fitting)\n])\n\n# Fit the pipeline on the training data\n# pipeline.fit(X_train)\n\n# Transform the test data\n# X_test_transformed = pipeline.transform(X_test)\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <fstream>\n#include <sstream>\n#include <map>\n#include <string>\n#include <algorithm>\n\n// Function to trim whitespace\nstd::string trim(const std::string& str) {\n    const std::string whitespace = \" \\t\\n\\r\";\n    const auto start = str.find_first_not_of(whitespace);\n    if(start == std::string::npos) return \"\";\n    const auto end = str.find_last_not_of(whitespace);\n    return str.substr(start, end - start + 1);\n}\n\n// Function to merge parent section into the child\nvoid mergeSections(std::map<std::string, std::string>& parent, std::map<std::string, std::string>& child) {\n    for (const auto &entry : parent) {\n        if (child.find(entry.first) == child.end()) {\n            child[entry.first] = entry.second;\n        }\n    }\n}\n\n// Parse INI file with section inheritance\nbool parseINI(const std::string &filename, std::map<std::string, std::map<std::string, std::string>>& sections) {\n    std::ifstream file(filename);\n    if (!file.is_open()) return false;\n\n    std::string line, currentSection;\n    while (getline(file, line)) {\n        line = trim(line);\n        if (line.empty() || line[0] == ';' || line[0] == '#') continue; // Skip comments\n\n        if (line[0] == '[' && line.back() == ']') { // Section\n            currentSection = line.substr(1, line.size() - 2); // Extract section name\n            auto colonPos = currentSection.find(':');\n            \n            if (colonPos != std::string::npos) {\n                std::string parentSection = trim(currentSection.substr(colonPos + 1));\n                currentSection = trim(currentSection.substr(0, colonPos));\n                if (sections.find(parentSection) != sections.end()) {\n                    sections[currentSection] = sections[parentSection];\n                }\n            }\n        } else if (!currentSection.empty()) { // Key-value pairs\n            const auto equalPos = line.find('=');\n            if (equalPos != std::string::npos) {\n                std::string key = trim(line.substr(0, equalPos));\n                std::string value = trim(line.substr(equalPos + 1));\n                sections[currentSection][key] = value;\n            }\n        }\n    }\n\n    file.close();\n    return true;\n}\n\nint main() {\n    std::map<std::string, std::map<std::string, std::string>> iniSections;\n    \n    if (!parseINI(\"config.ini\", iniSections)) {\n        std::cerr << \"Failed to open INI file.\" << std::endl;\n        return 1;\n    }\n\n    // Output parsed INI structure for verification\n    for (const auto &section : iniSections) {\n        std::cout << \"[\" << section.first << \"]\\n\";\n        for (const auto &kv : section.second) {\n            std::cout << kv.first << \"=\" << kv.second << \"\\n\";\n        }\n        std::cout << std::endl;\n    }\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "function autoCompleteResults_getElement(spec) {\n    var elem = null;\n\n    switch (spec.type) {\n      /**\n       * subtype: subtype to match\n       * value: value to match\n       */\n      case \"popup\":\n        elem = new elementslib.Lookup(this._controller.window.document, AUTOCOMPLETE_POPUP);\n        break;\n      case \"results\":\n        elem = new elementslib.Lookup(this._controller.window.document,\n                                      AUTOCOMPLETE_POPUP + '/anon({\"anonid\":\"richlistbox\"})');\n        break;\n      case \"result\":\n        elem = new elementslib.Elem(this._results.getNode().getItemAtIndex(spec.value));\n        break;\n      default:\n        throw new Error(arguments.callee.name + \": Unknown element type - \" + spec.type);\n    }\n\n    return elem;\n  }",
    "label": 0
  },
  {
    "codes": "func (s *CatalogEntry) SetTableName(v string) *CatalogEntry {\n\ts.TableName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function stringify (value, replacer, spaces, escape) {\n  // v8 checks arguments.length for optimizing simple call\n  // https://bugs.chromium.org/p/v8/issues/detail?id=4730\n  var json = replacer || spaces\n    ? JSON.stringify(value, replacer, spaces)\n    : JSON.stringify(value);\n\n  if (escape) {\n    json = json.replace(/[<>&]/g, function (c) {\n      switch (c.charCodeAt(0)) {\n        case 0x3c:\n          return '\\\\u003c'\n        case 0x3e:\n          return '\\\\u003e'\n        case 0x26:\n          return '\\\\u0026'\n        default:\n          return c\n      }\n    })\n  }\n\n  return json\n}",
    "label": 0
  },
  {
    "codes": "def json(self, attribs=None, recurse=True, ignorelist=False):\n        \"\"\"Serialises the FoLiA element and all its contents to a Python dictionary suitable for serialisation to JSON.\n\n        Example::\n\n            import json\n            json.dumps(word.json())\n\n        Returns:\n            dict\n        \"\"\"\n        jsonnode = {}\n\n        jsonnode['type'] = self.XMLTAG\n        if self.id:\n            jsonnode['id'] = self.id\n        if self.set:\n            jsonnode['set'] = self.set\n        if self.cls:\n            jsonnode['class'] = self.cls\n        if self.annotator:\n            jsonnode['annotator'] = self.annotator\n        if self.annotatortype:\n            if self.annotatortype == AnnotatorType.AUTO:\n                jsonnode['annotatortype'] = \"auto\"\n            elif self.annotatortype == AnnotatorType.MANUAL:\n                jsonnode['annotatortype'] = \"manual\"\n        if self.confidence is not None:\n            jsonnode['confidence'] = self.confidence\n        if self.n:\n            jsonnode['n'] = self.n\n        if self.auth:\n            jsonnode['auth'] = self.auth\n        if self.datetime:\n            jsonnode['datetime'] = self.datetime.strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n        if recurse: #pylint: disable=too-many-nested-blocks\n            jsonnode['children'] = []\n            if self.TEXTCONTAINER:\n                jsonnode['text'] = self.text()\n            if self.PHONCONTAINER:\n                jsonnode['phon'] = self.phon()\n            for child in self:\n                if self.TEXTCONTAINER and isstring(child):\n                    jsonnode['children'].append(child)\n                elif not self.PHONCONTAINER:\n                    #check ignore list\n                    ignore = False\n                    if ignorelist:\n                        for e in ignorelist:\n                            if isinstance(child,e):\n                                ignore = True\n                                break\n                    if not ignore:\n                        jsonnode['children'].append(child.json(attribs,recurse,ignorelist))\n\n        if attribs:\n            for attrib in attribs:\n                jsonnode[attrib] = attribs\n\n        return jsonnode",
    "label": 0
  },
  {
    "codes": "def render_request(self):\n        \"\"\"\n        Create a :class:`Data` object containing all fields known to the\n        :class:`Form`. If the :class:`Form` has a :attr:`LAYOUT` attribute, it\n        is used during generation.\n        \"\"\"\n\n        data = forms_xso.Data(type_=forms_xso.DataType.FORM)\n\n        try:\n            layout = self.LAYOUT\n        except AttributeError:\n            layout = list(self.DESCRIPTORS)\n\n        my_form_type = getattr(self, \"FORM_TYPE\", None)\n        if my_form_type is not None:\n            field_xso = forms_xso.Field()\n            field_xso.var = \"FORM_TYPE\"\n            field_xso.type_ = forms_xso.FieldType.HIDDEN\n            field_xso.values[:] = [my_form_type]\n            data.fields.append(field_xso)\n\n        for item in layout:\n            if isinstance(item, str):\n                field_xso = forms_xso.Field()\n                field_xso.type_ = forms_xso.FieldType.FIXED\n                field_xso.values[:] = [item]\n            else:\n                field_xso = item.__get__(\n                    self, type(self)\n                ).render()\n            data.fields.append(field_xso)\n\n        return data",
    "label": 0
  },
  {
    "codes": "```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\n# Define the Generator\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, img_channels, feature_map_size):\n        super(Generator, self).__init__()\n        self.net = nn.Sequential(\n            # Input is latent vector (z)\n            nn.ConvTranspose2d(latent_dim, feature_map_size * 8, 4, 1, 0),\n            nn.BatchNorm2d(feature_map_size * 8),\n            nn.ReLU(True),\n            # State size: (feature_map_size*8) x 4 x 4\n            nn.ConvTranspose2d(feature_map_size * 8, feature_map_size * 4, 4, 2, 1),\n            nn.BatchNorm2d(feature_map_size * 4),\n            nn.ReLU(True),\n            # State size: (feature_map_size*4) x 8 x 8\n            nn.ConvTranspose2d(feature_map_size * 4, feature_map_size * 2, 4, 2, 1),\n            nn.BatchNorm2d(feature_map_size * 2),\n            nn.ReLU(True),\n            # State size: (feature_map_size*2) x 16 x 16\n            nn.ConvTranspose2d(feature_map_size * 2, img_channels, 4, 2, 1),\n            nn.Tanh()\n            # Output size: img_channels x 32 x 32\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# Define the Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self, img_channels, feature_map_size):\n        super(Discriminator, self).__init__()\n        self.net = nn.Sequential(\n            # Input size: img_channels x 32 x 32\n            nn.Conv2d(img_channels, feature_map_size * 2, 4, 2, 1),\n            nn.LeakyReLU(0.2, inplace=True),\n            # State size: (feature_map_size*2) x 16 x 16\n            nn.Conv2d(feature_map_size * 2, feature_map_size * 4, 4, 2, 1),\n            nn.BatchNorm2d(feature_map_size * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # State size: (feature_map_size*4) x 8 x 8\n            nn.Conv2d(feature_map_size * 4, feature_map_size * 8, 4, 2, 1),\n            nn.BatchNorm2d(feature_map_size * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # State size: (feature_map_size*8) x 4 x 4\n            nn.Conv2d(feature_map_size * 8, 1, 4, 1, 0),\n            nn.Sigmoid()\n            # Output size: 1 (probability)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# Hyperparameters\nlatent_dim = 100\nimg_channels = 3\nfeature_map_size = 64\nbatch_size = 128\nnum_epochs = 50\nlr = 0.0002\nbeta1 = 0.5\n\n# Initialize Generator and Discriminator\ngenerator = Generator(latent_dim, img_channels, feature_map_size)\ndiscriminator = Discriminator(img_channels, feature_map_size)\n\n# Optimizers\noptimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n\n# Loss function\ncriterion = nn.BCELoss()\n\n# DataLoader\ntransform = transforms.Compose([\n    transforms.Resize(32),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndataset = datasets.ImageFolder(root='path_to_face_dataset', transform=transform)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Training loop\nfor epoch in range(num_epochs):\n    for i, (real_images, _) in enumerate(dataloader):\n        # Adversarial ground truths\n        real_labels = torch.ones(real_images.size(0), 1)\n        fake_labels = torch.zeros(real_images.size(0), 1)\n\n        # Train Discriminator\n        optimizer_D.zero_grad()\n        # Real images\n        real_output = discriminator(real_images)\n        d_loss_real = criterion(real_output, real_labels)\n        # Fake images\n        z = torch.randn(real_images.size(0), latent_dim, 1, 1)\n        fake_images = generator(z)\n        fake_output = discriminator(fake_images.detach())\n        d_loss_fake = criterion(fake_output, fake_labels)\n        # Total discriminator loss\n        d_loss = d_loss_real + d_loss_fake\n        d_loss.backward()\n        optimizer_D.step()\n\n        # Train Generator\n        optimizer_G.zero_grad()\n        fake_output = discriminator(fake_images)\n        g_loss = criterion(fake_output, real_labels)\n        g_loss.backward()\n        optimizer_G.step()\n\n        # Print losses\n        if i % 100 == 0:\n            print(f'Epoch [{epoch}/{num_epochs}] Batch {i}/{len(dataloader)} '\n                  f'Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}')\n\n    # Save generated images\n    with torch.no_grad():\n        z = torch.randn(64, latent_dim, 1, 1)\n        generated_images = generator(z)\n        plt.figure(figsize=(8, 8))\n        plt.axis(\"off\")\n        plt.imshow(torch.cat([\n            torch.cat([img for img in generated_images[i * 8:(i + 1) * 8]], dim=-1)\n            for i in range(8)], dim=-2).permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n        plt.savefig(f'generated_faces_epoch_{epoch}.png')\n        plt.close()\n```",
    "label": 1
  },
  {
    "codes": "```powershell\n# Define the registry paths to audit\n$registryPaths = @(\n    \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run\",\n    \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\",\n    \"HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\"\n)\n\n# Function to check permissions\nfunction Check-RegistryPermissions {\n    param (\n        [string]$path\n    )\n\n    # Get ACL for the registry path\n    $acl = Get-Acl -Path $path\n\n    # Check each access rule\n    foreach ($rule in $acl.Access) {\n        if ($rule.IdentityReference -eq \"BUILTIN\\Users\" -or $rule.IdentityReference -eq \"Everyone\") {\n            if ($rule.RegistryRights -match \"Write|FullControl\") {\n                Write-Host \"WARNING: Weak permissions found on $path for $($rule.IdentityReference)\"\n            }\n        }\n    }\n}\n\n# Iterate through each registry path and check permissions\nforeach ($path in $registryPaths) {\n    if (Test-Path $path) {\n        Check-RegistryPermissions -path $path\n    } else {\n        Write-Host \"Path $path does not exist.\"\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *CreatePartitionInput) SetCatalogId(v string) *CreatePartitionInput {\n\ts.CatalogId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```rust\nextern crate avro_rs;\nextern crate serde;\nextern crate serde_json;\n\nuse avro_rs::Schema;\nuse avro_rs::Writer;\nuse serde_json::Value;\nuse std::fs::File;\nuse std::io::{BufReader, BufWriter};\n\n// Function to read JSON file and output Avro format\nfn json_to_avro(json_file: &str, avro_file: &str, schema_file: &str) -> Result<(), Box<dyn std::error::Error>> {\n    // Open JSON file\n    let file = File::open(json_file)?;\n    let reader = BufReader::new(file);\n    let json_value: Value = serde_json::from_reader(reader)?;\n\n    // Open schema file\n    let schema_file = File::open(schema_file)?;\n    let schema_reader = BufReader::new(schema_file);\n    let schema_str: String = serde_json::from_reader(schema_reader)?;\n\n    // Parse schema\n    let schema = Schema::parse_str(&schema_str)?;\n\n    // Create Avro writer\n    let avro_output = File::create(avro_file)?;\n    let writer = BufWriter::new(avro_output);\n    let mut avro_writer = Writer::with_codec(&schema, writer, avro_rs::Codec::Deflate)?;\n\n    // Convert JSON to Avro and write\n    if let Some(json_array) = json_value.as_array() {\n        for record in json_array {\n            if let Some(record_map) = record.as_object() {\n                avro_writer.write(record_map)?;\n            }\n        }\n    }\n\n    avro_writer.flush()?;\n    Ok(())\n}\n\nfn main() {\n    // Input / Output file paths and schema path\n    let json_path = \"input.json\";\n    let avro_path = \"output.avro\";\n    let schema_path = \"schema.avsc\";\n\n    // Perform conversion\n    match json_to_avro(json_path, avro_path, schema_path) {\n        Ok(_) => println!(\"JSON to Avro conversion successful.\"),\n        Err(e) => eprintln!(\"Error during conversion: {:?}\", e),\n    }\n}\n```\n",
    "label": 1
  },
  {
    "codes": "public Observable<Void> purgeDeletedStorageAccountAsync(String vaultBaseUrl, String storageAccountName) {\n        return purgeDeletedStorageAccountWithServiceResponseAsync(vaultBaseUrl, storageAccountName).map(new Func1<ServiceResponse<Void>, Void>() {\n            @Override\n            public Void call(ServiceResponse<Void> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "function parse_BoundSheet8(blob, length, opts) {\n\tvar pos = blob.read_shift(4);\n\tvar hidden = blob.read_shift(1) & 0x03;\n\tvar dt = blob.read_shift(1);\n\tswitch(dt) {\n\t\tcase 0: dt = 'Worksheet'; break;\n\t\tcase 1: dt = 'Macrosheet'; break;\n\t\tcase 2: dt = 'Chartsheet'; break;\n\t\tcase 6: dt = 'VBAModule'; break;\n\t}\n\tvar name = parse_ShortXLUnicodeString(blob, 0, opts);\n\tif(name.length === 0) name = \"Sheet1\";\n\treturn { pos:pos, hs:hidden, dt:dt, name:name };\n}",
    "label": 0
  },
  {
    "codes": "public Observable<WorkerPoolResourceInner> updateMultiRolePoolAsync(String resourceGroupName, String name, WorkerPoolResourceInner multiRolePoolEnvelope) {\n        return updateMultiRolePoolWithServiceResponseAsync(resourceGroupName, name, multiRolePoolEnvelope).map(new Func1<ServiceResponse<WorkerPoolResourceInner>, WorkerPoolResourceInner>() {\n            @Override\n            public WorkerPoolResourceInner call(ServiceResponse<WorkerPoolResourceInner> response) {\n                return response.body();\n            }\n        });\n    }",
    "label": 0
  },
  {
    "codes": "```python\n# Definition for singly-linked list node\nclass ListNode:\n    def __init__(self, val=0, next=None):\n        self.val = val\n        self.next = next\n\n# Function to reverse the linked list\ndef reverse_linked_list(head):\n    prev = None  # Initialize previous node as None\n    current = head  # Start with the head of the list\n    \n    # Traverse the list and reverse the links\n    while current:\n        next_node = current.next  # Store the next node\n        current.next = prev  # Reverse the current node's pointer\n        prev = current  # Move prev to current node\n        current = next_node  # Move current to next node\n    \n    return prev  # prev will be the new head of the reversed list\n```",
    "label": 1
  },
  {
    "codes": "def query(self, sql, parameters=None):\n        \"\"\"A generator to issue a query on the server, mogrifying the\n        parameters against the sql statement. Results are returned as a\n        :py:class:`queries.Results` object which can act as an iterator and\n        has multiple ways to access the result data.\n\n        :param str sql: The SQL statement\n        :param dict parameters: A dictionary of query parameters\n        :rtype: queries.Results\n        :raises: queries.DataError\n        :raises: queries.DatabaseError\n        :raises: queries.IntegrityError\n        :raises: queries.InternalError\n        :raises: queries.InterfaceError\n        :raises: queries.NotSupportedError\n        :raises: queries.OperationalError\n        :raises: queries.ProgrammingError\n\n        \"\"\"\n        try:\n            self._cursor.execute(sql, parameters)\n        except psycopg2.Error as err:\n            self._incr_exceptions()\n            raise err\n        finally:\n            self._incr_executions()\n        return results.Results(self._cursor)",
    "label": 0
  },
  {
    "codes": "```python\nimport psutil\nimport time\nimport argparse\n\n# Function to get network usage per process\ndef get_network_usage():\n    processes = {}\n    for proc in psutil.process_iter(['pid', 'name']):\n        try:\n            connections = proc.connections()\n            if connections:\n                for conn in connections:\n                    if conn.status == psutil.CONN_ESTABLISHED:\n                        pid = proc.info['pid']\n                        name = proc.info['name']\n                        if pid not in processes:\n                            processes[pid] = {'name': name, 'sent': 0, 'recv': 0}\n                        io_counters = proc.io_counters()\n                        processes[pid]['sent'] += io_counters.bytes_sent\n                        processes[pid]['recv'] += io_counters.bytes_recv\n        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n            pass\n    return processes\n\n# Function to monitor network usage\ndef monitor_network_usage(interval=1):\n    previous_stats = get_network_usage()\n    while True:\n        time.sleep(interval)\n        current_stats = get_network_usage()\n        for pid, stats in current_stats.items():\n            if pid in previous_stats:\n                sent_diff = stats['sent'] - previous_stats[pid]['sent']\n                recv_diff = stats['recv'] - previous_stats[pid]['recv']\n                print(f\"PID: {pid}, Name: {stats['name']}, Sent: {sent_diff}, Recv: {recv_diff}\")\n            else:\n                print(f\"PID: {pid}, Name: {stats['name']}, Sent: {stats['sent']}, Recv: {stats['recv']}\")\n        previous_stats = current_stats\n\n# CLI argument parsing\ndef main():\n    parser = argparse.ArgumentParser(description=\"Monitor network bandwidth usage per process.\")\n    parser.add_argument('-i', '--interval', type=int, default=1, help=\"Interval in seconds to monitor network usage.\")\n    args = parser.parse_args()\n    monitor_network_usage(args.interval)\n\nif __name__ == \"__main__\":\n    main()\n```",
    "label": 1
  },
  {
    "codes": "function make_sector_list(sectors, dir_start, fat_addrs, ssz) {\n\tvar sl = sectors.length, sector_list = ([]);\n\tvar chkd = [], buf = [], buf_chain = [];\n\tvar modulus = ssz - 1, i=0, j=0, k=0, jj=0;\n\tfor(i=0; i < sl; ++i) {\n\t\tbuf = ([]);\n\t\tk = (i + dir_start); if(k >= sl) k-=sl;\n\t\tif(chkd[k]) continue;\n\t\tbuf_chain = [];\n\t\tfor(j=k; j>=0;) {\n\t\t\tchkd[j] = true;\n\t\t\tbuf[buf.length] = j;\n\t\t\tbuf_chain.push(sectors[j]);\n\t\t\tvar addr = fat_addrs[Math.floor(j*4/ssz)];\n\t\t\tjj = ((j*4) & modulus);\n\t\t\tif(ssz < 4 + jj) throw new Error(\"FAT boundary crossed: \" + j + \" 4 \"+ssz);\n\t\t\tif(!sectors[addr]) break;\n\t\t\tj = __readInt32LE(sectors[addr], jj);\n\t\t}\n\t\tsector_list[k] = ({nodes: buf, data:__toBuffer([buf_chain])});\n\t}\n\treturn sector_list;\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<List<OperationInner>>> listOperationsWithServiceResponseAsync(String resourceGroupName, String name) {\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (name == null) {\n            throw new IllegalArgumentException(\"Parameter name is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (this.client.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.apiVersion() is required and cannot be null.\");\n        }\n        return service.listOperations(resourceGroupName, name, this.client.subscriptionId(), this.client.apiVersion(), this.client.acceptLanguage(), this.client.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<List<OperationInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<List<OperationInner>>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<List<OperationInner>> clientResponse = listOperationsDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "```python\nimport pytest\nimport requests\n\n# Test case to validate the API response schema\ndef test_api_response_schema():\n    # Make a GET request to the API\n    response = requests.get('https://api.example.com/data')\n    \n    # Assert the status code is 200 (OK)\n    assert response.status_code == 200, \"Expected status code 200\"\n    \n    # Validate the response schema\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"id\": {\"type\": \"number\"},\n            \"name\": {\"type\": \"string\"},\n            \"is_active\": {\"type\": \"boolean\"}\n        },\n        \"required\": [\"id\", \"name\"]\n    }\n    \n    # Assert the response matches the schema\n    assert response.json() == schema, \"Response does not match the expected schema\"\n\n# Test case to validate the API response schema with different data\ndef test_api_response_schema_with_different_data():\n    # Make a GET request to the API with different parameters\n    response = requests.get('https://api.example.com/data?type=special')\n    \n    # Assert the status code is 200 (OK)\n    assert response.status_code == 200, \"Expected status code 200\"\n    \n    # Validate the response schema for different data\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"id\": {\"type\": \"number\"},\n            \"description\": {\"type\": \"string\"},\n            \"tags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n        },\n        \"required\": [\"id\", \"description\"]\n    }\n    \n    # Assert the response matches the schema\n    assert response.json() == schema, \"Response does not match the expected schema for different data\"\n\n# Test case to validate the API response schema with error handling\ndef test_api_response_schema_with_error_handling():\n    # Make a GET request to the API with invalid parameters\n    response = requests.get('https://api.example.com/data?invalid_param=123')\n    \n    # Assert the status code is 400 (Bad Request)\n    assert response.status_code == 400, \"Expected status code 400\"\n    \n    # Validate the error response schema\n    error_schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"error\": {\"type\": \"string\"},\n            \"message\": {\"type\": \"string\"}\n        },\n        \"required\": [\"error\", \"message\"]\n    }\n    \n    # Assert the error response matches the schema\n    assert response.json() == error_schema, \"Error response does not match the expected schema\"\n```",
    "label": 1
  },
  {
    "codes": "```go\npackage main\n\nimport (\n\t\"crypto/rand\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"strings\"\n)\n\n// generatePassword generates a random password of the specified length\n// using the provided character sets.\nfunc generatePassword(length int, charSets []string) (string, error) {\n\t// Combine all character sets into a single string\n\tallChars := strings.Join(charSets, \"\")\n\n\t// Precompute the length of the combined character set\n\tcharLen := big.NewInt(int64(len(allChars)))\n\n\t// Initialize a byte slice to store the password\n\tpassword := make([]byte, length)\n\n\t// Generate random bytes for each position in the password\n\tfor i := 0; i < length; i++ {\n\t\t// Generate a random index within the range of the combined character set\n\t\trandIndex, err := rand.Int(rand.Reader, charLen)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\t// Use the random index to select a character from the combined set\n\t\tpassword[i] = allChars[randIndex.Int64()]\n\t}\n\n\treturn string(password), nil\n}\n\nfunc main() {\n\t// Define the character sets to use for the password\n\tcharSets := []string{\n\t\t\"abcdefghijklmnopqrstuvwxyz\",\n\t\t\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\",\n\t\t\"0123456789\",\n\t\t\"!@#$%^&*()_+\",\n\t}\n\n\t// Specify the desired password length\n\tlength := 16\n\n\t// Generate the password\n\tpassword, err := generatePassword(length, charSets)\n\tif err != nil {\n\t\tfmt.Println(\"Error generating password:\", err)\n\t\treturn\n\t}\n\n\t// Print the generated password\n\tfmt.Println(\"Generated Password:\", password)\n}\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <stdexcept>\n\nclass DatabaseConnection {\npublic:\n    DatabaseConnection(const std::string &connectionString) {\n        // Imagine connecting to a database using the connection string\n        std::cout << \"Connecting to database...\" << std::endl;\n        if (connectionString.empty()) {\n            throw std::invalid_argument(\"Invalid connection string\");\n        }\n        // Simulate a successful connection\n        isConnected = true;\n    }\n    \n    ~DatabaseConnection() {\n        // Ensure the connection is closed properly\n        if (isConnected) {\n            std::cout << \"Disconnecting from database...\" << std::endl;\n            // Simulate closing the connection\n            isConnected = false;\n        }\n    }\n    \n    // Disallow copying\n    DatabaseConnection(const DatabaseConnection &) = delete;\n    DatabaseConnection &operator=(const DatabaseConnection &) = delete;\n\n    // Allow moving\n    DatabaseConnection(DatabaseConnection &&other) noexcept\n        : isConnected(other.isConnected) {\n        other.isConnected = false;\n    }\n\n    DatabaseConnection &operator=(DatabaseConnection &&other) noexcept {\n        if (this != &other) {\n            if (isConnected) {\n                // Close existing connection\n                std::cout << \"Disconnecting from database...\" << std::endl;\n                isConnected = false;\n            }\n            // Move the connection state\n            isConnected = other.isConnected;\n            other.isConnected = false;\n        }\n        return *this;\n    }\n\n    void executeQuery(const std::string &query) {\n        if (!isConnected) {\n            throw std::runtime_error(\"Not connected to a database\");\n        }\n        // Simulate executing a query\n        std::cout << \"Executing query: \" << query << std::endl;\n    }\n\nprivate:\n    bool isConnected = false;\n};\n\nvoid performDatabaseOperations() {\n    try {\n        DatabaseConnection dbConn(\"valid_connection_string\");\n        dbConn.executeQuery(\"SELECT * FROM users\");\n    } catch (const std::exception &e) {\n        std::cerr << \"Exception: \" << e.what() << std::endl;\n    }\n}\n\n// Example usage\nint main() {\n    performDatabaseOperations();\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "func (s *CreateTriggerInput) SetName(v string) *CreateTriggerInput {\n\ts.Name = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\n# Define elemental types and their weaknesses\nELEMENTS = {\n    'fire': 'water',\n    'water': 'earth',\n    'earth': 'air',\n    'air': 'fire'\n}\n\n# Define player and enemy classes\nclass Character:\n    def __init__(self, name, health, element):\n        self.name = name\n        self.health = health\n        self.element = element\n\n    def take_damage(self, damage):\n        self.health -= damage\n        if self.health < 0:\n            self.health = 0\n\n    def is_alive(self):\n        return self.health > 0\n\n# Combat function\ndef combat(player, enemy):\n    print(f\"A wild {enemy.name} appears!\")\n    while player.is_alive() and enemy.is_alive():\n        print(f\"{player.name} (HP: {player.health}) vs {enemy.name} (HP: {enemy.health})\")\n        print(\"Choose your action: [1] Attack, [2] Defend\")\n        action = input(\"> \")\n\n        if action == \"1\":\n            # Calculate damage based on elemental weakness\n            if ELEMENTS[player.element] == enemy.element:\n                damage = 20  # Weakness bonus\n                print(f\"{player.name} exploits {enemy.name}'s weakness!\")\n            else:\n                damage = 10\n            enemy.take_damage(damage)\n            print(f\"{player.name} attacks {enemy.name} for {damage} damage!\")\n        elif action == \"2\":\n            print(f\"{player.name} defends and takes reduced damage!\")\n            damage = 5\n            player.take_damage(damage)\n        else:\n            print(\"Invalid action! Try again.\")\n            continue\n\n        # Enemy's turn\n        if enemy.is_alive():\n            enemy_damage = 10\n            player.take_damage(enemy_damage)\n            print(f\"{enemy.name} attacks {player.name} for {enemy_damage} damage!\")\n\n    # Determine the winner\n    if player.is_alive():\n        print(f\"{player.name} defeated {enemy.name}!\")\n    else:\n        print(f\"{player.name} was defeated by {enemy.name}...\")\n\n# Example usage\nplayer = Character(\"Hero\", 100, 'fire')\nenemy = Character(\"Goblin\", 50, 'water')\n\ncombat(player, enemy)\n```",
    "label": 1
  },
  {
    "codes": "def _weightfun_spatial_distance(data, params, report):\n    \"\"\"\n    Creates the weights for the spatial distance method. See func: teneto.derive.derive.\n    \"\"\"\n    distance = getDistanceFunction(params['distance'])\n    weights = np.array([distance(data[n, :], data[t, :]) for n in np.arange(\n        0, data.shape[0]) for t in np.arange(0, data.shape[0])])\n    weights = np.reshape(weights, [data.shape[0], data.shape[0]])\n    np.fill_diagonal(weights, np.nan)\n    weights = 1 / weights\n    weights = (weights - np.nanmin(weights)) / \\\n        (np.nanmax(weights) - np.nanmin(weights))\n    np.fill_diagonal(weights, 1)\n    return weights, report",
    "label": 0
  },
  {
    "codes": "function parse_FontFlags(data) {\n\tvar d = data.read_shift(1);\n\tdata.l++;\n\tvar out = {\n\t\t/* fBold: d & 0x01 */\n\t\tfItalic: d & 0x02,\n\t\t/* fUnderline: d & 0x04 */\n\t\tfStrikeout: d & 0x08,\n\t\tfOutline: d & 0x10,\n\t\tfShadow: d & 0x20,\n\t\tfCondense: d & 0x40,\n\t\tfExtend: d & 0x80\n\t};\n\treturn out;\n}",
    "label": 0
  },
  {
    "codes": "Here is a possible implementation for a Next.js page that uses server-side rendering (SSR) to fetch data from a PostgreSQL database:\n\n```javascript\n// pages/index.js\n\nimport { useState, useEffect } from 'react';\n\n// Server-side function to get data from the database\nexport async function getServerSideProps() {\n  const { Pool } = require('pg');\n\n  // Create a new pool instance\n  const pool = new Pool({\n    user: 'yourUser',\n    host: 'localhost',\n    database: 'yourDatabase',\n    password: 'yourPassword',\n    port: 5432,\n  });\n\n  try {\n    // Connect to the Postgres client\n    const client = await pool.connect();\n\n    // Perform SQL query on your database\n    const res = await client.query('SELECT * FROM yourTable');\n\n    // Close the connection\n    client.release();\n\n    // Pass data to the page component as props\n    return { props: { data: res.rows } };\n  } catch (err) {\n    console.error(err);\n    return { props: { data: [] } };\n  }\n}\n\n// Component to display data\nconst HomePage = ({ data }) => {\n  return (\n    <div>\n      <h1>Data from PostgreSQL</h1>\n      <ul>\n        {data.map((item) => (\n          <li key={item.id}>{item.name}</li>\n        ))}\n      </ul>\n    </div>\n  );\n};\n\nexport default HomePage;\n```\n\nEnsure you replace `yourUser`, `yourDatabase`, `yourPassword`, `yourTable`, `localhost`, and `5432` with actual credentials and details specific to your database setup.",
    "label": 1
  },
  {
    "codes": "public ServiceFuture<KeyVerifyResult> verifyAsync(String vaultBaseUrl, String keyName, String keyVersion, JsonWebKeySignatureAlgorithm algorithm, byte[] digest, byte[] signature, final ServiceCallback<KeyVerifyResult> serviceCallback) {\n        return ServiceFuture.fromResponse(verifyWithServiceResponseAsync(vaultBaseUrl, keyName, keyVersion, algorithm, digest, signature), serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "```yaml\n# Chaos Engineering Experiment: Random Pod Termination\n\napiVersion: litmuschaos.io/v1alpha1\nkind: ChaosEngine\nmetadata:\n  name: pod-delete-chaos\n  namespace: default\nspec:\n  # To specify the application under test\n  appinfo: \n    appns: default\n    applabel: \"app=your-app-label\"  # Applicaton Label\n    appkind: \"deployment\"\n    \n  # Choose a chaos service account with necessary permissions\n  chaosServiceAccount: pod-delete-sa\n\n  # Specify the type of chaos experiment\n  experiments:\n    - name: pod-delete\n      spec:\n        components:\n          env:\n            # Duration of the chaos experiment\n            - name: TOTAL_CHAOS_DURATION\n              value: '60' # seconds\n            \n            # Interval between two successive pod terminations\n            - name: CHAOS_INTERVAL\n              value: '10' # seconds\n            \n            # Define the pods affected percentage\n            - name: PODS_AFFECTED_PERC\n              value: '50' # percentage\n\n            # Terminate all pods with a specific label\n            - name: TARGET_PODS \n              value: 'app=your-app-label'\n```\n",
    "label": 1
  },
  {
    "codes": "```python\nimport re\n\ndef validate_email(email):\n    # Regex pattern to validate email addresses\n    pattern = r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$'\n    \n    # Check if the email matches the pattern\n    if re.match(pattern, email):\n        return True\n    else:\n        return False\n```\n\n```python\n# Example usage:\n# print(validate_email(\"example@example.com\"))  # Output: True\n# print(validate_email(\"invalid-email@\"))       # Output: False\n```",
    "label": 1
  },
  {
    "codes": "func (s *ConnectionInput) Validate() error {\n\tinvalidParams := request.ErrInvalidParams{Context: \"ConnectionInput\"}\n\tif s.ConnectionProperties == nil {\n\t\tinvalidParams.Add(request.NewErrParamRequired(\"ConnectionProperties\"))\n\t}\n\tif s.ConnectionType == nil {\n\t\tinvalidParams.Add(request.NewErrParamRequired(\"ConnectionType\"))\n\t}\n\tif s.Name == nil {\n\t\tinvalidParams.Add(request.NewErrParamRequired(\"Name\"))\n\t}\n\tif s.Name != nil && len(*s.Name) < 1 {\n\t\tinvalidParams.Add(request.NewErrParamMinLen(\"Name\", 1))\n\t}\n\tif s.PhysicalConnectionRequirements != nil {\n\t\tif err := s.PhysicalConnectionRequirements.Validate(); err != nil {\n\t\t\tinvalidParams.AddNested(\"PhysicalConnectionRequirements\", err.(request.ErrInvalidParams))\n\t\t}\n\t}\n\n\tif invalidParams.Len() > 0 {\n\t\treturn invalidParams\n\t}\n\treturn nil\n}",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Page<ResourceMetricInner>>> listWebWorkerMetricsNextWithServiceResponseAsync(final String nextPageLink) {\n        return listWebWorkerMetricsNextSinglePageAsync(nextPageLink)\n            .concatMap(new Func1<ServiceResponse<Page<ResourceMetricInner>>, Observable<ServiceResponse<Page<ResourceMetricInner>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<ResourceMetricInner>>> call(ServiceResponse<Page<ResourceMetricInner>> page) {\n                    String nextPageLink = page.body().nextPageLink();\n                    if (nextPageLink == null) {\n                        return Observable.just(page);\n                    }\n                    return Observable.just(page).concatWith(listWebWorkerMetricsNextWithServiceResponseAsync(nextPageLink));\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "def connect(self, coro):\n        \"\"\"\n        The coroutine `coro` is connected to the signal. The coroutine must\n        return a true value, unless it wants to be disconnected from the\n        signal.\n\n        .. note::\n\n           This is different from the return value convention with\n           :attr:`AdHocSignal.STRONG` and :attr:`AdHocSignal.WEAK`.\n\n        :meth:`connect` returns a token which can be used with\n        :meth:`disconnect` to disconnect the coroutine.\n        \"\"\"\n        self.logger.debug(\"connecting %r\", coro)\n        return self._connect(coro)",
    "label": 0
  },
  {
    "codes": "```typescript\n// Define a base interface for geometric shapes\ninterface GeometricShape {\n    // Method to calculate the area of the shape\n    calculateArea(): number;\n}\n\n// Example implementation for a Circle\nclass Circle implements GeometricShape {\n    constructor(private radius: number) {}\n\n    calculateArea(): number {\n        return Math.PI * this.radius * this.radius;\n    }\n}\n\n// Example implementation for a Rectangle\nclass Rectangle implements GeometricShape {\n    constructor(private width: number, private height: number) {}\n\n    calculateArea(): number {\n        return this.width * this.height;\n    }\n}\n\n// Example implementation for a Triangle\nclass Triangle implements GeometricShape {\n    constructor(private base: number, private height: number) {}\n\n    calculateArea(): number {\n        return (this.base * this.height) / 2;\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "def attach_tracker(self, stanza, tracker=None):\n        \"\"\"\n        Return a new tracker or modify one to track the stanza.\n\n        :param stanza: Stanza to track.\n        :type stanza: :class:`aioxmpp.Message`\n        :param tracker: Existing tracker to attach to.\n        :type tracker: :class:`.tracking.MessageTracker`\n        :raises ValueError: if the stanza is of type\n            :attr:`~aioxmpp.MessageType.ERROR`\n        :raises ValueError: if the stanza contains a delivery receipt\n        :return: The message tracker for the stanza.\n        :rtype: :class:`.tracking.MessageTracker`\n\n        The `stanza` gets a :xep:`184` reciept request attached and internal\n        handlers are set up to update the `tracker` state once a confirmation\n        is received.\n\n        .. warning::\n\n           See the :ref:`api-tracking-memory`.\n\n        \"\"\"\n        if stanza.xep0184_received is not None:\n            raise ValueError(\n                \"requesting delivery receipts for delivery receipts is not \"\n                \"allowed\"\n            )\n        if stanza.type_ == aioxmpp.MessageType.ERROR:\n            raise ValueError(\n                \"requesting delivery receipts for errors is not supported\"\n            )\n\n        if tracker is None:\n            tracker = aioxmpp.tracking.MessageTracker()\n\n        stanza.xep0184_request_receipt = True\n        stanza.autoset_id()\n        self._bare_jid_maps[stanza.to, stanza.id_] = tracker\n        return tracker",
    "label": 0
  },
  {
    "codes": "```rust\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse std::time::{Duration, Instant};\nuse tokio::sync::Mutex;\nuse hyper::{Body, Request, Response, Server};\nuse hyper::service::{make_service_fn, service_fn};\nuse hyper::StatusCode;\n\n// Struct to hold rate limit information for each IP\nstruct RateLimiter {\n    requests: HashMap<String, (Instant, u32)>,\n    limit: u32,\n    window: Duration,\n}\n\nimpl RateLimiter {\n    fn new(limit: u32, window: Duration) -> Self {\n        RateLimiter {\n            requests: HashMap::new(),\n            limit,\n            window,\n        }\n    }\n\n    // Check if the IP has exceeded the rate limit\n    fn check(&mut self, ip: &str) -> bool {\n        let now = Instant::now();\n        let (start_time, count) = self.requests.entry(ip.to_string()).or_insert((now, 0));\n\n        if now.duration_since(*start_time) > self.window {\n            *start_time = now;\n            *count = 0;\n        }\n\n        if *count >= self.limit {\n            false // Rate limit exceeded\n        } else {\n            *count += 1;\n            true // Request allowed\n        }\n    }\n}\n\n// Shared state for the rate limiter\ntype SharedRateLimiter = Arc<Mutex<RateLimiter>>;\n\n// Handler for incoming HTTP requests\nasync fn handle_request(\n    req: Request<Body>,\n    rate_limiter: SharedRateLimiter,\n) -> Result<Response<Body>, hyper::Error> {\n    let ip = req\n        .headers()\n        .get(\"x-forwarded-for\")\n        .and_then(|v| v.to_str().ok())\n        .unwrap_or(\"unknown\");\n\n    let mut rate_limiter = rate_limiter.lock().await;\n\n    if !rate_limiter.check(ip) {\n        // Return 429 Too Many Requests if rate limit is exceeded\n        return Ok(Response::builder()\n            .status(StatusCode::TOO_MANY_REQUESTS)\n            .body(Body::from(\"Rate limit exceeded\"))\n            .unwrap());\n    }\n\n    // Process the request\n    Ok(Response::new(Body::from(\"Hello, World!\")))\n}\n\n#[tokio::main]\nasync fn main() {\n    // Initialize the rate limiter with a limit of 10 requests per 60 seconds\n    let rate_limiter = Arc::new(Mutex::new(RateLimiter::new(10, Duration::from_secs(60))));\n\n    // Create a service to handle requests\n    let make_svc = make_service_fn(move |_| {\n        let rate_limiter = Arc::clone(&rate_limiter);\n        async move {\n            Ok::<_, hyper::Error>(service_fn(move |req| {\n                handle_request(req, Arc::clone(&rate_limiter))\n            }))\n        }\n    });\n\n    // Bind the server to localhost:3000\n    let addr = ([127, 0, 0, 1], 3000).into();\n    let server = Server::bind(&addr).serve(make_svc);\n\n    // Run the server\n    if let Err(e) = server.await {\n        eprintln!(\"Server error: {}\", e);\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "def load(self, filename):\n        \"\"\"\n        Load a npz file. Supports only files previously saved by\n        :meth:`pypianoroll.Multitrack.save`.\n\n        Notes\n        -----\n        Attribute values will all be overwritten.\n\n        Parameters\n        ----------\n        filename : str\n            The name of the npz file to be loaded.\n\n        \"\"\"\n        def reconstruct_sparse(target_dict, name):\n            \"\"\"Return a reconstructed instance of `scipy.sparse.csc_matrix`.\"\"\"\n            return csc_matrix((target_dict[name+'_csc_data'],\n                               target_dict[name+'_csc_indices'],\n                               target_dict[name+'_csc_indptr']),\n                              shape=target_dict[name+'_csc_shape']).toarray()\n\n        with np.load(filename) as loaded:\n            if 'info.json' not in loaded:\n                raise ValueError(\"Cannot find 'info.json' in the npz file.\")\n            info_dict = json.loads(loaded['info.json'].decode('utf-8'))\n            self.name = info_dict['name']\n            self.beat_resolution = info_dict['beat_resolution']\n\n            self.tempo = loaded['tempo']\n            if 'downbeat' in loaded.files:\n                self.downbeat = loaded['downbeat']\n            else:\n                self.downbeat = None\n\n            idx = 0\n            self.tracks = []\n            while str(idx) in info_dict:\n                pianoroll = reconstruct_sparse(\n                    loaded, 'pianoroll_{}'.format(idx))\n                track = Track(pianoroll, info_dict[str(idx)]['program'],\n                              info_dict[str(idx)]['is_drum'],\n                              info_dict[str(idx)]['name'])\n                self.tracks.append(track)\n                idx += 1\n\n        self.check_validity()",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<TroubleshootingResultInner>> getTroubleshootingWithServiceResponseAsync(String resourceGroupName, String networkWatcherName, TroubleshootingParameters parameters) {\n        if (resourceGroupName == null) {\n            throw new IllegalArgumentException(\"Parameter resourceGroupName is required and cannot be null.\");\n        }\n        if (networkWatcherName == null) {\n            throw new IllegalArgumentException(\"Parameter networkWatcherName is required and cannot be null.\");\n        }\n        if (this.client.subscriptionId() == null) {\n            throw new IllegalArgumentException(\"Parameter this.client.subscriptionId() is required and cannot be null.\");\n        }\n        if (parameters == null) {\n            throw new IllegalArgumentException(\"Parameter parameters is required and cannot be null.\");\n        }\n        Validator.validate(parameters);\n        final String apiVersion = \"2018-06-01\";\n        Observable<Response<ResponseBody>> observable = service.getTroubleshooting(resourceGroupName, networkWatcherName, this.client.subscriptionId(), parameters, apiVersion, this.client.acceptLanguage(), this.client.userAgent());\n        return client.getAzureClient().getPostOrDeleteResultAsync(observable, new TypeToken<TroubleshootingResultInner>() { }.getType());\n    }",
    "label": 0
  },
  {
    "codes": "def lookup_srv(\n        domain: bytes,\n        service: str,\n        transport: str = \"tcp\",\n        **kwargs):\n    \"\"\"\n    Query the DNS for SRV records describing how the given `service` over the\n    given `transport` is implemented for the given `domain`. `domain` must be\n    an IDNA-encoded :class:`bytes` object; `service` must be a normal\n    :class:`str`.\n\n    Keyword arguments are passed to :func:`repeated_query`.\n\n    Return a list of tuples ``(prio, weight, (hostname, port))``, where\n    `hostname` is a IDNA-encoded :class:`bytes` object containing the hostname\n    obtained from the SRV record. The other fields are also as obtained from\n    the SRV records. The trailing dot is stripped from the `hostname`.\n\n    If the DNS query returns an empty result, :data:`None` is returned. If any\n    of the found SRV records has the root zone (``.``) as `hostname`, this\n    indicates that the service is not available at the given `domain` and\n    :class:`ValueError` is raised.\n    \"\"\"\n\n    record = b\".\".join([\n        b\"_\" + service.encode(\"ascii\"),\n        b\"_\" + transport.encode(\"ascii\"),\n        domain])\n\n    answer = yield from repeated_query(\n        record,\n        dns.rdatatype.SRV,\n        **kwargs)\n\n    if answer is None:\n        return None\n\n    items = [\n        (rec.priority, rec.weight, (str(rec.target), rec.port))\n        for rec in answer\n    ]\n\n    for i, (prio, weight, (host, port)) in enumerate(items):\n        if host == \".\":\n            raise ValueError(\n                \"protocol {!r} over {!r} not supported at {!r}\".format(\n                    service,\n                    transport,\n                    domain\n                )\n            )\n\n        items[i] = (prio, weight, (\n            host.rstrip(\".\").encode(\"ascii\"),\n            port))\n\n    return items",
    "label": 0
  },
  {
    "codes": "```csharp\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class MazeGenerator : MonoBehaviour\n{\n    public int width = 10; // Width of the maze\n    public int height = 10; // Height of the maze\n    public GameObject wallPrefab; // Prefab for the wall\n    public GameObject floorPrefab; // Prefab for the floor\n\n    private bool[,] visitedCells; // Track visited cells\n    private List<Vector2Int> directions = new List<Vector2Int>\n    {\n        new Vector2Int(1, 0),  // Right\n        new Vector2Int(-1, 0), // Left\n        new Vector2Int(0, 1),  // Up\n        new Vector2Int(0, -1)  // Down\n    };\n\n    void Start()\n    {\n        GenerateMaze();\n    }\n\n    void GenerateMaze()\n    {\n        visitedCells = new bool[width, height];\n        for (int x = 0; x < width; x++)\n        {\n            for (int y = 0; y < height; y++)\n            {\n                Instantiate(floorPrefab, new Vector3(x, 0, y), Quaternion.identity); // Create floor\n                if (x == 0 || x == width - 1 || y == 0 || y == height - 1)\n                {\n                    Instantiate(wallPrefab, new Vector3(x, 0.5f, y), Quaternion.identity); // Create border walls\n                }\n            }\n        }\n\n        CarvePassage(1, 1); // Start carving from the top-left corner\n    }\n\n    void CarvePassage(int x, int y)\n    {\n        visitedCells[x, y] = true;\n\n        // Shuffle directions to randomize maze generation\n        directions.Sort((a, b) => Random.Range(-1, 2));\n\n        foreach (var dir in directions)\n        {\n            int newX = x + dir.x * 2;\n            int newY = y + dir.y * 2;\n\n            if (newX > 0 && newX < width - 1 && newY > 0 && newY < height - 1 && !visitedCells[newX, newY])\n            {\n                // Remove wall between current cell and new cell\n                Instantiate(floorPrefab, new Vector3(x + dir.x, 0.5f, y + dir.y), Quaternion.identity);\n                CarvePassage(newX, newY); // Recursively carve passages\n            }\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "function parse_MTRSettings(blob) {\n\tvar fMTREnabled = blob.read_shift(4) !== 0x00;\n\tvar fUserSetThreadCount = blob.read_shift(4) !== 0x00;\n\tvar cUserThreadCount = blob.read_shift(4);\n\treturn [fMTREnabled, fUserSetThreadCount, cUserThreadCount];\n}",
    "label": 0
  },
  {
    "codes": "public ServiceFuture<List<DeletedSecretItem>> getDeletedSecretsNextAsync(final String nextPageLink, final ServiceFuture<List<DeletedSecretItem>> serviceFuture, final ListOperationCallback<DeletedSecretItem> serviceCallback) {\n        return AzureServiceFuture.fromPageResponse(\n            getDeletedSecretsNextSinglePageAsync(nextPageLink),\n            new Func1<String, Observable<ServiceResponse<Page<DeletedSecretItem>>>>() {\n                @Override\n                public Observable<ServiceResponse<Page<DeletedSecretItem>>> call(String nextPageLink) {\n                    return getDeletedSecretsNextSinglePageAsync(nextPageLink);\n                }\n            },\n            serviceCallback);\n    }",
    "label": 0
  },
  {
    "codes": "function( singleValue ) {\n\t\tvar\n\n\t\t\t// count of uncompleted subordinates\n\t\t\tremaining = arguments.length,\n\n\t\t\t// count of unprocessed arguments\n\t\t\ti = remaining,\n\n\t\t\t// subordinate fulfillment data\n\t\t\tresolveContexts = Array( i ),\n\t\t\tresolveValues = slice.call( arguments ),\n\n\t\t\t// the master Deferred\n\t\t\tmaster = jQuery.Deferred(),\n\n\t\t\t// subordinate callback factory\n\t\t\tupdateFunc = function( i ) {\n\t\t\t\treturn function( value ) {\n\t\t\t\t\tresolveContexts[ i ] = this;\n\t\t\t\t\tresolveValues[ i ] = arguments.length > 1 ? slice.call( arguments ) : value;\n\t\t\t\t\tif ( !( --remaining ) ) {\n\t\t\t\t\t\tmaster.resolveWith( resolveContexts, resolveValues );\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t};\n\n\t\t// Single- and empty arguments are adopted like Promise.resolve\n\t\tif ( remaining <= 1 ) {\n\t\t\tadoptValue( singleValue, master.done( updateFunc( i ) ).resolve, master.reject,\n\t\t\t\t!remaining );\n\n\t\t\t// Use .then() to unwrap secondary thenables (cf. gh-3000)\n\t\t\tif ( master.state() === \"pending\" ||\n\t\t\t\ttypeof( resolveValues[ i ] && resolveValues[ i ].then ) === \"function\" ) {\n\n\t\t\t\treturn master.then();\n\t\t\t}\n\t\t}\n\n\t\t// Multiple arguments are aggregated like Promise.all array elements\n\t\twhile ( i-- ) {\n\t\t\tadoptValue( resolveValues[ i ], updateFunc( i ), master.reject );\n\t\t}\n\n\t\treturn master.promise();\n\t}",
    "label": 0
  },
  {
    "codes": "```rust\nfn is_valid_isbn13(isbn: &str) -> bool {\n    // Remove hyphens and whitespace\n    let cleaned_isbn: String = isbn.chars().filter(|c| c.is_digit(10)).collect();\n\n    // Check if the length is 13 and contains only digits\n    if cleaned_isbn.len() != 13 || !cleaned_isbn.chars().all(|c| c.is_digit(10)) {\n        return false;\n    }\n\n    // Calculate the checksum\n    let checksum: u32 = cleaned_isbn\n        .chars()\n        .enumerate()\n        .map(|(i, c)| {\n            let digit = c.to_digit(10).unwrap();\n            if i % 2 == 0 {\n                digit\n            } else {\n                digit * 3\n            }\n        })\n        .sum();\n\n    // Validate the checksum\n    checksum % 10 == 0\n}\n```",
    "label": 1
  },
  {
    "codes": "```ini\n; PlatformIO Configuration for Multiple MCUs\n\n; Define default environment\n[platformio]\ndefault_envs = mcu1, mcu2\n\n; Environment for MCU1 (e.g., Arduino Uno)\n[env:mcu1]\nplatform = atmelavr\nboard = uno\nframework = arduino\nupload_speed = 115200\n; Additional build flags or libraries can be added here\nbuild_flags = \nlib_deps = \n\n; Environment for MCU2 (e.g., ESP32)\n[env:mcu2]\nplatform = espressif32\nboard = esp32dev\nframework = arduino\nupload_speed = 921600\nmonitor_speed = 115200\n; Additional build flags or libraries can be added here\nbuild_flags = \nlib_deps = \n\n; Common settings for all environments (if needed)\n[env]\n; Example: Set the default port for uploads\nupload_port = /dev/ttyUSB0\n\n; Additional configurations for other MCUs can be added similarly\n```\n",
    "label": 1
  },
  {
    "codes": "function transpileDerivedExchangeFile (folder, filename) {\n\n    try {\n\n        let contents = fs.readFileSync (folder + filename, 'utf8')\n\n        let { python2, python3, php, className, baseClass } = transpileDerivedExchangeClass (contents)\n\n        const python2Filename = python2Folder + filename.replace ('.js', '.py')\n        const python3Filename = python3Folder + filename.replace ('.js', '.py')\n        const phpFilename     = phpFolder     + filename.replace ('.js', '.php')\n\n        log.cyan ('Transpiling from', filename.yellow)\n\n        overwriteFile (python2Filename, python2)\n        overwriteFile (python3Filename, python3)\n        overwriteFile (phpFilename,     php)\n\n        return { className, baseClass }\n\n    } catch (e) {\n\n        log.red ('\\nFailed to transpile source code from', filename.yellow)\n        log.red ('See https://github.com/ccxt/ccxt/blob/master/CONTRIBUTING.md on how to build this library properly\\n')\n        throw e // rethrow it\n    }\n}",
    "label": 0
  },
  {
    "codes": "func (s *ListMultipartUploadsInput) SetAccountId(v string) *ListMultipartUploadsInput {\n\ts.AccountId = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport uuid\n\ndef generate_uuid4():\n    # Generate a UUID version 4\n    uuid_obj = uuid.uuid4()\n    # Convert to a string and remove hyphens\n    uuid_str = uuid_obj.hex\n    return uuid_str\n```\n",
    "label": 1
  },
  {
    "codes": "func (s *GetJobsOutput) SetJobs(v []*Job) *GetJobsOutput {\n\ts.Jobs = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport coremltools as ct\nfrom tensorflow import keras\n\n# Load the Keras model\nkeras_model = keras.models.load_model('path_to_your_keras_model.h5')\n\n# Convert the Keras model to CoreML format\ncoreml_model = ct.convert(keras_model, \n                          inputs=[ct.ImageType(name=\"image\", shape=keras_model.input_shape)])\n\n# Save the CoreML model\ncoreml_model.save('path_to_save_your_coreml_model.mlmodel')\n```",
    "label": 1
  },
  {
    "codes": "```rust\n// Function to calculate the Levenshtein distance between two strings\nfn levenshtein_distance(s1: &str, s2: &str) -> usize {\n    let len1 = s1.chars().count();\n    let len2 = s2.chars().count();\n    let mut dp = vec![vec![0; len2 + 1]; len1 + 1];\n\n    // Initialize the first row and column of the DP table\n    for i in 0..=len1 {\n        dp[i][0] = i;\n    }\n    for j in 0..=len2 {\n        dp[0][j] = j;\n    }\n\n    // Fill the DP table\n    for (i, c1) in s1.chars().enumerate() {\n        for (j, c2) in s2.chars().enumerate() {\n            let cost = if c1 == c2 { 0 } else { 1 };\n            dp[i + 1][j + 1] = (dp[i][j + 1] + 1)\n                .min(dp[i + 1][j] + 1)\n                .min(dp[i][j] + cost);\n        }\n    }\n\n    // Return the final result\n    dp[len1][len2]\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\n// Import necessary packages\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Pointcut;\nimport org.springframework.stereotype.Component;\n\n// Define the Aspect\n@Aspect\n@Component\npublic class CircuitBreakerAspect {\n\n    private int failureCount = 0;\n    private final int threshold = 3; // You can configure this threshold as needed\n    private final long timeout = 5000; // Time in milliseconds after which circuit will try to close\n    private long lastFailureTime = 0;\n\n    // Declare a Pointcut for the methods to wrap\n    @Pointcut(\"execution(* com.example.service.*.*(..))\") // Change package and method patterns as needed\n    public void serviceMethods() {}\n\n    // Around advice to implement circuit breaker\n    @Around(\"serviceMethods()\")\n    public Object wrapWithCircuitBreaker(ProceedingJoinPoint joinPoint) throws Throwable {\n        long currentTime = System.currentTimeMillis();\n\n        // Check if the circuit is OPEN\n        if (failureCount >= threshold) {\n            if (currentTime - lastFailureTime > timeout) {\n                // Attempt to reset the circuit\n                failureCount = 0;\n            } else {\n                throw new RuntimeException(\"Circuit is OPEN! Service is temporarily unavailable.\");\n            }\n        }\n\n        try {\n            // Proceed with the method invocation\n            Object result = joinPoint.proceed();\n            // Reset failure count on successful call\n            failureCount = 0;\n            return result;\n        } catch (Exception e) {\n            failureCount++;\n            lastFailureTime = currentTime;\n            throw new RuntimeException(\"Service method failed with exception: \" + e.getMessage());\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```csharp\nusing System;\nusing System.Management;\nusing System.ServiceProcess;\n\npublic class USBMonitorService : ServiceBase\n{\n    private ManagementEventWatcher usbWatcher;\n\n    protected override void OnStart(string[] args)\n    {\n        // Set up a WMI query to monitor USB device connections\n        WqlEventQuery query = new WqlEventQuery(\n            \"SELECT * FROM Win32_DeviceChangeEvent WHERE EventType = 2\");\n\n        usbWatcher = new ManagementEventWatcher(query);\n        usbWatcher.EventArrived += new EventArrivedEventHandler(USBDeviceConnected);\n        usbWatcher.Start();\n    }\n\n    protected override void OnStop()\n    {\n        usbWatcher.Stop();\n        usbWatcher.Dispose();\n    }\n\n    private void USBDeviceConnected(object sender, EventArrivedEventArgs e)\n    {\n        // Log or handle the USB device connection event\n        Log(\"USB device connected.\");\n    }\n\n    private void Log(string message)\n    {\n        // Implement logging logic here (e.g., write to a file or event log)\n        Console.WriteLine(message);\n    }\n\n    public static void Main()\n    {\n        ServiceBase.Run(new USBMonitorService());\n    }\n}\n```\n\n### Comments:\n- **WqlEventQuery**: Monitors USB device connections using WMI (Windows Management Instrumentation).\n- **ManagementEventWatcher**: Listens for WMI events related to USB device connections.\n- **EventArrivedEventHandler**: Handles the event when a USB device is connected.\n- **Log**: Placeholder for logging logic (e.g., writing to a file or event log).\n- **Main**: Entry point for the Windows service.",
    "label": 1
  },
  {
    "codes": "```java\nimport ddf.minim.*;\nimport ddf.minim.analysis.*;\n\nMinim minim;\nAudioInput in;\nFFT fft;\n\nvoid setup() {\n  size(512, 200);\n  \n  // Initialize Minim and get audio input\n  minim = new Minim(this);\n  in = minim.getLineIn(Minim.MONO, 1024);\n  \n  // Initialize FFT with the same size as the input\n  fft = new FFT(in.bufferSize(), in.sampleRate());\n  fft.logAverages(60, 7); // Choose parameters for log averaging\n}\n\nvoid draw() {\n  background(0);\n  stroke(255);\n  \n  // Perform FFT on the input buffer\n  fft.forward(in.mix);\n\n  // Draw the spectrum\n  for (int i = 0; i < fft.avgSize(); i++) {\n    float x = map(i, 0, fft.avgSize(), 0, width);\n    float y = map(fft.getAvg(i), 0, 10, height, 0); // Adjust 10 as needed for scaling\n    line(x, height, x, y);\n  }\n}\n\nvoid stop() {\n  // Close the input when stopping the sketch\n  minim.stop();\n  super.stop();\n}\n```",
    "label": 1
  },
  {
    "codes": "```kotlin\nfun hasCycle(graph: Map<Int, List<Int>>): Boolean {\n    val visited = mutableSetOf<Int>() // Track visited nodes\n    val recursionStack = mutableSetOf<Int>() // Track nodes in the current DFS path\n\n    fun dfs(node: Int): Boolean {\n        if (node in recursionStack) return true // Cycle detected\n        if (node in visited) return false // Already visited, no cycle\n\n        visited.add(node)\n        recursionStack.add(node)\n\n        for (neighbor in graph[node] ?: emptyList()) {\n            if (dfs(neighbor)) return true\n        }\n\n        recursionStack.remove(node) // Backtrack\n        return false\n    }\n\n    // Check all nodes in case of disconnected graph\n    for (node in graph.keys) {\n        if (dfs(node)) return true\n    }\n\n    return false\n}\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n#include <iostream>\n#include <vector>\n#include <queue>\n#include <thread>\n#include <future>\n#include <mutex>\n#include <condition_variable>\n#include <atomic>\n#include <functional>\n\nclass ThreadPool {\npublic:\n    ThreadPool(size_t numThreads) : stop(false) {\n        for (size_t i = 0; i < numThreads; ++i) {\n            workers.emplace_back([this] {\n                while (true) {\n                    std::function<void()> task;\n\n                    {\n                        std::unique_lock<std::mutex> lock(this->queueMutex);\n                        this->condition.wait(lock, [this] {\n                            return this->stop || !this->tasks.empty();\n                        });\n\n                        if (this->stop && this->tasks.empty()) {\n                            return;\n                        }\n\n                        task = std::move(this->tasks.front());\n                        this->tasks.pop();\n                    }\n\n                    task();\n                }\n            });\n        }\n    }\n\n    template<class F, class... Args>\n    auto enqueue(F&& f, Args&&... args) -> std::future<typename std::result_of<F(Args...)>::type> {\n        using return_type = typename std::result_of<F(Args...)>::type;\n\n        auto task = std::make_shared<std::packaged_task<return_type()>>(\n            std::bind(std::forward<F>(f), std::forward<Args>(args)...)\n        );\n\n        std::future<return_type> res = task->get_future();\n        {\n            std::unique_lock<std::mutex> lock(queueMutex);\n\n            if (stop) {\n                throw std::runtime_error(\"enqueue on stopped ThreadPool\");\n            }\n\n            tasks.emplace([task]() { (*task)(); });\n        }\n        condition.notify_one();\n        return res;\n    }\n\n    ~ThreadPool() {\n        {\n            std::unique_lock<std::mutex> lock(queueMutex);\n            stop = true;\n        }\n        condition.notify_all();\n        for (std::thread &worker : workers) {\n            worker.join();\n        }\n    }\n\nprivate:\n    std::vector<std::thread> workers;\n    std::queue<std::function<void()>> tasks;\n\n    std::mutex queueMutex;\n    std::condition_variable condition;\n    std::atomic<bool> stop;\n};\n\n// Example usage\nint main() {\n    ThreadPool pool(4);\n\n    auto result1 = pool.enqueue([](int a, int b) { return a + b; }, 1, 2);\n    auto result2 = pool.enqueue([](int a, int b) { return a * b; }, 3, 4);\n\n    std::cout << \"Result 1: \" << result1.get() << std::endl;\n    std::cout << \"Result 2: \" << result2.get() << std::endl;\n\n    return 0;\n}\n```",
    "label": 1
  },
  {
    "codes": "```java\nimport org.junit.jupiter.api.Test;\nimport static org.junit.jupiter.api.Assertions.*;\n\npublic class SortingAlgorithmTest {\n\n    @Test\n    public void testEmptyArray() {\n        int[] arr = {};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(new int[]{}, arr, \"Empty array should remain unchanged.\");\n    }\n\n    @Test\n    public void testSingleElementArray() {\n        int[] arr = {5};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(new int[]{5}, arr, \"Single element array should remain unchanged.\");\n    }\n\n    @Test\n    public void testAlreadySortedArray() {\n        int[] arr = {1, 2, 3, 4, 5};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(new int[]{1, 2, 3, 4, 5}, arr, \"Already sorted array should remain unchanged.\");\n    }\n\n    @Test\n    public void testReverseSortedArray() {\n        int[] arr = {5, 4, 3, 2, 1};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(new int[]{1, 2, 3, 4, 5}, arr, \"Reverse sorted array should be sorted in ascending order.\");\n    }\n\n    @Test\n    public void testArrayWithDuplicates() {\n        int[] arr = {3, 1, 2, 3, 1};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(new int[]{1, 1, 2, 3, 3}, arr, \"Array with duplicates should be sorted correctly.\");\n    }\n\n    @Test\n    public void testArrayWithNegativeNumbers() {\n        int[] arr = {-3, -1, -2, 0, 2};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(new int[]{-3, -2, -1, 0, 2}, arr, \"Array with negative numbers should be sorted correctly.\");\n    }\n\n    @Test\n    public void testArrayWithAllSameElements() {\n        int[] arr = {7, 7, 7, 7};\n        SortingAlgorithm.sort(arr);\n        assertArrayEquals(new int[]{7, 7, 7, 7}, arr, \"Array with all same elements should remain unchanged.\");\n    }\n\n    @Test\n    public void testLargeArray() {\n        int[] arr = new int[1000];\n        for (int i = 0; i < arr.length; i++) {\n            arr[i] = arr.length - i;\n        }\n        SortingAlgorithm.sort(arr);\n        for (int i = 0; i < arr.length - 1; i++) {\n            assertTrue(arr[i] <= arr[i + 1], \"Large array should be sorted in ascending order.\");\n        }\n    }\n}\n```",
    "label": 1
  },
  {
    "codes": "```cpp\n// DynamicWeatherSystem.h\n#pragma once\n\n#include \"CoreMinimal.h\"\n#include \"GameFramework/Actor.h\"\n#include \"DynamicWeatherSystem.generated.h\"\n\nUENUM(BlueprintType)\nenum class EWeatherType : uint8\n{\n    Sunny,\n    Rainy,\n    Snowy,\n    Foggy\n};\n\nUCLASS()\nclass YOURGAME_API ADynamicWeatherSystem : public AActor\n{\n    GENERATED_BODY()\n\npublic:\n    ADynamicWeatherSystem();\n\nprotected:\n    virtual void BeginPlay() override;\n\npublic:\n    virtual void Tick(float DeltaTime) override;\n\n    UFUNCTION(BlueprintCallable, Category = \"Weather\")\n    void TransitionToWeather(EWeatherType NewWeatherType, float TransitionDuration);\n\nprivate:\n    UPROPERTY(VisibleAnywhere, Category = \"Weather\")\n    EWeatherType CurrentWeather;\n\n    UPROPERTY(EditAnywhere, Category = \"Weather\")\n    float WeatherTransitionSpeed;\n\n    void UpdateWeatherEffects(float DeltaTime);\n    void ApplyWeatherEffects(EWeatherType WeatherType);\n};\n\n// DynamicWeatherSystem.cpp\n#include \"DynamicWeatherSystem.h\"\n#include \"Kismet/GameplayStatics.h\"\n\nADynamicWeatherSystem::ADynamicWeatherSystem()\n{\n    PrimaryActorTick.bCanEverTick = true;\n    CurrentWeather = EWeatherType::Sunny;\n    WeatherTransitionSpeed = 1.0f;\n}\n\nvoid ADynamicWeatherSystem::BeginPlay()\n{\n    Super::BeginPlay();\n    ApplyWeatherEffects(CurrentWeather);\n}\n\nvoid ADynamicWeatherSystem::Tick(float DeltaTime)\n{\n    Super::Tick(DeltaTime);\n    UpdateWeatherEffects(DeltaTime);\n}\n\nvoid ADynamicWeatherSystem::TransitionToWeather(EWeatherType NewWeatherType, float TransitionDuration)\n{\n    if (TransitionDuration > 0.0f)\n    {\n        WeatherTransitionSpeed = 1.0f / TransitionDuration;\n    }\n    else\n    {\n        WeatherTransitionSpeed = 1.0f;\n    }\n\n    CurrentWeather = NewWeatherType;\n}\n\nvoid ADynamicWeatherSystem::UpdateWeatherEffects(float DeltaTime)\n{\n    // Implement smooth transition logic here\n    // Example: Gradually change skybox, particle effects, lighting, etc.\n    // This is where you would interpolate between weather states\n}\n\nvoid ADynamicWeatherSystem::ApplyWeatherEffects(EWeatherType WeatherType)\n{\n    switch (WeatherType)\n    {\n    case EWeatherType::Sunny:\n        // Apply sunny weather effects\n        break;\n    case EWeatherType::Rainy:\n        // Apply rainy weather effects\n        break;\n    case EWeatherType::Snowy:\n        // Apply snowy weather effects\n        break;\n    case EWeatherType::Foggy:\n        // Apply foggy weather effects\n        break;\n    default:\n        break;\n    }\n}\n```\n\n### Comments:\n- **EWeatherType**: Enum to define different weather types.\n- **TransitionToWeather**: Blueprint callable function to transition to a new weather type.\n- **UpdateWeatherEffects**: Handles smooth transitions between weather states.\n- **ApplyWeatherEffects**: Applies specific effects based on the current weather type.",
    "label": 1
  },
  {
    "codes": "```bash\n#!/bin/bash\n\n# Exit immediately if a command exits with a non-zero status\nset -e\n\n# Variables for file and signature\nFILE=\"$1\"\nSIG_FILE=\"$2\"\nKEYRING=\"$3\"\n\n# Check if required files exist\nif [[ ! -f \"$FILE\" ]] || [[ ! -f \"$SIG_FILE\" ]]; then\n    echo \"Usage: $0 <file> <signature_file> <keyring>\"\n    exit 1\nfi\n\n# Import the keyring\ngpg --import \"$KEYRING\" 2>/dev/null\n\n# Verify the signature\ngpg --verify \"$SIG_FILE\" \"$FILE\"\n\n# Output success status\nif [[ $? -eq 0 ]]; then\n    echo \"Signature is valid and verified.\"\nelse\n    echo \"Signature verification failed!\"\n    exit 1\nfi\n```",
    "label": 1
  },
  {
    "codes": "func (s *BatchCreatePartitionInput) SetPartitionInputList(v []*PartitionInput) *BatchCreatePartitionInput {\n\ts.PartitionInputList = v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "```python\nimport pygame\nimport sys\n\n# Initialize Pygame\npygame.init()\n\n# Screen dimensions\nSCREEN_WIDTH = 800\nSCREEN_HEIGHT = 600\nscreen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n\n# Colors\nWHITE = (255, 255, 255)\nBLACK = (0, 0, 0)\n\n# Player settings\nplayer_width = 50\nplayer_height = 50\nplayer_x = 50\nplayer_y = SCREEN_HEIGHT - player_height\nplayer_vel_y = 0\nplayer_jump_count = 0\nplayer_jump_limit = 2\nplayer_speed = 5\ngravity = 0.5\n\n# Clock\nclock = pygame.time.Clock()\n\n# Main game loop\nrunning = True\nwhile running:\n    screen.fill(WHITE)\n    \n    # Event handling\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            running = False\n    \n    # Key presses\n    keys = pygame.key.get_pressed()\n    \n    # Horizontal movement\n    if keys[pygame.K_LEFT]:\n        player_x -= player_speed\n    if keys[pygame.K_RIGHT]:\n        player_x += player_speed\n    \n    # Jumping\n    if keys[pygame.K_SPACE] and player_jump_count < player_jump_limit:\n        player_vel_y = -10\n        player_jump_count += 1\n    \n    # Apply gravity\n    player_vel_y += gravity\n    player_y += player_vel_y\n    \n    # Ground collision\n    if player_y >= SCREEN_HEIGHT - player_height:\n        player_y = SCREEN_HEIGHT - player_height\n        player_vel_y = 0\n        player_jump_count = 0  # Reset jump count when on ground\n    \n    # Draw player\n    pygame.draw.rect(screen, BLACK, (player_x, player_y, player_width, player_height))\n    \n    # Update display\n    pygame.display.update()\n    clock.tick(60)\n\n# Quit Pygame\npygame.quit()\nsys.exit()\n```",
    "label": 1
  },
  {
    "codes": "```javascript\nconst xss = require('xss');\n\nfunction sanitizeInput(req, res, next) {\n  // Function to deeply sanitize input object\n  const sanitizeObject = (obj) => {\n    for (let key in obj) {\n      if (typeof obj[key] === 'string') {\n        obj[key] = xss(obj[key]);  // Sanitize string\n      } else if (typeof obj[key] === 'object' && obj[key] !== null) {\n        sanitizeObject(obj[key]);  // Recursive call for nested objects\n      }\n    }\n  };\n\n  // Sanitize request query, body, and params\n  if (req.query) sanitizeObject(req.query);\n  if (req.body) sanitizeObject(req.body);\n  if (req.params) sanitizeObject(req.params);\n\n  next(); // Move to the next middleware or route handler\n}\n\nmodule.exports = sanitizeInput;\n```",
    "label": 1
  },
  {
    "codes": "```ruby\n# my_ssh_config_generator.gemspec\nGem::Specification.new do |spec|\n  spec.name          = \"my_ssh_config_generator\"\n  spec.version       = \"0.1.0\"\n  spec.authors       = [\"Your Name\"]\n  spec.email         = [\"your.email@example.com\"]\n  spec.summary       = %q{Generates SSH config files from YAML templates}\n  spec.description   = %q{A gem to easily create SSH configuration files using simple YAML templates.}\n  spec.homepage      = \"http://your-gem-homepage.com\"\n  spec.license       = \"MIT\"\n\n  spec.files         = Dir[\"lib/**/*.rb\"]\n  spec.require_paths = [\"lib\"]\n\n  spec.add_dependency \"psych\"\nend\n```\n\n```ruby\n# lib/my_ssh_config_generator.rb\nrequire 'psych'\nrequire_relative 'my_ssh_config_generator/ssh_config_generator'\n\n# Main module for SSH Config Generator\nmodule MySSHConfigGenerator\n  def self.generate_from_yaml(yaml_path)\n    config = Psych.load_file(yaml_path)\n    generator = SSHConfigGenerator.new(config)\n    generator.generate\n  end\nend\n```\n\n```ruby\n# lib/my_ssh_config_generator/ssh_config_generator.rb\nmodule MySSHConfigGenerator\n  # Class responsible for generating SSH configuration\n  class SSHConfigGenerator\n    def initialize(config)\n      @config = config\n    end\n\n    def generate\n      @config.map do |host, settings|\n        build_host_config(host, settings)\n      end.join(\"\\n\")\n    end\n\n    private\n\n    def build_host_config(host, settings)\n      host_config = [\"Host #{host}\"]\n      settings.each do |key, value|\n        host_config << \"  #{key} #{value}\"\n      end\n      host_config.join(\"\\n\")\n    end\n  end\nend\n```\n```ruby\n# Usage Example (Not part of the gem codebase)\n# Assuming you have a YAML file 'ssh_config.yml' like:\n#\n# myserver:\n#   HostName: myserver.example.com\n#   User: user123\n#   Port: 22\n#\n# Run the following Ruby code to generate an SSH config string:\n#\n# require 'my_ssh_config_generator'\n# puts MySSHConfigGenerator.generate_from_yaml('ssh_config.yml')\n```\n```yaml\n# Sample YAML Template (ssh_config.yml)\nmyserver:\n  HostName: myserver.example.com\n  User: user123\n  Port: 22\n```",
    "label": 1
  },
  {
    "codes": "def resend_presence(self):\n        \"\"\"\n        Re-send the currently configured presence.\n\n        :return: Stanza token of the presence stanza or :data:`None` if the\n                 stream is not established.\n        :rtype: :class:`~.stream.StanzaToken`\n\n        .. note::\n\n           :meth:`set_presence` automatically broadcasts the new presence if\n           any of the parameters changed.\n        \"\"\"\n\n        if self.client.established:\n            return self.client.enqueue(self.make_stanza())",
    "label": 0
  },
  {
    "codes": "public Observable<ServiceResponse<Void>> purgeDeletedSecretWithServiceResponseAsync(String vaultBaseUrl, String secretName) {\n        if (vaultBaseUrl == null) {\n            throw new IllegalArgumentException(\"Parameter vaultBaseUrl is required and cannot be null.\");\n        }\n        if (secretName == null) {\n            throw new IllegalArgumentException(\"Parameter secretName is required and cannot be null.\");\n        }\n        if (this.apiVersion() == null) {\n            throw new IllegalArgumentException(\"Parameter this.apiVersion() is required and cannot be null.\");\n        }\n        String parameterizedHost = Joiner.on(\", \").join(\"{vaultBaseUrl}\", vaultBaseUrl);\n        return service.purgeDeletedSecret(secretName, this.apiVersion(), this.acceptLanguage(), parameterizedHost, this.userAgent())\n            .flatMap(new Func1<Response<ResponseBody>, Observable<ServiceResponse<Void>>>() {\n                @Override\n                public Observable<ServiceResponse<Void>> call(Response<ResponseBody> response) {\n                    try {\n                        ServiceResponse<Void> clientResponse = purgeDeletedSecretDelegate(response);\n                        return Observable.just(clientResponse);\n                    } catch (Throwable t) {\n                        return Observable.error(t);\n                    }\n                }\n            });\n    }",
    "label": 0
  },
  {
    "codes": "func (s *GetTableVersionsInput) SetTableName(v string) *GetTableVersionsInput {\n\ts.TableName = &v\n\treturn s\n}",
    "label": 0
  },
  {
    "codes": "function determineStepSize(min, max, unit, capacity) {\n\tvar range = max - min;\n\tvar interval = INTERVALS[unit];\n\tvar milliseconds = interval.size;\n\tvar steps = interval.steps;\n\tvar i, ilen, factor;\n\n\tif (!steps) {\n\t\treturn Math.ceil(range / (capacity * milliseconds));\n\t}\n\n\tfor (i = 0, ilen = steps.length; i < ilen; ++i) {\n\t\tfactor = steps[i];\n\t\tif (Math.ceil(range / (milliseconds * factor)) <= capacity) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn factor;\n}",
    "label": 0
  },
  {
    "codes": "function (chrs, buffer, pos, strict, opts) {\n                        if (opts.isInYearRange(chrs, opts.yearrange.minyear, opts.yearrange.maxyear)) {\n                            var dayMonthValue = buffer.join('').substr(0, 6);\n                            if (dayMonthValue != opts.leapday)\n                                return true;\n                            else {\n                                var year = parseInt(chrs, 10);//detect leap year\n                                if (year % 4 === 0)\n                                    if (year % 100 === 0)\n                                        if (year % 400 === 0)\n                                            return true;\n                                        else return false;\n                                    else return true;\n                                else return false;\n                            }\n                        } else return false;\n                    }",
    "label": 0
  },
  {
    "codes": "```python\nimport pygame\nimport sys\n\n# Constants\nSCREEN_WIDTH, SCREEN_HEIGHT = 800, 600\nPLAYER_WIDTH, PLAYER_HEIGHT = 50, 50\nGRAVITY = 0.5\nJUMP_STRENGTH = -10\nMAX_JUMPS = 2\n\n# Initialize Pygame\npygame.init()\nscreen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\nclock = pygame.time.Clock()\n\nclass Player(pygame.sprite.Sprite):\n    def __init__(self):\n        super().__init__()\n        self.image = pygame.Surface((PLAYER_WIDTH, PLAYER_HEIGHT))\n        self.image.fill((255, 0, 0))\n        self.rect = self.image.get_rect(midbottom=(SCREEN_WIDTH // 2, SCREEN_HEIGHT - 100))\n        self.velocity_y = 0\n        self.jumps_left = MAX_JUMPS\n\n    def update(self):\n        keys = pygame.key.get_pressed()\n        \n        # Horizontal movement\n        if keys[pygame.K_LEFT]:\n            self.rect.x -= 5\n        if keys[pygame.K_RIGHT]:\n            self.rect.x += 5\n\n        # Apply gravity\n        self.velocity_y += GRAVITY\n        self.rect.y += self.velocity_y\n        \n        # Ground collision check\n        if self.rect.bottom >= SCREEN_HEIGHT:\n            self.rect.bottom = SCREEN_HEIGHT\n            self.velocity_y = 0\n            self.jumps_left = MAX_JUMPS\n\n    def jump(self):\n        if self.jumps_left > 0:\n            self.velocity_y = JUMP_STRENGTH\n            self.jumps_left -= 1\n\nplayer = Player()\nall_sprites = pygame.sprite.Group(player)\n\n# Game loop\nwhile True:\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            pygame.quit()\n            sys.exit()\n        elif event.type == pygame.KEYDOWN:\n            if event.key == pygame.K_SPACE:\n                player.jump()\n\n    # Update all sprites\n    all_sprites.update()\n\n    # Draw everything\n    screen.fill((135, 206, 235))  # Sky blue background\n    all_sprites.draw(screen)\n\n    # Flip the display buffer\n    pygame.display.flip()\n    clock.tick(60)\n```\n",
    "label": 1
  }
]